<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Optimization Algorithms - BigDL Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/highlight.css">
  <link href="../../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Optimization Algorithms";
    var mkdocs_page_input_path = "APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods.md";
    var mkdocs_page_url = "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/";
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> BigDL Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-setup-bigdl/">Setup BigDL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prepare-data/">Prepare your Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prediction/">Use BigDL for Prediction Only</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-train/">Train a Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-monitor/">Monitor the Training</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-tune/">Tuning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-advanced/">Advanced Usage</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Install/Deploy</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/use-pre-built/">Use Pre-built Package</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/build-src/">Build from Source</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/deploy-python/">Enable Python Support</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/running-on-EC2/">Running On EC2</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/resources/">Resources</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-api/">API Usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/install-via-pip/">Install via pip</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-no-pip/">Use Python without Pip</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model/SequentialModel/">Sequential Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model/FunctionalAPI/">Functional API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Layers/Containers/merged-Containers/">Containers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Simple_Layers/merged-Simple_Layers/">Simple Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Convolution_Layers/merged-Convolution_Layers/">Convolution Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Pooling_Layers/merged-Pooling_Layers/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Linear_Layers/merged-Linear_Layers/">Linear Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Non-linear_Activations/merged-Non-linear_Activations/">Non-linear Activations</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Embedding_Layers/merged-Embedding_Layers/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/MergeSplit_Layers/merged-MergeSplit_Layers/">Merge/Split Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Math-Layers/merged-Math-Layers/">Math Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Padding_Layers/merged-Padding_Layers/">Padding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Normalization_Layers/merged-Normalization_Layers/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Dropout_Layers/merged-Dropout_Layers/">Dropout Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Distance_Layers/merged-Distance_Layers/">Distance Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Recurrent_Layers/merged-Recurrent_Layers/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Layers/Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Losses/merged-Losses/">Losses</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../DistriOptimizer/">Optimizer</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Optimization Algorithms</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#adam">Adam</a></li>
    

    <li class="toctree-l3"><a href="#sgd">SGD</a></li>
    

    <li class="toctree-l3"><a href="#adadelta">Adadelta</a></li>
    

    <li class="toctree-l3"><a href="#rmsprop">RMSprop</a></li>
    

    <li class="toctree-l3"><a href="#adamax">Adamax</a></li>
    

    <li class="toctree-l3"><a href="#adagrad">Adagrad</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Triggers/">Triggers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ResumeTraining/">Resume Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Initializers/merged-Initializers/">Initalizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Regularizers/merged-Regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Metrics/merged-Metrics/">Metrics</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Preprocessing/Transformer/">Transformer</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Load/Save Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/BigDLModel/">Load/Save a BigDL Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/TensorflowModel/">Load a Tensorflow Model</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/visualization-with-tensorboard/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../python-api-doc/">Python API Docs</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">BigDL Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Optimization &raquo;</li>
        
      
    
    <li>Optimization Algorithms</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="adam">Adam</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optim = new Adam(learningRate=1e-3, learningRateDecay=0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">optim = Adam(learningRate=1e-3, learningRateDecay-0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8, bigdl_type=&quot;float&quot;)
</code></pre>

<p>An implementation of Adam optimization, first-order gradient-based optimization of stochastic  objective  functions. http://arxiv.org/pdf/1412.6980.pdf</p>
<p><code>learningRate</code> learning rate. Default value is 1e-3. </p>
<p><code>learningRateDecay</code> learning rate decay. Default value is 0.0.</p>
<p><code>beta1</code> first moment coefficient. Default value is 0.9.</p>
<p><code>beta2</code> second moment coefficient. Default value is 0.999.</p>
<p><code>Epsilon</code> for numerical stability. Default value is 1e-8.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.optim._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val optm = new Adam(learningRate=0.002)
def rosenBrock(x: Tensor[Float]): (Float, Tensor[Float]) = {
    // (1) compute f(x)
    val d = x.size(1)

    // x1 = x(i)
    val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))
    // x(i + 1) - x(i)^2
    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))
    // 100 * (x(i + 1) - x(i)^2)^2
    x1.cmul(x1).mul(100)

    // x0 = x(i)
    val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))
    // 1-x(i)
    x0.mul(-1).add(1)
    x0.cmul(x0)
    // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2
    x1.add(x0)

    val fout = x1.sum()

    // (2) compute f(x)/dx
    val dxout = Tensor[Float]().resizeAs(x).zero()
    // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));
    x1.copy(x.narrow(1, 1, d - 1))
    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)
    x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)
    x1.add(x0)
    dxout.narrow(1, 1, d - 1).copy(x1)

    // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);
    x0.copy(x.narrow(1, 1, d - 1))
    x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)
    dxout.narrow(1, 2, d - 1).add(x0)

    (fout, dxout)
  }  
val x = Tensor(2).fill(0)
&gt; print(optm.optimize(rosenBrock, x))
(0.0019999996
0.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcD$sp of size 2],[D@302d88d8)
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">optim_method = Adam(learningrate=0.002)

optimizer = Optimizer(
    model=mlp_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=optim_method,
    end_trigger=MaxEpoch(20),
    batch_size=32)

</code></pre>

<h2 id="sgd">SGD</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = SGD(learningRate= 1e-3,learningRateDecay=0.0,
                      weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,
                      nesterov=false,learningRateSchedule=Default(),
                      learningRates=null,weightDecays=null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,
                   momentum=0.0,dampening=DOUBLEMAX,nesterov=False,
                   leaningrate_schedule=None,learningrates=None,
                   weightdecays=None,bigdl_type=&quot;float&quot;)
</code></pre>

<p>A plain implementation of SGD which provides optimize method. After setting 
optimization method when create Optimize, Optimize will call optimization method at the end of 
each iteration.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val optimMethod = new SGD[Float](learningRate= 1e-3,learningRateDecay=0.0,
                               weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,
                               nesterov=false,learningRateSchedule=Default(),
                               learningRates=null,weightDecays=null)
optimizer.setOptimMethod(optimMethod)
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">optim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,
                  momentum=0.0,dampening=DOUBLEMAX,nesterov=False,
                  leaningrate_schedule=None,learningrates=None,
                  weightdecays=None,bigdl_type=&quot;float&quot;)

optimizer = Optimizer(
    model=mlp_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=optim_method,
    end_trigger=MaxEpoch(20),
    batch_size=32)
</code></pre>

<h2 id="adadelta">Adadelta</h2>
<p><em>AdaDelta</em> implementation for <em>SGD</em> 
It has been proposed in <code>ADADELTA: An Adaptive Learning Rate Method</code>.
http://arxiv.org/abs/1212.5701.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = Adadelta(decayRate = 0.9, Epsilon = 1e-10)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = AdaDelta(decayrate = 0.9, epsilon = 1e-10)
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">optimizer.setOptimMethod(new Adadelta(0.9, 1e-10))

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">optimizer = Optimizer(
    model=mlp_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=Adadelta(0.9, 0.00001),
    end_trigger=MaxEpoch(20),
    batch_size=32)
</code></pre>

<h2 id="rmsprop">RMSprop</h2>
<p>An implementation of RMSprop (Reference: http://arxiv.org/pdf/1308.0850v5.pdf, Sec 4.2)
<em> learningRate : learning rate
</em> learningRateDecaye : learning rate decay
<em> decayRatee : decayRate, also called rho
</em> Epsilone : for numerical stability</p>
<h2 id="adamax">Adamax</h2>
<p>An implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf</p>
<p>Arguments:</p>
<ul>
<li>learningRate : learning rate</li>
<li>beta1 : first moment coefficient</li>
<li>beta2 : second moment coefficient</li>
<li>Epsilon : for numerical stability</li>
</ul>
<p>Returns:</p>
<p>the new x vector and the function list {fx}, evaluated before the update</p>
<h2 id="adagrad">Adagrad</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val adagrad = new Adagrad(learningRate = 1e-3,
                          learningRateDecay = 0.0,
                          weightDecay = 0.0)

</code></pre>

<p>An implementation of Adagrad. See the original paper:
 <a href="http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.optim._
import com.intel.analytics.bigdl.tensor._
val adagrad = Adagrad(0.01, 0.0, 0.0)
    def feval(x: Tensor[Float]): (Float, Tensor[Float]) = {
      // (1) compute f(x)
      val d = x.size(1)
      // x1 = x(i)
      val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))
      // x(i + 1) - x(i)^2
      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))
      // 100 * (x(i + 1) - x(i)^2)^2
      x1.cmul(x1).mul(100)
      // x0 = x(i)
      val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))
      // 1-x(i)
      x0.mul(-1).add(1)
      x0.cmul(x0)
      // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2
      x1.add(x0)
      val fout = x1.sum()
      // (2) compute f(x)/dx
      val dxout = Tensor[Float]().resizeAs(x).zero()
      // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));
      x1.copy(x.narrow(1, 1, d - 1))
      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)
      x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)
      x1.add(x0)
      dxout.narrow(1, 1, d - 1).copy(x1)
      // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);
      x0.copy(x.narrow(1, 1, d - 1))
      x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)
      dxout.narrow(1, 2, d - 1).add(x0)
      (fout, dxout)
    }
val x = Tensor(2).fill(0)
val config = T(&quot;learningRate&quot; -&gt; 1e-1)
for (i &lt;- 1 to 10) {
  adagrad.optimize(feval, x, config, config)
}
x after optimize: 0.27779138
0.07226955
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]
</code></pre>

<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,
                            tolFun=1e-5, tolX=1e-9, nCorrection=100,
                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = LBFGS(max_iter=20, max_eval=Double.MaxValue, \
                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \
                 learning_rate=1.0, line_search=None, line_search_options=None)
</code></pre>

<p>This implementation of L-BFGS relies on a user-provided line search function
(state.lineSearch). If this function is not provided, then a simple learningRate
is used to produce fixed size steps. Fixed size steps are much less costly than line
searches, and can be useful for stochastic problems.</p>
<p>The learning rate is used even when a line search is provided.This is also useful for
large-scale stochastic problems, where opfunc is a noisy approximation of f(x). In that
case, the learning rate allows a reduction of confidence in the step size.</p>
<p><strong>Parameters:</strong>
<em> <strong>maxIter</strong> - Maximum number of iterations allowed. Default: 20
</em> <strong>maxEval</strong> - Maximum number of function evaluations. Default: Double.MaxValue
<em> <strong>tolFun</strong> - Termination tolerance on the first-order optimality. Default: 1e-5
</em> <strong>tolX</strong> - Termination tol on progress in terms of func/param changes. Default: 1e-9
<em> <strong>learningRate</strong> - the learning rate. Default: 1.0
</em> <strong>lineSearch</strong> - A line search function. Default: None
* <strong>lineSearchOptions</strong> - If no line search provided, then a fixed step size is used. Default: None</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,
                            tolFun=1e-5, tolX=1e-9, nCorrection=100,
                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)
optimizer.setOptimMethod(optimMethod)
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">optim_method = LBFGS(max_iter=20, max_eval=DOUBLEMAX, \
                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \
                 learning_rate=1.0, line_search=None, line_search_options=None)

optimizer = Optimizer(
    model=mlp_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=optim_method,
    end_trigger=MaxEpoch(20),
    batch_size=32)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Triggers/" class="btn btn-neutral float-right" title="Triggers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../DistriOptimizer/" class="btn btn-neutral" title="Optimizer"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../DistriOptimizer/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Triggers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../../js/theme.js"></script>

</body>
</html>
