<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>Losses - BigDL Project</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/highlight.css">
  <link href="../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Losses";
    var mkdocs_page_input_path = "APIdocs/Losses/merged-Losses.md";
    var mkdocs_page_url = "/APIdocs/Losses/merged-Losses/";
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> BigDL Project</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../..">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../UserGuide/install/">Install</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../UserGuide/run/">Run</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../UserGuide/examples/">Examples</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../UserGuide/resources/">More Resources</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../PythonSupport/python-install/">Install</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../PythonSupport/python-run/">Run</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../PythonSupport/python-examples/">Examples</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../PythonSupport/python-resources/">More Examples and Tutorials</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../PythonSupport/tensorflow-support/">Tensorflow Support</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Programming Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../python-api-doc/">Python API Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Data/merged-Data/">Data</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../Model/Sequential/">Sequential Model</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Model/Functional/">Functional API</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Model/ModuleAPI/">Module API</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Containers/merged-Containers/">Containers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Simple-Layers/merged-Simple-Layers/">Simple Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Convolution-Layers/merged-Convolution-Layers/">Convolution Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Pooling-Layers/merged-Pooling-Layers/">Pooling Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Linear-Layers/merged-Linear-Layers/">Linear Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Non-linear-Activations/merged-Non-linear-Activations/">Non-linear Activations</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Embedding-Layers/merged-Embedding-Layers/">Embedding Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/MergeSplit-Layers/merged-MergeSplit-Layers/">Merge/Split Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Math-Layers/merged-Math-Layers/">Math Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Padding-Layers/merged-Padding-Layers/">Padding Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Normalization-Layers/merged-Normalization-Layers/">Normalization Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Dropout-Layers/merged-Dropout-Layers/">Dropout Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Distance-Layers/merged-Distance-Layers/">Distance Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Recurrent-Layers/merged-Recurrent-Layers/">Recurrent Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Layers/Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Losses</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#l1cost">L1Cost</a></li>
    

    <li class="toctree-l3"><a href="#timedistributedcriterion">TimeDistributedCriterion</a></li>
    

    <li class="toctree-l3"><a href="#marginrankingcriterion">MarginRankingCriterion</a></li>
    

    <li class="toctree-l3"><a href="#classnllcriterion">ClassNLLCriterion</a></li>
    

    <li class="toctree-l3"><a href="#softmaxwithcriterion">SoftmaxWithCriterion</a></li>
    

    <li class="toctree-l3"><a href="#smoothl1criterion">SmoothL1Criterion</a></li>
    

    <li class="toctree-l3"><a href="#smoothl1criterionwithweights">SmoothL1CriterionWithWeights</a></li>
    

    <li class="toctree-l3"><a href="#multimargincriterion">MultiMarginCriterion</a></li>
    

    <li class="toctree-l3"><a href="#hingeembeddingcriterion">HingeEmbeddingCriterion</a></li>
    

    <li class="toctree-l3"><a href="#margincriterion">MarginCriterion</a></li>
    

    <li class="toctree-l3"><a href="#cosineembeddingcriterion">CosineEmbeddingCriterion</a></li>
    

    <li class="toctree-l3"><a href="#bcecriterion">BCECriterion</a></li>
    

    <li class="toctree-l3"><a href="#dicecoefficientcriterion">DiceCoefficientCriterion</a></li>
    

    <li class="toctree-l3"><a href="#msecriterion">MSECriterion</a></li>
    

    <li class="toctree-l3"><a href="#softmargincriterion">SoftMarginCriterion</a></li>
    

    <li class="toctree-l3"><a href="#distkldivcriterion">DistKLDivCriterion</a></li>
    

    <li class="toctree-l3"><a href="#classsimplexcriterion">ClassSimplexCriterion</a></li>
    

    <li class="toctree-l3"><a href="#l1hingeembeddingcriterion">L1HingeEmbeddingCriterion</a></li>
    

    <li class="toctree-l3"><a href="#crossentropycriterion">CrossEntropyCriterion</a></li>
    

    <li class="toctree-l3"><a href="#parallelcriterion">ParallelCriterion</a></li>
    

    <li class="toctree-l3"><a href="#multilabelmargincriterion">MultiLabelMarginCriterion</a></li>
    

    <li class="toctree-l3"><a href="#multilabelsoftmargincriterion">MultiLabelSoftMarginCriterion</a></li>
    

    <li class="toctree-l3"><a href="#abscriterion">AbsCriterion</a></li>
    

    <li class="toctree-l3"><a href="#multicriterion">MultiCriterion</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Initializers/merged-Initializers/">Initalizers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Regularizers/merged-Regularizers/">Regularizers</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../Optimizers/DistriOptimizer/">Optimizer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Optimizers/Optim-Methods/merged-Optim-Methods/">Optimization Algorithms</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Optimizers/Triggers/">Triggers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Optimizers/ResumeTraining/">Resume Training</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Metrics/merged-Metrics/">Metrics</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../ProgrammingGuide/visualization-with-tensorboard/">Visualization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../MLPipeline/merged-MLPipeline/">MLPipeline</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">BigDL Project</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Programming Guide &raquo;</li>
        
      
    
    <li>Losses</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="l1cost">L1Cost</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = L1Cost[Float]()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = L1Cost()
</code></pre>

<p>Compute L1 norm for input, and sign of input</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val layer = L1Cost[Float]()
val input = Tensor[Float](2, 2).rand
val target = Tensor[Float](2, 2).rand

val output = layer.forward(input, target)
val gradInput = layer.backward(input, target)

&gt; println(input)
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.48145306      0.476887
0.23729686      0.5169516
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

&gt; println(target)
target: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.42999148      0.22272833
0.49723643      0.17884709
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

&gt; println(output)
output: Float = 1.7125885
&gt; println(gradInput)
gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0     1.0
1.0     1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = L1Cost()

input = np.random.uniform(0, 1, (2, 2)).astype(&quot;float32&quot;)
target = np.random.uniform(0, 1, (2, 2)).astype(&quot;float32&quot;)

output = layer.forward(input, target)
gradInput = layer.backward(input, target)

&gt; output
2.522411
&gt; gradInput
[array([[ 1.,  1.],
        [ 1.,  1.]], dtype=float32)]
</code></pre>

<h2 id="timedistributedcriterion">TimeDistributedCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = TimeDistributedCriterion(critrn, sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = TimeDistributedCriterion(critrn, sizeAverage)
</code></pre>

<p>This class is intended to support inputs with 3 or more dimensions.
Apply Any Provided Criterion to every temporal slice of an input.</p>
<p><code>critrn</code> embedded criterion</p>
<p><code>sizeAverage</code> whether to divide the sequence length. Default is false.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage

val criterion = ClassNLLCriterion[Double]()
val layer = TimeDistributedCriterion[Double](criterion, true)
val input = Tensor[Double](Storage(Array(
    1.0262627674932,
    -1.2412600935171,
    -1.0423174168648,
    -1.0262627674932,
    -1.2412600935171,
    -1.0423174168648,
    -0.90330565804228,
    -1.3686840144413,
    -1.0778380454479,
    -0.90330565804228,
    -1.3686840144413,
    -1.0778380454479,
    -0.99131220658219,
    -1.0559142847536,
    -1.2692712660404,
    -0.99131220658219,
    -1.0559142847536,
    -1.2692712660404))).resize(3, 2, 3)
val target = Tensor[Double](3, 2)
    target(Array(1, 1)) = 1
    target(Array(1, 2)) = 1
    target(Array(2, 1)) = 2
    target(Array(2, 2)) = 2
    target(Array(3, 1)) = 3
    target(Array(3, 2)) = 3
&gt; print(layer.forward(input, target))
0.8793184268272332
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *

criterion = ClassNLLCriterion()
layer = TimeDistributedCriterion(criterion, True)
input = np.array([1.0262627674932,
                      -1.2412600935171,
                      -1.0423174168648,
                      -1.0262627674932,
                      -1.2412600935171,
                      -1.0423174168648,
                      -0.90330565804228,
                      -1.3686840144413,
                      -1.0778380454479,
                      -0.90330565804228,
                      -1.3686840144413,
                      -1.0778380454479,
                      -0.99131220658219,
                      -1.0559142847536,
                      -1.2692712660404,
                      -0.99131220658219,
                      -1.0559142847536,
                      -1.2692712660404]).reshape(3,2,3)
target = np.array([[1,1],[2,2],[3,3]])                      
&gt;layer.forward(input, target)
0.8793184
</code></pre>

<h2 id="marginrankingcriterion">MarginRankingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val mse = new MarginRankingCriterion(margin=1.0, sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">mse = MarginRankingCriterion(margin=1.0, size_average=true)
</code></pre>

<p>Creates a criterion that measures the loss given an input x = {x1, x2},
a table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).
In batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size
batchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.
If y == 1 then it assumed the first input should be ranked higher (have a larger value) than
the second input, and vice-versa for y == -1.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.MarginRankingCriterion
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

import scala.util.Random

val input1Arr = Array(1, 2, 3, 4, 5)
val input2Arr = Array(5, 4, 3, 2, 1)

val target1Arr = Array(-1, 1, -1, 1, 1)

val input1 = Tensor(Storage(input1Arr.map(x =&gt; x.toFloat)))
val input2 = Tensor(Storage(input2Arr.map(x =&gt; x.toFloat)))

val input = T((1.toFloat, input1), (2.toFloat, input2))

val target1 = Tensor(Storage(target1Arr.map(x =&gt; x.toFloat)))
val target = T((1.toFloat, target1))

val mse = new MarginRankingCriterion()

val output = mse.forward(input, target)
val gradInput = mse.backward(input, target)

println(output)
println(gradInput)
</code></pre>

<p>The output will be,</p>
<pre><code>output: Float = 0.8                                                                                                                                                                    [21/154]
</code></pre>

<p>The gradInput will be,</p>
<pre><code>gradInput: com.intel.analytics.bigdl.utils.Table =
 {
        2: -0.0
           0.2
           -0.2
           0.0
           0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
        1: 0.0
           -0.2
           0.2
           -0.0
           -0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
 }
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

mse = MarginRankingCriterion()

input1 = np.array([1, 2, 3, 4, 5]).astype(&quot;float32&quot;)
input2 = np.array([5, 4, 3, 2, 1]).astype(&quot;float32&quot;)
input = [input1, input2]

target1 = np.array([-1, 1, -1, 1, 1]).astype(&quot;float32&quot;)
target = [target1, target1]

output = mse.forward(input, target)
gradInput = mse.backward(input, target)

print output
print gradInput
</code></pre>

<p>The output will be,</p>
<pre><code>0.8
</code></pre>

<p>The gradInput will be,</p>
<pre><code>[array([ 0. , -0.2,  0.2, -0. , -0. ], dtype=float32), array([-0. ,  0.2, -0.2,  0. ,  0. ], dtype=float32)] 
</code></pre>

<h2 id="classnllcriterion">ClassNLLCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = ClassNLLCriterion(weights = null, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = ClassNLLCriterion(weights=None, size_average=True)
</code></pre>

<p>The negative log likelihood criterion. It is useful to train a classification problem with n
classes. If provided, the optional argument weights should be a 1D Tensor assigning weight to
each of the classes. This is particularly useful when you have an unbalanced training set.</p>
<p>The input given through a <code>forward()</code> is expected to contain log-probabilities of each class:
input has to be a 1D Tensor of size <code>n</code>. Obtaining log-probabilities in a neural network is easily
achieved by adding a <code>LogSoftMax</code> layer in the last layer of your neural network. You may use
<code>CrossEntropyCriterion</code> instead, if you prefer not to add an extra layer to your network. This
criterion expects a class index (1 to the number of class) as target when calling
<code>forward(input, target)</code> and <code>backward(input, target)</code>.</p>
<p>The loss can be described as:
     loss(x, class) = -x[class]
 or in the case of the weights argument it is specified as follows:
     loss(x, class) = -weights[class] * x[class]
 Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when
 calculating losses in non-batch mode.</p>
<p>By default, the losses are averaged over observations for each minibatch. However, if the field
 <code>sizeAverage</code> is set to false, the losses are instead summed for each minibatch.</p>
<p><strong>Parameters:</strong></p>
<p><strong>weights</strong>     - weights of each element of the input</p>
<p><strong>sizeAverage</strong> - A boolean indicating whether normalizing by the number of elements in the input.
                  Default: true</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.ClassNLLCriterion
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val criterion = ClassNLLCriterion()
val input = Tensor(T(
              T(1f, 2f, 3f),
              T(2f, 3f, 4f),
              T(3f, 4f, 5f)
          ))

val target = Tensor(T(1f, 2f, 3f))

val loss = criterion.forward(input, target)
val grad = criterion.backward(input, target)

print(loss)
-3.0
println(grad)
-0.33333334 0.0 0.0
0.0 -0.33333334 0.0
0.0 0.0 -0.33333334
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *

criterion = ClassNLLCriterion()
input = np.array([
              [1.0, 2.0, 3.0],
              [2.0, 3.0, 4.0],
              [3.0, 4.0, 5.0]
          ])

target = np.array([1.0, 2.0, 3.0])

loss = criterion.forward(input, target)
gradient= criterion.backward(input, target)

print loss
-3.0
print gradient
-3.0
[[-0.33333334  0.          0.        ]
 [ 0.         -0.33333334  0.        ]
 [ 0.          0.         -0.33333334]]
</code></pre>

<h2 id="softmaxwithcriterion">SoftmaxWithCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)
</code></pre>

<p>Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued predictions through a softmax to
get a probability distribution over classes. It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as 
its gradient computation is more numerically stable.</p>
<ul>
<li>param ignoreLabel   (optional) Specify a label value that should be ignored when computing the loss.</li>
<li>param normalizeMode How to normalize the output loss.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.{Storage, Tensor}

val input = Tensor(1, 5, 2, 3).rand()
val target = Tensor(Storage(Array(2.0f, 4.0f, 2.0f, 4.0f, 1.0f, 2.0f))).resize(1, 1, 2, 3)

val model = SoftmaxWithCriterion[Float]()
val output = model.forward(input, target)

scala&gt; print(input)
(1,1,.,.) =
0.65131104  0.9332143   0.5618989   
0.9965054   0.9370902   0.108070895 

(1,2,.,.) =
0.46066576  0.9636703   0.8123812   
0.31076035  0.16386998  0.37894428  

(1,3,.,.) =
0.49111295  0.3704862   0.9938375   
0.87996656  0.8695406   0.53354675  

(1,4,.,.) =
0.8502225   0.9033509   0.8518651   
0.0692618   0.10121379  0.970959    

(1,5,.,.) =
0.9397213   0.49688303  0.75739735  
0.25074655  0.11416598  0.6594504   

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x2x3]

scala&gt; print(output)
1.6689054
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">input = np.random.randn(1, 5, 2, 3)
target = np.array([[[[2.0, 4.0, 2.0], [4.0, 1.0, 2.0]]]])

model = SoftmaxWithCriterion()
output = model.forward(input, target)

&gt;&gt;&gt; print input
[[[[ 0.78455689  0.01402084  0.82539628]
   [-1.06448238  2.58168413  0.60053703]]

  [[-0.48617618  0.44538094  0.46611658]
   [-1.41509329  0.40038991 -0.63505732]]

  [[ 0.91266769  1.68667933  0.92423611]
   [ 0.1465411   0.84637557  0.14917515]]

  [[-0.7060493  -2.02544114  0.89070726]
   [ 0.14535539  0.73980064 -0.33130613]]

  [[ 0.64538791 -0.44384233 -0.40112523]
   [ 0.44346658 -2.22303621  0.35715986]]]]

&gt;&gt;&gt; print output
2.1002123

</code></pre>

<h2 id="smoothl1criterion">SmoothL1Criterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val slc = SmoothL1Criterion(sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">slc = SmoothL1Criterion(size_average=True)
</code></pre>

<p>Creates a criterion that can be thought of as a smooth version of the AbsCriterion.
It uses a squared term if the absolute element-wise error falls below 1.
It is less sensitive to outliers than the MSECriterion and in some
cases prevents exploding gradients (e.g. see "Fast R-CNN" paper by Ross Girshick).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.{Tensor, Storage}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.SmoothL1Criterion

val slc = SmoothL1Criterion()

val inputArr = Array(
  0.17503996845335,
  0.83220188552514,
  0.48450597329065,
  0.64701424003579,
  0.62694586534053,
  0.34398410236463,
  0.55356747563928,
  0.20383032318205
)
val targetArr = Array(
  0.69956525065936,
  0.86074831243604,
  0.54923197557218,
  0.57388074393384,
  0.63334444304928,
  0.99680578662083,
  0.49997645849362,
  0.23869121982716
)

val input = Tensor(Storage(inputArr.map(x =&gt; x.toFloat))).reshape(Array(2, 2, 2))
val target = Tensor(Storage(targetArr.map(x =&gt; x.toFloat))).reshape(Array(2, 2, 2))

val output = slc.forward(input, target)
val gradInput = slc.backward(input, target)
</code></pre>

<p>The output is,</p>
<pre><code>output: Float = 0.0447365
</code></pre>

<p>The gradInput is,</p>
<pre><code>gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
-0.06556566     -0.003568299
-0.008090746    0.009141691

(2,.,.) =
-7.998273E-4    -0.08160271
0.0066988766    -0.0043576136
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

slc = SmoothL1Criterion()

input = np.array([
    0.17503996845335,
    0.83220188552514,
    0.48450597329065,
    0.64701424003579,
    0.62694586534053,
    0.34398410236463,
    0.55356747563928,
    0.20383032318205
])
input.reshape(2, 2, 2)

target = np.array([
    0.69956525065936,
    0.86074831243604,
    0.54923197557218,
    0.57388074393384,
    0.63334444304928,
    0.99680578662083,
    0.49997645849362,
    0.23869121982716
])

target.reshape(2, 2, 2)

output = slc.forward(input, target)
gradInput = slc.backward(input, target)

print output
print gradInput
</code></pre>

<h2 id="smoothl1criterionwithweights">SmoothL1CriterionWithWeights</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val smcod = SmoothL1CriterionWithWeights[Float](sigma: Float = 2.4f, num: Int = 2)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">smcod = SmoothL1CriterionWithWeights(sigma, num)
</code></pre>

<p>a smooth version of the AbsCriterion
It uses a squared term if the absolute element-wise error falls below 1.
It is less sensitive to outliers than the MSECriterion and in some cases
prevents exploding gradients (e.g. see "Fast R-CNN" paper by Ross Girshick).</p>
<pre><code>   d = (x - y) * w_in

  loss(x, y, w_in, w_out)
              | 0.5 * (sigma * d_i)^2 * w_out          if |d_i| &lt; 1 / sigma / sigma
   = 1/n \sum |
              | (|d_i| - 0.5 / sigma / sigma) * w_out   otherwise
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">val smcod = SmoothL1CriterionWithWeights[Float](2.4f, 2)

val inputArr = Array(1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03)
val targetArr = Array(0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58)
val inWArr = Array(-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8)
val outWArr = Array(-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1)

val input = Tensor(Storage(inputArr.map(x =&gt; x.toFloat)))
val target = T()
target.insert(Tensor(Storage(targetArr.map(x =&gt; x.toFloat))))
target.insert(Tensor(Storage(inWArr.map(x =&gt; x.toFloat))))
target.insert(Tensor(Storage(outWArr.map(x =&gt; x.toFloat))))

val output = smcod.forward(input, target)
val gradInput = smcod.backward(input, target)

&gt; println(output)
  output: Float = -2.17488
&gt; println(gradInput)
-0.010944003
0.425
0.63037443
-0.95
-0.1
0.07
0.120000005
-0.44000003
[com.intel.analytics.bigdl.tensor.DenseTensor of size 8]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">smcod = SmoothL1CriterionWithWeights(2.4, 2)

input = np.array([1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03]).astype(&quot;float32&quot;)
targetArr = np.array([0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58]).astype(&quot;float32&quot;)
inWArr = np.array([-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8]).astype(&quot;float32&quot;)
outWArr = np.array([-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1]).astype(&quot;float32&quot;)
target = [targetArr, inWArr, outWArr]

output = smcod.forward(input, target)
gradInput = smcod.backward(input, target)

&gt; output
-2.17488
&gt; gradInput
[array([-0.010944  ,  0.42500001,  0.63037443, -0.94999999, -0.1       ,
         0.07      ,  0.12      , -0.44000003], dtype=float32)]
</code></pre>

<h2 id="multimargincriterion">MultiMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val loss = MultiMarginCriterion(p=1,weights=null,margin=1.0,sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">loss = MultiMarginCriterion(p=1,weights=None,margin=1.0,size_average=True)
</code></pre>

<p>MultiMarginCriterion is a loss function that optimizes a multi-class classification hinge loss (margin-based loss) between input <code>x</code> and output <code>y</code> (<code>y</code> is the target class index).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.tensor.Storage

val input = Tensor(3,2).randn()
val target = Tensor(Storage(Array(2.0f, 1.0f, 2.0f)))
val loss = MultiMarginCriterion(1)
val output = loss.forward(input,target)
val grad = loss.backward(input,target)

scala&gt; print(input)
-0.45896783     -0.80141246
0.22560088      -0.13517438
0.2601126       0.35492152
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

scala&gt; print(target)
2.0
1.0
2.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]

scala&gt; print(output)
0.4811434

scala&gt; print(grad)
0.16666667      -0.16666667
-0.16666667     0.16666667
0.16666667      -0.16666667
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]


</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

input  = np.random.randn(3,2)
target = np.array([2,1,2])
print &quot;input=&quot;,input
print &quot;target=&quot;,target

loss = MultiMarginCriterion(1)
out = loss.forward(input, target)
print &quot;output of loss is : &quot;,out

grad_out = loss.backward(input,target)
print &quot;grad out of loss is : &quot;,grad_out
</code></pre>

<p>produces output</p>
<pre><code>input= [[ 0.46868305 -2.28562261]
 [ 0.8076243  -0.67809689]
 [-0.20342555 -0.66264743]]
target= [2 1 2]
creating: createMultiMarginCriterion
output of loss is :  0.8689213
grad out of loss is :  [[ 0.16666667 -0.16666667]
 [ 0.          0.        ]
 [ 0.16666667 -0.16666667]]


</code></pre>

<h2 id="hingeembeddingcriterion">HingeEmbeddingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = HingeEmbeddingCriterion(margin = 1, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = HingeEmbeddingCriterion(margin=1, size_average=True)
</code></pre>

<p>Creates a criterion that measures the loss given an input <code>x</code> which is a 1-dimensional vector and a label <code>y</code> (<code>1</code> or <code>-1</code>).
This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>
<pre><code>                 ⎧ x_i,                  if y_i ==  1
loss(x, y) = 1/n ⎨
                 ⎩ max(0, margin - x_i), if y_i == -1
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils.{T}
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.{T}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val loss = HingeEmbeddingCriterion(1, sizeAverage = false)
val input = Tensor(T(0.1f, 2.0f, 2.0f, 2.0f))
println(&quot;input: \n&quot; + input)
println(&quot;ouput: &quot;)

println(&quot;Target=1: &quot; + loss.forward(input, Tensor(4, 1).fill(1f)))

println(&quot;Target=-1: &quot; + loss.forward(input, Tensor(4, 1).fill(-1f)))
</code></pre>

<pre><code>input: 
0.1
2.0
2.0
2.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]
ouput: 
Target=1: 6.1
Target=-1: 0.9

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
input = np.array([0.1, 2.0, 2.0, 2.0])
target = np.full(4, 1)
print(&quot;input: &quot; )
print(input)
print(&quot;target: &quot;)
print(target)
print(&quot;output: &quot;)
print(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, target))
print(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, np.full(4, -1)))
</code></pre>

<pre><code>input: 
[ 0.1  2.   2.   2. ]
target: 
[1 1 1 1]
output: 
creating: createHingeEmbeddingCriterion
6.1
creating: createHingeEmbeddingCriterion
0.9
</code></pre>

<h2 id="margincriterion">MarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">criterion = MarginCriterion(margin=1.0, sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MarginCriterion(margin=1.0, sizeAverage=true, bigdl_type=&quot;float&quot;)
</code></pre>

<p>Creates a criterion that optimizes a two-class classification hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.
 * @param margin if unspecified, is by default 1.
 * @param sizeAverage whether to average the loss, is by default true</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val criterion = MarginCriterion(margin=1.0, sizeAverage=true)

val input = Tensor(3, 2).rand()
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.33753583      0.3575501
0.23477706      0.7240361
0.92835575      0.4737949
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

val target = Tensor(3, 2).rand()
target: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.27280563      0.7022703
0.3348442       0.43332106
0.08935371      0.17876455
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

criterion.forward(input, target)
res5: Float = 0.84946966
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">criterion = MarginCriterion(margin=1.0,size_average=True,bigdl_type=&quot;float&quot;)
input = np.random.rand(3, 2)
array([[ 0.20824672,  0.67299837],
       [ 0.80561452,  0.19564743],
       [ 0.42501441,  0.19408184]])

target = np.random.rand(3, 2)
array([[ 0.67882632,  0.61257846],
       [ 0.10111138,  0.75225082],
       [ 0.60404296,  0.31373273]])

criterion.forward(input, target)
0.8166871
</code></pre>

<h2 id="cosineembeddingcriterion">CosineEmbeddingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val cosineEmbeddingCriterion = CosineEmbeddingCriterion(margin  = 0.0, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">cosineEmbeddingCriterion = CosineEmbeddingCriterion( margin=0.0,size_average=True)
</code></pre>

<p>CosineEmbeddingCriterion creates a criterion that measures the loss given an input x = {x1, x2},
a table of two Tensors, and a Tensor label y with values 1 or -1.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
impot com.intel.analytics.bigdl.utils.T
val cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, false)
val input1 = Tensor(5).rand()
val input2 = Tensor(5).rand()
val input = T()
input(1.0) = input1
input(2.0) = input2
val target1 = Tensor(Storage(Array(-0.5f)))
val target = T()
target(1.0) = target1

&gt; print(input)
 {
    2.0: 0.4110882
         0.57726574
         0.1949834
         0.67670715
         0.16984987
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
    1.0: 0.16878392
         0.24124223
         0.8964794
         0.11156334
         0.5101486
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
 }

&gt; print(cosineEmbeddingCriterion.forward(input, target))
0.49919847

&gt; print(cosineEmbeddingCriterion.backward(input, target))
 {
    2: -0.045381278
       -0.059856333
       0.72547954
       -0.2268434
       0.3842142
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
    1: 0.30369008
       0.42463788
       -0.20637506
       0.5712836
       -0.06355385
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, False)
&gt; cosineEmbeddingCriterion.forward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])
0.6363636
&gt; cosineEmbeddingCriterion.backward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])
[array([ 0.07933884,  0.04958678,  0.01983471, -0.00991735, -0.03966942], dtype=float32), array([-0.03966942, -0.00991735,  0.01983471,  0.04958678,  0.07933884], dtype=float32)]

</code></pre>

<h2 id="bcecriterion">BCECriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = BCECriterion[Float]()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = BCECriterion()
</code></pre>

<p>This loss function measures the Binary Cross Entropy between the target and the output</p>
<pre><code> loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
</code></pre>

<p>or in the case of the weights argument being specified:</p>
<pre><code> loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
</code></pre>

<p>By default, the losses are averaged for each mini-batch over observations as well as over
 dimensions. However, if the field sizeAverage is set to false, the losses are instead summed.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
val criterion = BCECriterion[Float]()
val input = Tensor[Float](3, 1).rand

val target = Tensor[Float](3)
target(1) = 1
target(2) = 0
target(3) = 1

val output = criterion.forward(input, target)
val gradInput = criterion.backward(input, target)

&gt; println(target)
res25: com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0
0.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]

&gt; println(output)
output: Float = 0.9009579

&gt; println(gradInput)
gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-1.5277504
1.0736246
-0.336957
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">
criterion = BCECriterion()
input = np.random.uniform(0, 1, (3, 1)).astype(&quot;float32&quot;)
target = np.array([1, 0, 1])
output = criterion.forward(input, target)
gradInput = criterion.backward(input, target)

&gt; output
1.9218739
&gt; gradInput
[array([[-4.3074522 ],
        [ 2.24244714],
        [-1.22368968]], dtype=float32)]

</code></pre>

<h2 id="dicecoefficientcriterion">DiceCoefficientCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val loss = DiceCoefficientCriterion(sizeAverage=true, epsilon=1.0f)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">loss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)
</code></pre>

<p>DiceCoefficientCriterion is the Dice-Coefficient objective function. </p>
<p>Both <code>forward</code> and <code>backward</code> accept two tensors : input and target. The <code>forward</code> result is formulated as 
          <code>1 - (2 * (input intersection target) / (input union target))</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.tensor.Storage

val input = Tensor(2).randn()
val target = Tensor(Storage(Array(2.0f, 1.0f)))
val loss = DiceCoefficientCriterion(epsilon = 1.0f)
val output = loss.forward(input,target)
val grad = loss.backward(input,target)

scala&gt; print(input)
-0.50278
0.51387966
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(target)
2.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(output)
0.9958517

scala&gt; print(grad)
-0.99619853     -0.49758217
[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

input  = np.random.randn(2)
target = np.array([2,1],dtype='float64')

print &quot;input=&quot;, input
print &quot;target=&quot;, target
loss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)
out = loss.forward(input,target)
print &quot;output of loss is :&quot;,out

grad_out = loss.backward(input,target)
print &quot;grad out of loss is :&quot;,grad_out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input= [ 0.4440505  2.9430301]
target= [ 2.  1.]
creating: createDiceCoefficientCriterion
output of loss is : -0.17262316
grad out of loss is : [[-0.38274616 -0.11200322]]
</code></pre>

<h2 id="msecriterion">MSECriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MSECriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MSECriterion()
</code></pre>

<p>The mean squared error criterion e.g. input: a, target: b, total elements: n</p>
<pre><code>loss(a, b) = 1/n * sum(|a_i - b_i|^2)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><strong>sizeAverage</strong> - a boolean indicating whether to divide the sum of squared error by n.
 Default: true</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = MSECriterion()
val input = Tensor(T(
 T(1.0f, 2.0f),
 T(3.0f, 4.0f))
)
val target = Tensor(T(
 T(2.0f, 3.0f),
 T(4.0f, 5.0f))
)
val output = criterion.forward(input, target)
val gradient = criterion.backward(input, target)
-&gt; print(output)
1.0
-&gt; print(gradient)
-0.5    -0.5    
-0.5    -0.5    
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
criterion = MSECriterion()
input = np.array([
          [1.0, 2.0],
          [3.0, 4.0]
        ])
target = np.array([
           [2.0, 3.0],
           [4.0, 5.0]
         ])
output = criterion.forward(input, target)
gradient= criterion.backward(input, target)
-&gt; print output
1.0
-&gt; print gradient
[[-0.5 -0.5]
 [-0.5 -0.5]]
</code></pre>

<h2 id="softmargincriterion">SoftMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = SoftMarginCriterion(sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = SoftMarginCriterion(size_average)
</code></pre>

<p>Creates a criterion that optimizes a two-class classification logistic loss between
input x (a Tensor of dimension 1) and output y (which is a tensor containing either
1s or -1s).</p>
<pre><code>loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()
</code></pre>

<p><strong>Parameters:</strong>
* <strong>sizeAverage</strong> - A boolean indicating whether normalizing by the number of elements in the input.
                    Default: true</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = SoftMarginCriterion()
val input = Tensor(T(
 T(1.0f, 2.0f),
 T(3.0f, 4.0f))
)
val target = Tensor(T(
 T(1.0f, -1.0f),
 T(-1.0f, 1.0f))
)
val output = criterion.forward(input, target)
val gradient = criterion.backward(input, target)
-&gt; print(output)
1.3767318
-&gt; print(gradient)
-0.06723536     0.22019927      
0.23814353      -0.0044965525   
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
criterion = SoftMarginCriterion()
input = np.array([
          [1.0, 2.0],
          [3.0, 4.0]
        ])
target = np.array([
           [2.0, 3.0],
           [4.0, 5.0]
         ])
output = criterion.forward(input, target)
gradient = criterion.backward(input, target)
-&gt; print output
1.3767318
-&gt; print gradient
[[-0.06723536  0.22019927]
 [ 0.23814353 -0.00449655]]
</code></pre>

<h2 id="distkldivcriterion">DistKLDivCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val loss = DistKLDivCriterion[T](sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">loss = DistKLDivCriterion(size_average=True)
</code></pre>

<p>DistKLDivCriterion is the Kullback–Leibler divergence loss.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.tensor.Storage

val input = Tensor(2).randn()
val target = Tensor(Storage(Array(2.0f, 1.0f)))
val loss = DistKLDivCriterion()
val output = loss.forward(input,target)
val grad = loss.backward(input,target)

scala&gt; print(input)
-0.3854126
-0.7707398
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(target)
2.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(output)
1.4639297

scala&gt; print(grad)
-1.0
-0.5
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

input  = np.random.randn(2)
target = np.array([2,1])

print &quot;input=&quot;, input
print &quot;target=&quot;, target
loss = DistKLDivCriterion()
out = loss.forward(input,target)
print &quot;output of loss is :&quot;,out

grad_out = loss.backward(input,target)
print &quot;grad out of loss is :&quot;,grad_out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input= [-1.14333924  0.97662296]
target= [2 1]
creating: createDistKLDivCriterion
output of loss is : 1.348175
grad out of loss is : [-1.  -0.5]
</code></pre>

<h2 id="classsimplexcriterion">ClassSimplexCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = ClassSimplexCriterion(nClasses)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = ClassSimplexCriterion(nClasses)
</code></pre>

<p>ClassSimplexCriterion implements a criterion for classification.
It learns an embedding per class, where each class' embedding is a
point on an (N-1)-dimensional simplex, where N is the number of classes.</p>
<p><strong>Parameters:</strong>
* <strong>nClasses</strong> - An integer, the number of classes.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = ClassSimplexCriterion(5)
val input = Tensor(T(
 T(1.0f, 2.0f, 3.0f, 4.0f, 5.0f),
 T(4.0f, 5.0f, 6.0f, 7.0f, 8.0f)
))
val target = Tensor(2)
target(1) = 2.0f
target(2) = 1.0f
val output = criterion.forward(input, target)
val gradient = criterion.backward(input, target)
-&gt; print(output)
23.562702
-&gt; print(gradient)
0.25    0.20635083      0.6     0.8     1.0     
0.6     1.0     1.2     1.4     1.6     
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
criterion = ClassSimplexCriterion(5)
input = np.array([
   [1.0, 2.0, 3.0, 4.0, 5.0],
   [4.0, 5.0, 6.0, 7.0, 8.0]
])
target = np.array([2.0, 1.0])
output = criterion.forward(input, target)
gradient = criterion.backward(input, target)
-&gt; print output
23.562702
-&gt; print gradient
[[ 0.25        0.20635083  0.60000002  0.80000001  1.        ]
 [ 0.60000002  1.          1.20000005  1.39999998  1.60000002]]
</code></pre>

<h2 id="l1hingeembeddingcriterion">L1HingeEmbeddingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = L1HingeEmbeddingCriterion(margin)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = L1HingeEmbeddingCriterion(margin)
</code></pre>

<p>Creates a criterion that measures the loss given an input <code>x = {x1, x2}</code>, a table of two Tensors, and a label y (1 or -1).
This is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>
<pre><code>             ⎧ ||x1 - x2||_1,                  if y ==  1
loss(x, y) = ⎨
             ⎩ max(0, margin - ||x1 - x2||_1), if y == -1
</code></pre>

<p>The margin has a default value of 1, or can be set in the constructor.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val model = L1HingeEmbeddingCriterion(0.6)
val input1 = Tensor(T(1.0f, -0.1f))
val input2 = Tensor(T(2.0f, -0.2f))
val input = T(input1, input2)
val target = Tensor(1)
target(Array(1)) = 1.0f

val output = model.forward(input, target)

scala&gt; print(output)
1.1
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = L1HingeEmbeddingCriterion(0.6)
input1 = np.array(1.0, -0.1)
input2 = np.array(2.0, -0.2)
input = [input1, input2]
target = np.array([1.0])

output = model.forward(input, target)

&gt;&gt;&gt; print output
1.1
</code></pre>

<h2 id="crossentropycriterion">CrossEntropyCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = CrossEntropyCriterion(weights, sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = CrossEntropyCriterion(weights, sizeAverage)
</code></pre>

<p>This criterion combines LogSoftMax and ClassNLLCriterion in one single class.</p>
<p><code>weights</code> A tensor assigning weight to each of the classes</p>
<p><code>sizeAverage</code> whether to divide the sequence length. Default is true.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage

val layer = CrossEntropyCriterion[Double]()
val input = Tensor[Double](Storage(Array(
    1.0262627674932,
    -1.2412600935171,
    -1.0423174168648,
    -0.90330565804228,
    -1.3686840144413,
    -1.0778380454479,
    -0.99131220658219,
    -1.0559142847536,
    -1.2692712660404
    ))).resize(3, 3)
val target = Tensor[Double](3)
    target(Array(1)) = 1
    target(Array(2)) = 2
    target(Array(3)) = 3
&gt; print(layer.forward(input, target))
0.9483051199107635
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *

layer = CrossEntropyCriterion()
input = np.array([1.0262627674932,
                      -1.2412600935171,
                      -1.0423174168648,
                      -0.90330565804228,
                      -1.3686840144413,
                      -1.0778380454479,
                      -0.99131220658219,
                      -1.0559142847536,
                      -1.2692712660404
                      ]).reshape(3,3)
target = np.array([1, 2, 3])                      
&gt;layer.forward(input, target)
0.94830513
</code></pre>

<h2 id="parallelcriterion">ParallelCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val pc = ParallelCriterion(repeatTarget=false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">pc = ParallelCriterion(repeat_target=False)
</code></pre>

<p>ParallelCriterion is a weighted sum of other criterions each applied to a different input
and target. Set repeatTarget = true to share the target for criterions.
Use add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.{Tensor, Storage}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.{ParallelCriterion, ClassNLLCriterion, MSECriterion}

val pc = ParallelCriterion()

val input = T(Tensor(2, 10), Tensor(2, 10))
var i = 0
input[Tensor](1).apply1(_ =&gt; {i += 1; i})
input[Tensor](2).apply1(_ =&gt; {i -= 1; i})
val target = T(Tensor(Storage(Array(1.0f, 8.0f))), Tensor(2, 10).fill(1.0f))

val nll = ClassNLLCriterion()
val mse = MSECriterion()
pc.add(nll, 0.5).add(mse)

val output = pc.forward(input, target)
val gradInput = pc.backward(input, target)

println(output)
println(gradInput)

</code></pre>

<p>The output is,</p>
<pre><code>100.75

</code></pre>

<p>The gradInput is,</p>
<pre><code> {
        2: 1.8000001    1.7     1.6     1.5     1.4     1.3000001       1.2     1.1     1.0     0.90000004
           0.8  0.7     0.6     0.5     0.4     0.3     0.2     0.1     0.0     -0.1
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]
        1: -0.25        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0
           0.0  0.0     0.0     0.0     0.0     0.0     0.0     -0.25   0.0     0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

pc = ParallelCriterion()

input1 = np.arange(1, 21, 1).astype(&quot;float32&quot;)
input2 = np.arange(0, 20, 1).astype(&quot;float32&quot;)[::-1]
input1 = input1.reshape(2, 10)
input2 = input2.reshape(2, 10)

input = [input1, input2]

target1 = np.array([1.0, 8.0]).astype(&quot;float32&quot;)
target1 = target1.reshape(2)
target2 = np.full([2, 10], 1).astype(&quot;float32&quot;)
target2 = target2.reshape(2, 10)
target = [target1, target2]

nll = ClassNLLCriterion()
mse = MSECriterion()

pc.add(nll, weight = 0.5).add(mse)

print &quot;input = \n %s &quot; % input
print &quot;target = \n %s&quot; % target

output = pc.forward(input, target)
gradInput = pc.backward(input, target)

print &quot;output = %s &quot; % output
print &quot;gradInput = %s &quot; % gradInput
</code></pre>

<p>The console will output,</p>
<pre><code>input = 
 [array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],
       [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]], dtype=float32), array([[ 19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,  10.],
       [  9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.]], dtype=float32)] 
target = 
 [array([ 1.,  8.], dtype=float32), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)]
output = 100.75 
gradInput = [array([[-0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ]], dtype=float32), array([[ 1.80000007,  1.70000005,  1.60000002,  1.5       ,  1.39999998,
         1.30000007,  1.20000005,  1.10000002,  1.        ,  0.90000004],
       [ 0.80000001,  0.69999999,  0.60000002,  0.5       ,  0.40000001,
         0.30000001,  0.2       ,  0.1       ,  0.        , -0.1       ]], dtype=float32)]
</code></pre>

<h2 id="multilabelmargincriterion">MultiLabelMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val multiLabelMarginCriterion = MultiLabelMarginCriterion(sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">multiLabelMarginCriterion = MultiLabelMarginCriterion(size_average=True)
</code></pre>

<p>MultiLabelMarginCriterion creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x and output y </p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val multiLabelMarginCriterion = MultiLabelMarginCriterion(false)
val input = Tensor(4).rand()
val target = Tensor(4)
target(Array(1)) = 3
target(Array(2)) = 2
target(Array(3)) = 1
target(Array(4)) = 0

&gt; print(input)
0.40267515
0.5913795
0.84936756
0.05999674

&gt;  print(multiLabelMarginCriterion.forward(input, target))
0.33414197

&gt; print(multiLabelMarginCriterion.backward(input, target))
-0.25
-0.25
-0.25
0.75
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]


</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
multiLabelMarginCriterion = MultiLabelMarginCriterion(False)

&gt; multiLabelMarginCriterion.forward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))
0.975

&gt; multiLabelMarginCriterion.backward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))
[array([-0.25, -0.25, -0.25,  0.75], dtype=float32)]

</code></pre>

<h2 id="multilabelsoftmargincriterion">MultiLabelSoftMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MultiLabelSoftMarginCriterion(weights = null, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MultiLabelSoftMarginCriterion(weights=None, size_average=True)
</code></pre>

<p>MultiLabelSoftMarginCriterion is a multiLabel multiclass criterion based on sigmoid:</p>
<pre><code>l(x,y) = - sum_i y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i])
</code></pre>

<p>where <code>p[i] = exp(x[i]) / (1 + exp(x[i]))</code></p>
<p>If with weights,
 <code>l(x,y) = - sum_i weights[i] (y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i]))</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = MultiLabelSoftMarginCriterion()
val input = Tensor(3)
input(Array(1)) = 0.4f
input(Array(2)) = 0.5f
input(Array(3)) = 0.6f
val target = Tensor(3)
target(Array(1)) = 0
target(Array(2)) = 1
target(Array(3)) = 1

&gt; criterion.forward(input, target)
res0: Float = 0.6081934
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

criterion = MultiLabelSoftMarginCriterion()
input = np.array([0.4, 0.5, 0.6])
target = np.array([0, 1, 1])

&gt; criterion.forward(input, target)
0.6081934
</code></pre>

<h2 id="abscriterion">AbsCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = AbsCriterion(sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = AbsCriterion(sizeAverage)
</code></pre>

<p>Measures the mean absolute value of the element-wise difference between input and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = AbsCriterion()
val input = Tensor(T(1.0f, 2.0f, 3.0f))
val target = Tensor(T(4.0f, 5.0f, 6.0f))
val output = criterion.forward(input, target)

scala&gt; print(output)
3.0
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">criterion = AbsCriterion()
input = np.array([1.0, 2.0, 3.0])
target = np.array([4.0, 5.0, 6.0])
output=criterion.forward(input, target)

&gt;&gt;&gt; print output
3.0
</code></pre>

<h2 id="multicriterion">MultiCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MultiCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MultiCriterion()
</code></pre>

<p>MultiCriterion is a weighted sum of other criterions each applied to the same input and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = MultiCriterion()
val nll = ClassNLLCriterion()
val mse = MSECriterion()
criterion.add(nll, 0.5)
criterion.add(mse)

val input = Tensor(5).randn()
val target = Tensor(5)
target(Array(1)) = 1
target(Array(2)) = 2
target(Array(3)) = 3
target(Array(4)) = 2
target(Array(5)) = 1

val output = criterion.forward(input, target)

&gt; input
1.0641425
-0.33507252
1.2345984
0.08065767
0.531199
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]


&gt; output
res7: Float = 1.9633228
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

criterion = MultiCriterion()
nll = ClassNLLCriterion()
mse = MSECriterion()
criterion.add(nll, 0.5)
criterion.add(mse)

input = np.array([0.9682213801388531,
0.35258855644097503,
0.04584479998452568,
-0.21781499692588918,
-1.02721844006879])
target = np.array([1, 2, 3, 2, 1])

output = criterion.forward(input, target)

&gt; output
3.6099546
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Initializers/merged-Initializers/" class="btn btn-neutral float-right" title="Initalizers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../Layers/Utilities/merged-Utilities/" class="btn btn-neutral" title="Utilities"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../Layers/Utilities/merged-Utilities/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Initializers/merged-Initializers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../js/theme.js"></script>

</body>
</html>
