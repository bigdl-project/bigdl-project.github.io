
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>bigdl.nn.layer &#8212; BigDL  documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for bigdl.nn.layer</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Copyright 2016 The BigDL Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>


<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">JTensor</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">JavaValue</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">callBigDlFunc</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">callJavaFunc</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">get_spark_context</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">to_list</span>
<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">INTMAX</span><span class="p">,</span> <span class="n">INTMIN</span><span class="p">,</span> <span class="n">DOUBLEMAX</span>
<span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="k">import</span> <span class="n">L1Regularizer</span><span class="p">,</span> <span class="n">L2Regularizer</span><span class="p">,</span> <span class="n">L1L2Regularizer</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">long</span> <span class="o">=</span> <span class="nb">int</span>
    <span class="n">unicode</span> <span class="o">=</span> <span class="nb">str</span>

<div class="viewcode-block" id="Node"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node">[docs]</a><span class="k">class</span> <span class="nc">Node</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represent a node in a graph. The connections between nodes are directed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span> <span class="k">if</span> <span class="n">jvalue</span> <span class="k">else</span> <span class="n">callBigDlFunc</span><span class="p">(</span>
            <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">JavaValue</span><span class="o">.</span><span class="n">jvm_class_constructor</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>

<div class="viewcode-block" id="Node.of"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node.of">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">of</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Node</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>

<div class="viewcode-block" id="Node.element"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Node.element">[docs]</a>    <span class="k">def</span> <span class="nf">element</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">element</span><span class="p">())</span></div></div>


<div class="viewcode-block" id="Layer"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer">[docs]</a><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Layer is the basic component of a neural network</span>
<span class="sd">    and it&#39;s also the base class of layers.</span>
<span class="sd">    Layer can connect to others to construct a complex neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jvalue</span> <span class="k">if</span> <span class="n">jvalue</span> <span class="k">else</span> <span class="n">callBigDlFunc</span><span class="p">(</span>
            <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">JavaValue</span><span class="o">.</span><span class="n">jvm_class_constructor</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span> <span class="o">=</span> <span class="n">bigdl_type</span>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &gt;&gt;&gt; conv2 = SpatialConvolution(6, 12, 5, 5).set_name(&quot;conv2&quot;)</span>
<span class="sd">        creating: createSpatialConvolution</span>
<span class="sd">        &gt;&gt;&gt; print(conv2)</span>
<span class="sd">        SpatialConvolution[conv2](6 -&gt; 12, 5 x 5, 1, 1, 0, 0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">toString</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Some other modules point to current module</span>
<span class="sd">        :param x: upstream module nodes. x is either a Node or list of Node.</span>
<span class="sd">        :return: node containing current module</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">Node</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="s2">&quot;createNode&quot;</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="p">,</span>
                                     <span class="n">to_list</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<div class="viewcode-block" id="Layer.of"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.of">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">of</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a Python Layer base on the given java value</span>
<span class="sd">        :param jvalue: Java object create by Py4j</span>
<span class="sd">        :return: A Python Layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="Layer.set_name"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_name">[docs]</a>    <span class="k">def</span> <span class="nf">set_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Give this model a name. There would be a generated name</span>
<span class="sd">        consist of class name and UUID if user doesn&#39;t set it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">setName</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.name"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.name">[docs]</a>    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Name of this layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">getName</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.set_seed"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_seed">[docs]</a>    <span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        You can control the random seed which used to init weights for this model.</span>

<span class="sd">        :param seed: random seed</span>
<span class="sd">        :return: Model itself.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setModelSeed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.get_dtype"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.get_dtype">[docs]</a>    <span class="k">def</span> <span class="nf">get_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;float&quot;</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;float32&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;float64&quot;</span></div>

<div class="viewcode-block" id="Layer.check_input"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.check_input">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">check_input</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param input: ndarray or list of ndarray</span>
<span class="sd">        :return: (list of JTensor, isTable)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Error when checking: empty input&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;shape&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s1">&#39;Error when checking: expecting list of ndarray&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">],</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s1">&#39;Error when checking: expecting list of ndarray&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="nb">input</span><span class="p">)],</span> <span class="kc">False</span></div>

<div class="viewcode-block" id="Layer.convert_output"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.convert_output">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">convert_output</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">is</span> <span class="n">JTensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span>
        <span class="k">elif</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span></div>

<div class="viewcode-block" id="Layer.forward"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        Takes an input object, and computes the corresponding output of the module</span>

<span class="sd">        :param input: ndarray or list of ndarray</span>
<span class="sd">        :return: ndarray or list of ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jinput</span><span class="p">,</span> <span class="n">input_is_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                               <span class="s2">&quot;modelForward&quot;</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="n">jinput</span><span class="p">,</span>
                               <span class="n">input_is_table</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.backward"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        Performs a back-propagation step through the module, with respect to the given input. In</span>
<span class="sd">        general this method makes the assumption forward(input) has been called before, with the same</span>
<span class="sd">        input. This is necessary for optimization reasons. If you do not respect this rule, backward()</span>
<span class="sd">        will compute incorrect gradients.</span>

<span class="sd">        :param input: ndarray or list of ndarray</span>
<span class="sd">        :param grad_output: ndarray or list of ndarray</span>
<span class="sd">        :return: ndarray or list of ndarray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jinput</span><span class="p">,</span> <span class="n">input_is_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">jgrad_output</span><span class="p">,</span> <span class="n">grad_output_is_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                               <span class="s2">&quot;modelBackward&quot;</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                               <span class="n">jinput</span><span class="p">,</span>
                               <span class="n">input_is_table</span><span class="p">,</span>
                               <span class="n">jgrad_output</span><span class="p">,</span>
                               <span class="n">grad_output_is_table</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.zero_grad_parameters"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.zero_grad_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">zero_grad_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        If the module has parameters, this will zero the accumulation of the gradients with respect</span>
<span class="sd">        to these parameters. Otherwise, it does nothing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">zeroGradParameters</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.update_parameters"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.update_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        NB: It&#39;s for debug only, please use optimizer.optimize() in production.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                      <span class="s2">&quot;updateParameters&quot;</span><span class="p">,</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">learning_rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.reset"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the model weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">get_spark_context</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">reset</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Layer.parameters"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.parameters">[docs]</a>    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the model parameters which containing: weight, bias, gradBias, gradWeight</span>

<span class="sd">        :return: dict(layername -&gt; dict(parametername -&gt; ndarray))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">name_to_params</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="s2">&quot;modelGetParameters&quot;</span><span class="p">,</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">to_ndarray</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">dict</span><span class="p">((</span><span class="n">param_name</span><span class="p">,</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_dtype</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                             <span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span>
                        <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">((</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span>
                <span class="n">name_to_params</span><span class="o">.</span><span class="n">items</span><span class="p">())</span></div>

<div class="viewcode-block" id="Layer.predict"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_rdd</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Model inference base on the given data.</span>
<span class="sd">        You need to invoke collect() to trigger those action \</span>
<span class="sd">        as the returning result is an RDD.</span>

<span class="sd">        :param data_rdd: the data to be predict.</span>
<span class="sd">        :return: An RDD represent the predict result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                             <span class="s2">&quot;modelPredictRDD&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">data_rdd</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">data</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">())</span></div>

<div class="viewcode-block" id="Layer.test"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.test">[docs]</a>    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_rdd</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">val_methods</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A method to benchmark the model quality.</span>

<span class="sd">        :param val_rdd: the input data</span>
<span class="sd">        :param batch_size: batch size</span>
<span class="sd">        :param val_methods: a list of validation methods. i.e: Top1Accuracy,Top5Accuracy and Loss.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                             <span class="s2">&quot;modelTest&quot;</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">val_rdd</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">val_methods</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.set_weights"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.set_weights">[docs]</a>    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set weights for this layer</span>

<span class="sd">        :param weights: a list of numpy arrays which represent weight and bias</span>
<span class="sd">        :return:</span>

<span class="sd">        &gt;&gt;&gt; linear = Linear(3,2)</span>
<span class="sd">        creating: createLinear</span>
<span class="sd">        &gt;&gt;&gt; linear.set_weights([np.array([[1,2,3],[4,5,6]]), np.array([7,8])])</span>
<span class="sd">        &gt;&gt;&gt; weights = linear.get_weights()</span>
<span class="sd">        &gt;&gt;&gt; weights[0].shape == (2,3)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; weights[0][0]</span>
<span class="sd">        array([ 1.,  2.,  3.], dtype=float32)</span>
<span class="sd">        &gt;&gt;&gt; weights[1]</span>
<span class="sd">        array([ 7.,  8.], dtype=float32)</span>
<span class="sd">        &gt;&gt;&gt; relu = ReLU()</span>
<span class="sd">        creating: createReLU</span>
<span class="sd">        &gt;&gt;&gt; from py4j.protocol import Py4JJavaError</span>
<span class="sd">        &gt;&gt;&gt; try:</span>
<span class="sd">        ...     relu.set_weights([np.array([[1,2,3],[4,5,6]]), np.array([7,8])])</span>
<span class="sd">        ... except Py4JJavaError as err:</span>
<span class="sd">        ...     print(err.java_exception)</span>
<span class="sd">        ...</span>
<span class="sd">        java.lang.IllegalArgumentException: requirement failed: this layer does not have weight/bias</span>
<span class="sd">        &gt;&gt;&gt; relu.get_weights()</span>
<span class="sd">        The layer does not have weight/bias</span>
<span class="sd">        &gt;&gt;&gt; add = Add(2)</span>
<span class="sd">        creating: createAdd</span>
<span class="sd">        &gt;&gt;&gt; try:</span>
<span class="sd">        ...     add.set_weights([np.array([7,8]), np.array([1,2])])</span>
<span class="sd">        ... except Py4JJavaError as err:</span>
<span class="sd">        ...     print(err.java_exception)</span>
<span class="sd">        ...</span>
<span class="sd">        java.lang.IllegalArgumentException: requirement failed: the number of input weight/bias is not consistant with number of weight/bias of this layer</span>
<span class="sd">        &gt;&gt;&gt; cAdd = CAdd([4, 1])</span>
<span class="sd">        creating: createCAdd</span>
<span class="sd">        &gt;&gt;&gt; cAdd.set_weights(np.ones([4, 1]))</span>
<span class="sd">        &gt;&gt;&gt; (cAdd.get_weights()[0] == np.ones([4, 1])).all()</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">to_list</span><span class="p">(</span><span class="n">weights</span><span class="p">)]</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setWeights&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">tensors</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.get_weights"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.get_weights">[docs]</a>    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get weights for this layer</span>

<span class="sd">        :return: list of numpy arrays which represent weight and bias</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensorWeights</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span>
                              <span class="s2">&quot;getWeights&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensorWeights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">()</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensorWeights</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The layer does not have weight/bias&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="Layer.save"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">over_write</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;modelSave&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span>
                      <span class="n">over_write</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.setWRegularizer"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.setWRegularizer">[docs]</a>    <span class="k">def</span> <span class="nf">setWRegularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        set weight regularizer</span>
<span class="sd">        :param wRegularizer: weight regularizer</span>
<span class="sd">        :return:</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">wRegularizer</span> <span class="o">=</span> <span class="n">wRegularizer</span><span class="o">.</span><span class="n">value</span></div>

<div class="viewcode-block" id="Layer.setBRegularizer"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Layer.setBRegularizer">[docs]</a>    <span class="k">def</span> <span class="nf">setBRegularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        set bias regularizer</span>
<span class="sd">        :param wRegularizer: bias regularizer</span>
<span class="sd">        :return:</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">bRegularizer</span> <span class="o">=</span> <span class="n">bRegularizer</span><span class="o">.</span><span class="n">value</span></div></div>


<div class="viewcode-block" id="Container"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Container">[docs]</a><span class="k">class</span> <span class="nc">Container</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">     [[Container]] is a sub-class of Model that declares methods defined in all containers.</span>
<span class="sd">     A container usually contain some other modules which can be added through the &quot;add&quot; method</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Container</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">jvalue</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

<div class="viewcode-block" id="Container.add"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Container.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Model"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model">[docs]</a><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A graph container. Each node can have multiple inputs. The output of the node should be a</span>
<span class="sd">    tensor. The output tensor can be connected to multiple nodes. So the module in each node can</span>
<span class="sd">    have a tensor or table input, and should have a tensor output.</span>

<span class="sd">    The graph container can have multiple inputs and multiple outputs. If there&#39;s one input,</span>
<span class="sd">    the input data fed to the graph module should be a tensor. If there&#39;re multiple inputs,</span>
<span class="sd">    the input data fed to the graph module should be a table, which is actually an sequence of</span>
<span class="sd">    tensor. The order of the input tensors should be same with the order of the input nodes.</span>
<span class="sd">    This is also applied to the gradient from the module in the back propagation.</span>

<span class="sd">    If there&#39;s one output, the module output is a tensor. If there&#39;re multiple outputs, the module</span>
<span class="sd">    output is a table, which is actually an sequence of tensor. The order of the output tensors is</span>
<span class="sd">    same with the order of the output modules. This is also applied to the gradient passed to the</span>
<span class="sd">    module in the back propagation.</span>

<span class="sd">    All inputs should be able to connect to outputs through some paths in the graph.</span>
<span class="sd">    It is allowed that some successors of the inputs node are not connect to outputs.</span>
<span class="sd">    If so, these nodes will be excluded in the computation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inputs</span><span class="p">,</span>
                 <span class="n">outputs</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">to_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
                                    <span class="n">to_list</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>

<div class="viewcode-block" id="Model.load"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Bigdl model.</span>

<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadBigDL&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_torch"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_torch">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_torch</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Torch model.</span>

<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadTorch&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_caffe"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_caffe">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_caffe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">defPath</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">match_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Caffe model.</span>


<span class="sd">        :param model: A bigdl model definition \which equivalent to the pre-trained caffe model.</span>
<span class="sd">        :param defPath: The path containing the caffe model definition.</span>
<span class="sd">        :param modelPath: The path containing the pre-trained caffe model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadCaffe&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">defPath</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">,</span> <span class="n">match_all</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Layer</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_tensorflow"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Model.load_tensorflow">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_tensorflow</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">byte_order</span> <span class="o">=</span> <span class="s2">&quot;little_endian&quot;</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a pre-trained Tensorflow model.</span>
<span class="sd">        :param path: The path containing the pre-trained model.</span>
<span class="sd">        :return: A pre-trained model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;loadTF&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">byte_order</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Linear"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Linear">[docs]</a><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The [[Linear]] module applies a linear transformation to the input data,</span>
<span class="sd">    i.e. `y = Wx + b`. The input given in `forward(input)` must be either</span>
<span class="sd">    a vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must</span>
<span class="sd">    have the size of `inputSize`. If it is a matrix, then each row is assumed to be</span>
<span class="sd">    an input sample of given batch (the number of rows means the batch size and</span>
<span class="sd">    the number of columns should be equal to the `inputSize`).</span>

<span class="sd">    :param input_size the size the each input sample</span>
<span class="sd">    :param output_size the size of the module output of each sample</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param init_weight: the optional initial value for the weight</span>
<span class="sd">    :param init_bias: the optional initial value for the bias</span>
<span class="sd">    :param init_grad_weight: the optional initial value for the grad_weight</span>
<span class="sd">    :param init_grad_bias: the optional initial value for the grad_bias</span>


<span class="sd">    &gt;&gt;&gt; linear = Linear(100, 10, True, L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(10, 100)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(10)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([10, 100])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([10])</span>
<span class="sd">    &gt;&gt;&gt; linear = Linear(100, 10, True, L1Regularizer(0.5), L1Regularizer(0.5), init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                                     <span class="n">with_bias</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">,</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                     <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>

<div class="viewcode-block" id="Linear.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Linear.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                   <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the rectified linear unit (ReLU) function element-wise to the input Tensor,</span>
<span class="sd">     thus outputting a Tensor of the same dimension.</span>


<span class="sd">    ReLU is defined as: f(x) = max(0, x)</span>
<span class="sd">    Can optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; relu = ReLU()</span>
<span class="sd">    creating: createReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ip</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">ip</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the Tanh function element-wise to the input Tensor, thus outputting a Tensor of the same</span>
<span class="sd">    dimension. Tanh is defined as f(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).</span>


<span class="sd">    &gt;&gt;&gt; tanh = Tanh()</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Tanh</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Echo"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Echo">[docs]</a><span class="k">class</span> <span class="nc">Echo</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This module is for debug purpose, which can print activation and gradient in your model</span>
<span class="sd">    topology</span>


<span class="sd">    &gt;&gt;&gt; echo = Echo()</span>
<span class="sd">    creating: createEcho</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Echo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSoftMax"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LogSoftMax">[docs]</a><span class="k">class</span> <span class="nc">LogSoftMax</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the LogSoftMax function to an n-dimensional input Tensor.</span>
<span class="sd">    LogSoftmax is defined as: f_i(x) = log(1 / a exp(x_i))</span>
<span class="sd">    where a = sum_j[exp(x_j)].</span>


<span class="sd">    &gt;&gt;&gt; logSoftMax = LogSoftMax()</span>
<span class="sd">    creating: createLogSoftMax</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogSoftMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sequential"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sequential">[docs]</a><span class="k">class</span> <span class="nc">Sequential</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Sequential provides a means to plug layers together</span>
<span class="sd">    in a feed-forward fully connected manner.</span>


<span class="sd">    &gt;&gt;&gt; echo = Echo()</span>
<span class="sd">    creating: createEcho</span>
<span class="sd">    &gt;&gt;&gt; s = Sequential()</span>
<span class="sd">    creating: createSequential</span>
<span class="sd">    &gt;&gt;&gt; s = s.add(echo)</span>
<span class="sd">    &gt;&gt;&gt; s = s.add(s)</span>
<span class="sd">    &gt;&gt;&gt; s = s.add(echo)</span>


<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sequential</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a 2D convolution over an input image composed of several input planes.</span>
<span class="sd">    The input tensor in forward(input) is expected to be</span>
<span class="sd">    a 3D tensor (nInputPlane x height x width).</span>

<span class="sd">    :param n_input_plane The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param n_output_plane The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kernel_w The kernel width of the convolution</span>
<span class="sd">    :param kernel_h The kernel height of the convolution</span>
<span class="sd">    :param stride_w The step of the convolution in the width dimension.</span>
<span class="sd">    :param stride_h The step of the convolution in the height dimension</span>
<span class="sd">    :param pad_w The additional zeros added per width to the input planes.</span>
<span class="sd">    :param pad_h The additional zeros added per height to the input planes.</span>
<span class="sd">    :param n_group Kernel group number</span>
<span class="sd">    :param propagate_back Propagate gradient back</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param init_weight: the optional initial value for the weight</span>
<span class="sd">    :param init_bias: the optional initial value for the bias</span>
<span class="sd">    :param init_grad_weight: the optional initial value for the grad_weight</span>
<span class="sd">    :param init_grad_bias: the optional initial value for the grad_bias</span>
<span class="sd">    :param with_bias: the optional initial value for if need bias</span>

<span class="sd">    &gt;&gt;&gt; spatialConvolution = SpatialConvolution(6, 12, 5, 5)</span>
<span class="sd">    creating: createSpatialConvolution</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolution.setWRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolution.setBRegularizer(L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(1, 12, 6, 5, 5)</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.random.randn(12)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([1, 12, 6, 5, 5])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([12])</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolution = SpatialConvolution(6, 12, 5, 5, 1, 1, 0, 0, 1, True, L1Regularizer(0.5), L1Regularizer(0.5), init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createSpatialConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">kernel_h</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_input_plane</span><span class="p">,</span>
                                                 <span class="n">n_output_plane</span><span class="p">,</span>
                                                 <span class="n">kernel_w</span><span class="p">,</span>
                                                 <span class="n">kernel_h</span><span class="p">,</span>
                                                 <span class="n">stride_w</span><span class="p">,</span>
                                                 <span class="n">stride_h</span><span class="p">,</span>
                                                 <span class="n">pad_w</span><span class="p">,</span>
                                                 <span class="n">pad_h</span><span class="p">,</span>
                                                 <span class="n">n_group</span><span class="p">,</span>
                                                 <span class="n">propagate_back</span><span class="p">,</span>
                                                 <span class="n">wRegularizer</span><span class="p">,</span>
                                                 <span class="n">bRegularizer</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">),</span>
                                                 <span class="n">with_bias</span><span class="p">)</span>
<div class="viewcode-block" id="SpatialConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                  <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpatialMaxPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialMaxPooling">[docs]</a><span class="k">class</span> <span class="nc">SpatialMaxPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>
<span class="sd">    If the input image is a 3D tensor nInputPlane x height x width,</span>
<span class="sd">    the output image size will be nOutputPlane x oheight x owidth where</span>
<span class="sd">    owidth  = op((width  + 2*padW - kW) / dW + 1)</span>
<span class="sd">    oheight = op((height + 2*padH - kH) / dH + 1)</span>
<span class="sd">    op is a rounding operator. By default, it is floor.</span>
<span class="sd">    It can be changed by calling :ceil() or :floor() methods.</span>

<span class="sd">    :param kW:              kernel width</span>
<span class="sd">    :param kH:              kernel height</span>
<span class="sd">    :param dW:              step size in width</span>
<span class="sd">    :param dH:              step size in height</span>
<span class="sd">    :param padW:            padding in width</span>
<span class="sd">    :param padH:            padding in height</span>

<span class="sd">    &gt;&gt;&gt; spatialMaxPooling = SpatialMaxPooling(2, 2, 2, 2)</span>
<span class="sd">    creating: createSpatialMaxPooling</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># to_ceil: call floor() when False; call ceil() when True</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="p">,</span>
                 <span class="n">dh</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">to_ceil</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialMaxPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">kw</span><span class="p">,</span>
                                                <span class="n">kh</span><span class="p">,</span>
                                                <span class="n">dw</span><span class="p">,</span>
                                                <span class="n">dh</span><span class="p">,</span>
                                                <span class="n">pad_w</span><span class="p">,</span>
                                                <span class="n">pad_h</span><span class="p">,</span>
                                                <span class="n">to_ceil</span><span class="p">)</span></div>


<div class="viewcode-block" id="Select"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Select">[docs]</a><span class="k">class</span> <span class="nc">Select</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A Simple layer selecting an index of the input tensor in the given dimension</span>


<span class="sd">    :param dimension: the dimension to select</span>
<span class="sd">    :param index: the index of the dimension to be selected</span>


<span class="sd">    &gt;&gt;&gt; select = Select(1, 1)</span>
<span class="sd">    creating: createSelect</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Select</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>

<div class="viewcode-block" id="Recurrent"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Recurrent">[docs]</a><span class="k">class</span> <span class="nc">Recurrent</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Recurrent module is a container of rnn cells</span>
<span class="sd">    Different types of rnn cells can be added using add() function</span>


<span class="sd">    &gt;&gt;&gt; recurrent = Recurrent()</span>
<span class="sd">    creating: createRecurrent</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Recurrent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LSTM">[docs]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">|   Long Short Term Memory architecture.</span>
<span class="sd">|   Ref.</span>
<span class="sd">|   A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)</span>
<span class="sd">|   B. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</span>
<span class="sd">|   C. http://arxiv.org/pdf/1503.04069v1.pdf</span>
<span class="sd">|   D. https://github.com/wojzaremba/lstm</span>
<span class="sd">|   E. https://github.com/Element-Research/rnn/blob/master/FastLSTM.lua</span>


<span class="sd">    :param inputSize: the size of each input vector</span>
<span class="sd">    :param hiddenSize: Hidden unit size in the LSTM</span>
<span class="sd">    :param  p: is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287.pdf)</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; lstm = LSTM(4, 3, 0.5, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLSTM</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="LSTMPeephole"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LSTMPeephole">[docs]</a><span class="k">class</span> <span class="nc">LSTMPeephole</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">|   Long Short Term Memory architecture with peephole.</span>
<span class="sd">|   Ref. A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)</span>
<span class="sd">|   B. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</span>
<span class="sd">|   C. http://arxiv.org/pdf/1503.04069v1.pdf</span>
<span class="sd">|   D. https://github.com/wojzaremba/lstm</span>
<span class="sd">|   E. https://github.com/Element-Research/rnn/blob/master/LSTM.lua</span>


<span class="sd">    :param input_size: the size of each input vector</span>
<span class="sd">    :param hidden_size: Hidden unit size in the LSTM</span>
<span class="sd">    :param  p: is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287.pdf)</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; lstm = LSTMPeephole(4, 3, 0.5, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLSTMPeephole</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTMPeephole</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="GRU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GRU">[docs]</a><span class="k">class</span> <span class="nc">GRU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Gated Recurrent Units architecture.</span>
<span class="sd">    The first input in sequence uses zero value for cell and hidden state</span>


<span class="sd">|   Ref.</span>
<span class="sd">|   http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</span>
<span class="sd">|   https://github.com/Element-Research/rnn/blob/master/GRU.lua</span>


<span class="sd">    :param input_size: the size of each input vector</span>
<span class="sd">    :param hidden_size: Hidden unit size in GRU</span>
<span class="sd">    :param  p: is used for [[Dropout]] probability. For more details aboutRNN dropouts, please refer to[RnnDrop: A Novel Dropout for RNNs in ASR](http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/pdf/1512.05287.pdf)</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>



<span class="sd">    &gt;&gt;&gt; gru = GRU(4, 3, 0.5, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createGRU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>  <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GRU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="RnnCell"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RnnCell">[docs]</a><span class="k">class</span> <span class="nc">RnnCell</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple RNN. User can pass an activation function to the RNN.</span>


<span class="sd">    :param input_size: the size of each input vector</span>
<span class="sd">    :param hidden_size: Hidden unit size in simple RNN</span>
<span class="sd">    :param activation: activation function</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]](../regularizers.md),applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; reshape = RnnCell(4, 3, Tanh(), L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createTanh</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createRnnCell</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">activation</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RnnCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="TimeDistributed"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TimeDistributed">[docs]</a><span class="k">class</span> <span class="nc">TimeDistributed</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This layer is intended to apply contained layer to each temporal time slice</span>
<span class="sd">    of input tensor.</span>


<span class="sd">    For instance, The TimeDistributed Layer can feed each time slice of input tensor</span>
<span class="sd">    to the Linear layer.</span>
<span class="sd">    </span>
<span class="sd">    The input data format is [Batch, Time, Other dims]. For the contained layer, it must not change</span>
<span class="sd">    the Other dims length.</span>


<span class="sd">    &gt;&gt;&gt; td = TimeDistributed(Linear(2, 3))</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    creating: createTimeDistributed</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span></div>


<div class="viewcode-block" id="Concat"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Concat">[docs]</a><span class="k">class</span> <span class="nc">Concat</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Concat concatenates the output of one layer of &quot;parallel&quot;</span>
<span class="sd">    modules along the provided {@code dimension}: they take the</span>
<span class="sd">    same inputs, and their output is concatenated.</span>
<span class="sd">```</span>
<span class="sd">                    +-----------+</span>
<span class="sd">               +----&gt;  module1  -----+</span>
<span class="sd">               |    |           |    |</span>
<span class="sd">    input -----+----&gt;  module2  -----+----&gt; output</span>
<span class="sd">               |    |           |    |</span>
<span class="sd">               +----&gt;  module3  -----+</span>
<span class="sd">                    +-----------+</span>
<span class="sd">```</span>

<span class="sd">    :param dimension: dimension</span>


<span class="sd">    &gt;&gt;&gt; concat = Concat(2)</span>
<span class="sd">    creating: createConcat</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Concat</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">dimension</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialAveragePooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialAveragePooling">[docs]</a><span class="k">class</span> <span class="nc">SpatialAveragePooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 2D average-pooling operation in kWxkH regions by step size dWxdH steps.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>


<span class="sd">    :param kW: kernel width</span>
<span class="sd">    :param kH: kernel height</span>
<span class="sd">    :param dW: step width</span>
<span class="sd">    :param dH: step height</span>
<span class="sd">    :param padW: padding width</span>
<span class="sd">    :param padH: padding height</span>
<span class="sd">    :param ceilMode: whether the output size is to be ceiled or floored</span>
<span class="sd">    :param countIncludePad: whether to include padding when dividing thenumber of elements in pooling region</span>
<span class="sd">    :param divide: whether to do the averaging</span>


<span class="sd">    &gt;&gt;&gt; spatialAveragePooling = SpatialAveragePooling(7,7)</span>
<span class="sd">    creating: createSpatialAveragePooling</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">divide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialAveragePooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">kw</span><span class="p">,</span>
                                                    <span class="n">kh</span><span class="p">,</span>
                                                    <span class="n">dw</span><span class="p">,</span>
                                                    <span class="n">dh</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">,</span>
                                                    <span class="n">ceil_mode</span><span class="p">,</span>
                                                    <span class="n">count_include_pad</span><span class="p">,</span>
                                                    <span class="n">divide</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialBatchNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialBatchNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialBatchNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This file implements Batch Normalization as described in the paper:</span>
<span class="sd">    &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&quot;</span>
<span class="sd">    by Sergey Ioffe, Christian Szegedy</span>
<span class="sd">    This implementation is useful for inputs coming from convolution layers.</span>
<span class="sd">    For non-convolutional layers, see [[BatchNormalization]]</span>
<span class="sd">    The operation implemented is:</span>

<span class="sd">```</span>
<span class="sd">          ( x - mean(x) )</span>
<span class="sd">    y = -------------------- * gamma + beta</span>
<span class="sd">       standard-deviation(x)</span>
<span class="sd">```</span>

<span class="sd">    where gamma and beta are learnable parameters.</span>
<span class="sd">    The learning of gamma and beta is optional.</span>


<span class="sd">    &gt;&gt;&gt; spatialBatchNormalization = SpatialBatchNormalization(1)</span>
<span class="sd">    creating: createSpatialBatchNormalization</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.array([1.0])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.array([0.0])</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.array([0.0])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.array([0.0])</span>
<span class="sd">    &gt;&gt;&gt; spatialBatchNormalization = SpatialBatchNormalization(1, 1e-5, 0.1, True, init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createSpatialBatchNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialBatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                        <span class="n">n_output</span><span class="p">,</span>
                                                        <span class="n">eps</span><span class="p">,</span>
                                                        <span class="n">momentum</span><span class="p">,</span>
                                                        <span class="n">affine</span><span class="p">,</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                        <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>

<div class="viewcode-block" id="SpatialBatchNormalization.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialBatchNormalization.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpatialCrossMapLRN"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialCrossMapLRN">[docs]</a><span class="k">class</span> <span class="nc">SpatialCrossMapLRN</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies Spatial Local Response Normalization between different feature maps.</span>
<span class="sd">    The operation implemented is:</span>
<span class="sd">```</span>
<span class="sd">                                 x_f</span>
<span class="sd">    y_f =  -------------------------------------------------</span>
<span class="sd">            (k+(alpha/size)* sum_{l=l1 to l2} (x_l^2^))^beta^</span>
<span class="sd">```</span>

<span class="sd">    where x_f is the input at spatial locations h,w (not shown for simplicity) and feature map f,</span>
<span class="sd">    l1 corresponds to max(0,f-ceil(size/2)) and l2 to min(F, f-ceil(size/2) + size).</span>
<span class="sd">    Here, F is the number of feature maps.</span>

<span class="sd">    :param size:  the number of channels to sum over (for cross channel LRN) or the side length ofthe square region to sum over (for within channel LRN)</span>
<span class="sd">    :param alpha:  the scaling parameter</span>
<span class="sd">    :param beta:   the exponent</span>
<span class="sd">    :param k: a constant</span>


<span class="sd">    &gt;&gt;&gt; spatialCrossMapLRN = SpatialCrossMapLRN()</span>
<span class="sd">    creating: createSpatialCrossMapLRN</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
                 <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialCrossMapLRN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">size</span><span class="p">,</span>
                                                 <span class="n">alpha</span><span class="p">,</span>
                                                 <span class="n">beta</span><span class="p">,</span>
                                                 <span class="n">k</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Dropout">[docs]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Dropout masks(set to zero) parts of input using a bernoulli distribution.</span>
<span class="sd">    Each input element has a probability initP of being dropped. If scale is</span>
<span class="sd">    set, the outputs are scaled by a factor of 1/(1-initP) during training.</span>
<span class="sd">    During evaluating, output is the same as input.</span>


<span class="sd">    :param initP: probability to be dropped</span>
<span class="sd">    :param inplace: inplace model</span>
<span class="sd">    :param scale: if scale by a factor of 1/(1-initP)</span>


<span class="sd">    &gt;&gt;&gt; dropout = Dropout(0.4)</span>
<span class="sd">    creating: createDropout</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">init_p</span><span class="p">,</span>
                                      <span class="n">inplace</span><span class="p">,</span>
                                      <span class="n">scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="View"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.View">[docs]</a><span class="k">class</span> <span class="nc">View</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This module creates a new view of the input tensor using the sizes passed to the constructor.</span>
<span class="sd">    The method setNumInputDims() allows to specify the expected number of dimensions of the</span>
<span class="sd">    inputs of the modules. This makes it possible to use minibatch inputs when using a size -1</span>
<span class="sd">    for one of the dimensions.</span>


<span class="sd">    :param size: sizes use for creates a new view</span>


<span class="sd">    &gt;&gt;&gt; view = View([1024,2])</span>
<span class="sd">    creating: createView</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">sizes</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">View</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">sizes</span><span class="p">,</span>
                                   <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Abs"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Abs">[docs]</a><span class="k">class</span> <span class="nc">Abs</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    an element-wise abs operation</span>


<span class="sd">    &gt;&gt;&gt; abs = Abs()</span>
<span class="sd">    creating: createAbs</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Abs</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Add"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Add">[docs]</a><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    adds a bias term to input data ;</span>

<span class="sd">    :param input_size: size of input data</span>

<span class="sd">    &gt;&gt;&gt; add = Add(1)</span>
<span class="sd">    creating: createAdd</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">input_size</span><span class="p">)</span>
<div class="viewcode-block" id="Add.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Add.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="AddConstant"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.AddConstant">[docs]</a><span class="k">class</span> <span class="nc">AddConstant</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    adding a constant</span>


<span class="sd">    :param constant_scalar: constant value</span>
<span class="sd">    :param inplace: Can optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; addConstant = AddConstant(1e-5, True)</span>
<span class="sd">    creating: createAddConstant</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">constant_scalar</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AddConstant</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">constant_scalar</span><span class="p">,</span>
                                          <span class="n">inplace</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BatchNormalization">[docs]</a><span class="k">class</span> <span class="nc">BatchNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This layer implements Batch Normalization as described in the paper:</span>
<span class="sd">             &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">             Covariate Shift&quot;</span>
<span class="sd">    by Sergey Ioffe, Christian Szegedy https://arxiv.org/abs/1502.03167</span>


<span class="sd">    This implementation is useful for inputs NOT coming from convolution layers. For convolution</span>
<span class="sd">    layers, use nn.SpatialBatchNormalization.</span>


<span class="sd">    The operation implemented is:</span>
<span class="sd">```</span>
<span class="sd">                ( x - mean(x) )</span>
<span class="sd">         y = -------------------- * gamma + beta</span>
<span class="sd">             standard-deviation(x)</span>
<span class="sd">```</span>
<span class="sd">    where gamma and beta are learnable parameters.The learning of gamma and beta is optional.</span>


<span class="sd">    :param n_output: output feature map number</span>
<span class="sd">    :param eps: avoid divide zero</span>
<span class="sd">    :param momentum: momentum for weight update</span>
<span class="sd">    :param affine: affine operation on output or not</span>


<span class="sd">    &gt;&gt;&gt; batchNormalization = BatchNormalization(1, 1e-5, 1e-5, True)</span>
<span class="sd">    creating: createBatchNormalization</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; init_weight = np.random.randn(2)</span>
<span class="sd">    &gt;&gt;&gt; init_grad_weight = np.zeros([2])</span>
<span class="sd">    &gt;&gt;&gt; init_bias = np.zeros([2])</span>
<span class="sd">    &gt;&gt;&gt; init_grad_bias = np.zeros([2])</span>
<span class="sd">    &gt;&gt;&gt; batchNormalization = BatchNormalization(2, 1e-5, 1e-5, True, init_weight, init_bias, init_grad_weight, init_grad_bias)</span>
<span class="sd">    creating: createBatchNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">init_grad_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">n_output</span><span class="p">,</span>
                                                 <span class="n">eps</span><span class="p">,</span>
                                                 <span class="n">momentum</span><span class="p">,</span>
                                                 <span class="n">affine</span><span class="p">,</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_bias</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_weight</span><span class="p">),</span>
                                                 <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">init_grad_bias</span><span class="p">))</span>
<div class="viewcode-block" id="BatchNormalization.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BatchNormalization.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Bilinear"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Bilinear">[docs]</a><span class="k">class</span> <span class="nc">Bilinear</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    a bilinear transformation with sparse inputs,</span>
<span class="sd">    The input tensor given in forward(input) is a table containing both inputs x_1 and x_2,</span>
<span class="sd">    which are tensors of size N x inputDimension1 and N x inputDimension2, respectively.</span>

<span class="sd">    :param input_size1 input dimension of x_1</span>
<span class="sd">    :param input_size2 input dimension of x_2</span>
<span class="sd">    :param output_size output dimension</span>
<span class="sd">    :param bias_res whether use bias</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; bilinear = Bilinear(1, 1, 1, True, L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createBilinear</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size1</span><span class="p">,</span>
                 <span class="n">input_size2</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">bias_res</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bilinear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">input_size1</span><span class="p">,</span>
                                       <span class="n">input_size2</span><span class="p">,</span>
                                       <span class="n">output_size</span><span class="p">,</span>
                                       <span class="n">bias_res</span><span class="p">,</span>
                                       <span class="n">wRegularizer</span><span class="p">,</span>
                                       <span class="n">bRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="Bilinear.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Bilinear.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Bottle"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Bottle">[docs]</a><span class="k">class</span> <span class="nc">Bottle</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Bottle allows varying dimensionality input to be forwarded through any module</span>
<span class="sd">    that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.</span>

<span class="sd">    :param module: transform module</span>
<span class="sd">    :param n_input_dim: nInputDim dimensions of module</span>
<span class="sd">    :param n_output_dim1: output of nOutputDim dimensions</span>


<span class="sd">    &gt;&gt;&gt; bottle = Bottle(Linear(100,10), 1, 1)</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    creating: createBottle</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">module</span><span class="p">,</span>
                 <span class="n">n_input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">n_output_dim1</span><span class="o">=</span><span class="n">INTMAX</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">module</span><span class="p">,</span>
                                     <span class="n">n_input_dim</span><span class="p">,</span>
                                     <span class="n">n_output_dim1</span><span class="p">)</span></div>


<div class="viewcode-block" id="CAdd"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAdd">[docs]</a><span class="k">class</span> <span class="nc">CAdd</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This layer has a bias tensor with given size. The bias will be added element wise to the input</span>
<span class="sd">    tensor. If the element number of the bias tensor match the input tensor, a simply element wise</span>
<span class="sd">    will be done. Or the bias will be expanded to the same size of the input. The expand means</span>
<span class="sd">    repeat on unmatched singleton dimension(if some unmatched dimension isn&#39;t singleton dimension,</span>
<span class="sd">    it will report an error). If the input is a batch, a singleton dimension will be add to the</span>
<span class="sd">    first dimension before the expand.</span>


<span class="sd">    :param size: the size of the bias</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; cAdd = CAdd([1,2])</span>
<span class="sd">    creating: createCAdd</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CAdd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">size</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">)</span>

<div class="viewcode-block" id="CAdd.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAdd.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="CAddTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CAddTable">[docs]</a><span class="k">class</span> <span class="nc">CAddTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Merge the input tensors in the input table by element wise adding them together. The input</span>
<span class="sd">    table is actually an array of tensor with same size.</span>


<span class="sd">    :param inplace: reuse the input memory</span>


<span class="sd">    &gt;&gt;&gt; cAddTable = CAddTable(True)</span>
<span class="sd">    creating: createCAddTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CAddTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="CDivTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CDivTable">[docs]</a><span class="k">class</span> <span class="nc">CDivTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table with two Tensor and returns the component-wise division between them.</span>


<span class="sd">    &gt;&gt;&gt; cDivTable = CDivTable()</span>
<span class="sd">    creating: createCDivTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CDivTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CMaxTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMaxTable">[docs]</a><span class="k">class</span> <span class="nc">CMaxTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table of Tensors and outputs the max of all of them.</span>


<span class="sd">    &gt;&gt;&gt; cMaxTable = CMaxTable()</span>
<span class="sd">    creating: createCMaxTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMaxTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CMinTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMinTable">[docs]</a><span class="k">class</span> <span class="nc">CMinTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table of Tensors and outputs the min of all of them.</span>

<span class="sd">    &gt;&gt;&gt; cMinTable = CMinTable()</span>
<span class="sd">    creating: createCMinTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMinTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CMul"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMul">[docs]</a><span class="k">class</span> <span class="nc">CMul</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a component-wise multiplication to the incoming data</span>


<span class="sd">    :param size: size of the data</span>


<span class="sd">    &gt;&gt;&gt; cMul = CMul([1,2])</span>
<span class="sd">    creating: createCMul</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMul</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">size</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="p">)</span>

<div class="viewcode-block" id="CMul.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMul.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="CMulTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CMulTable">[docs]</a><span class="k">class</span> <span class="nc">CMulTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table of Tensors and outputs the multiplication of all of them.</span>


<span class="sd">    &gt;&gt;&gt; cMulTable = CMulTable()</span>
<span class="sd">    creating: createCMulTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CMulTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="CSubTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CSubTable">[docs]</a><span class="k">class</span> <span class="nc">CSubTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a table with two Tensor and returns the component-wise subtraction between them.</span>


<span class="sd">    &gt;&gt;&gt; cSubTable = CSubTable()</span>
<span class="sd">    creating: createCSubTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CSubTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Clamp"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Clamp">[docs]</a><span class="k">class</span> <span class="nc">Clamp</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Clamps all elements into the range [min_value, max_value].</span>
<span class="sd">    Output is identical to input in the range,</span>
<span class="sd">    otherwise elements less than min_value (or greater than max_value)</span>
<span class="sd">    are saturated to min_value (or max_value).</span>


<span class="sd">    :param min:</span>
<span class="sd">    :param max:</span>


<span class="sd">    &gt;&gt;&gt; clamp = Clamp(1, 3)</span>
<span class="sd">    creating: createClamp</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="nb">min</span><span class="p">,</span>
                 <span class="nb">max</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Clamp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="nb">min</span><span class="p">,</span>
                                    <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="Contiguous"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Contiguous">[docs]</a><span class="k">class</span> <span class="nc">Contiguous</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    used to make input, grad_output both contiguous</span>


<span class="sd">    &gt;&gt;&gt; contiguous = Contiguous()</span>
<span class="sd">    creating: createContiguous</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Contiguous</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Cosine"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Cosine">[docs]</a><span class="k">class</span> <span class="nc">Cosine</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Cosine calculates the cosine similarity of the input to k mean centers. The input given in</span>
<span class="sd">    forward(input) must be either a vector (1D tensor) or matrix (2D tensor). If the input is a</span>
<span class="sd">    vector, it must have the size of inputSize. If it is a matrix, then each row is assumed to be</span>
<span class="sd">    an input sample of given batch (the number of rows means the batch size and the number of</span>
<span class="sd">    columns should be equal to the inputSize).</span>


<span class="sd">    :param input_size: the size of each input sample</span>
<span class="sd">    :param output_size: the size of the module output of each sample</span>


<span class="sd">    &gt;&gt;&gt; cosine = Cosine(2,3)</span>
<span class="sd">    creating: createCosine</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Cosine</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">input_size</span><span class="p">,</span>
                                     <span class="n">output_size</span><span class="p">)</span>
<div class="viewcode-block" id="Cosine.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Cosine.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="CosineDistance"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.CosineDistance">[docs]</a><span class="k">class</span> <span class="nc">CosineDistance</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Outputs the cosine distance between inputs</span>


<span class="sd">    &gt;&gt;&gt; cosineDistance = CosineDistance()</span>
<span class="sd">    creating: createCosineDistance</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineDistance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="DotProduct"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.DotProduct">[docs]</a><span class="k">class</span> <span class="nc">DotProduct</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This is a simple table layer which takes a table of two tensors as input</span>
<span class="sd">    and calculate the dot product between them as outputs</span>


<span class="sd">    &gt;&gt;&gt; dotProduct = DotProduct()</span>
<span class="sd">    creating: createDotProduct</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DotProduct</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="ELU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ELU">[docs]</a><span class="k">class</span> <span class="nc">ELU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    D-A Clevert, Thomas Unterthiner, Sepp Hochreiter</span>
<span class="sd">    Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</span>
<span class="sd">    [http://arxiv.org/pdf/1511.07289.pdf]</span>


<span class="sd">    &gt;&gt;&gt; eLU = ELU(1e-5, True)</span>
<span class="sd">    creating: createELU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ELU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">alpha</span><span class="p">,</span>
                                  <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Euclidean"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Euclidean">[docs]</a><span class="k">class</span> <span class="nc">Euclidean</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Outputs the Euclidean distance of the input to outputSize centers</span>

<span class="sd">    :param inputSize: inputSize</span>
<span class="sd">    :param outputSize: outputSize</span>
<span class="sd">    :param T: Numeric type. Only support float/double now</span>


<span class="sd">    &gt;&gt;&gt; euclidean = Euclidean(1, 1, True)</span>
<span class="sd">    creating: createEuclidean</span>
<span class="sd">    &#39;&#39;&#39;</span>



    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">,</span>
                 <span class="n">fast_backward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Euclidean</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">input_size</span><span class="p">,</span>
                                        <span class="n">output_size</span><span class="p">,</span>
                                        <span class="n">fast_backward</span><span class="p">)</span>

<div class="viewcode-block" id="Euclidean.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Euclidean.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Exp"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Exp">[docs]</a><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies element-wise exp to input tensor.</span>

<span class="sd">    &gt;&gt;&gt; exp = Exp()</span>
<span class="sd">    creating: createExp</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Exp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="FlattenTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.FlattenTable">[docs]</a><span class="k">class</span> <span class="nc">FlattenTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This is a table layer which takes an arbitrarily deep table of Tensors</span>
<span class="sd">    (potentially nested) as input and a table of Tensors without any nested</span>
<span class="sd">    table will be produced</span>


<span class="sd">    &gt;&gt;&gt; flattenTable = FlattenTable()</span>
<span class="sd">    creating: createFlattenTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FlattenTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="GradientReversal"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.GradientReversal">[docs]</a><span class="k">class</span> <span class="nc">GradientReversal</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple module preserves the input, but takes the</span>
<span class="sd">    gradient from the subsequent layer, multiplies it by -lambda</span>
<span class="sd">    and passes it to the preceding layer. This can be used to maximise</span>
<span class="sd">    an objective function whilst using gradient descent, as described in</span>
<span class="sd">     [&quot;Domain-Adversarial Training of Neural Networks&quot;</span>
<span class="sd">     (http://arxiv.org/abs/1505.07818)]</span>


<span class="sd">    :param lambda: hyper-parameter lambda can be set dynamically during training</span>


<span class="sd">    &gt;&gt;&gt; gradientReversal = GradientReversal(1e-5)</span>
<span class="sd">    creating: createGradientReversal</span>
<span class="sd">    &gt;&gt;&gt; gradientReversal = GradientReversal()</span>
<span class="sd">    creating: createGradientReversal</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">the_lambda</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradientReversal</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">the_lambda</span><span class="p">)</span></div>


<div class="viewcode-block" id="HardShrink"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.HardShrink">[docs]</a><span class="k">class</span> <span class="nc">HardShrink</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This is a transfer layer which applies the hard shrinkage function</span>
<span class="sd">    element-wise to the input Tensor. The parameter lambda is set to 0.5</span>
<span class="sd">    by default</span>
<span class="sd">```</span>
<span class="sd">            x, if x &gt;  lambda</span>
<span class="sd">    f(x) =  x, if x &lt; -lambda</span>
<span class="sd">            0, otherwise</span>
<span class="sd">```</span>

<span class="sd">   :param the_lambda: a threshold value whose default value is 0.5</span>


<span class="sd">    &gt;&gt;&gt; hardShrink = HardShrink(1e-5)</span>
<span class="sd">    creating: createHardShrink</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">the_lambda</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HardShrink</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">the_lambda</span><span class="p">)</span></div>


<div class="viewcode-block" id="HardTanh"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.HardTanh">[docs]</a><span class="k">class</span> <span class="nc">HardTanh</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies HardTanh to each element of input, HardTanh is defined:</span>
<span class="sd">```</span>
<span class="sd">             |  maxValue, if x &gt; maxValue</span>
<span class="sd">      f(x) = |  minValue, if x &lt; minValue</span>
<span class="sd">             |  x, otherwise</span>
<span class="sd">```</span>
<span class="sd">    :param min_value: minValue in f(x), default is -1.</span>
<span class="sd">    :param max_value: maxValue in f(x), default is 1.</span>
<span class="sd">    :param inplace: whether enable inplace model.</span>


<span class="sd">    &gt;&gt;&gt; hardTanh = HardTanh(1e-5, 1e5, True)</span>
<span class="sd">    creating: createHardTanh</span>
<span class="sd">    &gt;&gt;&gt; hardTanh = HardTanh()</span>
<span class="sd">    creating: createHardTanh</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">min_value</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">max_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HardTanh</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">min_value</span><span class="p">,</span>
                                       <span class="n">max_value</span><span class="p">,</span>
                                       <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Index"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Index">[docs]</a><span class="k">class</span> <span class="nc">Index</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the Tensor index operation along the given dimension.</span>


<span class="sd">    :param dimension: the dimension to be indexed</span>


<span class="sd">    &gt;&gt;&gt; index = Index(1)</span>
<span class="sd">    creating: createIndex</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Index</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">dimension</span><span class="p">)</span></div>


<div class="viewcode-block" id="InferReshape"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.InferReshape">[docs]</a><span class="k">class</span> <span class="nc">InferReshape</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Reshape the input tensor with automatic size inference support.</span>
<span class="sd">    Positive numbers in the `size` argument are used to reshape the input to the</span>
<span class="sd">    corresponding dimension size.</span>
<span class="sd">    There are also two special values allowed in `size`:</span>
<span class="sd">       a. `0` means keep the corresponding dimension size of the input unchanged.</span>
<span class="sd">          i.e., if the 1st dimension size of the input is 2,</span>
<span class="sd">          the 1st dimension size of output will be set as 2 as well.</span>
<span class="sd">       b. `-1` means infer this dimension size from other dimensions.</span>
<span class="sd">          This dimension size is calculated by keeping the amount of output elements</span>
<span class="sd">          consistent with the input.</span>
<span class="sd">          Only one `-1` is allowable in `size`.</span>

<span class="sd">    For example,</span>
<span class="sd">       Input tensor with size: (4, 5, 6, 7)</span>
<span class="sd">       -&gt; InferReshape(Array(4, 0, 3, -1))</span>
<span class="sd">       Output tensor with size: (4, 5, 3, 14)</span>
<span class="sd">    The 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,</span>
<span class="sd">    and inferred the last dim as 14.</span>

<span class="sd">     :param size:      the target tensor size</span>
<span class="sd">     :param batch_mode: whether in batch mode</span>


<span class="sd">    &gt;&gt;&gt; inferReshape = InferReshape([4, 0, 3, -1], False)</span>
<span class="sd">    creating: createInferReshape</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span>
                 <span class="n">batch_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">InferReshape</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                           <span class="n">size</span><span class="p">,</span>
                                           <span class="n">batch_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="JoinTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.JoinTable">[docs]</a><span class="k">class</span> <span class="nc">JoinTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a table module which takes a table of Tensors as input and</span>
<span class="sd">    outputs a Tensor by joining them together along the dimension `dimension`.</span>


<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user need to specify the number of dimensions of each sample tensor in the</span>
<span class="sd">    batch using `nInputDims`.</span>


<span class="sd">    :param dimension: to be join in this dimension</span>
<span class="sd">    :param nInputDims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>


<span class="sd">    &gt;&gt;&gt; joinTable = JoinTable(1, 1)</span>
<span class="sd">    creating: createJoinTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">JoinTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">dimension</span><span class="p">,</span>
                                        <span class="n">n_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="L1Penalty"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.L1Penalty">[docs]</a><span class="k">class</span> <span class="nc">L1Penalty</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    adds an L1 penalty to an input (for sparsity).</span>
<span class="sd">    L1Penalty is an inline module that in its forward propagation copies the input Tensor</span>
<span class="sd">    directly to the output, and computes an L1 loss of the latent state (input) and stores</span>
<span class="sd">    it in the module&#39;s loss field. During backward propagation: gradInput = gradOutput + gradLoss.</span>


<span class="sd">    :param l1weight:</span>
<span class="sd">    :param sizeAverage:</span>
<span class="sd">    :param provideOutput:</span>


<span class="sd">    &gt;&gt;&gt; l1Penalty = L1Penalty(1, True, True)</span>
<span class="sd">    creating: createL1Penalty</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">l1weight</span><span class="p">,</span>
                 <span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">provide_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Penalty</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">l1weight</span><span class="p">,</span>
                                        <span class="n">size_average</span><span class="p">,</span>
                                        <span class="n">provide_output</span><span class="p">)</span></div>


<div class="viewcode-block" id="LeakyReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LeakyReLU">[docs]</a><span class="k">class</span> <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a transfer module that applies LeakyReLU, which parameter negval sets the slope of the</span>
<span class="sd">    negative part: LeakyReLU is defined as: f(x) = max(0, x) + negval * min(0, x)</span>


<span class="sd">    :param negval: sets the slope of the negative partl</span>
<span class="sd">    :param inplace: if it is true, doing the operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; leakyReLU = LeakyReLU(1e-5, True)</span>
<span class="sd">    creating: createLeakyReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">negval</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">negval</span><span class="p">,</span>
                                        <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Log">[docs]</a><span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the log function element-wise to the input Tensor,</span>
<span class="sd">     thus outputting a Tensor of the same dimension.</span>


<span class="sd">    &gt;&gt;&gt; log = Log()</span>
<span class="sd">    creating: createLog</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Log</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSigmoid"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LogSigmoid">[docs]</a><span class="k">class</span> <span class="nc">LogSigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is a transform layer corresponding to the sigmoid function:</span>
<span class="sd">    f(x) = Log(1 / (1 + e ^^ (-x)))</span>


<span class="sd">    &gt;&gt;&gt; logSigmoid = LogSigmoid()</span>
<span class="sd">    creating: createLogSigmoid</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogSigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="LookupTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LookupTable">[docs]</a><span class="k">class</span> <span class="nc">LookupTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    a convolution of width 1, commonly used for word embeddings</span>

<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>

<span class="sd">    &gt;&gt;&gt; lookupTable = LookupTable(1, 1, 1e-5, 1e-5, 1e-5, True, L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createLookupTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_index</span><span class="p">,</span>
                 <span class="n">n_output</span><span class="p">,</span>
                 <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">max_norm</span><span class="o">=</span><span class="n">DOUBLEMAX</span><span class="p">,</span>
                 <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">should_scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LookupTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">n_index</span><span class="p">,</span>
                                          <span class="n">n_output</span><span class="p">,</span>
                                          <span class="n">padding_value</span><span class="p">,</span>
                                          <span class="n">max_norm</span><span class="p">,</span>
                                          <span class="n">norm_type</span><span class="p">,</span>
                                          <span class="n">should_scale_grad_by_freq</span><span class="p">,</span>
                                          <span class="n">wRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="LookupTable.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.LookupTable.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="MM"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MM">[docs]</a><span class="k">class</span> <span class="nc">MM</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Module to perform matrix multiplication on two mini-batch inputs, producing a mini-batch.</span>


<span class="sd">    :param trans_a: specifying whether or not transpose the first input matrix</span>
<span class="sd">    :param trans_b: specifying whether or not transpose the second input matrix</span>


<span class="sd">    &gt;&gt;&gt; mM = MM(True, True)</span>
<span class="sd">    creating: createMM</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">trans_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">trans_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                 <span class="n">trans_a</span><span class="p">,</span>
                                 <span class="n">trans_b</span><span class="p">)</span></div>


<div class="viewcode-block" id="MV"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MV">[docs]</a><span class="k">class</span> <span class="nc">MV</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a module to perform matrix vector multiplication on two mini-batch inputs,</span>
<span class="sd">    producing a mini-batch.</span>


<span class="sd">    :param trans: whether make matrix transpose before multiplication</span>


<span class="sd">    &gt;&gt;&gt; mV = MV(True)</span>
<span class="sd">    creating: createMV</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">trans</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                 <span class="n">trans</span><span class="p">)</span></div>


<div class="viewcode-block" id="MapTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MapTable">[docs]</a><span class="k">class</span> <span class="nc">MapTable</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is a container for a single module which will be applied</span>
<span class="sd">    to all input elements. The member module is cloned as necessary to</span>
<span class="sd">    process all input elements.</span>


<span class="sd">    &gt;&gt;&gt; mapTable = MapTable(Linear(100,10))</span>
<span class="sd">    creating: createLinear</span>
<span class="sd">    creating: createMapTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">module</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MapTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="MaskedSelect"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MaskedSelect">[docs]</a><span class="k">class</span> <span class="nc">MaskedSelect</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Performs a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument with</span>
<span class="sd">    the input on the forward and backward passes.</span>

<span class="sd">    &gt;&gt;&gt; maskedSelect = MaskedSelect()</span>
<span class="sd">    creating: createMaskedSelect</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaskedSelect</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Max"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Max">[docs]</a><span class="k">class</span> <span class="nc">Max</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a max operation over dimension `dim`</span>


<span class="sd">   :param dim: max along this dimension</span>
<span class="sd">   :param num_input_dims: Optional. If in a batch model, set to the inputDims.</span>


<span class="sd">    &gt;&gt;&gt; max = Max(1)</span>
<span class="sd">    creating: createMax</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Max</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">dim</span><span class="p">,</span>
                                  <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Mean"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Mean">[docs]</a><span class="k">class</span> <span class="nc">Mean</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple layer which applies a mean operation over the given dimension. When nInputDims</span>
<span class="sd">    is provided, the input will be considered as batches. Then the mean operation will be applied</span>
<span class="sd">    in (dimension + 1). The input to this layer is expected to be a tensor, or a batch of</span>
<span class="sd">    tensors; when using mini-batch, a batch of sample tensors will be passed to the layer and the</span>
<span class="sd">    user need to specify the number of dimensions of each sample tensor in the batch using</span>
<span class="sd">    nInputDims.</span>


<span class="sd">    :param dimension: the dimension to be applied mean operation</span>
<span class="sd">    :param n_input_dims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimension would be consideredas batch size</span>
<span class="sd">    :param squeeze: default is true, which will squeeze the sum dimension; set it to false to keep the sum dimension </span>

<span class="sd">    &gt;&gt;&gt; mean = Mean(1, 1, True)</span>
<span class="sd">    creating: createMean</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">squeeze</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mean</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                   <span class="n">dimension</span><span class="p">,</span>
                                   <span class="n">n_input_dims</span><span class="p">,</span>
                                   <span class="n">squeeze</span><span class="p">)</span></div>


<div class="viewcode-block" id="Min"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Min">[docs]</a><span class="k">class</span> <span class="nc">Min</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a min operation over dimension `dim`.</span>


<span class="sd">    :param dim: min along this dimension</span>
<span class="sd">    :param num_input_dims: Optional. If in a batch model, set to the input_dim.</span>


<span class="sd">    &gt;&gt;&gt; min = Min(1)</span>
<span class="sd">    creating: createMin</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Min</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">dim</span><span class="p">,</span>
                                  <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="MixtureTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MixtureTable">[docs]</a><span class="k">class</span> <span class="nc">MixtureTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a table {gater, experts} as input and outputs the mixture of experts</span>
<span class="sd">    (a Tensor or table of Tensors) using a gater Tensor. When dim is provided, it specifies the</span>
<span class="sd">    dimension of the experts Tensor that will be interpolated (or mixed). Otherwise, the experts</span>
<span class="sd">    should take the form of a table of Tensors. This Module works for experts of dimension 1D or</span>
<span class="sd">    more, and for a 1D or 2D gater, i.e. for single examples or mini-batches.</span>


<span class="sd">    &gt;&gt;&gt; mixtureTable = MixtureTable()</span>
<span class="sd">    creating: createMixtureTable</span>
<span class="sd">    &gt;&gt;&gt; mixtureTable = MixtureTable(10)</span>
<span class="sd">    creating: createMixtureTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="n">INTMAX</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MixtureTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="Mul"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Mul">[docs]</a><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Multiply a single scalar factor to the incoming data</span>


<span class="sd">    &gt;&gt;&gt; mul = Mul()</span>
<span class="sd">    creating: createMul</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mul</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span>

<div class="viewcode-block" id="Mul.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Mul.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="MulConstant"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.MulConstant">[docs]</a><span class="k">class</span> <span class="nc">MulConstant</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Multiplies input Tensor by a (non-learnable) scalar constant.</span>
<span class="sd">    This module is sometimes useful for debugging purposes.</span>


<span class="sd">    :param scalar: scalar constant</span>
<span class="sd">    :param inplace: Can optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; mulConstant = MulConstant(2.5)</span>
<span class="sd">    creating: createMulConstant</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">scalar</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MulConstant</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">scalar</span><span class="p">,</span>
                                          <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Narrow"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Narrow">[docs]</a><span class="k">class</span> <span class="nc">Narrow</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Narrow is application of narrow operation in a module.</span>
<span class="sd">    The module further supports a negative length in order to handle inputs with an unknown size.</span>

<span class="sd">    &gt;&gt;&gt; narrow = Narrow(1, 1, 1)</span>
<span class="sd">    creating: createNarrow</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">offset</span><span class="p">,</span>
                 <span class="n">length</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Narrow</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                     <span class="n">dimension</span><span class="p">,</span>
                                     <span class="n">offset</span><span class="p">,</span>
                                     <span class="n">length</span><span class="p">)</span></div>


<div class="viewcode-block" id="NarrowTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.NarrowTable">[docs]</a><span class="k">class</span> <span class="nc">NarrowTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a table as input and outputs the subtable starting at index</span>
<span class="sd">    offset having length elements (defaults to 1 element). The elements can be either</span>
<span class="sd">    a table or a Tensor. If `length` is negative, it means selecting the elements from the</span>
<span class="sd">    offset to element which located at the abs(`length`) to the last element of the input.</span>


<span class="sd">    :param offset: the start index of table</span>
<span class="sd">    :param length: the length want to select</span>


<span class="sd">    &gt;&gt;&gt; narrowTable = NarrowTable(1, 1)</span>
<span class="sd">    creating: createNarrowTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">offset</span><span class="p">,</span>
                 <span class="n">length</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NarrowTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">offset</span><span class="p">,</span>
                                          <span class="n">length</span><span class="p">)</span></div>


<div class="viewcode-block" id="Normalize"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Normalize">[docs]</a><span class="k">class</span> <span class="nc">Normalize</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Normalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents</span>
<span class="sd">    division by zero when the input contains all zero elements (default = 1e-10).</span>
<span class="sd">    p can be the max value of double</span>


<span class="sd">    &gt;&gt;&gt; normalize = Normalize(1e-5, 1e-5)</span>
<span class="sd">    creating: createNormalize</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">p</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Normalize</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">p</span><span class="p">,</span>
                                        <span class="n">eps</span><span class="p">)</span></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PReLU">[docs]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies parametric ReLU, which parameter varies the slope of the negative part.</span>


<span class="sd">    PReLU: f(x) = max(0, x) + a * min(0, x)</span>


<span class="sd">    nOutputPlane&#39;s default value is 0, that means using PReLU in shared version and has</span>
<span class="sd">    only one parameters.</span>


<span class="sd">    Notice: Please don&#39;t use weight decay on this.</span>


<span class="sd">    :param n_output_plane: input map number. Default is 0.</span>


<span class="sd">    &gt;&gt;&gt; pReLU = PReLU(1)</span>
<span class="sd">    creating: createPReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">n_output_plane</span><span class="p">)</span>

<div class="viewcode-block" id="PReLU.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PReLU.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Padding"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Padding">[docs]</a><span class="k">class</span> <span class="nc">Padding</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This module adds pad units of padding to dimension dim of the input. If pad is negative,</span>
<span class="sd">    padding is added to the left, otherwise, it is added to the right of the dimension.</span>


<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user need to specify the number of dimensions of each sample tensor in the</span>
<span class="sd">    batch using n_input_dim.</span>


<span class="sd">    :param dim: the dimension to be applied padding operation</span>
<span class="sd">    :param pad: num of the pad units</span>
<span class="sd">    :param n_input_dim: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>
<span class="sd">    :param value: padding value</span>


<span class="sd">    &gt;&gt;&gt; padding = Padding(1, 1, 1, 1e-5, 1)</span>
<span class="sd">    creating: createPadding</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">pad</span><span class="p">,</span>
                 <span class="n">n_input_dim</span><span class="p">,</span>
                 <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">n_index</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Padding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">dim</span><span class="p">,</span>
                                      <span class="n">pad</span><span class="p">,</span>
                                      <span class="n">n_input_dim</span><span class="p">,</span>
                                      <span class="n">value</span><span class="p">,</span>
                                      <span class="n">n_index</span><span class="p">)</span></div>


<div class="viewcode-block" id="PairwiseDistance"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.PairwiseDistance">[docs]</a><span class="k">class</span> <span class="nc">PairwiseDistance</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a module that takes a table of two vectors as input and outputs</span>
<span class="sd">    the distance between them using the p-norm.</span>
<span class="sd">    The input given in `forward(input)` is a [[Table]] that contains two tensors which</span>
<span class="sd">    must be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,</span>
<span class="sd">    it must have the size of `inputSize`. If it is a matrix, then each row is assumed to be</span>
<span class="sd">    an input sample of the given batch (the number of rows means the batch size and</span>
<span class="sd">    the number of columns should be equal to the `inputSize`).</span>

<span class="sd">    :param norm: the norm of distance</span>


<span class="sd">    &gt;&gt;&gt; pairwiseDistance = PairwiseDistance(2)</span>
<span class="sd">    creating: createPairwiseDistance</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">norm</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PairwiseDistance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                               <span class="n">norm</span><span class="p">)</span></div>


<div class="viewcode-block" id="ParallelTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ParallelTable">[docs]</a><span class="k">class</span> <span class="nc">ParallelTable</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a container module that applies the i-th member module to the i-th</span>
<span class="sd">    input, and outputs an output in the form of Table</span>


<span class="sd">    &gt;&gt;&gt; parallelTable = ParallelTable()</span>
<span class="sd">    creating: createParallelTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Power"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Power">[docs]</a><span class="k">class</span> <span class="nc">Power</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply an element-wise power operation with scale and shift.</span>
<span class="sd">    f(x) = (shift + scale * x)^power^</span>

<span class="sd">    :param power: the exponent.</span>
<span class="sd">    :param scale: Default is 1.</span>
<span class="sd">    :param shift: Default is 0.</span>


<span class="sd">    &gt;&gt;&gt; power = Power(1e-5)</span>
<span class="sd">    creating: createPower</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">power</span><span class="p">,</span>
                 <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">shift</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Power</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">power</span><span class="p">,</span>
                                    <span class="n">scale</span><span class="p">,</span>
                                    <span class="n">shift</span><span class="p">)</span></div>


<div class="viewcode-block" id="RReLU"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RReLU">[docs]</a><span class="k">class</span> <span class="nc">RReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,</span>
<span class="sd">    thus outputting a Tensor of the same dimension. Informally the RReLU is also known as</span>
<span class="sd">    &#39;insanity&#39; layer. RReLU is defined as:</span>
<span class="sd">```</span>
<span class="sd">        f(x) = max(0,x) + a * min(0, x) where a ~ U(l, u).</span>
<span class="sd">```</span>

<span class="sd">    In training mode negative inputs are multiplied by a factor drawn from a uniform random</span>
<span class="sd">    distribution U(l, u).</span>


<span class="sd">    In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean factor</span>
<span class="sd">        a = (l + u) / 2.</span>


<span class="sd">    By default, l = 1/8 and u = 1/3. If l == u a RReLU effectively becomes a LeakyReLU.</span>


<span class="sd">    Regardless of operating in in-place mode a RReLU will internally allocate an input-sized</span>
<span class="sd">    noise tensor to store random factors for negative inputs.</span>


<span class="sd">    The backward() operation assumes that forward() has been called before.</span>


<span class="sd">    For reference see [Empirical Evaluation of Rectified Activations in Convolutional Network](</span>
<span class="sd">    http://arxiv.org/abs/1505.00853).</span>


<span class="sd">    :param lower: lower boundary of uniform random distribution</span>
<span class="sd">    :param upper: upper boundary of uniform random distribution</span>
<span class="sd">    :param inplace: optionally do its operation in-place without using extra state memory</span>


<span class="sd">    &gt;&gt;&gt; rReLU = RReLU(1e-5, 1e5, True)</span>
<span class="sd">    creating: createRReLU</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">lower</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">upper</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">lower</span><span class="p">,</span>
                                    <span class="n">upper</span><span class="p">,</span>
                                    <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ReLU6">[docs]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Same as ReLU except that the rectifying function f(x) saturates at x = 6</span>


<span class="sd">    :param inplace: either True = in-place or False = keeping separate state</span>


<span class="sd">    &gt;&gt;&gt; reLU6 = ReLU6(True)</span>
<span class="sd">    creating: createReLU6</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReLU6</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Replicate"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Replicate">[docs]</a><span class="k">class</span> <span class="nc">Replicate</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Replicate repeats input `nFeatures` times along its `dim` dimension.</span>
<span class="sd">    Notice: No memory copy, it set the stride along the `dim`-th dimension to zero.</span>


<span class="sd">    :param n_features: replicate times.</span>
<span class="sd">    :param dim: dimension to be replicated.</span>
<span class="sd">    :param n_dim: specify the number of non-batch dimensions.</span>


<span class="sd">    &gt;&gt;&gt; replicate = Replicate(2)</span>
<span class="sd">    creating: createReplicate</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_features</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_dim</span><span class="o">=</span><span class="n">INTMAX</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Replicate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">n_features</span><span class="p">,</span>
                                        <span class="n">dim</span><span class="p">,</span>
                                        <span class="n">n_dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="RoiPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.RoiPooling">[docs]</a><span class="k">class</span> <span class="nc">RoiPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Region of interest pooling</span>
<span class="sd">    The RoIPooling uses max pooling to convert the features inside any valid region of interest</span>
<span class="sd">    into a small feature map with a fixed spatial extent of pooledH * pooledW (e.g., 7 * 7)</span>
<span class="sd">    an RoI is a rectangular window into a conv feature map.</span>
<span class="sd">    Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its</span>
<span class="sd">    top-left corner (x1, y1) and its bottom-right corner (x2, y2).</span>
<span class="sd">    RoI max pooling works by dividing the h * w RoI window into an pooledH * pooledW grid of</span>
<span class="sd">    sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window</span>
<span class="sd">    into the corresponding output grid cell.</span>
<span class="sd">    Pooling is applied independently to each feature map channel</span>


<span class="sd">    :param pooled_w:      spatial extent in width</span>
<span class="sd">    :param pooled_h:      spatial extent in height</span>
<span class="sd">    :param spatial_scale: spatial scale</span>


<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; input_data = np.random.rand(2,2,6,8)</span>
<span class="sd">    &gt;&gt;&gt; input_rois = np.array([0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3],dtype=&#39;float64&#39;).reshape(4,5)</span>
<span class="sd">    &gt;&gt;&gt; m = RoiPooling(3,2,1.0)</span>
<span class="sd">    creating: createRoiPooling</span>
<span class="sd">    &gt;&gt;&gt; out = m.forward([input_data,input_rois])</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pooled_w</span><span class="p">,</span>
                 <span class="n">pooled_h</span><span class="p">,</span>
                 <span class="n">spatial_scale</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RoiPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">pooled_w</span><span class="p">,</span>
                                         <span class="n">pooled_h</span><span class="p">,</span>
                                         <span class="n">spatial_scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="Scale"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Scale">[docs]</a><span class="k">class</span> <span class="nc">Scale</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Scale is the combination of CMul and CAdd</span>
<span class="sd">    Computes the elementwise product of input and weight, with the shape of the weight &quot;expand&quot; to</span>
<span class="sd">    match the shape of the input.</span>
<span class="sd">    Similarly, perform a expand cdd bias and perform an elementwise add</span>


<span class="sd">    :param size: size of weight and bias</span>


<span class="sd">    &gt;&gt;&gt; scale = Scale([1,2])</span>
<span class="sd">    creating: createScale</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">size</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Scale</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                    <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="SelectTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SelectTable">[docs]</a><span class="k">class</span> <span class="nc">SelectTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a table as input and outputs the element at index `index`</span>
<span class="sd">    (positive or negative). This can be either a table or a Tensor.</span>
<span class="sd">    The gradients of the non-index elements are zeroed Tensors of the same size.</span>
<span class="sd">    This is true regardless of the depth of the encapsulated Tensor as the function used</span>
<span class="sd">    internally to do so is recursive.</span>


<span class="sd">    :param index: the index to be selected</span>


<span class="sd">    &gt;&gt;&gt; selectTable = SelectTable(1)</span>
<span class="sd">    creating: createSelectTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">index</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelectTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                          <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the Sigmoid function element-wise to the input Tensor,</span>
<span class="sd">    thus outputting a Tensor of the same dimension.</span>

<span class="sd">    &gt;&gt;&gt; sigmoid = Sigmoid()</span>
<span class="sd">    creating: createSigmoid</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftMax"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftMax">[docs]</a><span class="k">class</span> <span class="nc">SoftMax</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the</span>
<span class="sd">    elements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.</span>
<span class="sd">    Softmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)</span>
<span class="sd">    where shift = max_i(x_i).</span>


<span class="sd">    &gt;&gt;&gt; softMax = SoftMax()</span>
<span class="sd">    creating: createSoftMax</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftMin"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftMin">[docs]</a><span class="k">class</span> <span class="nc">SoftMin</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the</span>
<span class="sd">    elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.</span>
<span class="sd">    Softmin is defined as: f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)</span>
<span class="sd">    where shift = max_i(-x_i).</span>


<span class="sd">    &gt;&gt;&gt; softMin = SoftMin()</span>
<span class="sd">    creating: createSoftMin</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMin</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftPlus"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftPlus">[docs]</a><span class="k">class</span> <span class="nc">SoftPlus</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply the SoftPlus function to an n-dimensional input tensor.</span>
<span class="sd">    SoftPlus function: f_i(x) = 1/beta * log(1 + exp(beta * x_i))</span>


<span class="sd">    :param beta: Controls sharpness of transfer function</span>


<span class="sd">    &gt;&gt;&gt; softPlus = SoftPlus(1e-5)</span>
<span class="sd">    creating: createSoftPlus</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftPlus</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                       <span class="n">beta</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftShrink"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftShrink">[docs]</a><span class="k">class</span> <span class="nc">SoftShrink</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply the soft shrinkage function element-wise to the input Tensor</span>


<span class="sd">    SoftShrinkage operator:</span>
<span class="sd">```</span>
<span class="sd">           | x - lambda, if x &gt;  lambda</span>
<span class="sd">    f(x) = | x + lambda, if x &lt; -lambda</span>
<span class="sd">           | 0, otherwise</span>
<span class="sd">```</span>

<span class="sd">    :param the_lambda: lambda, default is 0.5</span>


<span class="sd">    &gt;&gt;&gt; softShrink = SoftShrink(1e-5)</span>
<span class="sd">    creating: createSoftShrink</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">the_lambda</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftShrink</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">the_lambda</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftSign"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SoftSign">[docs]</a><span class="k">class</span> <span class="nc">SoftSign</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply SoftSign function to an n-dimensional input Tensor.</span>


<span class="sd">    SoftSign function: f_i(x) = x_i / (1+|x_i|)</span>


<span class="sd">    &gt;&gt;&gt; softSign = SoftSign()</span>
<span class="sd">    creating: createSoftSign</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftSign</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialDilatedConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDilatedConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialDilatedConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply a 2D dilated convolution over an input image.</span>


<span class="sd">    The input tensor is expected to be a 3D or 4D(with batch) tensor.</span>


<span class="sd">    If input is a 3D tensor nInputPlane x height x width,</span>
<span class="sd">    owidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1</span>
<span class="sd">    oheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1</span>


<span class="sd">    Reference Paper: Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].</span>
<span class="sd">    arXiv preprint arXiv:1511.07122, 2015.</span>


<span class="sd">    :param n_input_plane: The number of expected input planes in the image given into forward().</span>
<span class="sd">    :param n_output_plane: The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kw: The kernel width of the convolution.</span>
<span class="sd">    :param kh: The kernel height of the convolution.</span>
<span class="sd">    :param dw: The step of the convolution in the width dimension. Default is 1.</span>
<span class="sd">    :param dh: The step of the convolution in the height dimension. Default is 1.</span>
<span class="sd">    :param pad_w: The additional zeros added per width to the input planes. Default is 0.</span>
<span class="sd">    :param pad_h: The additional zeros added per height to the input planes. Default is 0.</span>
<span class="sd">    :param dilation_w: The number of pixels to skip. Default is 1.</span>
<span class="sd">    :param dilation_h: The number of pixels to skip. Default is 1.</span>
<span class="sd">    :param init_method: Init method, Default, Xavier.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; spatialDilatedConvolution = SpatialDilatedConvolution(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialDilatedConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDilatedConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                        <span class="n">n_input_plane</span><span class="p">,</span>
                                                        <span class="n">n_output_plane</span><span class="p">,</span>
                                                        <span class="n">kw</span><span class="p">,</span>
                                                        <span class="n">kh</span><span class="p">,</span>
                                                        <span class="n">dw</span><span class="p">,</span>
                                                        <span class="n">dh</span><span class="p">,</span>
                                                        <span class="n">pad_w</span><span class="p">,</span>
                                                        <span class="n">pad_h</span><span class="p">,</span>
                                                        <span class="n">dilation_w</span><span class="p">,</span>
                                                        <span class="n">dilation_h</span><span class="p">,</span>
                                                        <span class="n">wRegularizer</span><span class="p">,</span>
                                                        <span class="n">bRegularizer</span><span class="p">)</span>
                                                        
<div class="viewcode-block" id="SpatialDilatedConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDilatedConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpatialFullConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialFullConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialFullConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply a 2D full convolution over an input image.</span>
<span class="sd">    The input tensor is expected to be a 3D or 4D(with batch) tensor. Note that instead</span>
<span class="sd">    of setting adjW and adjH, SpatialFullConvolution[Table, T] also accepts a table input</span>
<span class="sd">    with two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,</span>
<span class="sd">    and the size of sizeTensor is used to set the size of the output (will ignore the adjW and</span>
<span class="sd">    adjH values used to construct the module). This module can be used without a bias by setting</span>
<span class="sd">    parameter noBias = true while constructing the module.</span>


<span class="sd">    If input is a 3D tensor nInputPlane x height x width,</span>
<span class="sd">    owidth  = (width  - 1) * dW - 2*padW + kW + adjW</span>
<span class="sd">    oheight = (height - 1) * dH - 2*padH + kH + adjH</span>


<span class="sd">    Other frameworks call this operation &quot;In-network Upsampling&quot;, &quot;Fractionally-strided convolution&quot;,</span>
<span class="sd">    &quot;Backwards Convolution,&quot; &quot;Deconvolution&quot;, or &quot;Upconvolution.&quot;</span>


<span class="sd">    Reference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic</span>
<span class="sd">    segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</span>
<span class="sd">    2015: 3431-3440.</span>

<span class="sd">    :param nInputPlane The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param nOutputPlane The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param kW The kernel width of the convolution.</span>
<span class="sd">    :param kH The kernel height of the convolution.</span>
<span class="sd">    :param dW The step of the convolution in the width dimension. Default is 1.</span>
<span class="sd">    :param dH The step of the convolution in the height dimension. Default is 1.</span>
<span class="sd">    :param padW The additional zeros added per width to the input planes. Default is 0.</span>
<span class="sd">    :param padH The additional zeros added per height to the input planes. Default is 0.</span>
<span class="sd">    :param adjW Extra width to add to the output image. Default is 0.</span>
<span class="sd">    :param adjH Extra height to add to the output image. Default is 0.</span>
<span class="sd">    :param nGroup Kernel group number.</span>
<span class="sd">    :param noBias If bias is needed.</span>
<span class="sd">    :param initMethod Init method, Default, Xavier, Bilinear.</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>


<span class="sd">    &gt;&gt;&gt; spatialFullConvolution = SpatialFullConvolution(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialFullConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">adj_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialFullConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                     <span class="n">n_input_plane</span><span class="p">,</span>
                                                     <span class="n">n_output_plane</span><span class="p">,</span>
                                                     <span class="n">kw</span><span class="p">,</span>
                                                     <span class="n">kh</span><span class="p">,</span>
                                                     <span class="n">dw</span><span class="p">,</span>
                                                     <span class="n">dh</span><span class="p">,</span>
                                                     <span class="n">pad_w</span><span class="p">,</span>
                                                     <span class="n">pad_h</span><span class="p">,</span>
                                                     <span class="n">adj_w</span><span class="p">,</span>
                                                     <span class="n">adj_h</span><span class="p">,</span>
                                                     <span class="n">n_group</span><span class="p">,</span>
                                                     <span class="n">no_bias</span><span class="p">,</span>
                                                     <span class="n">wRegularizer</span><span class="p">,</span>
                                                     <span class="n">bRegularizer</span><span class="p">)</span>
<div class="viewcode-block" id="SpatialFullConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialFullConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpatialShareConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialShareConvolution">[docs]</a><span class="k">class</span> <span class="nc">SpatialShareConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>

<span class="sd">    &gt;&gt;&gt; spatialShareConvolution = SpatialShareConvolution(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialShareConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">kernel_w</span><span class="p">,</span>
                 <span class="n">kernel_h</span><span class="p">,</span>
                 <span class="n">stride_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">propagate_back</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialShareConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                      <span class="n">n_input_plane</span><span class="p">,</span>
                                                      <span class="n">n_output_plane</span><span class="p">,</span>
                                                      <span class="n">kernel_w</span><span class="p">,</span>
                                                      <span class="n">kernel_h</span><span class="p">,</span>
                                                      <span class="n">stride_w</span><span class="p">,</span>
                                                      <span class="n">stride_h</span><span class="p">,</span>
                                                      <span class="n">pad_w</span><span class="p">,</span>
                                                      <span class="n">pad_h</span><span class="p">,</span>
                                                      <span class="n">n_group</span><span class="p">,</span>
                                                      <span class="n">propagate_back</span><span class="p">)</span>
<div class="viewcode-block" id="SpatialShareConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialShareConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="VolumetricConvolution"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricConvolution">[docs]</a><span class="k">class</span> <span class="nc">VolumetricConvolution</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a 3D convolution over an input image composed of several input planes. The input tensor</span>
<span class="sd">    in forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).</span>

<span class="sd">    :param n_input_plane: The number of expected input planes in the image given into forward()</span>
<span class="sd">    :param n_output_plane: The number of output planes the convolution layer will produce.</span>
<span class="sd">    :param k_t: The kernel size of the convolution in time</span>
<span class="sd">    :param k_w: The kernel width of the convolution</span>
<span class="sd">    :param k_h: The kernel height of the convolution</span>
<span class="sd">    :param d_t: The step of the convolution in the time dimension. Default is 1</span>
<span class="sd">    :param d_w: The step of the convolution in the width dimension. Default is 1</span>
<span class="sd">    :param d_h: The step of the convolution in the height dimension. Default is 1</span>
<span class="sd">    :param pad_t: Additional zeros added to the input plane data on both sides of time axis.Default is 0. (kT-1)/2 is often used here.</span>
<span class="sd">    :param pad_w: The additional zeros added per width to the input planes.</span>
<span class="sd">    :param pad_h: The additional zeros added per height to the input planes.</span>
<span class="sd">    :param with_bias: whether with bias</span>
<span class="sd">    :param init_method: Init method, Default, Xavier, Bilinear.</span>


<span class="sd">    &gt;&gt;&gt; volumetricConvolution = VolumetricConvolution(6, 12, 5, 5, 5, 1, 1, 1)</span>
<span class="sd">    creating: createVolumetricConvolution</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="p">,</span>
                 <span class="n">n_output_plane</span><span class="p">,</span>
                 <span class="n">k_t</span><span class="p">,</span>
                 <span class="n">k_w</span><span class="p">,</span>
                 <span class="n">k_h</span><span class="p">,</span>
                 <span class="n">d_t</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">d_w</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">d_h</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VolumetricConvolution</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">n_input_plane</span><span class="p">,</span>
                                                    <span class="n">n_output_plane</span><span class="p">,</span>
                                                    <span class="n">k_t</span><span class="p">,</span>
                                                    <span class="n">k_w</span><span class="p">,</span>
                                                    <span class="n">k_h</span><span class="p">,</span>
                                                    <span class="n">d_t</span><span class="p">,</span>
                                                    <span class="n">d_w</span><span class="p">,</span>
                                                    <span class="n">d_h</span><span class="p">,</span>
                                                    <span class="n">pad_t</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">,</span>
                                                    <span class="n">with_bias</span><span class="p">)</span>

<div class="viewcode-block" id="VolumetricConvolution.set_init_method"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricConvolution.set_init_method">[docs]</a>    <span class="k">def</span> <span class="nf">set_init_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias_init_method</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setInitMethod&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">weight_init_method</span><span class="p">,</span> <span class="n">bias_init_method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="VolumetricMaxPooling"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.VolumetricMaxPooling">[docs]</a><span class="k">class</span> <span class="nc">VolumetricMaxPooling</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies 3D max-pooling operation in kTxkWxkH regions by step size dTxdWxdH.</span>
<span class="sd">    The number of output features is equal to the number of input planes / dT.</span>
<span class="sd">    The input can optionally be padded with zeros. Padding should be smaller than</span>
<span class="sd">    half of kernel size. That is, padT &lt; kT/2, padW &lt; kW/2 and padH &lt; kH/2</span>

<span class="sd">    :param k_t: The kernel size</span>
<span class="sd">    :param k_w: The kernel width</span>
<span class="sd">    :param k_h: The kernel height</span>
<span class="sd">    :param d_t: The step in the time dimension</span>
<span class="sd">    :param d_w: The step in the width dimension</span>
<span class="sd">    :param d_h: The step in the height dimension</span>
<span class="sd">    :param pad_t: The padding in the time dimension</span>
<span class="sd">    :param pad_w: The padding in the width dimension</span>
<span class="sd">    :param pad_h: The padding in the height dimension</span>


<span class="sd">    &gt;&gt;&gt; volumetricMaxPooling = VolumetricMaxPooling(5, 5, 5, 1, 1, 1)</span>
<span class="sd">    creating: createVolumetricMaxPooling</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">k_t</span><span class="p">,</span>
                 <span class="n">k_w</span><span class="p">,</span>
                 <span class="n">k_h</span><span class="p">,</span>
                 <span class="n">d_t</span><span class="p">,</span>
                 <span class="n">d_w</span><span class="p">,</span>
                 <span class="n">d_h</span><span class="p">,</span>
                 <span class="n">pad_t</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VolumetricMaxPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">k_t</span><span class="p">,</span>
                                                    <span class="n">k_w</span><span class="p">,</span>
                                                    <span class="n">k_h</span><span class="p">,</span>
                                                    <span class="n">d_t</span><span class="p">,</span>
                                                    <span class="n">d_w</span><span class="p">,</span>
                                                    <span class="n">d_h</span><span class="p">,</span>
                                                    <span class="n">pad_t</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialZeroPadding"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialZeroPadding">[docs]</a><span class="k">class</span> <span class="nc">SpatialZeroPadding</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Each feature map of a given input is padded with specified number of zeros.</span>
<span class="sd">    If padding values are negative, then input is cropped.</span>

<span class="sd">    :param padLeft: pad left position</span>
<span class="sd">    :param padRight: pad right position</span>
<span class="sd">    :param padTop: pad top position</span>
<span class="sd">    :param padBottom: pad bottom position</span>


<span class="sd">    &gt;&gt;&gt; spatialZeroPadding = SpatialZeroPadding(1, 1, 1, 1)</span>
<span class="sd">    creating: createSpatialZeroPadding</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pad_left</span><span class="p">,</span>
                 <span class="n">pad_right</span><span class="p">,</span>
                 <span class="n">pad_top</span><span class="p">,</span>
                 <span class="n">pad_bottom</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialZeroPadding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                 <span class="n">pad_left</span><span class="p">,</span>
                                                 <span class="n">pad_right</span><span class="p">,</span>
                                                 <span class="n">pad_top</span><span class="p">,</span>
                                                 <span class="n">pad_bottom</span><span class="p">)</span></div>


<div class="viewcode-block" id="SplitTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SplitTable">[docs]</a><span class="k">class</span> <span class="nc">SplitTable</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Creates a module that takes a Tensor as input and</span>
<span class="sd">    outputs several tables, splitting the Tensor along</span>
<span class="sd">    the specified dimension `dimension`. Please note the dimension starts from 1.</span>


<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user needs to specify the number of dimensions of each sample tensor in a</span>
<span class="sd">    batch using `nInputDims`.</span>


<span class="sd">    :param dimension: to be split along this dimension</span>
<span class="sd">    :param n_input_dims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>


<span class="sd">    &gt;&gt;&gt; splitTable = SplitTable(1, 1)</span>
<span class="sd">    creating: createSplitTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SplitTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                         <span class="n">dimension</span><span class="p">,</span>
                                         <span class="n">n_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sqrt"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sqrt">[docs]</a><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply an element-wise sqrt operation.</span>


<span class="sd">    &gt;&gt;&gt; sqrt = Sqrt()</span>
<span class="sd">    creating: createSqrt</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sqrt</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Square"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Square">[docs]</a><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply an element-wise square operation.</span>

<span class="sd">    &gt;&gt;&gt; square = Square()</span>
<span class="sd">    creating: createSquare</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Square</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Squeeze"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Squeeze">[docs]</a><span class="k">class</span> <span class="nc">Squeeze</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Delete singleton all dimensions or a specific dim.</span>


<span class="sd">    :param dim: Optional. The dimension to be delete. Default: delete all dimensions.</span>
<span class="sd">    :param num_input_dims: Optional. If in a batch model, set to the inputDims.</span>




<span class="sd">    &gt;&gt;&gt; squeeze = Squeeze(1)</span>
<span class="sd">    creating: createSqueeze</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Squeeze</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">dim</span><span class="p">,</span>
                                      <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sum"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Sum">[docs]</a><span class="k">class</span> <span class="nc">Sum</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    It is a simple layer which applies a sum operation over the given dimension.</span>
<span class="sd">    When nInputDims is provided, the input will be considered as a batches.</span>
<span class="sd">    Then the sum operation will be applied in (dimension + 1)</span>
<span class="sd">    The input to this layer is expected to be a tensor, or a batch of tensors;</span>
<span class="sd">    when using mini-batch, a batch of sample tensors will be passed to the layer and</span>
<span class="sd">    the user need to specify the number of dimensions of each sample tensor in the</span>
<span class="sd">    batch using `nInputDims`.</span>


<span class="sd">    :param dimension: the dimension to be applied sum operation</span>
<span class="sd">    :param n_input_dims: specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</span>
<span class="sd">    :param size_average: default is false, if it is true, it will return the mean instead</span>
<span class="sd">    :param squeeze: default is true, which will squeeze the sum dimension; set it to false to keep the sum dimension</span>


<span class="sd">    &gt;&gt;&gt; sum = Sum(1, 1, True, True)</span>
<span class="sd">    creating: createSum</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">n_input_dims</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">squeeze</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                  <span class="n">dimension</span><span class="p">,</span>
                                  <span class="n">n_input_dims</span><span class="p">,</span>
                                  <span class="n">squeeze</span><span class="p">,</span>
                                  <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="TanhShrink"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.TanhShrink">[docs]</a><span class="k">class</span> <span class="nc">TanhShrink</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A simple layer for each element of the input tensor, do the following operation</span>
<span class="sd">    during the forward process:</span>
<span class="sd">    [f(x) = tanh(x) - 1]</span>


<span class="sd">    &gt;&gt;&gt; tanhShrink = TanhShrink()</span>
<span class="sd">    creating: createTanhShrink</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TanhShrink</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Threshold"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Threshold">[docs]</a><span class="k">class</span> <span class="nc">Threshold</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Threshold input Tensor.</span>
<span class="sd">    If values in the Tensor smaller than th, then replace it with v</span>


<span class="sd">    :param th: the threshold to compare with</span>
<span class="sd">    :param v: the value to replace with</span>
<span class="sd">    :param ip: inplace mode</span>


<span class="sd">    &gt;&gt;&gt; threshold = Threshold(1e-5, 1e-5, True)</span>
<span class="sd">    creating: createThreshold</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">th</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                 <span class="n">v</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">ip</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Threshold</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">th</span><span class="p">,</span>
                                        <span class="n">v</span><span class="p">,</span>
                                        <span class="n">ip</span><span class="p">)</span></div>


<div class="viewcode-block" id="Unsqueeze"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Unsqueeze">[docs]</a><span class="k">class</span> <span class="nc">Unsqueeze</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Create an Unsqueeze layer.  Insert singleton dim (i.e., dimension 1) at position pos.</span>
<span class="sd">    For an input with dim = input.dim(),</span>
<span class="sd">    there are dim + 1 possible positions to insert the singleton dimension.</span>


<span class="sd">    :param pos: The position will be insert singleton.</span>
<span class="sd">    :param num_input_dims: Optional. If in a batch model, set to the inputDim</span>


<span class="sd">    &gt;&gt;&gt; unsqueeze = Unsqueeze(1, 1)</span>
<span class="sd">    creating: createUnsqueeze</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pos</span><span class="p">,</span>
                 <span class="n">num_input_dims</span><span class="o">=</span><span class="n">INTMIN</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Unsqueeze</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">pos</span><span class="p">,</span>
                                        <span class="n">num_input_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Reshape"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Reshape">[docs]</a><span class="k">class</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The forward(input) reshape the input tensor into a size(0) * size(1) * ... tensor, taking the</span>
<span class="sd">    elements row-wise.</span>


<span class="sd">    :param size: the reshape size</span>


<span class="sd">    &gt;&gt;&gt; reshape = Reshape([1, 28, 28])</span>
<span class="sd">    creating: createReshape</span>
<span class="sd">    &gt;&gt;&gt; reshape = Reshape([1, 28, 28], False)</span>
<span class="sd">    creating: createReshape</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">batch_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Reshape</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">batch_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="BiRecurrent"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.BiRecurrent">[docs]</a><span class="k">class</span> <span class="nc">BiRecurrent</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Create a Bidirectional recurrent layer</span>


<span class="sd">    :param merge: merge layer</span>


<span class="sd">    &gt;&gt;&gt; biRecurrent = BiRecurrent(CAddTable())</span>
<span class="sd">    creating: createCAddTable</span>
<span class="sd">    creating: createBiRecurrent</span>
<span class="sd">    &gt;&gt;&gt; biRecurrent = BiRecurrent()</span>
<span class="sd">    creating: createBiRecurrent</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">merge</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BiRecurrent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">merge</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConcatTable"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ConcatTable">[docs]</a><span class="k">class</span> <span class="nc">ConcatTable</span><span class="p">(</span><span class="n">Container</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ConcateTable is a container module like Concate. Applies an input</span>
<span class="sd">    to each member module, input can be a tensor or a table.</span>


<span class="sd">    ConcateTable usually works with CAddTable and CMulTable to</span>
<span class="sd">    implement element wise add/multiply on outputs of two modules.</span>


<span class="sd">    &gt;&gt;&gt; concatTable = ConcatTable()</span>
<span class="sd">    creating: createConcatTable</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConcatTable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Identity"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Identity">[docs]</a><span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Identity just return the input to output.</span>
<span class="sd">    It&#39;s useful in same parallel container to get an origin input.</span>


<span class="sd">    &gt;&gt;&gt; identity = Identity()</span>
<span class="sd">    creating: createIdentity</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Identity</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Reverse"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Reverse">[docs]</a><span class="k">class</span> <span class="nc">Reverse</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Reverse the input w.r.t given dimension.</span>
<span class="sd">    The input can be a Tensor or Table.</span>


<span class="sd">    :param dim:</span>


<span class="sd">    &gt;&gt;&gt; reverse = Reverse()</span>
<span class="sd">    creating: createReverse</span>
<span class="sd">    &gt;&gt;&gt; reverse = Reverse(1, False)</span>
<span class="sd">    creating: createReverse</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">is_inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Reverse</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                      <span class="n">dimension</span><span class="p">,</span>
                                      <span class="n">is_inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Transpose"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Transpose">[docs]</a><span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Transpose input along specified dimensions</span>


<span class="sd">    :param permutations: dimension pairs that need to swap</span>


<span class="sd">    &gt;&gt;&gt; transpose = Transpose([(1,2)])</span>
<span class="sd">    creating: createTranspose</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">permutations</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transpose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                        <span class="n">permutations</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialContrastiveNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialContrastiveNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialContrastiveNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Subtractive + divisive contrast normalization.</span>


<span class="sd">    :param n_input_plane:</span>
<span class="sd">    :param kernel:</span>
<span class="sd">    :param threshold:</span>
<span class="sd">    :param thresval:</span>


<span class="sd">    &gt;&gt;&gt; kernel = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialContrastiveNormalization = SpatialContrastiveNormalization(1, kernel)</span>
<span class="sd">    creating: createSpatialContrastiveNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialContrastiveNormalization = SpatialContrastiveNormalization()</span>
<span class="sd">    creating: createSpatialContrastiveNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">thresval</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialContrastiveNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                              <span class="n">n_input_plane</span><span class="p">,</span>
                                                              <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
                                                              <span class="n">threshold</span><span class="p">,</span>
                                                              <span class="n">thresval</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialConvolutionMap"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialConvolutionMap">[docs]</a><span class="k">class</span> <span class="nc">SpatialConvolutionMap</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This class is a generalization of SpatialConvolution.</span>
<span class="sd">    It uses a generic connection table between input and output features.</span>
<span class="sd">    The SpatialConvolution is equivalent to using a full connection table.</span>

<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices.</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>

<span class="sd">    &gt;&gt;&gt; ct = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialConvolutionMap = SpatialConvolutionMap(ct, 9, 9)</span>
<span class="sd">    creating: createSpatialConvolutionMap</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">conn_table</span><span class="p">,</span>
                 <span class="n">kw</span><span class="p">,</span>
                 <span class="n">kh</span><span class="p">,</span>
                 <span class="n">dw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_h</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialConvolutionMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                    <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">conn_table</span><span class="p">),</span>
                                                    <span class="n">kw</span><span class="p">,</span>
                                                    <span class="n">kh</span><span class="p">,</span>
                                                    <span class="n">dw</span><span class="p">,</span>
                                                    <span class="n">dh</span><span class="p">,</span>
                                                    <span class="n">pad_w</span><span class="p">,</span>
                                                    <span class="n">pad_h</span><span class="p">,</span>
                                                    <span class="n">wRegularizer</span><span class="p">,</span>
                                                    <span class="n">bRegularizer</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialDivisiveNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialDivisiveNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialDivisiveNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a spatial division operation on a series of 2D inputs using kernel for</span>
<span class="sd">    computing the weighted average in a neighborhood. The neighborhood is defined for</span>
<span class="sd">    a local spatial region that is the size as kernel and across all features. For</span>
<span class="sd">    an input image, since there is only one feature, the region is only spatial. For</span>
<span class="sd">    an RGB image, the weighted average is taken over RGB channels and a spatial region.</span>


<span class="sd">    If the kernel is 1D, then it will be used for constructing and separable 2D kernel.</span>
<span class="sd">    The operations will be much more efficient in this case.</span>


<span class="sd">    The kernel is generally chosen as a gaussian when it is believed that the correlation</span>
<span class="sd">    of two pixel locations decrease with increasing distance. On the feature dimension,</span>
<span class="sd">    a uniform average is used since the weighting across features is not known.</span>




<span class="sd">    :param nInputPlane: number of input plane, default is 1.</span>
<span class="sd">    :param kernel: kernel tensor, default is a 9 x 9 tensor.</span>
<span class="sd">    :param threshold: threshold</span>
<span class="sd">    :param thresval: threshhold value to replace withif data is smaller than theshold</span>


<span class="sd">    &gt;&gt;&gt; kernel = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialDivisiveNormalization = SpatialDivisiveNormalization(2,kernel)</span>
<span class="sd">    creating: createSpatialDivisiveNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialDivisiveNormalization = SpatialDivisiveNormalization()</span>
<span class="sd">    creating: createSpatialDivisiveNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">thresval</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialDivisiveNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                           <span class="n">n_input_plane</span><span class="p">,</span>
                                                           <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
                                                           <span class="n">threshold</span><span class="p">,</span>
                                                           <span class="n">thresval</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpatialSubtractiveNormalization"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.SpatialSubtractiveNormalization">[docs]</a><span class="k">class</span> <span class="nc">SpatialSubtractiveNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Applies a spatial subtraction operation on a series of 2D inputs using kernel for</span>
<span class="sd">    computing the weighted average in a neighborhood. The neighborhood is defined for</span>
<span class="sd">    a local spatial region that is the size as kernel and across all features. For a</span>
<span class="sd">    an input image, since there is only one feature, the region is only spatial. For</span>
<span class="sd">    an RGB image, the weighted average is taken over RGB channels and a spatial region.</span>


<span class="sd">    If the kernel is 1D, then it will be used for constructing and separable 2D kernel.</span>
<span class="sd">    The operations will be much more efficient in this case.</span>


<span class="sd">    The kernel is generally chosen as a gaussian when it is believed that the correlation</span>
<span class="sd">    of two pixel locations decrease with increasing distance. On the feature dimension,</span>
<span class="sd">    a uniform average is used since the weighting across features is not known.</span>


<span class="sd">    :param n_input_plane: number of input plane, default is 1.</span>
<span class="sd">    :param kernel: kernel tensor, default is a 9 x 9 tensor.</span>


<span class="sd">    &gt;&gt;&gt; kernel = np.ones([9,9]).astype(&quot;float32&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spatialSubtractiveNormalization = SpatialSubtractiveNormalization(2,kernel)</span>
<span class="sd">    creating: createSpatialSubtractiveNormalization</span>
<span class="sd">    &gt;&gt;&gt; spatialSubtractiveNormalization = SpatialSubtractiveNormalization()</span>
<span class="sd">    creating: createSpatialSubtractiveNormalization</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_input_plane</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialSubtractiveNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span>
                                                              <span class="n">n_input_plane</span><span class="p">,</span>
                                                              <span class="n">JTensor</span><span class="o">.</span><span class="n">from_ndarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">))</span></div>


<div class="viewcode-block" id="Pack"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.Pack">[docs]</a><span class="k">class</span> <span class="nc">Pack</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Stacks a list of n-dimensional tensors into one (n+1)-dimensional tensor.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; layer = Pack(1)</span>
<span class="sd">    creating: createPack</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimension</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Pack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span></div>

<div class="viewcode-block" id="ConvLSTMPeephole"><a class="viewcode-back" href="../../../bigdl.nn.html#bigdl.nn.layer.ConvLSTMPeephole">[docs]</a><span class="k">class</span> <span class="nc">ConvLSTMPeephole</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    </span>
<span class="sd">|   Convolution Long Short Term Memory architecture with peephole.</span>
<span class="sd">|   Ref. A.: https://arxiv.org/abs/1506.04214 (blueprint for this module)</span>
<span class="sd">|   B. https://github.com/viorik/ConvLSTM</span>

<span class="sd">    :param input_size: the size of each input</span>
<span class="sd">    :param hidden_size: Hidden unit size in the LSTM</span>
<span class="sd">    :param kernel_i Convolutional filter size to convolve input</span>
<span class="sd">    :param kernel_c Convolutional filter size to convolve cell</span>
<span class="sd">    :param stride The step of the convolution</span>
<span class="sd">    :param wRegularizer: instance of [[Regularizer]](eg. L1 or L2 regularization), applied to the input weights matrices</span>
<span class="sd">    :param uRegularizer: instance [[Regularizer]](eg. L1 or L2 regularization), applied to the recurrent weights matrices</span>
<span class="sd">    :param bRegularizer: instance of [[Regularizer]]applied to the bias.</span>
<span class="sd">    :param with_peephold: whether use last cell status control a gate.</span>

<span class="sd">    &gt;&gt;&gt; convlstm = ConvLSTMPeephole(4, 3, 3, 3, 1, L1Regularizer(0.5), L1Regularizer(0.5), L1Regularizer(0.5))</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createL1Regularizer</span>
<span class="sd">    creating: createConvLSTMPeephole</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">kernel_i</span><span class="p">,</span> <span class="n">kernel_c</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">wRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bRegularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_peephole</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvLSTMPeephole</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">kernel_i</span><span class="p">,</span> <span class="n">kernel_c</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                                               <span class="n">wRegularizer</span><span class="p">,</span> <span class="n">uRegularizer</span><span class="p">,</span> <span class="n">bRegularizer</span><span class="p">,</span> <span class="n">with_peephole</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">SparkContext</span>
    <span class="kn">from</span> <span class="nn">bigdl.nn</span> <span class="k">import</span> <span class="n">layer</span>
    <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">init_engine</span>
    <span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="k">import</span> <span class="n">create_spark_conf</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="s2">&quot;local[4]&quot;</span><span class="p">,</span> <span class="n">appName</span><span class="o">=</span><span class="s2">&quot;test layer&quot;</span><span class="p">,</span>
                      <span class="n">conf</span><span class="o">=</span><span class="n">create_spark_conf</span><span class="p">())</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span>
    <span class="n">init_engine</span><span class="p">()</span>

    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span><span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
                                                  <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">BigDL</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../bigdl.html">bigdl package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Intel.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>