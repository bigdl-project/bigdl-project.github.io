<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Math Layers - BigDL Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/highlight.css">
  <link href="../../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Math Layers";
    var mkdocs_page_input_path = "APIdocs/Layers/Math-Layers/merged-Math-Layers.md";
    var mkdocs_page_url = "/APIdocs/Layers/Math-Layers/merged-Math-Layers/";
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> BigDL Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-setup-bigdl/">Setup BigDL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prepare-data/">Prepare your Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prediction/">Use BigDL for Prediction Only</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-train/">Train a Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-monitor/">Monitor the Training</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-tune/">Tuning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-advanced/">Advanced Usage</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Install/Deploy</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/use-pre-built/">Use Pre-built Package</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/build-src/">Build from Source</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/deploy-python/">Enable Python Support</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/running-on-EC2/">Running On EC2</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/resources/">Resources</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-api/">API Usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/install-via-pip/">Install via pip</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-no-pip/">Use Python without Pip</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model/SequentialModel/">Sequential Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model/FunctionalAPI/">Functional API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../Containers/merged-Containers/">Containers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Simple_Layers/merged-Simple_Layers/">Simple Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Convolution_Layers/merged-Convolution_Layers/">Convolution Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Pooling_Layers/merged-Pooling_Layers/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Linear_Layers/merged-Linear_Layers/">Linear Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Non-linear_Activations/merged-Non-linear_Activations/">Non-linear Activations</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Embedding_Layers/merged-Embedding_Layers/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../MergeSplit_Layers/merged-MergeSplit_Layers/">Merge/Split Layers</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Math Layers</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#scale">Scale</a></li>
    

    <li class="toctree-l3"><a href="#min">Min</a></li>
    

    <li class="toctree-l3"><a href="#add">Add</a></li>
    

    <li class="toctree-l3"><a href="#bilinear">BiLinear</a></li>
    

    <li class="toctree-l3"><a href="#clamp">Clamp</a></li>
    

    <li class="toctree-l3"><a href="#square">Square</a></li>
    

    <li class="toctree-l3"><a href="#mean">Mean</a></li>
    

    <li class="toctree-l3"><a href="#power">Power</a></li>
    

    <li class="toctree-l3"><a href="#cmul">CMul</a></li>
    

    <li class="toctree-l3"><a href="#addconstant">AddConstant</a></li>
    

    <li class="toctree-l3"><a href="#abs">Abs</a></li>
    

    <li class="toctree-l3"><a href="#log">Log</a></li>
    

    <li class="toctree-l3"><a href="#sum">Sum</a></li>
    

    <li class="toctree-l3"><a href="#sqrt">Sqrt</a></li>
    

    <li class="toctree-l3"><a href="#exp">Exp</a></li>
    

    <li class="toctree-l3"><a href="#max">Max</a></li>
    

    <li class="toctree-l3"><a href="#cosine">Cosine</a></li>
    

    <li class="toctree-l3"><a href="#mul">Mul</a></li>
    

    <li class="toctree-l3"><a href="#mulconstant">MulConstant</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Padding_Layers/merged-Padding_Layers/">Padding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Normalization_Layers/merged-Normalization_Layers/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Dropout_Layers/merged-Dropout_Layers/">Dropout Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Distance_Layers/merged-Distance_Layers/">Distance Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Recurrent_Layers/merged-Recurrent_Layers/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Losses/merged-Losses/">Losses</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Optimizers/DistriOptimizer/">Optimizer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Optim_Methods/merged-Optim_Methods/">Optimization Algorithms</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Triggers/">Triggers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/ResumeTraining/">Resume Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Initializers/merged-Initializers/">Initalizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Regularizers/merged-Regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Metrics/merged-Metrics/">Metrics</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Preprocessing/Transformer/">Transformer</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Load/Save Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/BigDLModel/">Load/Save a BigDL Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/TensorflowModel/">Load a Tensorflow Model</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/visualization-with-tensorboard/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../python-api-doc/">Python API Docs</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">BigDL Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Math Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="scale">Scale</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = Scale(Array(2, 1))
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = scale = Scale([2, 1])
</code></pre>

<p>Scale is the combination of cmul and cadd. <code>Scale(size).forward(input) == CAdd(size).forward(CMul(size).forward(input))</code>
Computes the elementwise product of input and weight, with the shape of the weight "expand" to
match the shape of the input.Similarly, perform a expand cdd bias and perform an elementwise add.
<code>output = input .* weight .+ bias (element wise)</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.{T, Table}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val input = Tensor(2, 3).fill(1f)
println(&quot;input:&quot;)
println(input)
val scale = Scale(Array(2, 1))
val weight = Tensor(2, 1).fill(2f)
val bias = Tensor(2, 1).fill(3f)
scale.setWeightsBias(Array(weight, bias))
println(&quot;Weight:&quot;)
println(weight)
println(&quot;bias:&quot;)
println(bias)
println(&quot;output:&quot;)
print(scale.forward(input))
</code></pre>

<pre><code>input:
1.0 1.0 1.0 
1.0 1.0 1.0 
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
Weight:
2.0 
2.0 
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]
bias:
3.0 
3.0 
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]
output:
5.0 5.0 5.0 
5.0 5.0 5.0 
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.layer import *
input = np.ones([2, 3])
print(&quot;input:&quot;)
print(input)
scale = Scale([2, 1])
weight = np.full([2, 1], 2)
bias = np.full([2, 1], 3)
print(&quot;weight: &quot;)
print(weight)
print(&quot;bias: &quot;)
print(bias)
scale.set_weights([weight, bias])
print(&quot;output: &quot;)
print(scale.forward(input))

</code></pre>

<pre><code>input:
[[ 1.  1.  1.]
 [ 1.  1.  1.]]
creating: createScale
weight: 
[[2]
 [2]]
bias: 
[[3]
 [3]]
output: 
[[ 5.  5.  5.]
 [ 5.  5.  5.]]
</code></pre>

<h2 id="min">Min</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val min = Min(dim, numInputDims)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">min = Min(dim, num_input_dims)
</code></pre>

<p>Applies a min operation over dimension <code>dim</code>.</p>
<p><strong>Parameters:</strong>
<em> <strong>dim</strong> - A integer. The dimension to min along.
</em> <strong>numInputDims</strong> - An optional integer indicating the number of input dimensions.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val min = Min(2)
val input = Tensor(T(
 T(1.0f, 2.0f),
 T(3.0f, 4.0f))
)
val gradOutput = Tensor(T(
 1.0f,
 1.0f
))
val output = min.forward(input)
val gradient = min.backward(input, gradOutput)
-&gt; print(output)
1.0
3.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]

-&gt; print(gradient)
1.0     0.0     
1.0     0.0     
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
min = Min(2)
input = np.array([
  [1.0, 2.0],
  [3.0, 4.0]
])

grad_output = np.array([1.0, 1.0])
output = min.forward(input)
gradient = min.backward(input, grad_output)
-&gt; print output
[ 1.  3.]
-&gt; print gradient
[[ 1.  0.]
 [ 1.  0.]]
</code></pre>

<h2 id="add">Add</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val addLayer = Add[T](inputSize)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">add_layer = Add(input_size)
</code></pre>

<p>A.K.A BiasAdd. This layer adds input tensor with a parameter tensor and output the result.
If the input is 1D, this layer just do a element-wise add. If the input has multiple dimentions,
this layer will treat the first dimension as batch dimension, resize the input tensor to a 2D 
tensor(batch-dimension x input_size) and do a broadcast add between the 2D tensor and the 
parameter.</p>
<p>Please note that the parameter will be trained in the back propagation.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor

val addLayer = Add[Float](4)
addLayer.bias.set(Tensor[Float](T(1.0f, 2.0f, 3.0f, 4.0f)))
addLayer.forward(Tensor[Float](T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))))
addLayer.backward(Tensor[Float](T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))),
    Tensor[Float](T(T(0.1f, 0.1f, 0.1f, 0.1f), T(0.3f, 0.3f, 0.3f, 0.3f))))
</code></pre>

<p>Its output should be</p>
<pre><code>2.0     3.0     4.0     5.0
4.0     5.0     6.0     7.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]

0.1     0.1     0.1     0.1
0.3     0.3     0.3     0.3
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import Add
import numpy as np

add_layer = Add(4)
add_layer.set_weights([np.array([1.0, 2.0, 3.0, 4.0])])
add_layer.forward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]))
add_layer.backward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]),
    np.array([[0.1, 0.1, 0.1, 0.1], [0.3, 0.3, 0.3, 0.3]]))
</code></pre>

<p>Its output should be</p>
<pre><code>array([[ 2.,  3.,  4.,  5.],
       [ 4.,  5.,  6.,  7.]], dtype=float32)

array([[ 0.1       ,  0.1       ,  0.1       ,  0.1       ],
       [ 0.30000001,  0.30000001,  0.30000001,  0.30000001]], dtype=float32)   
</code></pre>

<h2 id="bilinear">BiLinear</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = BiLinear(
  inputSize1,
  inputSize2,
  outputSize,
  biasRes = true,
  wRegularizer = null,
  bRegularizer = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = BiLinear(
    input_size1,
    input_size2,
    output_size,
    bias_res=True,
    wRegularizer=None,
    bRegularizer=None)
</code></pre>

<p>A bilinear transformation with sparse inputs.
The input tensor given in forward(input) is a table containing both inputs x_1 and x_2,
which are tensors of size N x inputDimension1 and N x inputDimension2, respectively.</p>
<p><strong>Parameters:</strong></p>
<p><strong>inputSize1</strong>   dimension of input x_1</p>
<p><strong>inputSize2</strong>   dimension of input x_2</p>
<p><strong>outputSize</strong>   output dimension</p>
<p><strong>biasRes</strong>  The layer can be trained without biases by setting bias = false. otherwise true</p>
<p><strong>wRegularizer</strong> : instance of <code>Regularizer</code>
             (eg. L1 or L2 regularization), applied to the input weights matrices.</p>
<p><strong>bRegularizer</strong> : instance of <code>Regularizer</code>
             applied to the bias.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.Bilinear
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val layer = Bilinear(3, 2, 3)
val input1 = Tensor(T(
  T(-1f, 2f, 3f),
  T(-2f, 3f, 4f),
  T(-3f, 4f, 5f)
))
val input2 = Tensor(T(
  T(-2f, 3f),
  T(-1f, 2f),
  T(-3f, 4f)
))
val input = T(input1, input2)

val gradOutput = Tensor(T(
  T(3f, 4f, 5f),
  T(2f, 3f, 4f),
  T(1f, 2f, 3f)
))

val output = layer.forward(input)
val grad = layer.backward(input, gradOutput)

println(output)
-0.14168167 -8.697224   -10.097688
-0.20962894 -7.114827   -8.568602
0.16706467  -19.751905  -24.516418
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

println(grad)
 {
    2: 13.411718    -18.695072
       14.674414    -19.503393
       13.9599  -17.271534
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]
    1: -5.3747015   -17.803686  -17.558662
       -2.413877    -8.373887   -8.346823
       -2.239298    -11.249412  -14.537216
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
 }
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = Bilinear(3, 2, 3)
input_1 = np.array([
  [-1.0, 2.0, 3.0],
  [-2.0, 3.0, 4.0],
  [-3.0, 4.0, 5.0]
])

input_2 = np.array([
  [-3.0, 4.0],
  [-2.0, 3.0],
  [-1.0, 2.0]
])

input = [input_1, input_2]

gradOutput = np.array([
  [3.0, 4.0, 5.0],
  [2.0, 3.0, 4.0],
  [1.0, 2.0, 5.0]
])

output = layer.forward(input)
grad = layer.backward(input, gradOutput)

print output
[[-0.5  1.5  2.5]
 [-1.5  2.5  3.5]
 [-2.5  3.5  4.5]]
[[ 3.  4.  5.]
 [ 2.  3.  4.]
 [ 1.  2.  5.]]

print grad
[array([[ 11.86168194, -14.02727222,  -6.16624403],
       [  6.72984409,  -7.96572971,  -2.89302039],
       [  5.52902842,  -5.76724434,  -1.46646953]], dtype=float32), array([[ 13.22105694,  -4.6879468 ],
       [ 14.39296341,  -6.71434498],
       [ 20.93929482, -13.02455521]], dtype=float32)]
</code></pre>

<h2 id="clamp">Clamp</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = Clamp(min, max)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = Clamp(min, max)
</code></pre>

<p>A kind of hard tanh activition function with integer min and max
- param min min value
- param max max value</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor

val model = Clamp(-10, 10)
val input = Tensor(2, 2, 2).rand()
val output = model.forward(input)

scala&gt; print(input)
(1,.,.) =
0.95979714  0.27654588  
0.35592428  0.49355772  

(2,.,.) =
0.2624511   0.78833413  
0.967827    0.59160346  

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]

scala&gt; print(output)
(1,.,.) =
0.95979714  0.27654588  
0.35592428  0.49355772  

(2,.,.) =
0.2624511   0.78833413  
0.967827    0.59160346  

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = Clamp(-10, 10)
input = np.random.randn(2, 2, 2)
output = model.forward(input)

&gt;&gt;&gt; print(input)
[[[-0.66763755  1.15392566]
  [-2.10846048  0.46931736]]

 [[ 1.74174638 -1.04323311]
  [-1.91858729  0.12624046]]]

&gt;&gt;&gt; print(output)
[[[-0.66763753  1.15392566]
  [-2.10846043  0.46931735]]

 [[ 1.74174643 -1.04323316]
  [-1.91858733  0.12624046]]
</code></pre>

<h2 id="square">Square</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Square()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Square()
</code></pre>

<p>Square apply an element-wise square operation.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val module = Square()

println(module.forward(Tensor.range(1, 6, 1)))
</code></pre>

<p>Output is</p>
<pre><code>com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0
4.0
9.0
16.0
25.0
36.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 6]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

module = Square()
print(module.forward(np.arange(1, 7, 1)))
</code></pre>

<p>Output is</p>
<pre><code>[array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)]
</code></pre>

<h2 id="mean">Mean</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = Mean(dimension=1, nInputDims=-1, squeeze=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = Mean(dimension=1,n_input_dims=-1, squeeze=True)
</code></pre>

<p>Mean is a module that simply applies a mean operation over the given dimension - specified by <code>dimension</code> (starting from 1).</p>
<p>The input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using <code>nInputDims</code>.  When input is one tensor, do not specify <code>nInputDims</code> or set it = -1, otherwise input will be interpreted as batch of tensors. </p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">scala&gt; 
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val input = Tensor(2, 2, 2).randn()
val m1 = Mean()
val output1 = m1.forward(input)
val m2 = Mean(2,1,true)
val output2 = m2.forward(input)

scala&gt; print(input)
(1,.,.) =
-0.52021635     -1.8250599
-0.2321481      -2.5672712

(2,.,.) =
4.007425        -0.8705412
1.6506456       -0.2470611

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]

scala&gt; print(output1)
1.7436042       -1.3478005
0.7092488       -1.4071661
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

scala&gt; print(output2)
-0.37618223     -2.1961656
2.8290353       -0.5588012
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

input = np.random.rand(2,2,2)
print &quot;input is :&quot;,input

m1 = Mean()
out = m1.forward(input)
print &quot;output m1 is :&quot;,out

m2 = Mean(2,1,True)
out = m2.forward(input)
print &quot;output m2 is :&quot;,out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input is : [[[ 0.01990713  0.37740696]
  [ 0.67689963  0.67715705]]

 [[ 0.45685026  0.58995121]
  [ 0.33405769  0.86351324]]]
creating: createMean
output m1 is : [array([[ 0.23837869,  0.48367909],
       [ 0.50547862,  0.77033514]], dtype=float32)]
creating: createMean
output m2 is : [array([[ 0.34840336,  0.527282  ],
       [ 0.39545399,  0.72673225]], dtype=float32)]
</code></pre>

<h2 id="power">Power</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Power(power, scale=1, shift=0)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Power(power, scale=1.0, shift=0.0)
</code></pre>

<p>Apply an element-wise power operation with scale and shift.</p>
<p>f(x) = (shift + scale * x)^power^</p>
<p><code>power</code> the exponent.
 <code>scale</code> Default is 1.
 <code>shift</code> Default is 0.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val power = Power(2, 1, 1)
val input = Tensor(Storage(Array(0.0, 1, 2, 3, 4, 5)), 1, Array(2, 3))
&gt; print(power.forward(input))
1.0     4.0      9.0    
16.0        25.0     36.0   
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *

power = Power(2.0, 1.0, 1.0)
input = np.array([[0.0, 1, 2], [3, 4, 5]])
&gt;power.forward(input)
array([[  1.,   4.,   9.],
       [ 16.,  25.,  36.]], dtype=float32)

</code></pre>

<h2 id="cmul">CMul</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = CMul(size, wRegularizer = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = CMul(size, wRegularizer=None)
</code></pre>

<p>This layer has a weight tensor with given size. The weight will be multiplied element wise to
the input tensor. If the element number of the weight tensor match the input tensor, a simply
element wise multiply will be done. Or the bias will be expanded to the same size of the input.
The expand means repeat on unmatched singleton dimension(if some unmatched dimension isn't
singleton dimension, it will report an error). If the input is a batch, a singleton dimension
will be add to the first dimension before the expand.</p>
<p><code>size</code> the size of the bias, which is an array of bias shape</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val layer = CMul(Array(2, 1))
val input = Tensor(2, 3)
var i = 0
input.apply1(_ =&gt; {i += 1; i})
&gt; print(layer.forward(input))
-0.29362988     -0.58725977     -0.88088965
1.9482219       2.4352775       2.9223328
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *

layer = CMul([2,1])
input = np.array([[1, 2, 3], [4, 5, 6]])
&gt;layer.forward(input)
array([[-0.17618844, -0.35237688, -0.52856529],
       [ 0.85603124,  1.07003903,  1.28404689]], dtype=float32)
</code></pre>

<h2 id="addconstant">AddConstant</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = AddConstant(constant_scalar,inplace= false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = AddConstant(constant_scalar,inplace=False,bigdl_type=&quot;float&quot;)
</code></pre>

<p>Element wise add a constant scalar to input tensor
<em> @param constant_scalar constant value
</em> @param inplace Can optionally do its operation in-place without using extra state memory</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val module = AddConstant(3.0)
val input = Tensor(2,3).rand()
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.40684703      0.077655114     0.42314094
0.55392265      0.8650696       0.3621729
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]

module.forward(input)
res11: com.intel.analytics.bigdl.tensor.Tensor[Float] =
3.406847        3.077655        3.423141
3.5539227       3.8650696       3.3621728
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">module = AddConstant(3.0,inplace=False,bigdl_type=&quot;float&quot;)
input = np.array([[1, 2, 3],[4, 5, 6]])
module.forward(input)
[array([
[ 4.,  5.,  6.],
[ 7.,  8.,  9.]], dtype=float32)]
</code></pre>

<h2 id="abs">Abs</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = Abs()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = Abs()
</code></pre>

<p>An element-wise abs operation.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val abs = new Abs
val input = Tensor(2)
input(1) = 21f
input(2) = -29f
print(abs.forward(input))
</code></pre>

<p><code>output is:　21.0　29.0</code></p>
<p><strong>Python example:</strong></p>
<pre><code class="python">abs = Abs()
input = np.array([21, -29, 30])
print(abs.forward(input))
</code></pre>

<p><code>output is: [array([ 21.,  29.,  30.], dtype=float32)]</code></p>
<h2 id="log">Log</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val log = Log()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">log = Log()
</code></pre>

<p>The Log module applies a log transformation to the input data</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val log = Log()
val input = Tensor(T(1.0f, Math.E.toFloat))
val gradOutput = Tensor(T(1.0f, 1.0f))
val output = log.forward(input)
val gradient = log.backward(input, gradOutput)
-&gt; print(output)
0.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]

-&gt; print(gradient)
1.0
0.36787945
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
import math
log = Log()
input = np.array([1.0, math.e])
grad_output = np.array([1.0, 1.0])
output = log.forward(input)
gradient = log.backward(input, grad_output)

-&gt; print output
[ 0.  1.]

-&gt; print gradient
[ 1.          0.36787945]
</code></pre>

<h2 id="sum">Sum</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = Sum(dimension=1,nInputDims=-1,sizeAverage=false,squeeze=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = Sum(dimension=1,n_input_dims=-1,size_average=False,squeeze=True)
</code></pre>

<p>Sum is a module that simply applies a sum operation over the given dimension - specified by the argument <code>dimension</code> (starting from 1). </p>
<p>The input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using <code>nInputDims</code>.  When input is one tensor, do not specify <code>nInputDims</code> or set it = -1, otherwise input will be interpreted as batch of tensors. </p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt; 
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val input = Tensor(2, 2, 2).randn()
val m1 = Sum(2)
val output1 = m1.forward(input)
val m2 = Sum(2, 1, true)
val output2 = m2.forward(input)

scala&gt; print(input)
(1,.,.) =
-0.003314678    0.96401167
0.79000163      0.78624517

(2,.,.) =
-0.29975495     0.24742787
0.8709072       0.4381108

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]

scala&gt; print(output1)
0.78668696      1.7502568
0.5711522       0.68553865
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

scala&gt; print(output2)
0.39334348      0.8751284
0.2855761       0.34276932
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

input=np.random.rand(2,2,2)
print &quot;input is :&quot;,input
module = Sum(2)
out = module.forward(input)
print &quot;output 1 is :&quot;,out
module = Sum(2,1,True)
out = module.forward(input)
print &quot;output 2 is :&quot;,out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input is : [[[ 0.7194801   0.99120677]
  [ 0.07446639  0.056318  ]]

 [[ 0.08639016  0.17173268]
  [ 0.71686986  0.30503663]]]
creating: createSum
output 1 is : [array([[ 0.7939465 ,  1.04752481],
       [ 0.80325997,  0.47676933]], dtype=float32)]
creating: createSum
output 2 is : [array([[ 0.39697325,  0.5237624 ],
       [ 0.40162998,  0.23838466]], dtype=float32)]
</code></pre>

<h2 id="sqrt">Sqrt</h2>
<p>Apply an element-wise sqrt operation.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val sqrt = new Sqrt
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">sqrt = Sqrt()
</code></pre>

<p>Apply an element-wise sqrt operation.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.Sqrt
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val input = Tensor(3, 5).range(1, 15, 1)
val sqrt = new Sqrt
val output = sqrt.forward(input)
println(output)

val gradOutput = Tensor(3, 5).range(2, 16, 1)
val gradInput = sqrt.backward(input, gradOutput)
println(gradOutput
</code></pre>

<p>The output will be,</p>
<pre><code>output: com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0     1.4142135       1.7320508       2.0     2.236068
2.4494898       2.6457512       2.828427        3.0     3.1622777
3.3166249       3.4641016       3.6055512       3.7416575       3.8729835
</code></pre>

<p>The gradInput will be,</p>
<pre><code>gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0     1.0606601       1.1547005       1.25    1.3416407
1.428869        1.5118579       1.5909902       1.6666667       1.7392527
1.8090681       1.8763883       1.9414507       2.0044594       2.065591
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

sqrt = Sqrt()

input = np.arange(1, 16, 1).astype(&quot;float32&quot;)
input = input.reshape(3, 5)

output = sqrt.forward(input)
print output

gradOutput = np.arange(2, 17, 1).astype(&quot;float32&quot;)
gradOutput = gradOutput.reshape(3, 5)

gradInput = sqrt.backward(input, gradOutput)
print gradInput
</code></pre>

<p>The output will be:</p>
<pre><code>[array([[ 1.        ,  1.41421354,  1.73205078,  2.        ,  2.23606801],
       [ 2.44948983,  2.64575124,  2.82842708,  3.        ,  3.1622777 ],
       [ 3.31662488,  3.46410155,  3.60555124,  3.7416575 ,  3.87298346]], dtype=float32)]
</code></pre>

<p>The gradInput will be:</p>
<pre><code>[array([[ 1.        ,  1.06066012,  1.15470052,  1.25      ,  1.34164071],
       [ 1.42886901,  1.51185787,  1.59099019,  1.66666675,  1.73925269],
       [ 1.80906808,  1.87638831,  1.94145072,  2.00445938,  2.0655911 ]], dtype=float32)]
</code></pre>

<h2 id="exp">Exp</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val exp = Exp()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">exp = Exp()
</code></pre>

<p>Exp applies element-wise exp operation to input tensor</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val exp = Exp()
val input = Tensor(3, 3).rand()
&gt; print(input)
0.0858663   0.28117087  0.85724664  
0.62026995  0.29137492  0.07581586  
0.22099794  0.45131826  0.78286386  
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]
&gt; print(exp.forward(input))
1.0896606   1.32468     2.356663    
1.85943     1.3382663   1.078764    
1.2473209   1.5703809   2.1877286   
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
exp = Exp()
&gt; exp.forward(np.array([[1, 2, 3],[1, 2, 3]]))
[array([[  2.71828175,   7.38905621,  20.08553696],
       [  2.71828175,   7.38905621,  20.08553696]], dtype=float32)]

</code></pre>

<h2 id="max">Max</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = Max(dim = 1, numInputDims = Int.MinValue)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = Max(dim, num_input_dims=INTMIN)
</code></pre>

<p>Applies a max operation over dimension <code>dim</code>.</p>
<p><strong>Parameters:</strong></p>
<p><strong>dim</strong> max along this dimension</p>
<p><strong>numInputDims</strong> Optional. If in a batch model, set to the inputDims.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.Max
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val layer = Max(1, 1)
val input = Tensor(T(
  T(-1f, 2f, 3f),
  T(-2f, 3f, 4f),
  T(-3f, 4f, 5f)
))

val gradOutput = Tensor(T(3f, 4f, 5f))

val output = layer.forward(input)
val grad = layer.backward(input, gradOutput)

println(output)
3.0
4.0
5.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]

println(grad)
0.0 0.0 3.0
0.0 0.0 4.0
0.0 0.0 5.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = Max(1, 1)
input = np.array([
  [-1.0, 2.0, 3.0],
  [-2.0, 3.0, 4.0],
  [-3.0, 4.0, 5.0]
])

gradOutput = np.array([3.0, 4.0, 5.0])

output = layer.forward(input)
grad = layer.backward(input, gradOutput)

print output
[ 3.  4.  5.]

print grad
[[ 0.  0.  3.]
 [ 0.  0.  4.]
 [ 0.  0.  5.]]
``
## CAdd ##

**Scala:**
```scala
val module = CAdd(size,bRegularizer=null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = CAdd(size,bRegularizer=None,bigdl_type=&quot;float&quot;)
</code></pre>

<p>This layer has a bias tensor with given size. The bias will be added element wise to the input
tensor. If the element number of the bias tensor match the input tensor, a simply element wise
will be done. Or the bias will be expanded to the same size of the input. The expand means
repeat on unmatched singleton dimension(if some unmatched dimension isn't singleton dimension,
it will report an error). If the input is a batch, a singleton dimension will be add to the first
dimension before the expand.</p>
<ul>
<li>@param size the size of the bias </li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val module = CAdd(Array(2, 1),bRegularizer=null)
val input = Tensor(2, 3).rand()
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.52146345      0.86262375      0.74210143
0.15882674      0.026310394     0.28394955
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]

module.forward(input)
res12: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.97027373      1.311434        1.1909117
-0.047433108    -0.17994945     0.07768971
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">module = CAdd([2, 1],bRegularizer=None,bigdl_type=&quot;float&quot;)
input = np.random.rand(2, 3)
array([[ 0.71239789,  0.65869477,  0.50425182],
       [ 0.40333312,  0.64843273,  0.07286636]])

module.forward(input)
array([[ 0.89537328,  0.84167016,  0.68722725],
       [ 0.1290929 ,  0.37419251, -0.20137388]], dtype=float32)
</code></pre>

<h2 id="cosine">Cosine</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = Cosine(inputSize, outputSize)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = Cosine(input_size, output_size)
</code></pre>

<p>Cosine is a module used to  calculate the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> of the input to <code>outputSize</code> centers, i.e. this layer has the weights <code>w_j</code>, for <code>j = 1,..,outputSize</code>, where <code>w_j</code> are vectors of dimension <code>inputSize</code>.</p>
<p>The distance <code>y_j</code> between center <code>j</code> and input <code>x</code> is formulated as <code>y_j = (x · w_j) / ( || w_j || * || x || )</code>.</p>
<p>The input given in <code>forward(input)</code> must be either a vector (1D tensor) or matrix (2D tensor). If the input is a
vector, it must have the size of <code>inputSize</code>. If it is a matrix, then each row is assumed to be an input sample of given batch (the number of rows means the batch size and the number of columns should be equal to the <code>inputSize</code>).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val m = Cosine(2, 3)
val input = Tensor(3, 2).rand()
val output = m.forward(input)

scala&gt; print(input)
0.48958543      0.38529378
0.28814933      0.66979927
0.3581584       0.67365724
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

scala&gt; print(output)
0.998335        0.9098057       -0.71862763
0.8496431       0.99756527      -0.2993874
0.8901594       0.9999207       -0.37689084
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

input=np.random.rand(2,3)
print &quot;input is :&quot;,input
module = Cosine(3,3)
module.forward(input)
print &quot;output is :&quot;,out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input is : [[ 0.31156943  0.85577626  0.4274042 ]
 [ 0.79744055  0.66431136  0.05657437]]
creating: createCosine
output is : [array([[-0.73284394, -0.28076306, -0.51965958],
       [-0.9563939 , -0.42036989, -0.08060561]], dtype=float32)]


</code></pre>

<h2 id="mul">Mul</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Mul[Float]()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Mul()
</code></pre>

<p>Multiply a singla scalar factor to the incoming data</p>
<pre><code>                 +----Mul----+
 input -----+---&gt; input * weight -----+----&gt; output
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">val mul = Mul[Float]()

&gt; print(mul.forward(Tensor(1, 5).rand()))
-0.03212923     -0.019040342    -9.136753E-4    -0.014459004    -0.04096878
[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">mul = Mul()
input = np.random.uniform(0, 1, (1, 5)).astype(&quot;float32&quot;)

&gt; mul.forward(input)
[array([[ 0.72429317,  0.7377845 ,  0.09136307,  0.40439236,  0.29011244]], dtype=float32)]

</code></pre>

<h2 id="mulconstant">MulConstant</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = MulConstant(scalar, inplace)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = MulConstant(const, inplace)
</code></pre>

<p>Multiplies input Tensor by a (non-learnable) scalar constant.
This module is sometimes useful for debugging purposes.</p>
<p><strong>Parameters:</strong>
<em> <strong>constant</strong> - scalar constant
</em> <strong>inplace</strong> - Can optionally do its operation in-place without using extra state memory. Default: false</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val input = Tensor(T(
 T(1.0f, 2.0f),
 T(3.0f, 4.0f))
)
val gradOutput = Tensor(T(
 T(1.0f, 1.0f),
 T(1.0f, 1.0f))
)
val scalar = 2.0
val module = MulConstant(scalar)
val output = module.forward(input)
val gradient = module.backward(input, gradOutput)
-&gt; print(output)
2.0     4.0     
6.0     8.0     
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

-&gt; print(gradient)
2.0     2.0     
2.0     2.0     
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
input = np.array([
          [1.0, 2.0],
          [3.0, 4.0]
        ])
grad_output = np.array([
           [1.0, 1.0],
           [1.0, 1.0]
         ])
scalar = 2.0
module = MulConstant(scalar)
output = module.forward(input)
gradient = module.backward(input, grad_output)
-&gt; print output
[[ 2.  4.]
 [ 6.  8.]]
-&gt; print gradient
[[ 2.  2.]
 [ 2.  2.]]
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Padding_Layers/merged-Padding_Layers/" class="btn btn-neutral float-right" title="Padding Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../MergeSplit_Layers/merged-MergeSplit_Layers/" class="btn btn-neutral" title="Merge/Split Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../MergeSplit_Layers/merged-MergeSplit_Layers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Padding_Layers/merged-Padding_Layers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../../js/theme.js"></script>

</body>
</html>
