<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Containers - BigDL Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/highlight.css">
  <link href="../../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Containers";
    var mkdocs_page_input_path = "APIdocs/Layers/Containers/merged-Containers.md";
    var mkdocs_page_url = "/APIdocs/Layers/Containers/merged-Containers/";
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> BigDL Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-setup-bigdl/">Setup BigDL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prepare-data/">Prepare your Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prediction/">Use BigDL for Prediction Only</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-train/">Train a Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-monitor/">Monitor the Training</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-tune/">Tuning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-advanced/">Advanced Usage</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Install/Deploy</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/use-pre-built/">Use Pre-built Package</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/build-src/">Build from Source</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/deploy-python/">Enable Python Support</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/running-on-EC2/">Running On EC2</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/resources/">Resources</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-api/">API Usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/install-via-pip/">Install via pip</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-no-pip/">Use Python without Pip</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model/SequentialModel/">Sequential Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model/FunctionalAPI/">Functional API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Containers</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#graph">Graph</a></li>
    

    <li class="toctree-l3"><a href="#bottle">Bottle</a></li>
    

    <li class="toctree-l3"><a href="#container">Container</a></li>
    

    <li class="toctree-l3"><a href="#timedistributed">TimeDistributed</a></li>
    

    <li class="toctree-l3"><a href="#maptable">MapTable</a></li>
    

    <li class="toctree-l3"><a href="#concattable">ConcatTable</a></li>
    

    <li class="toctree-l3"><a href="#paralleltable">ParallelTable</a></li>
    

    <li class="toctree-l3"><a href="#concat">Concat</a></li>
    

    <li class="toctree-l3"><a href="#sequential">Sequential</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Simple_Layers/merged-Simple_Layers/">Simple Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Convolution_Layers/merged-Convolution_Layers/">Convolution Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Pooling_Layers/merged-Pooling_Layers/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Linear_Layers/merged-Linear_Layers/">Linear Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Non-linear_Activations/merged-Non-linear_Activations/">Non-linear Activations</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Embedding_Layers/merged-Embedding_Layers/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../MergeSplit_Layers/merged-MergeSplit_Layers/">Merge/Split Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Math-Layers/merged-Math-Layers/">Math Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Padding_Layers/merged-Padding_Layers/">Padding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Normalization_Layers/merged-Normalization_Layers/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Dropout_Layers/merged-Dropout_Layers/">Dropout Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Distance_Layers/merged-Distance_Layers/">Distance Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Recurrent_Layers/merged-Recurrent_Layers/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Losses/merged-Losses/">Losses</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Optimizers/DistriOptimizer/">Optimizer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Optim_Methods/merged-Optim_Methods/">Optimization Algorithms</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Triggers/">Triggers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/ResumeTraining/">Resume Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Initializers/merged-Initializers/">Initalizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Regularizers/merged-Regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Metrics/merged-Metrics/">Metrics</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Preprocessing/Transformer/">Transformer</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Load/Save Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/BigDLModel/">Load/Save a BigDL Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/TensorflowModel/">Load a Tensorflow Model</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/visualization-with-tensorboard/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../python-api-doc/">Python API Docs</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">BigDL Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Containers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="graph">Graph</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val graph = Graph[Float](Array(Node), Array(Node))
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = Model([Node], [Node])
</code></pre>

<p>A graph container. Each node can have multiple inputs. The output of the node should be a tensor.
 The output tensor can be connected to multiple nodes. So the module in each node can have a
 tensor or table input, and should have a tensor output.</p>
<p>The graph container can have multiple inputs and multiple outputs. If there's one input, the
 input data fed to the graph module should be a tensor. If there're multiple inputs, the input
 data fed to the graph module should be a table, which is actually an sequence of tensor. The
 order of the input tensors should be same with the order of the input nodes. This is also
 applied to the gradient from the module in the back propagation.</p>
<p>All of the input modules must accept a tensor input. If your input module accept multiple
 tensors as input, you should add some Input module before it as input nodes and connect the
 output of the Input modules to that module.</p>
<p>If there's one output, the module output is a tensor. If there're multiple outputs, the module
 output is a table, which is actually an sequence of tensor. The order of the output tensors is
 same with the order of the output modules. This is also applied to the gradient passed to the
 module in the back propagation.</p>
<p>All inputs should be able to connect to outputs through some paths in the graph. It is
 allowed that some successors of the inputs node are not connect to outputs. If so, these nodes
 will be excluded in the computation.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
val input1 = Input[Float]()
val input2 = Input[Float]()
val cadd = CAddTable[Float]().apply(input1, input2)
val graph = Graph[Float](Array(input1, input2), cadd)

val output = graph.forward(T(Tensor[Float](T(0.1f, 0.2f, -0.3f, -0.4f)),
                             Tensor[Float](T(0.5f, 0.4f, -0.2f, -0.1f))))
val gradInput = graph.backward(T(Tensor[Float](T(0.1f, 0.2f, -0.3f, -0.4f)),
                                 Tensor[Float](T(0.5f, 0.4f, -0.2f, -0.1f))),
                               Tensor[Float](T(0.1f, 0.2f, 0.3f, 0.4f)))

&gt; println(output)
output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
0.6
0.6
-0.5
-0.5
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]

&gt; println(gradInput)
gradInput: com.intel.analytics.bigdl.nn.abstractnn.Activity =
 {
        2: 0.1
           0.2
           0.3
           0.4
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]
        1: 0.1
           0.2
           0.3
           0.4
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]
 }



</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">
fc1 = Linear(4, 2)()
fc2 = Linear(4, 2)()
cadd = CAddTable()([fc1, fc2])
output1 = ReLU()(cadd)
output2 = Threshold(10.0)(cadd)
model = Model([fc1, fc2], [output1, output2])
fc1.element().set_weights([np.ones((4, 2)), np.ones((2, ))])
fc2.element().set_weights([np.ones((4, 2)) * 2, np.ones((2, )) * 2])

output = model.forward([np.array([0.1, 0.2, -0.3, -0.4]),
                        np.array([0.5, 0.4, -0.2, -0.1])])

gradInput = model.backward([np.array([0.1, 0.2, -0.3, -0.4]),
                            np.array([0.5, 0.4, -0.2, -0.1])],
                           [np.array([1.0, 2.0]),
                                    np.array([3.0, 4.0])])
weights = fc1.element().get_weights()[0]

&gt; output
[array([ 3.79999971,  3.79999971], dtype=float32),
 array([ 0.,  0.], dtype=float32)]
&gt; gradInput
[array([ 3.,  3.,  3.,  3.], dtype=float32),
 array([ 6.,  6.,  6.,  6.], dtype=float32)]

</code></pre>

<h2 id="bottle">Bottle</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = Bottle(module, nInputDim, nOutputDim)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = Bottle(module, nInputDim, nOutputDim)
</code></pre>

<p>Bottle allows varying dimensionality input to be forwarded through any module that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val model = Bottle(Linear[Float](3, 2), 2, 2)
val input = Tensor(2, 3, 3).rand()

scala&gt; print(input)
(1,.,.) =
0.7843752   0.17286697  0.20767091  
0.8594811   0.9100018   0.8448141   
0.7683892   0.36661968  0.76637685  

(2,.,.) =
0.7163263   0.083962396 0.81222403  
0.7947034   0.09976136  0.114404656 
0.14890474  0.43289232  0.1489096   

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3x3] 

val output = model.forward(input)

scala&gt; print(output)
(1,.,.) =
-0.31146684 0.40719786  
-0.51778656 0.58715886  
-0.51676923 0.4027511   

(2,.,.) =
-0.5498678  0.29658738  
-0.280177   0.39901164  
-0.2387946  0.24809375  

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = Bottle(Linear(3, 2), 2, 2)

input = np.random.randn(2, 3, 3)
output = model.forward(input)

&gt;&gt;&gt; print(input)
[[[ 0.42370589 -1.7938942   0.56666373]
  [-1.78501381  0.55676471 -0.50150367]
  [-1.59262182  0.82079469  1.1873599 ]]

 [[ 0.95799792 -0.71447244  1.05344083]
  [-0.07838376 -0.88780484 -1.80491177]
  [ 0.99996222  1.39876002 -0.16326094]]]
&gt;&gt;&gt; print(output)
[[[ 0.26298434  0.74947536]
  [-1.24375117 -0.33148435]
  [-1.35218966  0.17042145]]

 [[ 0.08041853  0.91245329]
  [-0.08317742 -0.13909879]
  [-0.52287608  0.3667658 ]]]
</code></pre>

<h2 id="container">Container</h2>
<p>Container is a subclass of abstract class AbstractModule, which
declares methods defined in all containers. A container usually
contains some other modules in the <code>modules</code> variable. It overrides
many module methods such that calls are propogated to the contained
modules.</p>
<h2 id="timedistributed">TimeDistributed</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = TimeDistributed[T](layer)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = TimeDistributed(layer)
</code></pre>

<p>This layer is intended to apply contained layer to each temporal time slice
of input tensor.</p>
<p>The input data format is [Batch, Time, Other dims]. For the contained layer, it must not change
the Other dims length.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor

val layer = TimeDistributed[Float](Sum[Float](1, squeeze = false, nInputDims = 2))
val input = Tensor[Float](T(T(
  T(
    T(1.0f, 2.0f),
    T(3.0f, 4.0f)
  ),
  T(
    T(2.0f, 3.0f),
    T(4.0f, 5.0f)
  )
)))
layer.forward(input)
layer.backward(input, Tensor[Float](T(T(
  T(
    T(0.1f, 0.2f)
  ),
  T(
    T(0.3f, 0.4f)
  )
))))
</code></pre>

<p>Its output should be</p>
<pre><code>(1,1,.,.) =
4.0     6.0

(1,2,.,.) =
6.0     8.0

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1x2]

(1,1,.,.) =
0.1     0.2
0.1     0.2

(1,2,.,.) =
0.3     0.4
0.3     0.4

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import TimeDistributed,Sum
import numpy as np

layer = TimeDistributed(Sum(1, squeeze = False, n_input_dims = 2))

input = np.array([[
  [
    [1.0, 2.0],
    [3.0, 4.0]
  ],
  [
    [2.0, 3.0],
    [4.0, 5.0]
  ]
]])
layer.forward(input)
layer.backward(input, np.array([[
  [
    [0.1, 0.2]
  ],
  [
    [0.3, 0.4]
  ]
]]))
</code></pre>

<p>Its output should be</p>
<pre><code>array([[[[ 4.,  6.]],

        [[ 6.,  8.]]]], dtype=float32)

array([[[[ 0.1       ,  0.2       ],
         [ 0.1       ,  0.2       ]],

        [[ 0.30000001,  0.40000001],
         [ 0.30000001,  0.40000001]]]], dtype=float32)
</code></pre>

<h2 id="maptable">MapTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val mod = MapTable(module=null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">mod = MapTable(module=None)
</code></pre>

<p>This class is a container for a single module which will be applied
to all input elements. The member module is cloned as necessary to
process all input elements.</p>
<p><code>module</code> a member module.  </p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T 

val map = MapTable()
map.add(Linear(10, 3))
val input = T(
      Tensor(10).randn(),
      Tensor(10).randn())
&gt; print(map.forward(input))
{
    2: 0.2444828
       -1.1700082
       0.15887381
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
    1: 0.06696482
       0.18692614
       -1.432079
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
 }
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *

map = MapTable()
map.add(Linear(10, 3))
input = [np.random.rand(10), np.random.rand(10)]
&gt;map.forward(input)
[array([ 0.69586945, -0.70547599, -0.05802459], dtype=float32),
 array([ 0.47995114, -0.67459631, -0.52500772], dtype=float32)]
</code></pre>

<h2 id="concattable">ConcatTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = ConcatTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = ConcatTable()
</code></pre>

<p>ConcateTable is a container module like Concate. Applies an input
to each member module, input can be a tensor or a table.</p>
<p>ConcateTable usually works with CAddTable and CMulTable to
 implement element wise add/multiply on outputs of two modules.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val mlp = ConcatTable()
mlp.add(Linear(3, 2))
mlp.add(Linear(3, 4))

&gt; print(mlp.forward(Tensor(2, 3).rand()))

{
    2: -0.37111914  0.8542446   -0.362602   -0.75522065 
       -0.28713673  0.6021913   -0.16525984 -0.44689763 
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]
    1: -0.79941726  0.8303885   
       -0.8342782   0.89961016  
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *

mlp = ConcatTable()
mlp.add(Linear(3, 2))   
mlp.add(Linear(3, 4))
&gt; mlp.forward(np.array([[1, 2, 3], [1, 2, 3]]))
out: [array([[ 1.16408789, -0.1508013 ],
             [ 1.16408789, -0.1508013 ]], dtype=float32),
      array([[-0.24672163, -0.56958938, -0.51390374,  0.64546645],
             [-0.24672163, -0.56958938, -0.51390374,  0.64546645]], dtype=float32)]

</code></pre>

<h2 id="paralleltable">ParallelTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = ParallelTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = ParallelTable()
</code></pre>

<p>It is a container module that applies the i-th member module to the i-th
 input, and outputs an output in the form of Table</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T 
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val module = ParallelTable()
val log = Log()
val exp = Exp()
module.add(log)
module.add(exp)
val input1 = Tensor(3, 3).rand(0, 1)
val input2 = Tensor(3).rand(0, 1)
val input = T(1 -&gt; input1, 2 -&gt; input2)
&gt; print(module.forward(input))
 {
        2: 2.6996834
           2.0741253
           1.0625387
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
        1: -1.073425    -0.6672964      -1.8160943
           -0.54094607  -1.3029919      -1.7064717
           -0.66175103  -0.08288143     -1.1840979
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
 }
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *

module = ParallelTable()
log = Log()
exp = Exp()
module.add(log)
module.add(exp)
input1 = np.random.rand(3,3)
input2 = np.random.rand(3)
&gt;module.forward([input1, input2])
[array([[-1.27472472, -2.18216252, -0.60752904],
        [-2.76213861, -1.77966928, -0.13652121],
        [-1.47725129, -0.03578046, -1.37736678]], dtype=float32),
 array([ 1.10634041,  1.46384597,  1.96525407], dtype=float32)]
</code></pre>

<h2 id="concat">Concat</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Concat(dimension)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Concat(dimension)
</code></pre>

<p>Concat is a container who concatenates the output of it's submodules along the
provided <code>dimension</code>: all submodules take the same inputs, and their output is
concatenated.</p>
<pre><code>                 +----Concat----+
            +----&gt;  submodule1  -----+
            |    |              |    |
 input -----+----&gt;  submodule2  -----+----&gt; output
            |    |              |    |
            +----&gt;  submodule3  -----+
                 +--------------+
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val mlp = Concat(2)
mlp.add(Linear(3,2))
mlp.add(Linear(3,4))

println(mlp.forward(Tensor(2, 3).rand()))
</code></pre>

<p>Output is</p>
<pre><code>-0.17087375 0.12954286  0.15685591  -0.027277306    0.38549712  -0.20375136
-0.9473443  0.030516684 0.23380546  0.625985    -0.031360716    0.40449825
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

mlp = Concat(2)
mlp.add(Linear(3,2))
mlp.add(Linear(3,4))
print(mlp.forward(np.array([[1, 2, 3], [1, 2, 3]])))
</code></pre>

<p>Output is</p>
<pre><code>[array([
[-0.71994132,  2.17439198, -1.46522939,  0.64588934,  2.61534023, -2.39528942],
[-0.89125222,  5.49583197, -2.8865242 ,  1.44914722,  5.26639175, -6.26586771]]
      dtype=float32)]

</code></pre>

<h2 id="sequential">Sequential</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Sequential()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">seq = Sequential()
</code></pre>

<p>Sequential provides a means to plug layers together
in a feed-forward fully connected manner.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

import com.intel.analytics.bigdl.nn.{Sequential, Linear}

val module = Sequential()
module.add(Linear(10, 25))
module.add(Linear(25, 10))

val input = Tensor(10).range(1, 10, 1)
val gradOutput = Tensor(10).range(1, 10, 1)

val output = module.forward(input).toTensor
val gradInput = module.backward(input, gradOutput).toTensor

println(output)
println(gradInput)
</code></pre>

<p>The output is,</p>
<pre><code>-2.3750305
2.4512818
1.6998017
-0.47432393
4.3048754
-0.044168986
-1.1643536
0.60341483
2.0216258
2.1190155
[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]
</code></pre>

<p>The gradInput is,</p>
<pre><code>2.593382
-1.4137214
-1.8271983
1.229643
0.51384985
1.509845
2.9537349
1.088281
0.2618509
1.4840821
[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

seq = Sequential()
seq.add(Linear(10, 25))
seq.add(Linear(25, 10))

input = np.arange(1, 11, 1).astype(&quot;float32&quot;)
input = input.reshape(1, 10)

output = seq.forward(input)
print output

gradOutput = np.arange(1, 11, 1).astype(&quot;float32&quot;)
gradOutput = gradOutput.reshape(1, 10)

gradInput = seq.backward(input, gradOutput)
print gradInput
</code></pre>

<p>The output is,</p>
<pre><code>[array([[ 1.08462083, -2.03257799, -0.5400058 ,  0.27452484,  1.85562158,
         1.64338267,  2.45694995,  1.70170391, -2.12998056, -1.28924525]], dtype=float32)]
</code></pre>

<p>The gradInput is,</p>
<pre><code>
[array([[ 1.72007763,  1.64403224,  2.52977395, -1.00021958,  0.1134415 ,
         2.06711197,  2.29631734, -3.39587498,  1.01093054, -0.54482007]], dtype=float32)]
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Simple_Layers/merged-Simple_Layers/" class="btn btn-neutral float-right" title="Simple Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../../Model/FunctionalAPI/" class="btn btn-neutral" title="Functional API"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../../Model/FunctionalAPI/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Simple_Layers/merged-Simple_Layers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../../js/theme.js"></script>

</body>
</html>
