<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Non-linear Activations - BigDL Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/highlight.css">
  <link href="../../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Non-linear Activations";
    var mkdocs_page_input_path = "APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations.md";
    var mkdocs_page_url = "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/";
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> BigDL Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-setup-bigdl/">Setup BigDL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prepare-data/">Prepare your Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prediction/">Use BigDL for Prediction Only</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-train/">Train a Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-monitor/">Monitor the Training</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-tune/">Tuning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-advanced/">Advanced Usage</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Install/Deploy</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/use-pre-built/">Use Pre-built Package</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/build-src/">Build from Source</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/deploy-python/">Enable Python Support</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/running-on-EC2/">Running On EC2</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/resources/">Resources</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-api/">API Usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/install-via-pip/">Install via pip</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-no-pip/">Use Python without Pip</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model/SequentialModel/">Sequential Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model/FunctionalAPI/">Functional API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../Containers/merged-Containers/">Containers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Simple_Layers/merged-Simple_Layers/">Simple Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Convolution_Layers/merged-Convolution_Layers/">Convolution Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Pooling_Layers/merged-Pooling_Layers/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Linear_Layers/merged-Linear_Layers/">Linear Layers</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Non-linear Activations</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#softsign">SoftSign</a></li>
    

    <li class="toctree-l3"><a href="#relu6">ReLU6</a></li>
    

    <li class="toctree-l3"><a href="#tanhshrink">TanhShrink</a></li>
    

    <li class="toctree-l3"><a href="#softmax">SoftMax</a></li>
    

    <li class="toctree-l3"><a href="#prelu">PReLU</a></li>
    

    <li class="toctree-l3"><a href="#relu">ReLU</a></li>
    

    <li class="toctree-l3"><a href="#softmin">SoftMin</a></li>
    

    <li class="toctree-l3"><a href="#elu">ELU</a></li>
    

    <li class="toctree-l3"><a href="#softshrink">SoftShrink</a></li>
    

    <li class="toctree-l3"><a href="#sigmoid">Sigmoid</a></li>
    

    <li class="toctree-l3"><a href="#tanh">Tanh</a></li>
    

    <li class="toctree-l3"><a href="#softplus">SoftPlus</a></li>
    

    <li class="toctree-l3"><a href="#l1penalty">L1Penalty</a></li>
    

    <li class="toctree-l3"><a href="#hardshrink">HardShrink</a></li>
    

    <li class="toctree-l3"><a href="#rrelu">RReLU</a></li>
    

    <li class="toctree-l3"><a href="#hardtanh">HardTanh</a></li>
    

    <li class="toctree-l3"><a href="#leakyrelu">LeakyReLU</a></li>
    

    <li class="toctree-l3"><a href="#logsigmoid">LogSigmoid</a></li>
    

    <li class="toctree-l3"><a href="#logsoftmax">LogSoftMax</a></li>
    

    <li class="toctree-l3"><a href="#threshold">Threshold</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Embedding_Layers/merged-Embedding_Layers/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../MergeSplit_Layers/merged-MergeSplit_Layers/">Merge/Split Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Math-Layers/merged-Math-Layers/">Math Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Padding_Layers/merged-Padding_Layers/">Padding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Normalization_Layers/merged-Normalization_Layers/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Dropout_Layers/merged-Dropout_Layers/">Dropout Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Distance_Layers/merged-Distance_Layers/">Distance Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Recurrent_Layers/merged-Recurrent_Layers/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Losses/merged-Losses/">Losses</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Optimizers/DistriOptimizer/">Optimizer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Optim_Methods/merged-Optim_Methods/">Optimization Algorithms</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Triggers/">Triggers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/ResumeTraining/">Resume Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Initializers/merged-Initializers/">Initalizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Regularizers/merged-Regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Metrics/merged-Metrics/">Metrics</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Preprocessing/Transformer/">Transformer</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Load/Save Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/BigDLModel/">Load/Save a BigDL Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/TensorflowModel/">Load a Tensorflow Model</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/visualization-with-tensorboard/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../python-api-doc/">Python API Docs</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">BigDL Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Non-linear Activations</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="softsign">SoftSign</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val softSign = SoftSign()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">softSign = SoftSign()
</code></pre>

<p>SoftSign applies SoftSign function to the input tensor</p>
<p>SoftSign function: <code>f_i(x) = x_i / (1+|x_i|)</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val softSign = SoftSign()
val input = Tensor(3, 3).rand()

&gt; print(input)
0.6733504   0.7566517   0.43793806  
0.09683273  0.05829774  0.4567967   
0.20021072  0.11158377  0.31668025

&gt; print(softSign.forward(input))
0.40239656  0.4307352   0.30455974  
0.08828395  0.05508633  0.31356242  
0.16681297  0.10038269  0.24051417  
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]


</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
softSign=SoftSign()
&gt; softSign.forward(np.array([[1, 2, 4],[-1, -2, -4]]))
[array([[ 0.5       ,  0.66666669,  0.80000001],
       [-0.5       , -0.66666669, -0.80000001]], dtype=float32)]

</code></pre>

<h2 id="relu6">ReLU6</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = ReLU6(inplace = false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = ReLU6(inplace=False)
</code></pre>

<p>Same as ReLU except that the rectifying function f(x) saturates at x = 6 
ReLU6 is defined as:
<code>f(x) = min(max(0, x), 6)</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val module = ReLU6()

println(module.forward(Tensor.range(-2, 8, 1)))
</code></pre>

<p>Output is</p>
<pre><code>com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.0
0.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
6.0
6.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 11]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

module = ReLU6()

print(module.forward(np.arange(-2, 9, 1)))
</code></pre>

<p>Output is</p>
<pre><code>[array([ 0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  6.,  6.], dtype=float32)]
</code></pre>

<h2 id="tanhshrink">TanhShrink</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val tanhShrink = TanhShrink()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">tanhShrink = TanhShrink()
</code></pre>

<p>TanhShrink applies element-wise Tanh and Shrink function to the input</p>
<p>TanhShrink function : <code>f(x) = scala.math.tanh(x) - 1</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val tanhShrink = TanhShrink()
val input = Tensor(3, 3).rand()

&gt; print(input)
0.7056571   0.25239098  0.75746965  
0.89736927  0.31193605  0.23842576  
0.69492024  0.7512544   0.8386124   
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]

&gt; print(tanhShrink.forward(input))
0.09771085  0.0052260756    0.11788553  
0.18235475  0.009738684 0.004417494 
0.09378672  0.1153577   0.153539    
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
tanhShrink = TanhShrink()

&gt;  tanhShrink.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
[array([[ 0.23840582,  1.03597236,  2.00494528],
       [ 3.00067067,  4.0000906 ,  5.0000124 ],
       [ 6.00000191,  7.        ,  8.        ]], dtype=float32)]

</code></pre>

<h2 id="softmax">SoftMax</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = SoftMax()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = SoftMax()
</code></pre>

<p>Applies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the
elements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.
Softmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)
where shift = max_i(x_i).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val layer = SoftMax()
val input = Tensor(3)
input.apply1(_ =&gt; 1.0f * 10)
val gradOutput = Tensor(T(
1.0f,
0.0f,
0.0f
))
val output = layer.forward(input)
val gradient = layer.backward(input, gradOutput)
-&gt; print(output)
0.33333334
0.33333334
0.33333334
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
-&gt; print(gradient)
0.22222221
-0.11111112
-0.11111112
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
layer = SoftMax()
input = np.ones(3)*10
grad_output = np.array([1.0, 0.0, 0.0])
output = layer.forward(input)
gradient = layer.backward(input, grad_output)
-&gt; print output
[ 0.33333334  0.33333334  0.33333334]
-&gt; print gradient
[ 0.22222221 -0.11111112 -0.11111112]
</code></pre>

<h2 id="prelu">PReLU</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = PReLU(nOutputPlane = 0)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = PReLU(nOutputPlane=0)
</code></pre>

<p>Applies parametric ReLU, which parameter varies the slope of the negative part.</p>
<pre><code>PReLU: f(x) = max(0, x) + a * min(0, x)
</code></pre>

<p>nOutputPlane's default value is 0, that means using PReLU in shared version and has
only one parameters. nOutputPlane is the input map number(Default is 0).</p>
<p>Notice: Please don't use weight decay on this.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val module = PReLU(2)
val input = Tensor(2, 2, 3).randn()
val output = module.forward(input)

&gt; input
(1,.,.) =
-0.17810068 -0.69607687 0.25582042
-1.2140307  -1.5410945  1.0209005

(2,.,.) =
0.2826971   0.6370953   0.21471702
-0.16203058 -0.5643519  0.816576

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x3]

&gt; output
(1,.,.) =
-0.04452517 -0.17401922 0.25582042
-0.3035077  -0.38527364 1.0209005

(2,.,.) =
0.2826971   0.6370953   0.21471702
-0.040507644    -0.14108798 0.816576

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

module = PReLU(2)
input = np.random.randn(2, 2, 3)
output = module.forward(input)

&gt; input
[[[ 2.50596953 -0.06593339 -1.90273409]
  [ 0.2464341   0.45941315 -0.41977094]]

 [[-0.8584367   2.19389229  0.93136755]
  [-0.39209027  0.16507514 -0.35850447]]]

&gt; output
[array([[[ 2.50596952, -0.01648335, -0.47568351],
         [ 0.24643411,  0.45941314, -0.10494273]],

        [[-0.21460918,  2.19389224,  0.93136758],
         [-0.09802257,  0.16507514, -0.08962612]]], dtype=float32)]
</code></pre>

<h2 id="relu">ReLU</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val relu = ReLU(ip = false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">relu = ReLU(ip)
</code></pre>

<p>ReLU applies the element-wise rectified linear unit (ReLU) function to the input</p>
<p><code>ip</code> illustrate if the ReLU fuction is done on the origin input</p>
<pre><code>ReLU function : `f(x) = max(0, x)`
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val relu = ReLU(false)

val input = Tensor(3, 3).rand()
&gt; print(input)
0.13486342  0.8986828   0.2648762   
0.56467545  0.7727274   0.65959305  
0.01554346  0.9552375   0.2434533   
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]

&gt; print(relu.forward(input))
0.13486342  0.8986828   0.2648762   
0.56467545  0.7727274   0.65959305  
0.01554346  0.9552375   0.2434533   
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]


</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
relu = ReLU(False)
&gt; relu.forward(np.array([[-1, -2, -3], [0, 0, 0], [1, 2, 3]]))
[array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 1.,  2.,  3.]], dtype=float32)]

</code></pre>

<h2 id="softmin">SoftMin</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val sm = SoftMin()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">sm = SoftMin()
</code></pre>

<p>Applies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the
elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.
Softmin is defined as: f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)
where shift = max_i(-x_i).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.SoftMin
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val sm = SoftMin()
val input = Tensor(3, 3).range(1, 3 * 3)

val output = sm.forward(input)

val gradOutput = Tensor(3, 3).range(1, 3 * 3).apply1(x =&gt; (x / 10.0).toFloat)
val gradInput = sm.backward(input, gradOutput)

</code></pre>

<p>The output will be,</p>
<pre><code>output: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.66524094      0.24472848      0.09003057
0.66524094      0.24472848      0.09003057
0.66524094      0.24472848      0.09003057
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p>The gradInput will be,</p>
<pre><code>gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.02825874      -0.014077038    -0.014181711
0.028258756     -0.01407703     -0.01418171
0.028258756     -0.014077038    -0.014181707
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

sm = SoftMin()

input = np.arange(1, 10, 1).astype(&quot;float32&quot;)
input = input.reshape(3, 3)

output = sm.forward(input)
print output

gradOutput = np.arange(1, 10, 1).astype(&quot;float32&quot;)
gradOutput = np.vectorize(lambda t: t / 10)(gradOutput)
gradOutput = gradOutput.reshape(3, 3)

gradInput = sm.backward(input, gradOutput)
print gradInput

</code></pre>

<h2 id="elu">ELU</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = ELU(alpha = 1.0, inplace = false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = ELU(alpha=1.0, inplace=False)
</code></pre>

<p>Applies exponential linear unit (<code>ELU</code>), which parameter a varies the convergence value of the exponential function below zero:</p>
<p><code>ELU</code> is defined as:</p>
<pre><code>f(x) = max(0, x) + min(0, alpha * (exp(x) - 1))
</code></pre>

<p>The output dimension is always equal to input dimension.</p>
<p>For reference see <a href="http://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a>.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val xs = Tensor(4).randn()
println(xs)
println(ELU(4).forward(xs))
</code></pre>

<pre><code>1.0217569
-0.17189966
1.4164596
0.69361746
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]

1.0217569
-0.63174534
1.4164596
0.69361746
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.layer import *

xs = np.linspace(-3, 3, num=200)
go = np.ones(200)

def f(a):
    return ELU(a).forward(xs)[0]
def df(a):
    m = ELU(a)
    m.forward(xs)
    return m.backward(xs, go)[0]

plt.plot(xs, f(0.1), '-', label='fw ELU, alpha = 0.1')
plt.plot(xs, f(1.0), '-', label='fw ELU, alpha = 0.1')
plt.plot(xs, df(0.1), '-', label='dw ELU, alpha = 0.1')
plt.plot(xs, df(1.0), '-', label='dw ELU, alpha = 0.1')

plt.legend(loc='best', shadow=True, fancybox=True)
plt.show()

</code></pre>

<p><img alt="" src="../../../../../Image/ELU.png" /></p>
<h2 id="softshrink">SoftShrink</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = SoftShrink(lambda = 0.5)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = SoftShrink(the_lambda=0.5)
</code></pre>

<p>Apply the soft shrinkage function element-wise to the input Tensor</p>
<p>SoftShrinkage operator:</p>
<pre><code>       ⎧ x - lambda, if x &gt;  lambda
f(x) = ⎨ x + lambda, if x &lt; -lambda
       ⎩ 0, otherwise
</code></pre>

<p><strong>Parameters:</strong></p>
<p><strong>lambda</strong>     - a factor, default is 0.5</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.SoftShrink
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val activation = SoftShrink()
val input = Tensor(T(
  T(-1f, 2f, 3f),
  T(-2f, 3f, 4f),
  T(-3f, 4f, 5f)
))

val gradOutput = Tensor(T(
  T(3f, 4f, 5f),
  T(2f, 3f, 4f),
  T(1f, 2f, 3f)
))

val output = activation.forward(input)
val grad = activation.backward(input, gradOutput)

println(output)
-0.5    1.5 2.5
-1.5    2.5 3.5
-2.5    3.5 4.5
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

println(grad)
3.0 4.0 5.0
2.0 3.0 4.0
1.0 2.0 3.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="python">activation = SoftShrink()
input = np.array([
  [-1.0, 2.0, 3.0],
  [-2.0, 3.0, 4.0],
  [-3.0, 4.0, 5.0]
])

gradOutput = np.array([
  [3.0, 4.0, 5.0],
  [2.0, 3.0, 4.0],
  [1.0, 2.0, 5.0]
])

output = activation.forward(input)
grad = activation.backward(input, gradOutput)

print output
[[-0.5  1.5  2.5]
 [-1.5  2.5  3.5]
 [-2.5  3.5  4.5]]

print grad
[[ 3.  4.  5.]
 [ 2.  3.  4.]
 [ 1.  2.  5.]]
</code></pre>

<h2 id="sigmoid">Sigmoid</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Sigmoid()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Sigmoid()
</code></pre>

<p>Applies the Sigmoid function element-wise to the input Tensor,
thus outputting a Tensor of the same dimension.</p>
<p>Sigmoid is defined as: f(x) = 1 / (1 + exp(-x))</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val layer = new Sigmoid()
val input = Tensor(2, 3)
var i = 0
input.apply1(_ =&gt; {i += 1; i})
&gt; print(layer.forward(input))
0.7310586   0.880797    0.95257413  
0.98201376  0.9933072   0.9975274   
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *

layer = Sigmoid()
input = np.array([[1, 2, 3], [4, 5, 6]])
&gt;layer.forward(input)
array([[ 0.7310586 ,  0.88079703,  0.95257413],
       [ 0.98201376,  0.99330717,  0.99752742]], dtype=float32)
</code></pre>

<h2 id="tanh">Tanh</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val activation = Tanh()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">activation = Tanh()
</code></pre>

<p>Applies the Tanh function element-wise to the input Tensor,
thus outputting a Tensor of the same dimension.
Tanh is defined as</p>
<pre><code>f(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.Tanh
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val activation = Tanh()
val input = Tensor(T(
  T(1f, 2f, 3f),
  T(2f, 3f, 4f),
  T(3f, 4f, 5f)
))

val gradOutput = Tensor(T(
  T(3f, 4f, 5f),
  T(2f, 3f, 4f),
  T(1f, 2f, 3f)
))

val output = activation.forward(input)
val grad = activation.backward(input, gradOutput)

println(output)
0.7615942   0.9640276   0.9950548
0.9640276   0.9950548   0.9993293
0.9950548   0.9993293   0.9999092
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

println(grad)
1.259923    0.28260326  0.049329996
0.14130163  0.029597998 0.0053634644
0.009865999 0.0026817322    5.4466724E-4
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">activation = Tanh()
input = np.array([
  [1.0, 2.0, 3.0],
  [2.0, 3.0, 4.0],
  [3.0, 4.0, 5.0]
])

gradOutput = np.array([
  [3.0, 4.0, 5.0],
  [2.0, 3.0, 4.0],
  [1.0, 2.0, 5.0]
])

output = activation.forward(input)
grad = activation.backward(input, gradOutput)

print output
[[ 0.76159418  0.96402758  0.99505478]
 [ 0.96402758  0.99505478  0.99932933]
 [ 0.99505478  0.99932933  0.99990922]]

print grad
[[  1.25992298e+00   2.82603264e-01   4.93299961e-02]
 [  1.41301632e-01   2.95979977e-02   5.36346436e-03]
 [  9.86599922e-03   2.68173218e-03   9.07778740e-04]]
</code></pre>

<h2 id="softplus">SoftPlus</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = SoftPlus(beta = 1.0)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = SoftPlus(beta = 1.0)
</code></pre>

<p>Apply the SoftPlus function to an n-dimensional input tensor.
SoftPlus function: </p>
<pre><code>f_i(x) = 1/beta * log(1 + exp(beta * x_i))
</code></pre>

<ul>
<li>param beta Controls sharpness of transfer function</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor

val model = SoftPlus()
val input = Tensor(2, 3, 4).rand()
val output = model.forward(input)

scala&gt; println(input)
(1,.,.) =
0.9812126   0.7044107   0.0657767   0.9173636   
0.20853543  0.76482195  0.60774535  0.47837523  
0.62954164  0.56440496  0.28893307  0.40742245  

(2,.,.) =
0.18701692  0.7700966   0.98496467  0.8958407   
0.037015386 0.34626052  0.36459026  0.8460807   
0.051016055 0.6742781   0.14469075  0.07565566  

scala&gt; println(output)
(1,.,.) =
1.2995617   1.1061354   0.7265762   1.2535294   
0.80284095  1.1469617   1.0424956   0.9606715   
1.0566612   1.0146512   0.8480129   0.91746557  

(2,.,.) =
0.7910212   1.1505641   1.3022922   1.2381986   
0.71182615  0.88119024  0.8919668   1.203121    
0.7189805   1.0860726   0.7681072   0.7316903   

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

model = SoftPlus()
input = np.random.randn(2, 3, 4)
output = model.forward(input)

&gt;&gt;&gt; print(input)
[[[ 0.82634972 -0.09853824  0.97570235  1.84464617]
  [ 0.38466503  0.08963732  1.29438774  1.25204527]
  [-0.01910449 -0.19560752 -0.81769143 -1.06365733]]

 [[-0.56284365 -0.28473239 -0.58206869 -1.97350909]
  [-0.28303919 -0.59735361  0.73282102  0.0176838 ]
  [ 0.63439133  1.84904987 -1.24073643  2.13275833]]]
&gt;&gt;&gt; print(output)
[[[ 1.18935537  0.6450913   1.2955569   1.99141073]
  [ 0.90386271  0.73896986  1.53660071  1.50351918]
  [ 0.68364054  0.60011864  0.36564925  0.29653603]]

 [[ 0.45081255  0.56088102  0.44387865  0.1301229 ]
  [ 0.56160825  0.43842646  1.12523568  0.70202816]
  [ 1.0598278   1.99521446  0.2539995   2.24475574]]]
</code></pre>

<h2 id="l1penalty">L1Penalty</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val l1Penalty = L1Penalty(l1weight, sizeAverage = false, provideOutput = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">l1Penalty = L1Penalty( l1weight, size_average=False, provide_output=True)
</code></pre>

<p>L1Penalty adds an L1 penalty to an input 
For forward, the output is the same as input and a L1 loss of the latent state will be calculated each time
For backward, gradInput = gradOutput + gradLoss</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val l1Penalty = L1Penalty(1, true, true)
val input = Tensor(3, 3).rand()

&gt; print(input)
0.0370419   0.03080979  0.22083037  
0.1547358   0.018475588 0.8102709   
0.86393493  0.7081842   0.13717912  
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]


&gt; print(l1Penalty.forward(input))
0.0370419   0.03080979  0.22083037  
0.1547358   0.018475588 0.8102709   
0.86393493  0.7081842   0.13717912  
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]   

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
l1Penalty = L1Penalty(1, True, True)

&gt;&gt;&gt; l1Penalty.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
[array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.],
       [ 7.,  8.,  9.]], dtype=float32)]

</code></pre>

<h2 id="hardshrink">HardShrink</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = HardShrink(lambda = 0.5)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = HardShrink(the_lambda=0.5)
</code></pre>

<p>Applies the hard shrinkage function element-wise to the input Tensor. lambda is set to 0.5 by default.</p>
<p>HardShrinkage operator is defined as:</p>
<pre><code>       ⎧ x, if x &gt;  lambda
f(x) = ⎨ x, if x &lt; -lambda
       ⎩ 0, otherwise
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

import com.intel.analytics.bigdl.utils._

def randomn(): Float = RandomGenerator.RNG.uniform(-10, 10)
val input = Tensor(3, 4)
input.apply1(x =&gt; randomn().toFloat)

val layer = new HardShrink(8)
println(&quot;input:&quot;)
println(input)
println(&quot;output:&quot;)
println(layer.forward(input))
</code></pre>

<pre><code>input:
8.53746839798987    -2.25314284209162   2.838596091605723   0.7181660132482648  
0.8278933027759194  8.986027473583817   -3.6885232804343104 -2.4018199276179075 
-9.51015486381948   2.6402589259669185  5.438693333417177   -6.577442386187613  
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]
output:
8.53746839798987    0.0 0.0 0.0 
0.0 8.986027473583817   0.0 0.0 
-9.51015486381948   0.0 0.0 0.0 
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.layer import *

input = np.linspace(-5, 5, num=10)
layer = HardShrink(the_lambda=3.0)
print(&quot;input:&quot;)
print(input)
print(&quot;output: &quot;)
print(layer.forward(input))
</code></pre>

<pre><code>creating: createHardShrink
input:
[-5.         -3.88888889 -2.77777778 -1.66666667 -0.55555556  0.55555556
  1.66666667  2.77777778  3.88888889  5.        ]
output: 
[-5.         -3.88888884  0.          0.          0.          0.          0.
  0.          3.88888884  5.        ]

</code></pre>

<h2 id="rrelu">RReLU</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = RReLU[T](lower, upper, inPlace)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = RReLU(lower, upper, inPlace)
</code></pre>

<p>Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,
thus outputting a Tensor of the same dimension. Informally the RReLU is also known as 'insanity' layer.</p>
<p>RReLU is defined as: f(x) = max(0,x) + a * min(0, x) where a ~ U(l, u).</p>
<p>In training mode negative inputs are multiplied by a factor drawn from a uniform random
distribution U(l, u). In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean
factor a = (l + u) / 2.</p>
<p>By default, l = 1/8 and u = 1/3. If l == u a RReLU effectively becomes a LeakyReLU.</p>
<p>Regardless of operating in in-place mode a RReLU will internally allocate an input-sized noise tensor to store random factors for negative inputs.</p>
<p>The backward() operation assumes that forward() has been called before.</p>
<p>For reference see <a href="http://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolutional Network</a>.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor

val layer = RReLU[Float]()
layer.forward(Tensor[Float](T(1.0f, 2.0f, -1.0f, -2.0f)))
layer.backward(Tensor[Float](T(1.0f, 2.0f, -1.0f, -2.0f)),
Tensor[Float](T(0.1f, 0.2f, -0.1f, -0.2f)))
</code></pre>

<p>There's random factor. An output is like</p>
<pre><code>1.0
2.0
-0.24342789
-0.43175703
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]

0.1
0.2
-0.024342788
-0.043175705
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import RReLU
import numpy as np

layer = RReLU()
layer.forward(np.array([1.0, 2.0, -1.0, -2.0]))
layer.backward(np.array([1.0, 2.0, -1.0, -2.0]),
  np.array([0.1, 0.2, -0.1, -0.2]))
</code></pre>

<p>There's random factor. An output is like</p>
<pre><code>array([ 1.,  2., -0.15329693, -0.40423378], dtype=float32)

array([ 0.1, 0.2, -0.01532969, -0.04042338], dtype=float32)
</code></pre>

<h2 id="hardtanh">HardTanh</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val activation = HardTanh(
    minValue = -1,
    maxValue = 1,
    inplace = false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">activation = HardTanh(
    min_value=-1.0,
    max_value=1.0,
    inplace=False)
</code></pre>

<p>Applies non-linear function HardTanh to each element of input, HardTanh is defined:</p>
<pre><code>           ⎧  maxValue, if x &gt; maxValue
    f(x) = ⎨  minValue, if x &lt; minValue
           ⎩  x, otherwise
</code></pre>

<p><strong>Parameters:</strong></p>
<p><strong>minValue</strong> - minValue in f(x), default is -1.</p>
<p><strong>maxValue</strong> - maxValue in f(x), default is 1.</p>
<p><strong>inplace</strong>  - weather inplace update output from input. default is false.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.HardTanh
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val activation = HardTanh()
val input = Tensor(T(
  T(-1f, 2f, 3f),
  T(-2f, 3f, 4f),
  T(-3f, 4f, 5f)
))

val gradOutput = Tensor(T(
  T(3f, 4f, 5f),
  T(2f, 3f, 4f),
  T(1f, 2f, 3f)
))

val output = activation.forward(input)
val grad = activation.backward(input, gradOutput)

println(output)
-1.0    1.0 1.0
-1.0    1.0 1.0
-1.0    1.0 1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

println(grad)
0.0 0.0 0.0
0.0 0.0 0.0
0.0 0.0 0.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">activation = HardTanh()
input = np.array([
  [-1.0, 2.0, 3.0],
  [-2.0, 3.0, 4.0],
  [-3.0, 4.0, 5.0]
])

gradOutput = np.array([
  [3.0, 4.0, 5.0],
  [2.0, 3.0, 4.0],
  [1.0, 2.0, 5.0]
])

output = activation.forward(input)
grad = activation.backward(input, gradOutput)

print output
[[-1.  1.  1.]
 [-1.  1.  1.]
 [-1.  1.  1.]]

print grad
[[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]
</code></pre>

<h2 id="leakyrelu">LeakyReLU</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">layer = LeakyReLU(negval=0.01,inplace=false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = LeakyReLU(negval=0.01,inplace=False,bigdl_type=&quot;float&quot;)
</code></pre>

<p>It is a transfer module that applies LeakyReLU, which parameter
negval sets the slope of the negative part:
 LeakyReLU is defined as:
  f(x) = max(0, x) + negval * min(0, x)</p>
<ul>
<li>@param negval sets the slope of the negative partl, default is 0.01</li>
<li>@param inplace if it is true, doing the operation in-place without
                using extra state memory, default is false</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val layer = LeakyReLU(negval=0.01,inplace=false)
val input = Tensor(3, 2).rand(-1, 1)
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.6923256      -0.14086828
0.029539397     0.477964
0.5202874       0.10458552
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

layer.forward(input)
res7: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.006923256    -0.0014086828
0.029539397     0.477964
0.5202874       0.10458552
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = LeakyReLU(negval=0.01,inplace=False,bigdl_type=&quot;float&quot;)
input = np.random.rand(3, 2)
array([[ 0.19502378,  0.40498206],
       [ 0.97056004,  0.35643192],
       [ 0.25075111,  0.18904582]])

layer.forward(input)
array([[ 0.19502378,  0.40498206],
       [ 0.97056001,  0.35643193],
       [ 0.25075111,  0.18904583]], dtype=float32)
</code></pre>

<h2 id="logsigmoid">LogSigmoid</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val activation = LogSigmoid()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">activation = LogSigmoid()
</code></pre>

<p>This class is a activation layer corresponding to the non-linear function sigmoid function:</p>
<pre><code>f(x) = Log(1 / (1 + e ^ (-x)))
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.LogSigmoid
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val activation = LogSigmoid()
val input = Tensor(T(
  T(1f, 2f, 3f),
  T(2f, 3f, 4f),
  T(3f, 4f, 5f)
))

val gradOutput = Tensor(T(
  T(3f, 4f, 5f),
  T(2f, 3f, 4f),
  T(1f, 2f, 3f)
))

val output = activation.forward(input)
val grad = activation.backward(input, gradOutput)

println(output)
-0.3132617  -0.12692802 -0.04858735
-0.12692802 -0.04858735 -0.01814993
-0.04858735 -0.01814993 -0.0067153485
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

println(grad)
0.8068244   0.47681168  0.23712938
0.23840584  0.14227761  0.07194484
0.047425874 0.03597242  0.020078553
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">activation = LogSigmoid()
input = np.array([
  [1.0, 2.0, 3.0],
  [2.0, 3.0, 4.0],
  [3.0, 4.0, 5.0]
])

gradOutput = np.array([
  [3.0, 4.0, 5.0],
  [2.0, 3.0, 4.0],
  [1.0, 2.0, 5.0]
])

output = activation.forward(input)
grad = activation.backward(input, gradOutput)

print output
[[-0.31326169 -0.12692802 -0.04858735]
 [-0.12692802 -0.04858735 -0.01814993]
 [-0.04858735 -0.01814993 -0.00671535]]

print grad
[[ 0.80682439  0.47681168  0.23712938]
 [ 0.23840584  0.14227761  0.07194484]
 [ 0.04742587  0.03597242  0.03346425]]
</code></pre>

<h2 id="logsoftmax">LogSoftMax</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = LogSoftMax()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = LogSoftMax()
</code></pre>

<p>The LogSoftMax module applies a LogSoftMax transformation to the input data
which is defined as:</p>
<pre><code>f_i(x) = log(1 / a exp(x_i))
where a = sum_j[exp(x_j)]
</code></pre>

<p>The input given in <code>forward(input)</code> must be either
a vector (1D tensor) or matrix (2D tensor).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor

val model = LogSoftMax()
val input = Tensor(2, 5).rand()
val output = model.forward(input)

scala&gt; print(input)
0.4434036   0.64535594  0.7516194   0.11752353  0.5216674   
0.57294756  0.744955    0.62644184  0.0052207764    0.900162    
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x5]

scala&gt; print(output)
-1.6841899  -1.4822376  -1.3759742  -2.01007    -1.605926   
-1.6479948  -1.4759872  -1.5945004  -2.2157214  -1.3207803  
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = LogSoftMax()
input = np.random.randn(4, 10)
output = model.forward(input)

&gt;&gt;&gt; print(input)
[[ 0.10805365  0.11392282  1.31891713 -0.62910637 -0.80532589  0.57976863
  -0.44454368  0.26292944  0.8338328   0.32305099]
 [-0.16443839  0.12010763  0.62978233 -1.57224143 -2.16133614 -0.60932395
  -0.22722708  0.23268273  0.00313597  0.34585582]
 [ 0.55913444 -0.7560615   0.12170887  1.40628806  0.97614582  1.20417145
  -1.60619173 -0.54483025  1.12227399 -0.79976189]
 [-0.05540945  0.86954458  0.34586427  2.52004267  0.6998163  -1.61315173
  -0.76276874  0.38332142  0.66351792 -0.30111399]]

&gt;&gt;&gt; print(output)
[[-2.55674744 -2.55087829 -1.34588397 -3.2939074  -3.47012711 -2.08503246
  -3.10934472 -2.40187168 -1.83096838 -2.34175014]
 [-2.38306785 -2.09852171 -1.58884704 -3.79087067 -4.37996578 -2.82795334
  -2.44585633 -1.98594666 -2.21549344 -1.87277353]
 [-2.31549931 -3.63069534 -2.75292492 -1.46834576 -1.89848804 -1.67046237
  -4.48082542 -3.41946411 -1.75235975 -3.67439556]
 [-3.23354769 -2.30859375 -2.83227396 -0.6580956  -2.47832203 -4.79128981
  -3.940907   -2.79481697 -2.5146203  -3.47925234]]
</code></pre>

<h2 id="threshold">Threshold</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Threshold(threshold, value, ip)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Threshold(threshold, value, ip)
</code></pre>

<p>Thresholds each element of the input Tensor.
Threshold is defined as:</p>
<pre><code>     ⎧ x        if x &gt;= threshold
 y = ⎨ 
     ⎩ value    if x &lt;  threshold
</code></pre>

<ul>
<li>threshold: The value to threshold at</li>
<li>value: The value to replace with</li>
<li>ip: can optionally do the operation in-place</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val module = Threshold(1, 0.8)
val input = Tensor(2, 2, 2).randn()
val output = module.forward(input)

&gt; input
(1,.,.) =
2.0502799   -0.37522468
-1.2704345  -0.22533786

(2,.,.) =
1.1959263   1.6670992
-0.24333914 1.4424673

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]

&gt; output
(1,.,.) =
(1,.,.) =
2.0502799   0.8
0.8 0.8

(2,.,.) =
1.1959263   1.6670992
0.8 1.4424673

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

module = Threshold(1.0, 0.8)
input = np.random.randn(2, 2, 2)
output = module.forward(input)

&gt; input
[[[-0.43226865 -1.09160093]
  [-0.20280088  0.68196767]]

 [[ 2.32017942  1.00003307]
  [-0.46618767  0.57057167]]]

&gt; output
[array([[[ 0.80000001,  0.80000001],
        [ 0.80000001,  0.80000001]],

       [[ 2.32017946,  1.00003302],
        [ 0.80000001,  0.80000001]]], dtype=float32)]
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Embedding_Layers/merged-Embedding_Layers/" class="btn btn-neutral float-right" title="Embedding Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../Linear_Layers/merged-Linear_Layers/" class="btn btn-neutral" title="Linear Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../Linear_Layers/merged-Linear_Layers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Embedding_Layers/merged-Embedding_Layers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../../js/theme.js"></script>

</body>
</html>
