<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Merge/Split Layers - BigDL Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/highlight.css">
  <link href="../../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Merge/Split Layers";
    var mkdocs_page_input_path = "APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers.md";
    var mkdocs_page_url = "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/";
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> BigDL Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-setup-bigdl/">Setup BigDL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prepare-data/">Prepare your Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-prediction/">Use BigDL for Prediction Only</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-train/">Train a Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-monitor/">Monitor the Training</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-tune/">Tuning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/ug-advanced/">Advanced Usage</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Install/Deploy</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/use-pre-built/">Use Pre-built Package</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/build-src/">Build from Source</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/deploy-python/">Enable Python Support</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/running-on-EC2/">Running On EC2</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/resources/">Resources</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-api/">API Usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/install-via-pip/">Install via pip</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-no-pip/">Use Python without Pip</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model/SequentialModel/">Sequential Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model/FunctionalAPI/">Functional API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../Containers/merged-Containers/">Containers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Simple_Layers/merged-Simple_Layers/">Simple Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Convolution_Layers/merged-Convolution_Layers/">Convolution Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Pooling_Layers/merged-Pooling_Layers/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Linear_Layers/merged-Linear_Layers/">Linear Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Non-linear_Activations/merged-Non-linear_Activations/">Non-linear Activations</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Embedding_Layers/merged-Embedding_Layers/">Embedding Layers</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Merge/Split Layers</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#pack">Pack</a></li>
    

    <li class="toctree-l3"><a href="#mm">MM</a></li>
    

    <li class="toctree-l3"><a href="#cmaxtable">CMaxTable</a></li>
    

    <li class="toctree-l3"><a href="#splittable">SplitTable</a></li>
    

    <li class="toctree-l3"><a href="#dotproduct">DotProduct</a></li>
    

    <li class="toctree-l3"><a href="#csubtable">CSubTable</a></li>
    

    <li class="toctree-l3"><a href="#cdivtable">CDivTable</a></li>
    

    <li class="toctree-l3"><a href="#jointable">JoinTable</a></li>
    

    <li class="toctree-l3"><a href="#selecttable">SelectTable</a></li>
    

    <li class="toctree-l3"><a href="#narrowtable">NarrowTable</a></li>
    

    <li class="toctree-l3"><a href="#caddtable">CAddTable</a></li>
    

    <li class="toctree-l3"><a href="#cmultable">CMulTable</a></li>
    

    <li class="toctree-l3"><a href="#mv">MV</a></li>
    

    <li class="toctree-l3"><a href="#flattentable">FlattenTable</a></li>
    

    <li class="toctree-l3"><a href="#cmintable">CMinTable</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../Math-Layers/merged-Math-Layers/">Math Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Padding_Layers/merged-Padding_Layers/">Padding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Normalization_Layers/merged-Normalization_Layers/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Dropout_Layers/merged-Dropout_Layers/">Dropout Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Distance_Layers/merged-Distance_Layers/">Distance Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Recurrent_Layers/merged-Recurrent_Layers/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Losses/merged-Losses/">Losses</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Optimizers/DistriOptimizer/">Optimizer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Optim_Methods/merged-Optim_Methods/">Optimization Algorithms</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/Triggers/">Triggers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Optimizers/ResumeTraining/">Resume Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Initializers/merged-Initializers/">Initalizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Regularizers/merged-Regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../Metrics/merged-Metrics/">Metrics</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Preprocessing/Transformer/">Transformer</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Load/Save Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/BigDLModel/">Load/Save a BigDL Model</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Model_LoadSave/TensorflowModel/">Load a Tensorflow Model</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/visualization-with-tensorboard/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../python-api-doc/">Python API Docs</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../UserGuide/known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">BigDL Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Merge/Split Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="pack">Pack</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Pack(dim)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Pack(dim)
</code></pre>

<p>Pack is used to stack a list of n-dimensional tensors into one (n+1)-dimensional tensor.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val module = Pack(2)
val input1 = Tensor(2, 2).randn()
val input2 = Tensor(2, 2).randn()
val input = T()
input(1) = input1
input(2) = input2

val output = module.forward(input)

&gt; input
 {
    2: -0.8737048   -0.7337217
       0.7268678    -0.53470045
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]
    1: -1.3062215   -0.58756566
       0.8921608    -1.8087773
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]
 }


&gt; output
(1,.,.) =
-1.3062215  -0.58756566
-0.8737048  -0.7337217

(2,.,.) =
0.8921608   -1.8087773
0.7268678   -0.53470045

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

module = Pack(2)
input1 = np.random.randn(2, 2)
input2 = np.random.randn(2, 2)
input = [input1, input2]
output = module.forward(input)

&gt; input
[array([[ 0.92741416, -3.29826586],
       [-0.03147819, -0.10049306]]), array([[-0.27146461, -0.25729802],
       [ 0.1316149 ,  1.27620145]])]

&gt; output
array([[[ 0.92741418, -3.29826593],
        [-0.27146462, -0.25729802]],

       [[-0.03147819, -0.10049306],
        [ 0.13161489,  1.27620149]]], dtype=float32)
</code></pre>

<h2 id="mm">MM</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = MM(transA=false,transB=false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = MM(trans_a=False,trans_b=False)
</code></pre>

<p>MM is a module that performs matrix multiplication on two mini-batch inputs, producing one mini-batch.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.utils.T

val input = T(1 -&gt; Tensor(3, 3).randn(), 2 -&gt; Tensor(3, 3).randn())
val m1 = MM()
val output1 = m1.forward(input)
val m2 = MM(true,true)
val output2 = m2.forward(input)

scala&gt; print(input)
 {
        2: -0.62020904  -0.18690863     0.34132162
           -0.5359324   -0.09937895     0.86147165
           -2.6607985   -1.426654       2.3428898
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]
        1: -1.3087689   0.048720464     0.69583243
           -0.52055264  -1.5275089      -1.1569321
           0.28093573   -0.29353273     -0.9505267
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]
 }

scala&gt; print(output1)
-1.0658705      -0.7529337      1.225519
4.2198563       1.8996398       -4.204146
2.512235        1.3327343       -2.38396
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

scala&gt; print(output2)
1.0048954       0.99516183      4.8832207
0.15509865      -0.12717877     1.3618765
-0.5397563      -1.0767963      -2.4279075
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

input1=np.random.rand(3,3)
input2=np.random.rand(3,3)
input = [input1,input2]
print &quot;input is :&quot;,input
out = MM().forward(input)
print &quot;output is :&quot;,out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input is : [array([[ 0.13696046,  0.92653165,  0.73585328],
       [ 0.28167852,  0.06431783,  0.15710073],
       [ 0.21896166,  0.00780161,  0.25780671]]), array([[ 0.11232797,  0.17023931,  0.92430042],
       [ 0.86629537,  0.07630215,  0.08584417],
       [ 0.47087278,  0.22992833,  0.59257503]])]
creating: createMM
output is : [array([[ 1.16452789,  0.26320592,  0.64217824],
       [ 0.16133308,  0.08898225,  0.35897085],
       [ 0.15274818,  0.09714822,  0.3558259 ]], dtype=float32)]
</code></pre>

<h2 id="cmaxtable">CMaxTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = CMaxTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = CMaxTable()
</code></pre>

<p>CMaxTable is a module that takes a table of Tensors and outputs the max of all of them.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt; 
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.utils.T

val input1 = Tensor(3).randn()
val input2 =  Tensor(3).randn()
val input = T(input1, input2)
val m = CMaxTable()
val output = m.forward(input)
val gradOut = Tensor(3).randn()
val gradIn = m.backward(input,gradOut)

scala&gt; print(input)
 {
        2: -0.38613814
           0.74074316
           -1.753783
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
        1: -1.6037064
           -2.3297918
           -0.7160026
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
 }

scala&gt; print(output)
-0.38613814
0.74074316
-0.7160026
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]

scala&gt; print(gradOut)
-1.4526331
0.7070323
0.29294914
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]

scala&gt; print(gradIn)
 {
        2: -1.4526331
           0.7070323
           0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
        1: 0.0
           0.0
           0.29294914
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

input1 = np.random.rand(3)
input2 = np.random.rand(3)
print &quot;input is :&quot;,input1,input2

m = CMaxTable()
out = m.forward([input1,input2])
print &quot;output of m is :&quot;,out

grad_out = np.random.rand(3)
grad_in = m.backward([input1, input2],grad_out)
print &quot;grad input of m is :&quot;,grad_in
</code></pre>

<p>produces output:</p>
<pre><code class="python">input is : [ 0.48649797  0.22131348  0.45667796] [ 0.73207053  0.74290136  0.03169769]
creating: createCMaxTable
output of m is : [array([ 0.73207051,  0.74290138,  0.45667794], dtype=float32)]
grad input of m is : [array([ 0.        ,  0.        ,  0.86938971], dtype=float32), array([ 0.04140199,  0.4787094 ,  0.        ], dtype=float32)]
</code></pre>

<h2 id="splittable">SplitTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = SplitTable[T](dim)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = SplitTable(dim)
</code></pre>

<p>SplitTable takes a Tensor as input and outputs several tables,
splitting the Tensor along the specified dimension <code>dimension</code>. Please note
the dimension starts from 1.</p>
<p>The input to this layer is expected to be a tensor, or a batch of tensors;
when using mini-batch, a batch of sample tensors will be passed to the layer and
the user needs to specify the number of dimensions of each sample tensor in a
batch using <code>nInputDims</code>.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor

val layer = SplitTable[Float](2)
layer.forward(Tensor(T(
  T(1.0f, 2.0f, 3.0f),
  T(4.0f, 5.0f, 6.0f),
  T(7.0f, 8.0f, 9.0f)
)))
layer.backward(Tensor(T(
  T(1.0f, 2.0f, 3.0f),
  T(4.0f, 5.0f, 6.0f),
  T(7.0f, 8.0f, 9.0f)
)), T(
  Tensor[Float](T(0.1f, 0.2f, 0.3f)),
  Tensor[Float](T(0.4f, 0.5f, 0.6f)),
  Tensor[Float](T(0.7f, 0.8f, 0.9f))
))
</code></pre>

<p>Its output should be </p>
<pre><code> {
        2: 2.0
           5.0
           8.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
        1: 1.0
           4.0
           7.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
        3: 3.0
           6.0
           9.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
 }

0.1     0.4     0.7
0.2     0.5     0.8
0.3     0.6     0.9
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import SplitTable
import numpy as np

layer = SplitTable(2)
layer.forward(np.array([
  [1.0, 2.0, 3.0],
  [4.0, 5.0, 6.0],
  [7.0, 8.0, 9.0]
]))

layer.backward(np.array([
  [1.0, 2.0, 3.0],
  [4.0, 5.0, 6.0],
  [7.0, 8.0, 9.0]
]), [
  np.array([0.1, 0.2, 0.3]),
  np.array([0.4, 0.5, 0.6]),
  np.array([0.7, 0.8, 0.9])
])
</code></pre>

<p>Its output should be</p>
<pre><code>[
  array([ 1.,  4.,  7.], dtype=float32),
  array([ 2.,  5.,  8.], dtype=float32),
  array([ 3.,  6.,  9.], dtype=float32)
]

array([[ 0.1       ,  0.40000001,  0.69999999],
       [ 0.2       ,  0.5       ,  0.80000001],
       [ 0.30000001,  0.60000002,  0.89999998]], dtype=float32)
</code></pre>

<h2 id="dotproduct">DotProduct</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = DotProduct()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = DotProduct()
</code></pre>

<p>Outputs the dot product (similarity) between inputs</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.{T, Table}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val mlp = DotProduct()
val x = Tensor(3).fill(1f)
val y = Tensor(3).fill(2f)
println(&quot;input:&quot;)
println(x)
println(y)
println(&quot;output:&quot;)
println(mlp.forward(T(x, y)))
</code></pre>

<pre><code>input:
1.0
1.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
2.0
2.0
2.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
output:
6.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.layer import *

mlp = DotProduct()
x = np.array([1, 1, 1])
y = np.array([2, 2, 2])
print(&quot;input:&quot;)
print(x)
print(y)
print(&quot;output:&quot;)
print(mlp.forward([x, y]))

</code></pre>

<pre><code>creating: createDotProduct
input:
[1 1 1]
[2 2 2]
output:
[ 6.]
</code></pre>

<h2 id="csubtable">CSubTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = CSubTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = CSubTable()
</code></pre>

<p>Takes a sequence with two Tensor and returns the component-wise subtraction between them.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T

val model = CSubTable()
val input1 = Tensor(5).rand()
val input2 = Tensor(5).rand()
val input = T(input1, input2)
val output = model.forward(input)

scala&gt; print(input)
 {
    2: 0.29122078
       0.17347474
       0.14127742
       0.2249051
       0.12171601
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
    1: 0.6202152
       0.70417005
       0.21334995
       0.05191216
       0.4209623
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]

scala&gt; print(output)
0.3289944
0.5306953
0.072072536
-0.17299294
0.2992463
[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = CSubTable()
input1 = np.random.randn(5)
input2 = np.random.randn(5)
input = [input1, input2]
output = model.forward(input)
</code></pre>

<p>output is</p>
<pre><code>array([-1.15087152,  0.6169951 ,  2.41840839,  1.34374809,  1.39436531], dtype=float32)
</code></pre>

<h2 id="cdivtable">CDivTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = CDivTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = CDivTable()
</code></pre>

<p>Takes a table with two Tensor and returns the component-wise division between them.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val module = CDivTable()
val input = T(1 -&gt; Tensor(2,3).rand(), 2 -&gt; Tensor(2,3).rand())
input: com.intel.analytics.bigdl.utils.Table =
 {
        2: 0.802295     0.7113872       0.29395157
           0.6562403    0.06519115      0.20099664
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]
        1: 0.7435388    0.59126955      0.10225375
           0.46819785   0.10572237      0.9861797
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]
 }

module.forward(input)
res6: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.9267648       0.8311501       0.34785917
0.7134549       1.6217289       4.906449
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">module = CDivTable()
input = [np.array([[1, 2, 3],[4, 5, 6]]), np.array([[1, 4, 9],[6, 10, 3]])]
module.forward(input)
[array([
[ 1.,                   0.5     ,    0.33333334],
[ 0.66666669, 0.5       ,  2.        ]], dtype=float32)]
</code></pre>

<h2 id="jointable">JoinTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = JoinTable(dimension, nInputDims)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = JoinTable(dimension, n_input_dims)
</code></pre>

<p>It is a table module which takes a table of Tensors as input and
outputs a Tensor by joining them together along the dimension <code>dimension</code>.</p>
<p>The input to this layer is expected to be a tensor, or a batch of tensors;
when using mini-batch, a batch of sample tensors will be passed to the layer and
the user need to specify the number of dimensions of each sample tensor in the
batch using <code>nInputDims</code>.</p>
<p><strong>Parameters:</strong></p>
<p><strong>dimension</strong>  - to be join in this dimension</p>
<p><strong>nInputDims</strong> - specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.JoinTable
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val layer = JoinTable(2, 2)
val input1 = Tensor(T(
  T(
    T(1f, 2f, 3f),
    T(2f, 3f, 4f),
    T(3f, 4f, 5f))
))

val input2 = Tensor(T(
  T(
    T(3f, 4f, 5f),
    T(2f, 3f, 4f),
    T(1f, 2f, 3f))
))

val input = T(input1, input2)

val gradOutput = Tensor(T(
  T(
    T(1f, 2f, 3f, 3f, 4f, 5f),
    T(2f, 3f, 4f, 2f, 3f, 4f),
    T(3f, 4f, 5f, 1f, 2f, 3f)
)))

val output = layer.forward(input)
val grad = layer.backward(input, gradOutput)

println(output)
(1,.,.) =
1.0 2.0 3.0 3.0 4.0 5.0
2.0 3.0 4.0 2.0 3.0 4.0
3.0 4.0 5.0 1.0 2.0 3.0

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x6]

println(grad)
 {
    2: (1,.,.) =
       3.0  4.0 5.0
       2.0  3.0 4.0
       1.0  2.0 3.0

       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]
    1: (1,.,.) =
       1.0  2.0 3.0
       2.0  3.0 4.0
       3.0  4.0 5.0

       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]
 }
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = JoinTable(2, 2)
input1 = np.array([
 [
    [1.0, 2.0, 3.0],
    [2.0, 3.0, 4.0],
    [3.0, 4.0, 5.0]
  ]
])

input2 = np.array([
  [
    [3.0, 4.0, 5.0],
    [2.0, 3.0, 4.0],
    [1.0, 2.0, 3.0]
  ]
])

input = [input1, input2]

gradOutput = np.array([
  [
    [1.0, 2.0, 3.0, 3.0, 4.0, 5.0],
    [2.0, 3.0, 4.0, 2.0, 3.0, 4.0],
    [3.0, 4.0, 5.0, 1.0, 2.0, 3.0]
  ]
])

output = layer.forward(input)
grad = layer.backward(input, gradOutput)

print output
[[[ 1.  2.  3.  3.  4.  5.]
  [ 2.  3.  4.  2.  3.  4.]
  [ 3.  4.  5.  1.  2.  3.]]]

print grad
[array([[[ 1.,  2.,  3.],
        [ 2.,  3.,  4.],
        [ 3.,  4.,  5.]]], dtype=float32), array([[[ 3.,  4.,  5.],
        [ 2.,  3.,  4.],
        [ 1.,  2.,  3.]]], dtype=float32)]
</code></pre>

<h2 id="selecttable">SelectTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = SelectTable(index: Int)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = SelectTable(dimension)
</code></pre>

<p>Select one element from a table by a given index.
In Scala API, table is kind of like HashMap with one-base index as the key.
In python, table is a just a list.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.{T, Table}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val input = T(Tensor(2,3).randn(), Tensor(2,3).randn())

println(&quot;input: &quot;)
println(input)
println(&quot;output:&quot;)
println(SelectTable(1).forward(input)) // Select and output the first element of the input which shape is (2, 3)
println(SelectTable(2).forward(input)) // Select and output the second element of the input which shape is (2, 3)

</code></pre>

<pre><code>input: 
 {
    2: 2.005436370849835    0.09670211785545313 1.186779895312918   
       2.238415300857082    0.241626512721254   0.15765709974113828 
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
    1: 0.5668905654052705   -1.3205159007397167 -0.5431464848526197 
       -0.11582559521074104 0.7671830693813515  -0.39992781407893574    
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
 }
output:
0.5668905654052705  -1.3205159007397167 -0.5431464848526197 
-0.11582559521074104    0.7671830693813515  -0.39992781407893574    
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
2.005436370849835   0.09670211785545313 1.186779895312918   
2.238415300857082   0.241626512721254   0.15765709974113828 
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.layer import *

input = [np.random.random((2,3)), np.random.random((2, 1))]
print(&quot;input:&quot;)
print(input)
print(&quot;output:&quot;)
print(SelectTable(1).forward(input)) # Select and output the first element of the input which shape is (2, 3)
</code></pre>

<pre><code>input:
[array([[ 0.07185111,  0.26140439,  0.9437582 ],
       [ 0.50278191,  0.83923974,  0.06396735]]), array([[ 0.84955122],
       [ 0.16053703]])]
output:
creating: createSelectTable
[[ 0.07185111  0.2614044   0.94375819]
 [ 0.50278193  0.83923972  0.06396735]]

</code></pre>

<h2 id="narrowtable">NarrowTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val narrowTable = NarrowTable(offset, length = 1)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">narrowTable = NarrowTable(offset, length = 1)
</code></pre>

<p>NarrowTable takes a table as input and returns a subtable starting from index <code>offset</code> having <code>length</code> elements</p>
<p>Negative <code>length</code> means the last element is located at Abs|length| to the last element of input</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.utils.T
val narrowTable = NarrowTable(1, 1)

val input = T()
input(1.0) = Tensor(2, 2).rand()
input(2.0) = Tensor(2, 2).rand()
input(3.0) = Tensor(2, 2).rand()
&gt; print(input)
 {
    2.0: 0.27686104 0.9040761   
         0.75969505 0.8008061   
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]
    1.0: 0.94122535 0.46173728  
         0.43302807 0.1670979   
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]
    3.0: 0.43944374 0.49336782  
         0.7274511  0.67777634  
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]
 }
&gt;  print(narrowTable.forward(input))
 {
    1: 0.94122535   0.46173728  
       0.43302807   0.1670979   
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
narrowTable = NarrowTable(1, 1)
&gt; narrowTable.forward([np.array([1, 2, 3]), np.array([4, 5, 6])])
[array([ 1.,  2.,  3.], dtype=float32)]

</code></pre>

<h2 id="caddtable">CAddTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = CAddTable(inplace = false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = CAddTable(inplace=False)
</code></pre>

<p>CAddTable merges the input tensors in the input table by element-wise adding. The input table is actually an array of tensor with same size.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._

val mlp = Sequential()
mlp.add(ConcatTable().add(Identity()).add(Identity()))
mlp.add(CAddTable())

println(mlp.forward(Tensor.range(1, 3, 1)))
</code></pre>

<p>Output is</p>
<pre><code>com.intel.analytics.bigdl.nn.abstractnn.Activity =
2.0
4.0
6.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

mlp = Sequential()
mlp.add(ConcatTable().add(Identity()).add(Identity()))
mlp.add(CAddTable())

print(mlp.forward(np.arange(1, 4, 1)))
</code></pre>

<p>Output is</p>
<pre><code>[array([ 2.,  4.,  6.], dtype=float32)]
</code></pre>

<h2 id="cmultable">CMulTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = CMulTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = CMulTable()
</code></pre>

<p>Takes a sequence of Tensors and outputs the multiplication of all of them.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T

val model = CMulTable()
val input1 = Tensor(5).rand()
val input2 = Tensor(5).rand()
val input = T(input1, input2)
val output = model.forward(input)

scala&gt; print(input)
 {
    2: 0.13224044
       0.5460452
       0.33032498
       0.6317603
       0.6665052
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
    1: 0.28694472
       0.45169437
       0.36891535
       0.9126049
       0.41318864
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
 }

scala&gt; print(output)
0.037945695
0.24664554
0.12186196
0.57654756
0.27539238
[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = CMulTable()
input1 = np.random.randn(5)
input2 = np.random.randn(5)
input = [input1, input2]
output = model.forward(input)

&gt;&gt;&gt; print(input)
[array([ 0.28183274, -0.6477487 , -0.21279841,  0.22725124,  0.54748552]), array([-0.78673028, -1.08337196, -0.62710066,  0.37332587, -1.40708162])]

&gt;&gt;&gt; print(output)
[-0.22172636  0.70175284  0.13344601  0.08483877 -0.77035683]
</code></pre>

<h2 id="mv">MV</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = MV(trans = false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = MV(trans=False)
</code></pre>

<p>It is a module to perform matrix vector multiplication on two mini-batch inputs, producing a mini-batch.</p>
<p><code>trans</code> means whether make matrix transpose before multiplication.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.utils.T

val module = MV()

println(module.forward(T(Tensor.range(1, 12, 1).resize(2, 2, 3), Tensor.range(1, 6, 1).resize(2, 3))))
</code></pre>

<p>Output is</p>
<pre><code>com.intel.analytics.bigdl.tensor.Tensor[Float] =
14.0    32.0
122.0   167.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">module = MV()

print(module.forward([np.arange(1, 13, 1).reshape(2, 2, 3), np.arange(1, 7, 1).reshape(2, 3)]))
</code></pre>

<p>Output is</p>
<pre><code>[array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]
</code></pre>

<h2 id="flattentable">FlattenTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = FlattenTable()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = FlattenTable()
</code></pre>

<p>FlattenTable takes an arbitrarily deep table of Tensors (potentially nested) as input and a table of Tensors without any nested table will be produced</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val module = FlattenTable()
val t1 = Tensor(3).randn()
val t2 = Tensor(3).randn()
val t3 = Tensor(3).randn()
val input = T(t1, T(t2, T(t3)))

val output = module.forward(input)

&gt; input
 {
    2:  {
        2:  {
            1: 0.5521984
               -0.4160644
               -0.698762
               [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
            }
        1: -1.7380241
           0.60336906
           -0.8751049
           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
        }
    1: 1.0529885
       -0.792229
       0.8395628
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
 }


&gt; output
{
    2: -1.7380241
       0.60336906
       -0.8751049
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
    1: 1.0529885
       -0.792229
       0.8395628
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
    3: 0.5521984
       -0.4160644
       -0.698762
       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

module = Sequential()
# this will create a nested table
nested = ConcatTable().add(Identity()).add(Identity())
module.add(nested).add(FlattenTable())
t1 = np.random.randn(3)
t2 = np.random.randn(3)
input = [t1, t2]
output = module.forward(input)

&gt; input
[array([-2.21080689, -0.48928043, -0.26122161]), array([-0.8499716 ,  1.63694575, -0.31109292])]

&gt; output
[array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),
 array([-0.84997159,  1.63694572, -0.31109291], dtype=float32),
 array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),
 array([-0.84997159,  1.63694572, -0.31109291], dtype=float32)]

</code></pre>

<h2 id="cmintable">CMinTable</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = CMinTable[T]()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = CMinTable()
</code></pre>

<p>CMinTable takes a bunch of tensors as inputs. These tensors must have
same shape. This layer will merge them by doing an element-wise comparision
and use the min value.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor

val layer = CMinTable[Float]()
layer.forward(T(
  Tensor[Float](T(1.0f, 5.0f, 2.0f)),
  Tensor[Float](T(3.0f, 4.0f, -1.0f)),
  Tensor[Float](T(5.0f, 7.0f, -5.0f))
))
layer.backward(T(
  Tensor[Float](T(1.0f, 5.0f, 2.0f)),
  Tensor[Float](T(3.0f, 4.0f, -1.0f)),
  Tensor[Float](T(5.0f, 7.0f, -5.0f))
), Tensor[Float](T(0.1f, 0.2f, 0.3f)))
</code></pre>

<p>Its output should be</p>
<pre><code>1.0
4.0
-5.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]

{
  2: 0.0
     0.2
     0.0
     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
  1: 0.1
     0.0
     0.0
     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
  3: 0.0
     0.0
     0.3
  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]
}
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import CMinTable
import numpy as np

layer = CMinTable()
layer.forward([
  np.array([1.0, 5.0, 2.0]),
  np.array([3.0, 4.0, -1.0]),
  np.array([5.0, 7.0, -5.0])
])

layer.backward([
  np.array([1.0, 5.0, 2.0]),
  np.array([3.0, 4.0, -1.0]),
  np.array([5.0, 7.0, -5.0])
], np.array([0.1, 0.2, 0.3]))

</code></pre>

<p>Its output should be</p>
<pre><code>array([ 1.,  4., -5.], dtype=float32)

[array([ 0.1, 0., 0.], dtype=float32),
array([ 0., 0.2, 0.], dtype=float32),
array([ 0., 0., 0.30000001], dtype=float32)]

</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Math-Layers/merged-Math-Layers/" class="btn btn-neutral float-right" title="Math Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../Embedding_Layers/merged-Embedding_Layers/" class="btn btn-neutral" title="Embedding Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../Embedding_Layers/merged-Embedding_Layers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Math-Layers/merged-Math-Layers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../../js/theme.js"></script>

</body>
</html>
