<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Recurrent Layers - BigDL Project</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/highlight.css">
  <link href="../../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Recurrent Layers";
    var mkdocs_page_input_path = "APIdocs/Layers/Recurrent-Layers/merged-Recurrent-Layers.md";
    var mkdocs_page_url = "/APIdocs/Layers/Recurrent-Layers/merged-Recurrent-Layers/";
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> BigDL Project</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../release/">Releases</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../getting-started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../UserGuide/install/">Install</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/run/">Run</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/examples/">Examples</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../UserGuide/resources/">More Resources</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Python Support</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-install/">Install</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-run/">Run</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-examples/">Examples</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/python-resources/">More Examples and Tutorials</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../PythonSupport/tensorflow-support/">Tensorflow Support</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Programming Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../scaladoc/">Scala Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../python-api-doc/">Python API Docs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Data/merged-Data/">Data</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Model</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../../Model/Sequential/">Sequential Model</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../../Model/Functional/">Functional API</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../../Model/ModuleAPI/">Module API</a>
                </li>
    </ul>
                </li>
                <li class=" current">
                    
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../Containers/merged-Containers/">Containers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Simple-Layers/merged-Simple-Layers/">Simple Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Convolution-Layers/merged-Convolution-Layers/">Convolution Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Pooling-Layers/merged-Pooling-Layers/">Pooling Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Linear-Layers/merged-Linear-Layers/">Linear Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Non-linear-Activations/merged-Non-linear-Activations/">Non-linear Activations</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Embedding-Layers/merged-Embedding-Layers/">Embedding Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../MergeSplit-Layers/merged-MergeSplit-Layers/">Merge/Split Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Math-Layers/merged-Math-Layers/">Math Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Padding-Layers/merged-Padding-Layers/">Padding Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Normalization-Layers/merged-Normalization-Layers/">Normalization Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Dropout-Layers/merged-Dropout-Layers/">Dropout Layers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Distance-Layers/merged-Distance-Layers/">Distance Layers</a>
                </li>
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">Recurrent Layers</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#rnn">RNN</a></li>
    

    <li class="toctree-l4"><a href="#recurrent">Recurrent</a></li>
    

    <li class="toctree-l4"><a href="#birecurrent">BiRecurrent</a></li>
    

    <li class="toctree-l4"><a href="#lstmpeephole">LSTMPeephole</a></li>
    

    <li class="toctree-l4"><a href="#gru">GRU</a></li>
    

    <li class="toctree-l4"><a href="#lstm">LSTM</a></li>
    

    </ul>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../Utilities/merged-Utilities/">Utilities</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../../Losses/merged-Losses/">Losses</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Initializers/merged-Initializers/">Initalizers</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../Regularizers/merged-Regularizers/">Regularizers</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Optimization</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../../Optimizers/DistriOptimizer/">Optimizer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../../Optimizers/Optim-Methods/merged-Optim-Methods/">Optimization Algorithms</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../../Optimizers/Triggers/">Triggers</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../../Optimizers/ResumeTraining/">Resume Training</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../../Metrics/merged-Metrics/">Metrics</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../ProgrammingGuide/visualization/">Visualization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../MLPipeline/merged-MLPipeline/">MLPipeline</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../powered-by/">Powered by</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../../known-issues/">Known Issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">BigDL Project</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Latest Docs</a> &raquo;</li>
    
      
        
          <li>Programming Guide &raquo;</li>
        
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Recurrent Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/intel-analytics/BigDL/"> Fork on GitHub </a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="rnn">RNN</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val rnnCell = RnnCell[Double](inputSize, hiddenSize, activation, wRegularizer, uRegularizer, bRegularizer)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">rnnCell = RnnCell(input_size, hidden_size, Tanh(), w_regularizer, u_regularizer, b_regularizer)
</code></pre>

<p>Implementation of vanilla recurrent neural network cell
i2h: weight matrix of input to hidden units
h2h: weight matrix of hidden units to themselves through time
The updating is defined as:
h_t = f(i2h * x_t + h2h * h_{t-1})</p>
<p><strong>Parameters:</strong>
<em> <strong>inputSize</strong> - input size. Default: 4
</em> <strong>hiddenSize</strong> - hidden layer size. Default: 3
<em> <strong>activation</strong> - activation function f for non-linearity
</em> <strong>wRegularizer</strong> - instance of <code>Regularizer</code>(eg. L1 or L2 regularization), applied to the input weights matrices. Default: null
<em> <strong>uRegularizer</strong> - instance of <code>Regularizer</code>(eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null
</em> <strong>bRegularizer</strong> - instance of <code>Regularizer</code>(eg. L1 or L2 regularization), applied to the bias. Default: null</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val hiddenSize = 2
val inputSize = 2
val outputSize = 2
val seqLength = 2
val input = Tensor(T(
  T(1.0f, 2.0f),
  T(2.0f, 3.0f)
)).resize(Array(1, seqLength, inputSize))
val gradOutput = Tensor(T(
  T(2.0f, 3.0f),
  T(4.0f, 5.0f)
)).resize(Array(1, seqLength, inputSize))
val rec = Recurrent()

val model = Sequential()
    .add(rec.add(RnnCell(inputSize, hiddenSize, Tanh())))
    .add(TimeDistributed(Linear(hiddenSize, outputSize)))
val output = model.forward(input)
val gradient = model.backward(input, gradOutput)
-&gt; print(output)
# There's random factor. An output could be
(1,.,.) =
0.41442442      0.1663357       
0.5339842       0.57332826      

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]
-&gt; print(gradient)
# There's random factor. An output could be
(1,.,.) =
1.1512008       2.181274        
-0.4805725      1.6620052       

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
hidden_size = 2
input_size = 2
output_size = 2
seq_length = 2
input = np.array([[
  [1.0, 2.0],
  [2.0, 3.0]
]])
grad_output = np.array([[
  [2.0, 3.0],
  [4.0, 5.0]
]])
rec = Recurrent()

model = Sequential() \
    .add(rec.add(RnnCell(input_size, hidden_size, Tanh()))) \
    .add(TimeDistributed(Linear(hidden_size, output_size)))
output = model.forward(input)
gradient = model.backward(input, grad_output)
-&gt; print output
# There's random factor. An output could be
[[[-0.67860311  0.80307233]
  [-0.77462083  0.97191858]]]

-&gt; print gradient
# There's random factor. An output could be
[[[-0.90771425  1.24791598]
  [-0.70141178  0.97821164]]]
</code></pre>

<h2 id="recurrent">Recurrent</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = Recurrent()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = Recurrent()
</code></pre>

<p>Recurrent module is a container of rnn cells. Different types of rnn cells can be added using add() function.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.RandomGenerator.RNG
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val hiddenSize = 4
val inputSize = 5
val module = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))
val input = Tensor(Array(1, 5, inputSize))
for (i &lt;- 1 to 5) {
  val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0)*inputSize).toInt
  input.setValue(1, i, rdmInput, 1.0f)
}

val output = module.forward(input)

&gt; input
(1,.,.) =
0.0 1.0 0.0 0.0 0.0
0.0 1.0 0.0 0.0 0.0
0.0 0.0 1.0 0.0 0.0
0.0 1.0 0.0 0.0 0.0
0.0 0.0 1.0 0.0 0.0

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x5]

&gt; output
(1,.,.) =
-0.44992247 -0.50529593 -0.033753205    -0.29562786
-0.19734861 -0.5647412  0.07520321  -0.35515767
-0.6771096  -0.4985356  -0.5806829  -0.47552463
-0.06949129 -0.53153497 0.11510986  -0.34098053
-0.71635246 -0.5226476  -0.5929389  -0.46533492

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

hiddenSize = 4
inputSize = 5
module = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))
input = np.zeros((1, 5, 5))
input[0][0][4] = 1
input[0][1][0] = 1
input[0][2][4] = 1
input[0][3][3] = 1
input[0][4][0] = 1

output = module.forward(input)

&gt; output
[array([[[ 0.7526533 ,  0.29162994, -0.28749418, -0.11243925],
         [ 0.33291328, -0.07243762, -0.38017112,  0.53216213],
         [ 0.83854133,  0.07213539, -0.34503224,  0.33690596],
         [ 0.44095358,  0.27467242, -0.05471399,  0.46601957],
         [ 0.451913  , -0.33519334, -0.61357468,  0.56650752]]], dtype=float32)]
</code></pre>

<h2 id="birecurrent">BiRecurrent</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = BiRecurrent(merge=null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = BiRecurrent(merge=None,bigdl_type=&quot;float&quot;)
</code></pre>

<p>This layer implement a bidirectional recurrent neural network
 * @param merge concat or add the output tensor of the two RNNs. Default is add</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val module = BiRecurrent(CAddTable())
.add(RnnCell(6, 4, Sigmoid()))
val input = Tensor(Array(1, 2, 6)).rand()
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
0.55511624      0.44330198      0.9025551       0.26096714      0.3434667       0.20060952
0.24903035      0.24026379      0.89252585      0.23025699      0.8131796       0.4013688

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x6]

module.forward(input)
res10: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
1.3577285       0.8861933       0.52908427      0.86278
1.2850789       0.82549953      0.5560188       0.81468254

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">module = BiRecurrent(CAddTable()).add(RnnCell(6, 4, Sigmoid()))
input = np.random.rand(1, 2, 6)
array([[[ 0.75637438,  0.2642816 ,  0.61973312,  0.68565282,  0.73571443,
          0.17167681],
        [ 0.16439321,  0.06853251,  0.42257202,  0.42814042,  0.15706152,
          0.57866659]]])

module.forward(input)
array([[[ 0.69091094,  0.97150528,  0.9562254 ,  1.14894259],
        [ 0.83814102,  1.11358368,  0.96752423,  1.00913286]]], dtype=float32)
</code></pre>

<h2 id="lstmpeephole">LSTMPeephole</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = LSTMPeephole(
  inputSize = 4,
  hiddenSize = 3,
  p = 0.0,
  wRegularizer = null,
  uRegularizer = null,
  bRegularizer = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = LSTMPeephole(
  input_size,
  hidden_size,
  p=0.0,
  wRegularizer=None,
  uRegularizer=None,
  bRegularizer=None)
</code></pre>

<p>Long Short Term Memory architecture with peephole.
Ref. A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)
B. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf
C. http://arxiv.org/pdf/1503.04069v1.pdf
D. https://github.com/wojzaremba/lstm</p>
<ul>
<li>param inputSize the size of each input vector</li>
<li>param hiddenSize Hidden unit size in the LSTM</li>
<li>param  p is used for [[Dropout]] probability. For more details about
           RNN dropouts, please refer to
           [RnnDrop: A Novel Dropout for RNNs in ASR]
           (http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)
           [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]
           (https://arxiv.org/pdf/1512.05287.pdf)</li>
<li>param wRegularizer: instance of [[Regularizer]]
                   (eg. L1 or L2 regularization), applied to the input weights matrices.</li>
<li>param uRegularizer: instance [[Regularizer]]
          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.</li>
<li>param bRegularizer: instance of [[Regularizer]]
          applied to the bias.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.RandomGenerator._

val hiddenSize = 4
val inputSize = 6
val outputSize = 5
val seqLength = 5
val batchSize = 1

val input = Tensor(Array(batchSize, seqLength, inputSize))
for (b &lt;- 1 to batchSize) {
  for (i &lt;- 1 to seqLength) {
    val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0) * inputSize).toInt
    input.setValue(b, i, rdmInput, 1.0f)
  }
}

val rec = Recurrent(hiddenSize)
val model = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))
val output = model.forward(input).toTensor

scala&gt; print(input)
(1,.,.) =
1.0 0.0 0.0 0.0 0.0 0.0 
0.0 0.0 0.0 0.0 0.0 1.0 
0.0 1.0 0.0 0.0 0.0 0.0 
0.0 0.0 0.0 0.0 0.0 1.0 
1.0 0.0 0.0 0.0 0.0 0.0 

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x6]

scala&gt; print(output)
(1,.,.) =
0.34764957  -0.31453514 -0.45646006 -0.42966008 -0.13651063 
0.3624894   -0.2926056  -0.4347164  -0.40951455 -0.1775867  
0.33391106  -0.29304913 -0.4748538  -0.45285955 -0.14919288 
0.35499972  -0.29385415 -0.4419502  -0.42135617 -0.17544147 
0.32911295  -0.30237123 -0.47175884 -0.4409852  -0.15733294 

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
import numpy as np

hiddenSize = 4
inputSize = 6
outputSize = 5
seqLength = 5
batchSize = 1

input = np.random.randn(batchSize, seqLength, inputSize)
rec = Recurrent(hiddenSize)
model = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))
output = model.forward(input)

&gt;&gt;&gt; print(input)
[[[ 0.73624017 -0.91135209 -0.30627796 -1.07902111 -1.13549159  0.52868762]
  [-0.07251559 -0.45596589  1.64020513  0.53218623  1.37993166 -0.47724947]
  [-1.24958366 -1.22220259 -0.52454306  0.17382396  1.77666173 -1.2961758 ]
  [ 0.45407533  0.82944329  0.02155243  1.82168093 -0.06022129  2.23823013]
  [ 1.09100802  0.28555387 -0.94312648  0.55774033 -0.54895792  0.79885853]]]

&gt;&gt;&gt; print(output)
[[[ 0.4034881  -0.26156989  0.46799076  0.06283229  0.11794794]
  [ 0.37359846 -0.17925361  0.31623816  0.06038529  0.10813089]
  [ 0.34150451 -0.16565879  0.25264332  0.1187657   0.05118144]
  [ 0.40773875 -0.2028828   0.24765283  0.0986848   0.12132661]
  [ 0.40263647 -0.22403356  0.38489845  0.04720671  0.1686969 ]]]
</code></pre>

<h2 id="gru">GRU</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val gru = GRU(inputSize, outputSize, p, wRegularizer, uRegularizer, bRegularizer)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">gru = GRU(inputSize, outputSize, p, w_regularizer, u_regularizer, b_regularizer)
</code></pre>

<p>Gated Recurrent Units architecture. The first input in sequence uses zero value for cell and hidden state.</p>
<p>Ref.
 1. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/
 2. https://github.com/Element-Research/rnn/blob/master/GRU.lua</p>
<p><strong>Parameters:</strong>
<em> <strong>inputSize</strong> - the size of each input vector
</em> <strong>outputSize</strong> - hidden unit size in GRU
<em> <strong>p</strong> - is used for [[Dropout]] probability. For more details about
          RNN dropouts, please refer to
           <a href="http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf">RnnDrop: A Novel Dropout for RNNs in ASR</a>
            and <a href="https://arxiv.org/pdf/1512.05287.pdf">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a>. Default: 0.0
</em> <strong>wRegularizer</strong> - instance of <code>Regularizer</code>(eg. L1 or L2 regularization), applied to the input weights matrices. Default: null
<em> <strong>uRegularizer</strong> - instance of <code>Regularizer</code>(eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null
</em> <strong>bRegularizer</strong> - instance of <code>Regularizer</code>(eg. L1 or L2 regularization), applied to the bias. Default: null</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val hiddenSize = 2
val inputSize = 2
val outputSize = 2
val seqLength = 2
val input = Tensor(T(
  T(1.0f, 2.0f),
  T(2.0f, 3.0f)
)).resize(Array(1, seqLength, inputSize))
val gradOutput = Tensor(T(
  T(2.0f, 3.0f),
  T(4.0f, 5.0f)
)).resize(Array(1, seqLength, inputSize))
val rec = Recurrent()

val model = Sequential()
    .add(rec.add(GRU(inputSize, hiddenSize)))
    .add(TimeDistributed(Linear(hiddenSize, outputSize)))
val output = model.forward(input)
val gradient = model.backward(input, gradOutput)

-&gt; print(output)
# There's random factor. An output could be
(1,.,.) =
0.3833429       0.0082434565    
-0.041063666    -0.08152798     

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]


-&gt; print(gradient)
# There's random factor. An output could be
(1,.,.) =
-0.7684499      -0.49320614     
-0.98002595     -0.47857404     

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
hidden_size = 2
input_size = 2
output_size = 2
seq_length = 2
input = np.array([[
  [1.0, 2.0],
  [2.0, 3.0]
]])
grad_output = np.array([[
  [2.0, 3.0],
  [4.0, 5.0]
]])
rec = Recurrent()

model = Sequential() \
    .add(rec.add(GRU(input_size, hidden_size))) \
    .add(TimeDistributed(Linear(hidden_size, output_size)))
output = model.forward(input)
gradient = model.backward(input, grad_output)
-&gt; print output
# There's random factor. An output could be
[[[ 0.27857888  0.20263115]
  [ 0.29470384  0.22594413]]]
-&gt; print gradient
[[[-0.32956457  0.27405274]
  [-0.32718879  0.32963118]]]
</code></pre>

<h2 id="lstm">LSTM</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val lstm = LSTM(inputSize, hiddenSize)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">lstm = LSTM(input_size, hidden_size)
</code></pre>

<p>Long Short Term Memory architecture.</p>
<p>Ref:</p>
<ol>
<li>http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)</li>
<li>http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf</li>
<li>http://arxiv.org/pdf/1503.04069v1.pdf</li>
<li>https://github.com/wojzaremba/lstm</li>
</ol>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.optim.SGD
import com.intel.analytics.bigdl.utils.RandomGenerator._
import com.intel.analytics.bigdl.tensor.{Storage, Tensor}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val hiddenSize = 4
val inputSize = 6
val outputSize = 5
val seqLength = 5
val seed = 100

RNG.setSeed(seed)
val input = Tensor(Array(1, seqLength, inputSize))
val labels = Tensor(Array(1, seqLength))
for (i &lt;- 1 to seqLength) {
  val rdmLabel = Math.ceil(RNG.uniform(0, 1) * outputSize).toInt
  val rdmInput = Math.ceil(RNG.uniform(0, 1) * inputSize).toInt
  input.setValue(1, i, rdmInput, 1.0f)
  labels.setValue(1, i, rdmLabel)
}

println(input)
val rec = Recurrent(hiddenSize)
val model = Sequential().add(
  rec.add(
      LSTM(inputSize, hiddenSize))).add(
        TimeDistributed(Linear(hiddenSize, outputSize)))

val criterion = TimeDistributedCriterion(
  CrossEntropyCriterion(), false)

val sgd = new SGD(learningRate=0.1, learningRateDecay=5e-7, weightDecay=0.1, momentum=0.002)

val (weight, grad) = model.getParameters()

val output = model.forward(input).toTensor
val _loss = criterion.forward(output, labels)
model.zeroGradParameters()
val gradInput = criterion.backward(output, labels)
model.backward(input, gradInput)

def feval(x: Tensor[Float]): (Float, Tensor[Float]) = {
  val output = model.forward(input).toTensor
  val _loss = criterion.forward(output, labels)
  model.zeroGradParameters()
  val gradInput = criterion.backward(output, labels)
  model.backward(input, gradInput)
  (_loss, grad)
}

var loss: Array[Float] = null
for (i &lt;- 1 to 100) {
  loss = sgd.optimize(feval, weight)._2
  println(s&quot;${i}-th loss = ${loss(0)}&quot;)
}
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

hidden_size = 4
input_size = 6
output_size = 5
seq_length = 5

input = np.random.uniform(0, 1, [1, seq_length, input_size]).astype(&quot;float32&quot;)
labels = np.random.uniform(1, 5, [1, seq_length]).astype(&quot;int&quot;)

print labels
print input

rec = Recurrent()
rec.add(LSTM(input_size, hidden_size))

model = Sequential()
model.add(rec)
model.add(TimeDistributed(Linear(hidden_size, output_size)))

criterion = TimeDistributedCriterion(CrossEntropyCriterion(), False)

sgd = SGD(learningrate=0.1, learningrate_decay=5e-7)

weight, grad = model.parameters()

output = model.forward(input)
loss = criterion.forward(input, labels)
gradInput = criterion.backward(output, labels)
model.backward(input, gradInput)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Utilities/merged-Utilities/" class="btn btn-neutral float-right" title="Utilities">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../Distance-Layers/merged-Distance-Layers/" class="btn btn-neutral" title="Distance Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../Distance-Layers/merged-Distance-Layers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Utilities/merged-Utilities/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../../../js/theme.js"></script>

</body>
</html>
