<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Run - BigDL Project</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Use an Interactive Shell", url: "#use-an-interactive-shell", children: [
          ]},
          {title: "Run a BigDL Program", url: "#run-a-bigdl-program", children: [
          ]},
          {title: "Running on EC2", url: "#running-on-ec2", children: [
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

    <h2 id="use-an-interactive-shell"><strong>Use an Interactive Shell</strong></h2>
<p>You can try BigDL easily using the Spark interactive shell. Run below command to start spark shell with BigDL support:</p>
<pre><code class="bash">$ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \
  --jars bigdl-VERSION-jar-with-dependencies.jar
</code></pre>

<p>You will see a welcome message looking like below:</p>
<pre><code>Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)
Spark context available as sc.
scala&gt; 
</code></pre>

<p>To use BigDL, you should first initialize the engine as below. </p>
<pre><code class="scala">scala&gt;import com.intel.analytics.bigdl.utils.Engine
scala&gt;Engine.init
</code></pre>

<p>Once the engine is successfully initialted, you'll be able to play with BigDL API's. 
For instance, to experiment with the <code>Tensor</code> APIs in BigDL, you may try below code:</p>
<pre><code class="scala">scala&gt; import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Tensor

scala&gt; Tensor[Double](2,2).fill(1.0)
res9: com.intel.analytics.bigdl.tensor.Tensor[Double] =
1.0     1.0
1.0     1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<hr />
<h2 id="run-a-bigdl-program"><strong>Run a BigDL Program</strong></h2>
<p>You can run a BigDL program, e.g., the <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/vgg">VGG</a> training, as a standard Spark program (running in either local mode or cluster mode) as follows:</p>
<ol>
<li>Download the CIFAR-10 data from <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">here</a>. Remember to choose the binary version.</li>
</ol>
<pre><code>  # Spark local mode
  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \
  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \
  -f path_to_your_cifar_folder \
  -b batch_size

  # Spark standalone mode
  spark-submit --master spark://... --executor-cores cores_per_executor \
  --total-executor-cores total_cores_for_the_job \
  --class com.intel.analytics.bigdl.models.vgg.Train \
  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \
  -f path_to_your_cifar_folder \
  -b batch_size

  # Spark yarn mode
  spark-submit --master yarn --deploy-mode client \
  --executor-cores cores_per_executor \
  --num-executors executors_number \
  --class com.intel.analytics.bigdl.models.vgg.Train \
  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \
  -f path_to_your_cifar_folder \
  -b batch_size
</code></pre>

<p>The parameters used in the above command are:</p>
<ul>
<li>
<p>-f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.</p>
</li>
<li>
<p>-b: The mini-batch size. The mini-batch size is expected to be a multiple of <em>total cores</em> used in the job. In this example, the mini-batch size is suggested to be set to <em>total cores * 4</em></p>
</li>
</ul>
<p>If you are to run your own program, do remember to create SparkContext and initialize the engine before call other BigDL API's, as shown below. </p>
<pre><code class="scala"> // Scala code example
 val conf = Engine.createSparkConf()
 val sc = new SparkContext(conf)
 Engine.init
</code></pre>

<hr />
<h2 id="running-on-ec2"><strong>Running on EC2</strong></h2>
<p><br/>
<span style="font-size: 1.5em; font-weight:bold">1. AMI</span>
<br/></p>
<p>To make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.</p>
<table>
<thead>
<tr>
<th>BigDL version</th>
<th>AMI version</th>
<th>Date</th>
<th>AMI ID</th>
<th>AMI Name</th>
<th>Region</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>0.2S</td>
<td>Mar 13, 2017</td>
<td>ami-37b73957</td>
<td>BigDL Client 0.2S</td>
<td>US West (Oregon)</td>
<td>Active</td>
</tr>
<tr>
<td>master</td>
<td>0.2S</td>
<td>Apr 10, 2017</td>
<td>ami-8c87099a</td>
<td>BigDL Client 0.2S</td>
<td>US East (N. Virginia)</td>
<td>Active</td>
</tr>
<tr>
<td>0.1.0</td>
<td>0.1.0</td>
<td>Apr 10, 2017</td>
<td>ami-9a8818fa</td>
<td>BigDL Client 0.1.0</td>
<td>US West (Oregon)</td>
<td>Active</td>
</tr>
<tr>
<td>0.1.0</td>
<td>0.1.0</td>
<td>Apr 10, 2017</td>
<td>ami-6476f872</td>
<td>BigDL Client 0.1.0</td>
<td>US East (N. Virginia)</td>
<td>Active</td>
</tr>
</tbody>
</table>
<p>Please note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.</p>
<p>After launching the AMI on EC2, please log on to the instance and run a "bootstrap.sh" script to download example scripts.</p>
<pre><code class="bash">./bootstrap.sh
</code></pre>

<p><br/>
<span style="font-size: 1.5em; font-weight:bold">2. Before You Start</span>
<br/></p>
<p>Before running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to <a href="https://github.com/amplab/spark-ec2">https://github.com/amplab/spark-ec2</a> for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.</p>
<p><br/>
 <span style="font-size: 1.5em; font-weight:bold">3. Run BigDL Examples</span>
 <br/></p>
<p>You can run BigDL examples using the <code>run.example.sh</code> script in home directory of your BigDL Client instance (e.g. <code>/home/ubuntu/</code>) with the following parameters:
* Mandatory parameters:
  * <code>-m|--model</code> which model to train, including
    * lenet: train the <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet">LeNet</a> example
    * vgg: train the <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/vgg">VGG</a> example
    * inception-v1: train the <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception">Inception v1</a> example
    * perf: test the training speed using the <a href="https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception/Inception_v1.scala">Inception v1</a> model with dummy data</p>
<ul>
<li>
<p><code>-s|--spark-url</code> the master URL for the Spark cluster</p>
</li>
<li>
<p><code>-n|--nodes</code> number of Spark slave nodes</p>
</li>
<li>
<p><code>-o|--cores</code> number of cores used on each node</p>
</li>
<li>
<p><code>-r|--memory</code> memory used on each node, e.g. 200g</p>
</li>
<li>
<p><code>-b|--batch-size</code> batch size when training the model; it is expected to be a multiple of "nodes * cores"</p>
</li>
<li>
<p><code>-f|--hdfs-data-dir</code> HDFS directory for the input images (for the "inception-v1" model training only)</p>
</li>
<li>
<p>Optional parameters:</p>
</li>
<li>
<p><code>-e|--max-epoch</code> the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified</p>
</li>
<li>
<p><code>-p|--spark</code> by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify "spark_2.0" here (it is highly recommended to use <em><strong>Java 8</strong></em> when running BigDL for Spark 2.0, otherwise you may observe very poor performance)</p>
</li>
<li>
<p><code>-l|--learning-rate</code> by default the the example will use an initial learning rate of "0.01"; you can specify a different value here</p>
</li>
</ul>
<p>After the training, you can check the log files and generated models in the home directory (e.g., <code>/home/ubuntu/</code>).  </p>
<p><br/>
  <span style="font-size: 1.5em; font-weight:bold">4. Run the "inception-v1" example</span>
  <br/> </p>
<p>You can refer to the <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception">Inception v1</a> example to prepare the input <a href="http://image-net.org/index">ImageNet</a> data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.</p>
<ul>
<li>Download and prepare the input image data (a subset of the <a href="http://sergeykarayev.com/files/1311.3715v3.pdf">Flickr Style</a> data)</li>
</ul>
<pre><code class="bash">  ./download.sh $HDFS-NAMENODE
</code></pre>

<p>After the download completes, the downloaded images are stored in <code>hdfs://HDFS-NAMENODE:9000/seq</code>. (If the download fails with error "Unable to establish SSL connection." please check your network connection and retry this later.)</p>
<ul>
<li>To run the "inception-v1" example on a 4-worker Spark cluster (using, say, the "m4.10xlarge" instance), run the example command below: </li>
</ul>
<pre><code>  nohup bash ./run.example.sh --model inception-v1  \
         --spark-url spark://SPARK-MASTER:7077    \
         --nodes 4 --cores 20 --memory 150g       \
         --batch-size 400 --learning-rate 0.0898  \
         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \
         --spark spark_2.0 --max-epoch 4 \
         &gt; incep.log 2&gt;&amp;1 &amp;     
</code></pre>

<ul>
<li>View output of the training in the log file generated by the previous step:</li>
</ul>
<pre><code class="bash">  $ tail -f incep.log
  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
</code></pre>

<p><br/>
   <span style="font-size: 1.5em; font-weight:bold">5. Run the "perf" example</  span>
   <br/></p>
<p>To run the "perf" example on a 4-worker Spark cluster (using, say, the "m4.10xlarge" instance), you may try the example command below: </p>
<pre><code class="bash">  nohup bash ./run.example.sh --model perf  \
       --spark-url spark://SPARK-MASTER:7077    \
       --nodes 4 --cores 20 --memory 150g       \
       --spark spark_2.0 --max-epoch 4 \
       &gt; perf.log 2&gt;&amp;1 &amp;
</code></pre>

<hr />

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>