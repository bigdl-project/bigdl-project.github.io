{
    "docs": [
        {
            "location": "/", 
            "text": "What is BigDL\n\n\n\n\nBigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.\n\n\n\n\n\n\nRich deep learning support.\n Modeled after \nTorch\n, BigDL provides comprehensive support for deep learning, including numeric computing (via \nTensor\n) and high level \nneural networks\n; in addition, users can load pre-trained \nCaffe\n or \nTorch\n models into Spark programs using BigDL.\n\n\n\n\n\n\nExtremely high performance.\n To achieve high performance, BigDL uses \nIntel MKL\n and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source \nCaffe\n, \nTorch\n or \nTensorFlow\n on a single-node Xeon (i.e., comparable with mainstream GPU).\n\n\n\n\n\n\nEfficiently scale-out.\n BigDL can efficiently scale out to perform data analytics at \"Big Data scale\", by leveraging \nApache Spark\n (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark. \n\n\n\n\n\n\nWhy BigDL?\n\n\nYou may want to write your deep learning programs using BigDL if:\n\n\n\n\n\n\nYou want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).\n\n\n\n\n\n\nYou want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.\n\n\n\n\n\n\nYou want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)\n\n\n\n\n\n\nGetting Help\n\n\n\n\n\n\nYou can join the \nBigDL Google Group\n (or subscribe to the \nMail List\n) for more questions and discussions on BigDL\n\n\n\n\n\n\nYou can post bug reports and feature requests at the \nIssue Page", 
            "title": "Home"
        }, 
        {
            "location": "/#what-is-bigdl", 
            "text": "BigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.    Rich deep learning support.  Modeled after  Torch , BigDL provides comprehensive support for deep learning, including numeric computing (via  Tensor ) and high level  neural networks ; in addition, users can load pre-trained  Caffe  or  Torch  models into Spark programs using BigDL.    Extremely high performance.  To achieve high performance, BigDL uses  Intel MKL  and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source  Caffe ,  Torch  or  TensorFlow  on a single-node Xeon (i.e., comparable with mainstream GPU).    Efficiently scale-out.  BigDL can efficiently scale out to perform data analytics at \"Big Data scale\", by leveraging  Apache Spark  (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark.", 
            "title": "What is BigDL"
        }, 
        {
            "location": "/#why-bigdl", 
            "text": "You may want to write your deep learning programs using BigDL if:    You want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).    You want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.    You want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)", 
            "title": "Why BigDL?"
        }, 
        {
            "location": "/#getting-help", 
            "text": "You can join the  BigDL Google Group  (or subscribe to the  Mail List ) for more questions and discussions on BigDL    You can post bug reports and feature requests at the  Issue Page", 
            "title": "Getting Help"
        }, 
        {
            "location": "/release/", 
            "text": "Download \n\n\n\n\n\n\n\n\nDownload\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\nBigDL Nightly Build Download\n\n\nlatest Docs\n\n\n\n\n\n\nBigDL 0.1.1 Download\n\n\nBigDL 0.1.1 Docs\n\n\n\n\n\n\nBigDL 0.1.0 Download\n\n\nN/A\n\n\n\n\n\n\n\n\nThese are built BigDL packages including dependency and python files. You can download these packages instead of building them by yourself. This is useful when you want to do something like run some examples or develop python code.\n\n\nBigDL Nightly Build\n\n\nHere are the folders for nightly build packages. The packages are built from latest master code. You can download the .zip files with a timestamp suffix in the name. \n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\nBigDL 0.1.1\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\nBigDL 0.1.0\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload", 
            "title": "Releases"
        }, 
        {
            "location": "/release/#download", 
            "text": "Download  Docs      BigDL Nightly Build Download  latest Docs    BigDL 0.1.1 Download  BigDL 0.1.1 Docs    BigDL 0.1.0 Download  N/A     These are built BigDL packages including dependency and python files. You can download these packages instead of building them by yourself. This is useful when you want to do something like run some examples or develop python code.", 
            "title": "Download"
        }, 
        {
            "location": "/release/#bigdl-nightly-build", 
            "text": "Here are the folders for nightly build packages. The packages are built from latest master code. You can download the .zip files with a timestamp suffix in the name.       Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download", 
            "title": "BigDL Nightly Build"
        }, 
        {
            "location": "/release/#bigdl-011", 
            "text": "Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download", 
            "title": "BigDL 0.1.1"
        }, 
        {
            "location": "/release/#bigdl-010", 
            "text": "Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download", 
            "title": "BigDL 0.1.0"
        }, 
        {
            "location": "/powered-by/", 
            "text": "Intel\u2019s BigDL on Databricks\n\n\n\n\n\n\nUse BigDL on AZure HDInsight\n\n\n\n\nA more detailed post for \nHow to use BigDL on Apache Spark for Azure HDInsight\n\n\n\n\n\n\n\n\nBigDL on AliCloud E-MapReduce (in Chinese)\n\n\n\n\n\n\nRunning BigDL, Deep Learning for Apache Spark, on AWS\n\n\n\n\n\n\nBigDL on CDH and Cloudera Data Science Workbench", 
            "title": "Powered by"
        }, 
        {
            "location": "/UserGuide/getting-started/", 
            "text": "Before running a BigDL program\n\n\nBefore running a BigDL program, you need to install \nSpark\n. Then you can download a pre-built BigDL package and run your program.  Refer to \nUse Pre-built packages\n\n\n\n\nStart an Interactive shell\n\n\nYou can quickly experiment with BigDL codes as a Spark program using the interactive Spark shell by running:\n\n\n$ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \\\n  --jars bigdl-VERSION-jar-with-dependencies.jar\n\n\n\n\nThen you can see something like:\n\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala\n \n\n\n\n\nFor instance, to experiment with the \nTensor\n APIs in BigDL, you may then try:\n\n\nscala\n import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala\n Tensor[Double](2,2).fill(1.0)\nres9: com.intel.analytics.bigdl.tensor.Tensor[Double] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nFor more details about the BigDL APIs, please refer to the \nProgramming Guide\n.\n\n\n\n\nRun a BigDL Program\n\n\nYou can run a BigDL program, e.g., the \nVGG\n training, as a standard Spark program (running in either local mode or cluster mode) as follows:\n\n\n\n\nDownload the CIFAR-10 data from \nhere\n. Remember to choose the binary version.\n\n\n\n\n  # Spark local mode\n  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark standalone mode\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark yarn mode\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n\n\n\nThe parameters used in the above command are:\n\n\n\n\n\n\n-f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.\n\n\n\n\n\n\n-b: The mini-batch size. The mini-batch size is expected to be a multiple of \ntotal cores\n used in the job. In this example, the mini-batch size is suggested to be set to \ntotal cores * 4\n\n\n\n\n\n\n\n\nNext Steps\n\n\n\n\n\n\nTo learn the details of Python support in BigDL, you can check out the \nPython Support Page\n\n\n\n\n\n\nTo learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the \nExamples Page\n\n\n\n\n\n\nYou can check out the \nDocuments Page\n for more details (including Models, Examples, Programming Guide, etc.)\n\n\n\n\n\n\nYou can join the \nBigDL Google Group\n (or subscribe to the \nmail list\n) for more questions and discussions on BigDL\n\n\n\n\n\n\nYou can post bug reports and feature requests at the \nIssue Page", 
            "title": "Getting Started"
        }, 
        {
            "location": "/UserGuide/getting-started/#before-running-a-bigdl-program", 
            "text": "Before running a BigDL program, you need to install  Spark . Then you can download a pre-built BigDL package and run your program.  Refer to  Use Pre-built packages", 
            "title": "Before running a BigDL program"
        }, 
        {
            "location": "/UserGuide/getting-started/#start-an-interactive-shell", 
            "text": "You can quickly experiment with BigDL codes as a Spark program using the interactive Spark shell by running:  $ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \\\n  --jars bigdl-VERSION-jar-with-dependencies.jar  Then you can see something like:  Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala    For instance, to experiment with the  Tensor  APIs in BigDL, you may then try:  scala  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala  Tensor[Double](2,2).fill(1.0)\nres9: com.intel.analytics.bigdl.tensor.Tensor[Double] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  For more details about the BigDL APIs, please refer to the  Programming Guide .", 
            "title": "Start an Interactive shell"
        }, 
        {
            "location": "/UserGuide/getting-started/#run-a-bigdl-program", 
            "text": "You can run a BigDL program, e.g., the  VGG  training, as a standard Spark program (running in either local mode or cluster mode) as follows:   Download the CIFAR-10 data from  here . Remember to choose the binary version.     # Spark local mode\n  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark standalone mode\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark yarn mode\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size  The parameters used in the above command are:    -f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.    -b: The mini-batch size. The mini-batch size is expected to be a multiple of  total cores  used in the job. In this example, the mini-batch size is suggested to be set to  total cores * 4", 
            "title": "Run a BigDL Program"
        }, 
        {
            "location": "/UserGuide/getting-started/#next-steps", 
            "text": "To learn the details of Python support in BigDL, you can check out the  Python Support Page    To learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the  Examples Page    You can check out the  Documents Page  for more details (including Models, Examples, Programming Guide, etc.)    You can join the  BigDL Google Group  (or subscribe to the  mail list ) for more questions and discussions on BigDL    You can post bug reports and feature requests at the  Issue Page", 
            "title": "Next Steps"
        }, 
        {
            "location": "/UserGuide/examples/", 
            "text": "This section is a short introduction of some classic examples/tutorials. They can give you a clear idea of how to build simple deep learning programs using BigDL. Besides these examples, BigDL also provides plenty of models ready for re-use and examples in both Scala and Python - refer to \nResources\n section for details. \n\n\n\n\nTraining LeNet on MNIST - The \"hello world\" for deep learning\n\n\nThis tutorial is an explanation of what is happening in the \nlenet\n example, which trains \nLeNet-5\n on the \nMNIST data\n using BigDL.\n\n\nA BigDL program starts with \nimport com.intel.analytics.bigdl._\n; it then \ncreates the \nSparkContext\n using the \nSparkConf\n returned by the \nEngine\n; after that, it \ninitializes the \nEngine\n.\n\n\n  val conf = Engine.createSparkConf()\n      .setAppName(\nTrain Lenet on MNIST\n)\n      .set(\nspark.task.maxFailures\n, \n1\n)\n  val sc = new SparkContext(conf)\n  Engine.init\n\n\n\n\nEngine.createSparkConf\n will return a \nSparkConf\n populated with some appropriate configuration. And \nEngine.init\n will verify and read some environment information(e.g. executor numbers and executor cores) from the \nSparkContext\n. You can find more information about the initialization in the \nProgramming Guilde\n\n\nAfter the initialization, we need to:\n\n\n\n\nCreate the LeNet model\n by calling the \nLeNet5()\n, which creates the LeNet-5 convolutional network model as follows:\n\n\n\n\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())\n\n\n\n\n\n\nLoad the data by \ncreating the \nDataSet\n (either a distributed or local one depending on whether it runs on Spark or not), and then \napplying a series of \nTransformer\n (e.g., \nSampleToGreyImg\n, \nGreyImgNormalizer\n and \nGreyImgToBatch\n):\n\n\n\n\n    val trainSet = (if (sc.isDefined) {\n        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)\n      } else {\n        DataSet.array(load(trainData, trainLabel))\n      }) -\n SampleToGreyImg(28, 28) -\n GreyImgNormalizer(trainMean, trainStd) -\n GreyImgToBatch(\n        param.batchSize)\n\n\n\n\nAfter that, we \ncreate the \nOptimizer\n (either a distributed or local one depending on whether it runs on Spark or not) by specifying the \nDataSet\n, the model and the \nCriterion\n (which, given input and target, computes gradient per given loss function):\n\n\n  val optimizer = Optimizer(\n    model = model,\n    dataset = trainSet,\n    criterion = ClassNLLCriterion[Float]())\n\n\n\n\nFinally (after optionally specifying the validation data and methods for the \nOptimizer\n), we \ntrain the model by calling \nOptimizer.optimize()\n:\n\n\n  optimizer\n    .setValidation(\n      trigger = Trigger.everyEpoch,\n      dataset = validationSet,\n      vMethods = Array(new Top1Accuracy))\n    .setState(state)\n    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))\n    .optimize()\n\n\n\n\n\n\nText Classification - Working with Spark RDD\n\n\nThis tutorial describes the \ntext_classification\n example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by \nthis Keras tutorial\n).\n\n\nAfter importing \ncom.intel.analytics.bigdl._\n and some initialization, the \nexample\n broadcasts the pre-trained world embedding and loads the input data using RDD transformations:\n\n\n  // For large dataset, you might want to get such RDD[(String, Float)] from HDFS\n  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)\n  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)\n  val word2MetaBC = sc.broadcast(word2Meta)\n  val word2VecBC = sc.broadcast(word2Vec)\n  val vectorizedRdd = dataRdd\n      .map {case (text, label) =\n (toTokens(text, word2MetaBC.value), label)}\n      .map {case (tokens, label) =\n (shaping(tokens, sequenceLen), label)}\n      .map {case (tokens, label) =\n (vectorization(\n        tokens, embeddingDim, word2VecBC.value), label)}\n\n\n\n\nThe \nexample\n then converts the processed data (\nvectorizedRdd\n) to an RDD of Sample, and randomly splits the sample RDD (\nsampleRDD\n) into training data (\ntrainingRDD\n) and validation data (\nvalRDD\n):\n\n\n  val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) =\n\n        Sample(\n          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))\n            .transpose(1, 2).contiguous(),\n          labelTensor = Tensor(Array(label), Array(1)))\n      }\n\n  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(\n    Array(trainingSplit, 1 - trainingSplit))\n\n\n\n\nAfter that, the \nexample\n builds the CNN model, creates the \nOptimizer\n, pass the RDD of training data (\ntrainingRDD\n) to the \nOptimizer\n (with specific batch size), and finally trains the model (using \nAdagrad\n as the optimization method, and setting relevant hyper parameters in \nstate\n):\n\n\n  val optimizer = Optimizer(\n    model = buildModel(classNum),\n    sampleRDD = trainingRDD,\n    criterion = new ClassNLLCriterion[Float](),\n    batchSize = param.batchSize\n  )\n  val state = T(\nlearningRate\n -\n 0.01, \nlearningRateDecay\n -\n 0.0002)\n  optimizer\n    .setState(state)\n    .setOptimMethod(new Adagrad())\n    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n    .setEndWhen(Trigger.maxEpoch(2))\n    .optimize()\n\n\n\n\n\n\nImage Classification\n - Working with Spark DataFrame and ML pipeline\n\n\nThis tutorial describes the \nimage_classification\n example, which loads a BigDL (\nInception\n) model or Torch (\nResnet\n) model that is trained on \nImageNet\n data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark \nML pipeline\n.\n\n\nAfter importing \ncom.intel.analytics.bigdl._\n and some initialization, the \nexample\n first \nloads\n the specified model:\n\n\n  def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)\n    (implicit ev: TensorNumeric[T]): Module[T] = {\n    val model = param.modelType match {\n      case TorchModel =\n\n        Module.loadTorch[T](param.modelPath)\n      case BigDlModel =\n\n        Module.load[T](param.modelPath)\n      case _ =\n throw new IllegalArgumentException(s\n${param.modelType}\n)\n    }\n    model\n  }\n\n\n\n\nIt then creates \nDLClassifer\n (a Spark ML pipelines \nTransformer\n) that predicts the input value based on the specified deep learning model:\n\n\n  val model = loadModel(param)\n  val valTrans = new DLClassifier()\n    .setInputCol(\nfeatures\n)\n    .setOutputCol(\npredict\n)\n\n  val paramsTrans = ParamMap(\n    valTrans.modelTrain -\n model,\n    valTrans.batchShape -\n\n    Array(param.batchSize, 3, imageSize, imageSize))\n\n\n\n\nAfter that, the \nexample\n  loads the input images into a \nDataFrame\n, and then predicts the class of each each image using the \nDLClassifer\n:\n\n\n  val valRDD = sc.parallelize(imageSet).repartition(partitionNum)\n  val transf = RowToByteRecords() -\n\n      SampleToBGRImg() -\n\n      BGRImgCropper(imageSize, imageSize) -\n\n      BGRImgNormalizer(testMean, testStd) -\n\n      BGRImgToImageVector()\n\n  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)\n\n  valTrans.transform(valDF, paramsTrans)\n      .select(\nimageName\n, \npredict\n)\n      .show(param.showNum)\n\n\n\n\n\n\nTutorial: Text Classification - Working with BigDL Python API\n\n\nThis tutorial describes the \ntextclassifier\n example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by \nthis Keras tutorial\n)\n\n\nThe example first creates the \nSparkContext\n using the SparkConf\nreturn by the\ncreate_spark_conf()` method, and then initialize the engine:\n\n\n  sc = SparkContext(appName=\ntext_classifier\n,\n                    conf=create_spark_conf())\n  init_engine()\n\n\n\n\nIt then loads the \n20 Newsgroup dataset\n into RDD, and transforms the input data into an RDD of \nSample\n. (Each \nSample\n in essence contains a tuple of two NumPy ndarray representing the feature and label).\n\n\n  texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])   \n\n\n\n\nAfter that, the example creates the neural network model as follows:\n\n\ndef build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \ncnn\n:\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \nlstm\n:\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \ngru\n:\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model\n\n\n\n\nFinally the example creates the \nOptimizer\n (which accepts both the model and the training Sample RDD) and trains the model by calling \nOptimizer.optimize()\n:\n\n\noptimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=\nAdagrad\n,\n    state=state)\n...\ntrain_model = optimizer.optimize()\n\n\n\n\n\n\nMore Resources\n\n\nBesides above examples, BigDL also provides plenty of models ready for re-use and examples in both Scala and Python. Refer to \nResources\n section for more details.", 
            "title": "Examples"
        }, 
        {
            "location": "/UserGuide/examples/#training-lenet-on-mnist-the-hello-world-for-deep-learning", 
            "text": "This tutorial is an explanation of what is happening in the  lenet  example, which trains  LeNet-5  on the  MNIST data  using BigDL.  A BigDL program starts with  import com.intel.analytics.bigdl._ ; it then  creates the  SparkContext  using the  SparkConf  returned by the  Engine ; after that, it  initializes the  Engine .    val conf = Engine.createSparkConf()\n      .setAppName( Train Lenet on MNIST )\n      .set( spark.task.maxFailures ,  1 )\n  val sc = new SparkContext(conf)\n  Engine.init  Engine.createSparkConf  will return a  SparkConf  populated with some appropriate configuration. And  Engine.init  will verify and read some environment information(e.g. executor numbers and executor cores) from the  SparkContext . You can find more information about the initialization in the  Programming Guilde  After the initialization, we need to:   Create the LeNet model  by calling the  LeNet5() , which creates the LeNet-5 convolutional network model as follows:       val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())   Load the data by  creating the  DataSet  (either a distributed or local one depending on whether it runs on Spark or not), and then  applying a series of  Transformer  (e.g.,  SampleToGreyImg ,  GreyImgNormalizer  and  GreyImgToBatch ):       val trainSet = (if (sc.isDefined) {\n        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)\n      } else {\n        DataSet.array(load(trainData, trainLabel))\n      }) -  SampleToGreyImg(28, 28) -  GreyImgNormalizer(trainMean, trainStd) -  GreyImgToBatch(\n        param.batchSize)  After that, we  create the  Optimizer  (either a distributed or local one depending on whether it runs on Spark or not) by specifying the  DataSet , the model and the  Criterion  (which, given input and target, computes gradient per given loss function):    val optimizer = Optimizer(\n    model = model,\n    dataset = trainSet,\n    criterion = ClassNLLCriterion[Float]())  Finally (after optionally specifying the validation data and methods for the  Optimizer ), we  train the model by calling  Optimizer.optimize() :    optimizer\n    .setValidation(\n      trigger = Trigger.everyEpoch,\n      dataset = validationSet,\n      vMethods = Array(new Top1Accuracy))\n    .setState(state)\n    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))\n    .optimize()", 
            "title": "Training LeNet on MNIST - The \"hello world\" for deep learning"
        }, 
        {
            "location": "/UserGuide/examples/#text-classification-working-with-spark-rdd", 
            "text": "This tutorial describes the  text_classification  example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by  this Keras tutorial ).  After importing  com.intel.analytics.bigdl._  and some initialization, the  example  broadcasts the pre-trained world embedding and loads the input data using RDD transformations:    // For large dataset, you might want to get such RDD[(String, Float)] from HDFS\n  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)\n  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)\n  val word2MetaBC = sc.broadcast(word2Meta)\n  val word2VecBC = sc.broadcast(word2Vec)\n  val vectorizedRdd = dataRdd\n      .map {case (text, label) =  (toTokens(text, word2MetaBC.value), label)}\n      .map {case (tokens, label) =  (shaping(tokens, sequenceLen), label)}\n      .map {case (tokens, label) =  (vectorization(\n        tokens, embeddingDim, word2VecBC.value), label)}  The  example  then converts the processed data ( vectorizedRdd ) to an RDD of Sample, and randomly splits the sample RDD ( sampleRDD ) into training data ( trainingRDD ) and validation data ( valRDD ):    val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) = \n        Sample(\n          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))\n            .transpose(1, 2).contiguous(),\n          labelTensor = Tensor(Array(label), Array(1)))\n      }\n\n  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(\n    Array(trainingSplit, 1 - trainingSplit))  After that, the  example  builds the CNN model, creates the  Optimizer , pass the RDD of training data ( trainingRDD ) to the  Optimizer  (with specific batch size), and finally trains the model (using  Adagrad  as the optimization method, and setting relevant hyper parameters in  state ):    val optimizer = Optimizer(\n    model = buildModel(classNum),\n    sampleRDD = trainingRDD,\n    criterion = new ClassNLLCriterion[Float](),\n    batchSize = param.batchSize\n  )\n  val state = T( learningRate  -  0.01,  learningRateDecay  -  0.0002)\n  optimizer\n    .setState(state)\n    .setOptimMethod(new Adagrad())\n    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n    .setEndWhen(Trigger.maxEpoch(2))\n    .optimize()", 
            "title": "Text Classification - Working with Spark RDD"
        }, 
        {
            "location": "/UserGuide/examples/#image-classification-working-with-spark-dataframe-and-ml-pipeline", 
            "text": "This tutorial describes the  image_classification  example, which loads a BigDL ( Inception ) model or Torch ( Resnet ) model that is trained on  ImageNet  data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark  ML pipeline .  After importing  com.intel.analytics.bigdl._  and some initialization, the  example  first  loads  the specified model:    def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)\n    (implicit ev: TensorNumeric[T]): Module[T] = {\n    val model = param.modelType match {\n      case TorchModel = \n        Module.loadTorch[T](param.modelPath)\n      case BigDlModel = \n        Module.load[T](param.modelPath)\n      case _ =  throw new IllegalArgumentException(s ${param.modelType} )\n    }\n    model\n  }  It then creates  DLClassifer  (a Spark ML pipelines  Transformer ) that predicts the input value based on the specified deep learning model:    val model = loadModel(param)\n  val valTrans = new DLClassifier()\n    .setInputCol( features )\n    .setOutputCol( predict )\n\n  val paramsTrans = ParamMap(\n    valTrans.modelTrain -  model,\n    valTrans.batchShape - \n    Array(param.batchSize, 3, imageSize, imageSize))  After that, the  example   loads the input images into a  DataFrame , and then predicts the class of each each image using the  DLClassifer :    val valRDD = sc.parallelize(imageSet).repartition(partitionNum)\n  val transf = RowToByteRecords() - \n      SampleToBGRImg() - \n      BGRImgCropper(imageSize, imageSize) - \n      BGRImgNormalizer(testMean, testStd) - \n      BGRImgToImageVector()\n\n  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)\n\n  valTrans.transform(valDF, paramsTrans)\n      .select( imageName ,  predict )\n      .show(param.showNum)", 
            "title": "Image Classification - Working with Spark DataFrame and ML pipeline"
        }, 
        {
            "location": "/UserGuide/examples/#tutorial-text-classification-working-with-bigdl-python-api", 
            "text": "This tutorial describes the  textclassifier  example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by  this Keras tutorial )  The example first creates the  SparkContext  using the SparkConf return by the create_spark_conf()` method, and then initialize the engine:    sc = SparkContext(appName= text_classifier ,\n                    conf=create_spark_conf())\n  init_engine()  It then loads the  20 Newsgroup dataset  into RDD, and transforms the input data into an RDD of  Sample . (Each  Sample  in essence contains a tuple of two NumPy ndarray representing the feature and label).    texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])     After that, the example creates the neural network model as follows:  def build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() ==  cnn :\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() ==  lstm :\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() ==  gru :\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model  Finally the example creates the  Optimizer  (which accepts both the model and the training Sample RDD) and trains the model by calling  Optimizer.optimize() :  optimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method= Adagrad ,\n    state=state)\n...\ntrain_model = optimizer.optimize()", 
            "title": "Tutorial: Text Classification - Working with BigDL Python API"
        }, 
        {
            "location": "/UserGuide/examples/#more-resources", 
            "text": "Besides above examples, BigDL also provides plenty of models ready for re-use and examples in both Scala and Python. Refer to  Resources  section for more details.", 
            "title": "More Resources"
        }, 
        {
            "location": "/UserGuide/ug-setup-bigdl/", 
            "text": "Build and Install\n\n\n\n\n\n\nInitialize Engine", 
            "title": "Setup BigDL"
        }, 
        {
            "location": "/UserGuide/ug-prepare-data/", 
            "text": "Transformer and all preprocessing resources", 
            "title": "Prepare your Data"
        }, 
        {
            "location": "/UserGuide/ug-prediction/", 
            "text": "", 
            "title": "Use BigDL for Prediction Only"
        }, 
        {
            "location": "/UserGuide/ug-train/", 
            "text": "How to Build a Model\n\n\nMore desriptions goes here.\n\n\n\n\nModule\n\n\nCreate modules\n\n\nConstruct complex networks\n\n\nBuild neural network models\n\n\n\n\n\n\nCriterion\n\n\n\n\nHow to Load/Save a model\n\n\n\n\nCheckpoint\n\n\nLoad/Save Model\n\n\n\n\nHow to Train\n\n\n\n\nValidation\n\n\nCheckpoint\n\n\n\n\nStop/Resume the Training\n\n\nUse pretrained Model and Layer", 
            "title": "Train a Model"
        }, 
        {
            "location": "/UserGuide/ug-train/#how-to-build-a-model", 
            "text": "More desriptions goes here.   Module  Create modules  Construct complex networks  Build neural network models    Criterion", 
            "title": "How to Build a Model"
        }, 
        {
            "location": "/UserGuide/ug-train/#how-to-loadsave-a-model", 
            "text": "Checkpoint  Load/Save Model", 
            "title": "How to Load/Save a model"
        }, 
        {
            "location": "/UserGuide/ug-train/#how-to-train", 
            "text": "Validation  Checkpoint", 
            "title": "How to Train"
        }, 
        {
            "location": "/UserGuide/ug-train/#stopresume-the-training", 
            "text": "", 
            "title": "Stop/Resume the Training"
        }, 
        {
            "location": "/UserGuide/ug-train/#use-pretrained-model-and-layer", 
            "text": "", 
            "title": "Use pretrained Model and Layer"
        }, 
        {
            "location": "/UserGuide/ug-monitor/", 
            "text": "Logging\n\n\nVisualization", 
            "title": "Monitor the Training"
        }, 
        {
            "location": "/UserGuide/ug-monitor/#logging", 
            "text": "", 
            "title": "Logging"
        }, 
        {
            "location": "/UserGuide/ug-monitor/#visualization", 
            "text": "", 
            "title": "Visualization"
        }, 
        {
            "location": "/UserGuide/ug-tune/", 
            "text": "", 
            "title": "Tuning"
        }, 
        {
            "location": "/UserGuide/ug-advanced/", 
            "text": "", 
            "title": "Advanced Usage"
        }, 
        {
            "location": "/UserGuide/use-pre-built/", 
            "text": "Download\n\n\nYou can download the BigDL release (currently v0.1.0) and nightly build from the \nRelease Page\n\n\n\n\nLink with a BigDL release\n\n\nCurrently, BigDL releases are hosted on maven central; here's an example to add the BigDL dependency to your own project:\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.bigdl\n/groupId\n\n    \nartifactId\nbigdl\n/artifactId\n\n    \nversion\n${BIGDL_VERSION}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.bigdl\n % \nbigdl\n % \n${BIGDL_VERSION}\n\n\n\n\n\nSince currently only BigDL 0.1.0 is released, ${BIGDL_VERSION} must be set to 0.1.0 here.\n\n\nNote\n: the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.bigdl\n/groupId\n\n    \nartifactId\nbigdl-SPARK_2.0\n/artifactId\n\n    \nversion\n${BIGDL_VERSION}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.bigdl\n % \nbigdl-SPARK_2.0\n % \n${BIGDL_VERSION}\n\n\n\n\n\nIf your project runs on MacOS, you should add the dependency below,\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.bigdl.native\n/groupId\n\n    \nartifactId\nmkl-java-mac\n/artifactId\n\n    \nversion\n${BIGDL_VERSION}\n/version\n\n    \nexclusions\n\n        \nexclusion\n\n            \ngroupId\ncom.intel.analytics.bigdl.native\n/groupId\n\n            \nartifactId\nbigdl-native\n/artifactId\n\n        \n/exclusion\n\n    \n/exclusions\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.bigdl.native\n % \nmkl-java-mac\n % \n${BIGDL_VERSION}\n from \nhttp://repo1.maven.org/maven2/com/intel/analytics/bigdl/native/mkl-java-mac/${BIGDL_VERSION}/mkl-java-mac-${BIGDL_VERSION}.jar\n\n\n\n\n\n\n\nLink with a development version\n\n\nCurrently, BigDL development version is hosted on \nSonaType\n. \n\n\nTo link your application with the latest BigDL development version, you should add some dependencies like \nLinking with BigDL releases\n, but set ${BIGDL_VERSION} to 0.2.0-SNAPSHOT, and add below repository to your pom.xml.\n\n\nrepository\n\n    \nid\nsonatype\n/id\n\n    \nname\nsonatype repository\n/name\n\n    \nurl\nhttps://oss.sonatype.org/content/groups/public/\n/url\n\n    \nreleases\n\n        \nenabled\ntrue\n/enabled\n\n    \n/releases\n\n    \nsnapshots\n\n        \nenabled\ntrue\n/enabled\n\n    \n/snapshots\n\n\n/repository\n\n\n\n\n\nSBT developers can use\n\n\nresolvers += \nSonatype OSS Snapshots\n at \nhttps://oss.sonatype.org/content/repositories/snapshots", 
            "title": "Use Pre-built Package"
        }, 
        {
            "location": "/UserGuide/use-pre-built/#download", 
            "text": "You can download the BigDL release (currently v0.1.0) and nightly build from the  Release Page", 
            "title": "Download"
        }, 
        {
            "location": "/UserGuide/use-pre-built/#link-with-a-bigdl-release", 
            "text": "Currently, BigDL releases are hosted on maven central; here's an example to add the BigDL dependency to your own project:  dependency \n     groupId com.intel.analytics.bigdl /groupId \n     artifactId bigdl /artifactId \n     version ${BIGDL_VERSION} /version  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.bigdl  %  bigdl  %  ${BIGDL_VERSION}   Since currently only BigDL 0.1.0 is released, ${BIGDL_VERSION} must be set to 0.1.0 here.  Note : the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this  dependency \n     groupId com.intel.analytics.bigdl /groupId \n     artifactId bigdl-SPARK_2.0 /artifactId \n     version ${BIGDL_VERSION} /version  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.bigdl  %  bigdl-SPARK_2.0  %  ${BIGDL_VERSION}   If your project runs on MacOS, you should add the dependency below,  dependency \n     groupId com.intel.analytics.bigdl.native /groupId \n     artifactId mkl-java-mac /artifactId \n     version ${BIGDL_VERSION} /version \n     exclusions \n         exclusion \n             groupId com.intel.analytics.bigdl.native /groupId \n             artifactId bigdl-native /artifactId \n         /exclusion \n     /exclusions  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.bigdl.native  %  mkl-java-mac  %  ${BIGDL_VERSION}  from  http://repo1.maven.org/maven2/com/intel/analytics/bigdl/native/mkl-java-mac/${BIGDL_VERSION}/mkl-java-mac-${BIGDL_VERSION}.jar", 
            "title": "Link with a BigDL release"
        }, 
        {
            "location": "/UserGuide/use-pre-built/#link-with-a-development-version", 
            "text": "Currently, BigDL development version is hosted on  SonaType .   To link your application with the latest BigDL development version, you should add some dependencies like  Linking with BigDL releases , but set ${BIGDL_VERSION} to 0.2.0-SNAPSHOT, and add below repository to your pom.xml.  repository \n     id sonatype /id \n     name sonatype repository /name \n     url https://oss.sonatype.org/content/groups/public/ /url \n     releases \n         enabled true /enabled \n     /releases \n     snapshots \n         enabled true /enabled \n     /snapshots  /repository   SBT developers can use  resolvers +=  Sonatype OSS Snapshots  at  https://oss.sonatype.org/content/repositories/snapshots", 
            "title": "Link with a development version"
        }, 
        {
            "location": "/UserGuide/build-src/", 
            "text": "Download BigDL Source\n\n\nBigDL source code is available at \nGitHub\n\n\n$ git clone https://github.com/intel-analytics/BigDL.git\n\n\n\n\nBy default, \ngit clone\n will download the development version of BigDL, if you want a release version, you can use command \ngit checkout\n to change the version. Available release versions is \nBigDL releases\n.\n\n\n\n\nEnvironment Settings\n\n\nThe following instructions are aligned with master code.\n\n\nMaven 3 is needed to build BigDL, you can download it from the \nmaven website\n.\n\n\nAfter installing Maven 3, please set the environment variable MAVEN_OPTS as follows:\n\n\n$ export MAVEN_OPTS=\n-Xmx2g -XX:ReservedCodeCacheSize=512m\n\n\n\n\n\nWhen compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d. \n\n\n\n\nBuild with script (Recommended)\n\n\nIt is highly recommended that you build BigDL using the \nmake-dist.sh script\n. And it will handle the MAVEN_OPTS variable.\n\n\nOnce downloaded, you can build BigDL with the following commands:\n\n\n$ bash make-dist.sh\n\n\n\n\nAfter that, you can find a \ndist\n folder, which contains all the needed files to run a BigDL program. The files in \ndist\n include:\n\n \ndist/lib/bigdl-VERSION-jar-with-dependencies.jar\n: This jar package contains all dependencies except Spark classes.\n\n \ndist/lib/bigdl-VERSION-python-api.zip\n: This zip package contains all Python files of BigDL.\n* \ndist/conf/spark-bigdl.conf\n: This file contains necessary property configurations. \nEngine.createSparkConf\n will populate these properties, so try to use that method in your code. Or you need to pass the file to Spark with the \"--properties-file\" option. \n\n\n\n\nBuild for macOS\n\n\nThe instructions above will only build for Linux. To build BigDL for macOS, pass \n-P mac\n to the \nmake-dist.sh\n script as follows:\n\n\n$ bash make-dist.sh -P mac\n\n\n\n\n\n\nBuild for Spark 2.0 and above\n\n\nThe instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 and above (which uses Scala 2.11 by default), pass \n-P spark_2.x\n to the \nmake-dist.sh\n script:\n\n\n$ bash make-dist.sh -P spark_2.x\n\n\n\n\nIt is highly recommended to use \nJava 8\n when running with Spark 2.x; otherwise you may observe very poor performance.\n\n\n\n\nBuild for Scala 2.10 or 2.11\n\n\nBy default, \nmake-dist.sh\n uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0.x or 2.1.x. To override the default behaviors, you can pass \n-P scala_2.10\n or \n-P scala_2.11\n to \nmake-dist.sh\n as appropriate.\n\n\n\n\nBuild native libs\n\n\nNote that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:\n\n\n\n\n\n\nDownload and install \nIntel Parallel Studio XE\n in your Linux box.\n\n\n\n\n\n\nPrepare build environment as follows:\n    \n{r, engine='sh'}\n    $ source \ninstall-dir\n/bin/compilervars.sh intel64\n    $ source PATH_TO_MKL/bin/mklvars.sh intel64\n\n    where the \nPATH_TO_MKL\n is the installation directory of the MKL.\n\n\n\n\n\n\nFull build\n\n\n\n\n\n\nClone BigDL as follows:\n   \n{r, engine='sh'}\n   git clone git@github.com:intel-analytics/BigDL.git --recursive\n\n   For already cloned repos, just use:\n   \n{r, engine='sh'}\n   git submodule update --init --recursive\n\n   If the Intel MKL is not installed to the default path \n/opt/intel\n, please pass your libiomp5.so's directory path to\n   the \nmake-dist.sh\n script:\n   \n{r, engine='sh'}\n   $ bash make-dist.sh -P full-build -DiompLibDir=\nPATH_TO_LIBIOMP5_DIR\n\n   Otherwise, only pass \n-P full-build\n to the \nmake-dist.sh\n script:\n   \n{r, engine='sh'}\n   $ bash make-dist.sh -P full-build\n\n\n\n\nBuild with Maven\n\n\nTo build BigDL directly using Maven, run the command below:\n\n\n$ mvn clean package -DskipTests\n\n\n\n\nAfter that, you can find that the three jar packages in \nPATH_To_BigDL\n/target/, where \nPATH_To_BigDL\n is the path to the directory of the BigDL. \n\n\nNote that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:\n\n\n\n\n-P mac\n: build for maxOS\n\n\n-P spark_2.x\n: build for Spark 2.0 and above (using Scala 2.11). (Again, it is highly recommended to use \nJava 8\n when running with Spark 2.0; otherwise you may observe very poor performance.)\n\n\n-P full-build\n: full build\n\n\n-P scala_2.10\n (or \n-P scala_2.11\n): build using Scala 2.10 (or Scala 2.11) \n\n\n\n\n\n\nSetup IDE\n\n\nWe set the scope of spark related library to \nprovided\n in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.\n\n\nThis will cause a problem in IDE. When you run applications, it will throw \nNoClassDefFoundError\n because the library scope is \nprovided\n.\n\n\nYou can easily change the scopes by the \nall-in-one\n profile.\n\n\n\n\nIn Intellij, go to View -\n Tools Windows -\n Maven Projects. Then in the Maven Projects panel, Profiles -\n click \"all-in-one\".", 
            "title": "Build from Source"
        }, 
        {
            "location": "/UserGuide/build-src/#download-bigdl-source", 
            "text": "BigDL source code is available at  GitHub  $ git clone https://github.com/intel-analytics/BigDL.git  By default,  git clone  will download the development version of BigDL, if you want a release version, you can use command  git checkout  to change the version. Available release versions is  BigDL releases .", 
            "title": "Download BigDL Source"
        }, 
        {
            "location": "/UserGuide/build-src/#environment-settings", 
            "text": "The following instructions are aligned with master code.  Maven 3 is needed to build BigDL, you can download it from the  maven website .  After installing Maven 3, please set the environment variable MAVEN_OPTS as follows:  $ export MAVEN_OPTS= -Xmx2g -XX:ReservedCodeCacheSize=512m   When compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d.", 
            "title": "Environment Settings"
        }, 
        {
            "location": "/UserGuide/build-src/#build-with-script-recommended", 
            "text": "It is highly recommended that you build BigDL using the  make-dist.sh script . And it will handle the MAVEN_OPTS variable.  Once downloaded, you can build BigDL with the following commands:  $ bash make-dist.sh  After that, you can find a  dist  folder, which contains all the needed files to run a BigDL program. The files in  dist  include:   dist/lib/bigdl-VERSION-jar-with-dependencies.jar : This jar package contains all dependencies except Spark classes.   dist/lib/bigdl-VERSION-python-api.zip : This zip package contains all Python files of BigDL.\n*  dist/conf/spark-bigdl.conf : This file contains necessary property configurations.  Engine.createSparkConf  will populate these properties, so try to use that method in your code. Or you need to pass the file to Spark with the \"--properties-file\" option.", 
            "title": "Build with script (Recommended)"
        }, 
        {
            "location": "/UserGuide/build-src/#build-for-macos", 
            "text": "The instructions above will only build for Linux. To build BigDL for macOS, pass  -P mac  to the  make-dist.sh  script as follows:  $ bash make-dist.sh -P mac", 
            "title": "Build for macOS"
        }, 
        {
            "location": "/UserGuide/build-src/#build-for-spark-20-and-above", 
            "text": "The instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 and above (which uses Scala 2.11 by default), pass  -P spark_2.x  to the  make-dist.sh  script:  $ bash make-dist.sh -P spark_2.x  It is highly recommended to use  Java 8  when running with Spark 2.x; otherwise you may observe very poor performance.", 
            "title": "Build for Spark 2.0 and above"
        }, 
        {
            "location": "/UserGuide/build-src/#build-for-scala-210-or-211", 
            "text": "By default,  make-dist.sh  uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0.x or 2.1.x. To override the default behaviors, you can pass  -P scala_2.10  or  -P scala_2.11  to  make-dist.sh  as appropriate.", 
            "title": "Build for Scala 2.10 or 2.11"
        }, 
        {
            "location": "/UserGuide/build-src/#build-native-libs", 
            "text": "Note that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:    Download and install  Intel Parallel Studio XE  in your Linux box.    Prepare build environment as follows:\n     {r, engine='sh'}\n    $ source  install-dir /bin/compilervars.sh intel64\n    $ source PATH_TO_MKL/bin/mklvars.sh intel64 \n    where the  PATH_TO_MKL  is the installation directory of the MKL.    Full build    Clone BigDL as follows:\n    {r, engine='sh'}\n   git clone git@github.com:intel-analytics/BigDL.git --recursive \n   For already cloned repos, just use:\n    {r, engine='sh'}\n   git submodule update --init --recursive \n   If the Intel MKL is not installed to the default path  /opt/intel , please pass your libiomp5.so's directory path to\n   the  make-dist.sh  script:\n    {r, engine='sh'}\n   $ bash make-dist.sh -P full-build -DiompLibDir= PATH_TO_LIBIOMP5_DIR \n   Otherwise, only pass  -P full-build  to the  make-dist.sh  script:\n    {r, engine='sh'}\n   $ bash make-dist.sh -P full-build", 
            "title": "Build native libs"
        }, 
        {
            "location": "/UserGuide/build-src/#build-with-maven", 
            "text": "To build BigDL directly using Maven, run the command below:  $ mvn clean package -DskipTests  After that, you can find that the three jar packages in  PATH_To_BigDL /target/, where  PATH_To_BigDL  is the path to the directory of the BigDL.   Note that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:   -P mac : build for maxOS  -P spark_2.x : build for Spark 2.0 and above (using Scala 2.11). (Again, it is highly recommended to use  Java 8  when running with Spark 2.0; otherwise you may observe very poor performance.)  -P full-build : full build  -P scala_2.10  (or  -P scala_2.11 ): build using Scala 2.10 (or Scala 2.11)", 
            "title": "Build with Maven"
        }, 
        {
            "location": "/UserGuide/build-src/#setup-ide", 
            "text": "We set the scope of spark related library to  provided  in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.  This will cause a problem in IDE. When you run applications, it will throw  NoClassDefFoundError  because the library scope is  provided .  You can easily change the scopes by the  all-in-one  profile.   In Intellij, go to View -  Tools Windows -  Maven Projects. Then in the Maven Projects panel, Profiles -  click \"all-in-one\".", 
            "title": "Setup IDE"
        }, 
        {
            "location": "/UserGuide/deploy-python/", 
            "text": "For how to deploy Python, refer to \nUsing Python without Pip\n and \nInstall via Pip", 
            "title": "Enable Python Support"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/", 
            "text": "Running on EC2\n\n\n\n\n\n\n1. AMI\n\n\n2. Before You Start\n\n\n3. Run BigDL examples\n\n\n3.1 Run the \"inception-v1\" example\n\n\n3.2 Run the \"perf\" example\n\n\n\n\n\n\n\n\n1. AMI\n\n\nTo make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.\n\n\n\n\n\n\n\n\nBigDL version\n\n\nAMI version\n\n\nDate\n\n\nAMI ID\n\n\nAMI Name\n\n\nRegion\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nmaster\n\n\n0.2S\n\n\nMar 13, 2017\n\n\nami-37b73957\n\n\nBigDL Client 0.2S\n\n\nUS West (Oregon)\n\n\nActive\n\n\n\n\n\n\nmaster\n\n\n0.2S\n\n\nApr 10, 2017\n\n\nami-8c87099a\n\n\nBigDL Client 0.2S\n\n\nUS East (N. Virginia)\n\n\nActive\n\n\n\n\n\n\n0.1.0\n\n\n0.1.0\n\n\nApr 10, 2017\n\n\nami-9a8818fa\n\n\nBigDL Client 0.1.0\n\n\nUS West (Oregon)\n\n\nActive\n\n\n\n\n\n\n0.1.0\n\n\n0.1.0\n\n\nApr 10, 2017\n\n\nami-6476f872\n\n\nBigDL Client 0.1.0\n\n\nUS East (N. Virginia)\n\n\nActive\n\n\n\n\n\n\n\n\nPlease note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.\n\n\nAfter launching the AMI on EC2, please log on to the instance and run a \"bootstrap.sh\" script to download example scripts.\n\n\n./bootstrap.sh\n\n\n\n\n2. Before You Start\n\n\nBefore running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to \nhttps://github.com/amplab/spark-ec2\n for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.\n\n\n3. Run BigDL examples\n\n\nYou can run BigDL examples using the \nrun.example.sh\n script in home directory of your BigDL Client instance (e.g. \n/home/ubuntu/\n) with the following parameters:\n* Mandatory parameters:\n  * \n-m|--model\n which model to train, including\n    * lenet: train the \nLeNet\n example\n    * vgg: train the \nVGG\n example\n    * inception-v1: train the \nInception v1\n example\n    * perf: test the training speed using the \nInception v1\n model with dummy data\n\n\n\n\n\n\n-s|--spark-url\n the master URL for the Spark cluster\n\n\n\n\n\n\n-n|--nodes\n number of Spark slave nodes\n\n\n\n\n\n\n-o|--cores\n number of cores used on each node\n\n\n\n\n\n\n-r|--memory\n memory used on each node, e.g. 200g\n\n\n\n\n\n\n-b|--batch-size\n batch size when training the model; it is expected to be a multiple of \"nodes * cores\"\n\n\n\n\n\n\n-f|--hdfs-data-dir\n HDFS directory for the input images (for the \"inception-v1\" model training only)\n\n\n\n\n\n\nOptional parameters:\n\n\n\n\n\n\n-e|--max-epoch\n the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified\n\n\n\n\n\n\n-p|--spark\n by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify \"spark_2.0\" here (it is highly recommended to use \nJava 8\n when running BigDL for Spark 2.0, otherwise you may observe very poor performance)\n\n\n\n\n\n\n-l|--learning-rate\n by default the the example will use an initial learning rate of \"0.01\"; you can specify a different value here\n\n\n\n\n\n\nAfter the training, you can check the log files and generated models in the home directory (e.g., \n/home/ubuntu/\n).  \n\n\n3.1 Run the \"inception-v1\" example\n\n\nYou can refer to the \nInception v1\n example to prepare the input \nImageNet\n data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.\n\n\n\n\nDownload and prepare the input image data (a subset of the \nFlickr Style\n data)\n\n\n\n\n  ./download.sh $HDFS-NAMENODE\n\n\n\n\nAfter the download completes, the downloaded images are stored in \nhdfs://HDFS-NAMENODE:9000/seq\n. (If the download fails with error \"Unable to establish SSL connection.\" please check your network connection and retry this later.)\n\n\n\n\nTo run the \"inception-v1\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), run the example command below: \n\n\n\n\n  nohup bash ./run.example.sh --model inception-v1  \\\n         --spark-url spark://SPARK-MASTER:7077    \\\n         --nodes 4 --cores 20 --memory 150g       \\\n         --batch-size 400 --learning-rate 0.0898  \\\n         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \\\n         --spark spark_2.0 --max-epoch 4 \\\n         \n incep.log 2\n1 \n     \n\n\n\n\n\n\nView output of the training in the log file generated by the previous step:\n\n\n\n\n  $ tail -f incep.log\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n\n\n\n\n3.2 Run the \"perf\" example\n\n\nTo run the \"perf\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), you may try the example command below: \n\n\n  nohup bash ./run.example.sh --model perf  \\\n       --spark-url spark://SPARK-MASTER:7077    \\\n       --nodes 4 --cores 20 --memory 150g       \\\n       --spark spark_2.0 --max-epoch 4 \\\n       \n perf.log 2\n1", 
            "title": "Running On EC2"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/#running-on-ec2", 
            "text": "1. AMI  2. Before You Start  3. Run BigDL examples  3.1 Run the \"inception-v1\" example  3.2 Run the \"perf\" example", 
            "title": "Running on EC2"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/#1-ami", 
            "text": "To make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.     BigDL version  AMI version  Date  AMI ID  AMI Name  Region  Status      master  0.2S  Mar 13, 2017  ami-37b73957  BigDL Client 0.2S  US West (Oregon)  Active    master  0.2S  Apr 10, 2017  ami-8c87099a  BigDL Client 0.2S  US East (N. Virginia)  Active    0.1.0  0.1.0  Apr 10, 2017  ami-9a8818fa  BigDL Client 0.1.0  US West (Oregon)  Active    0.1.0  0.1.0  Apr 10, 2017  ami-6476f872  BigDL Client 0.1.0  US East (N. Virginia)  Active     Please note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.  After launching the AMI on EC2, please log on to the instance and run a \"bootstrap.sh\" script to download example scripts.  ./bootstrap.sh", 
            "title": "1. AMI"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/#2-before-you-start", 
            "text": "Before running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to  https://github.com/amplab/spark-ec2  for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.", 
            "title": "2. Before You Start"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/#3-run-bigdl-examples", 
            "text": "You can run BigDL examples using the  run.example.sh  script in home directory of your BigDL Client instance (e.g.  /home/ubuntu/ ) with the following parameters:\n* Mandatory parameters:\n  *  -m|--model  which model to train, including\n    * lenet: train the  LeNet  example\n    * vgg: train the  VGG  example\n    * inception-v1: train the  Inception v1  example\n    * perf: test the training speed using the  Inception v1  model with dummy data    -s|--spark-url  the master URL for the Spark cluster    -n|--nodes  number of Spark slave nodes    -o|--cores  number of cores used on each node    -r|--memory  memory used on each node, e.g. 200g    -b|--batch-size  batch size when training the model; it is expected to be a multiple of \"nodes * cores\"    -f|--hdfs-data-dir  HDFS directory for the input images (for the \"inception-v1\" model training only)    Optional parameters:    -e|--max-epoch  the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified    -p|--spark  by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify \"spark_2.0\" here (it is highly recommended to use  Java 8  when running BigDL for Spark 2.0, otherwise you may observe very poor performance)    -l|--learning-rate  by default the the example will use an initial learning rate of \"0.01\"; you can specify a different value here    After the training, you can check the log files and generated models in the home directory (e.g.,  /home/ubuntu/ ).", 
            "title": "3. Run BigDL examples"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/#31-run-the-inception-v1-example", 
            "text": "You can refer to the  Inception v1  example to prepare the input  ImageNet  data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.   Download and prepare the input image data (a subset of the  Flickr Style  data)     ./download.sh $HDFS-NAMENODE  After the download completes, the downloaded images are stored in  hdfs://HDFS-NAMENODE:9000/seq . (If the download fails with error \"Unable to establish SSL connection.\" please check your network connection and retry this later.)   To run the \"inception-v1\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), run the example command below:      nohup bash ./run.example.sh --model inception-v1  \\\n         --spark-url spark://SPARK-MASTER:7077    \\\n         --nodes 4 --cores 20 --memory 150g       \\\n         --batch-size 400 --learning-rate 0.0898  \\\n         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \\\n         --spark spark_2.0 --max-epoch 4 \\\n           incep.log 2 1          View output of the training in the log file generated by the previous step:     $ tail -f incep.log\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.", 
            "title": "3.1 Run the \"inception-v1\" example"
        }, 
        {
            "location": "/UserGuide/running-on-EC2/#32-run-the-perf-example", 
            "text": "To run the \"perf\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), you may try the example command below:     nohup bash ./run.example.sh --model perf  \\\n       --spark-url spark://SPARK-MASTER:7077    \\\n       --nodes 4 --cores 20 --memory 150g       \\\n       --spark spark_2.0 --max-epoch 4 \\\n         perf.log 2 1", 
            "title": "3.2 Run the \"perf\" example"
        }, 
        {
            "location": "/UserGuide/resources/", 
            "text": "Scala Models\n\n\nBigDL provides loads of popular models ready for use in your application. Some of them are listed blow. See all in \nscala neural network models\n. \n\n\n\n\nLeNet\n: it demonstrates how to use BigDL to train and evaluate the \nLeNet-5\n network on MNIST data.\n\n\nInception\n: it demonstrates how to use BigDL to train and evaluate \nInception v1\n and \nInception v2\n architecture on the ImageNet data.\n\n\nVGG\n: it demonstrates how to use BigDL to train and evaluate a \nVGG-like\n network on CIFAR-10 data.\n\n\nResNet\n: it demonstrates how to use BigDL to train and evaluate the \nResNet\n architecture on CIFAR-10 data.\n\n\nRNN\n: it demonstrates how to use BigDL to build and train a simple recurrent neural network \n(RNN) for language model\n.\n\n\nAuto-encoder\n: it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.\n\n\n\n\n\n\nScala Examples\n\n\nBigDL ships plenty of Scala examples to show how to use BigDL to solve real problems. Some are listed blow. See all of them in \nscala deep learning examples\n \n\n\n\n\ntext_classification\n: it demonstrates how to use BigDL to build a \ntext classifier\n using a simple convolutional neural network (CNN) model.\n\n\nimage_classification\n: it demonstrates how to load a BigDL or \nTorch\n model trained on ImageNet data (e.g., \nInception\n or \nResNet\n), and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.\n\n\nload_model\n: it demonstrates how to use BigDL to load a pre-trained \nTorch\n or \nCaffe\n model into Spark program for prediction.\n\n\n\n\n\n\nPython Models/Examples\n\n\nBigDL also provides a lot of Python models and examples. Some are listed blow. See all in \npython models\n and \npython examples\n\n\n\n\nLeNet\n: it demonstrates how to use BigDL Python APIs to train and evaluate the \nLeNet-5\n network on MNIST data.\n\n\nText Classifier\n:  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.\n\n\nJupyter Notebook Tutorial\n: it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.\n\n\n\n\n\n\nPython Tutorial Notebooks\n\n\nBigDL Tutorials Notebooks\n - A series of notebooks that step-by- step introduce you how to do data science on Apache Spark and BigDL framework", 
            "title": "Resources"
        }, 
        {
            "location": "/UserGuide/resources/#scala-models", 
            "text": "BigDL provides loads of popular models ready for use in your application. Some of them are listed blow. See all in  scala neural network models .    LeNet : it demonstrates how to use BigDL to train and evaluate the  LeNet-5  network on MNIST data.  Inception : it demonstrates how to use BigDL to train and evaluate  Inception v1  and  Inception v2  architecture on the ImageNet data.  VGG : it demonstrates how to use BigDL to train and evaluate a  VGG-like  network on CIFAR-10 data.  ResNet : it demonstrates how to use BigDL to train and evaluate the  ResNet  architecture on CIFAR-10 data.  RNN : it demonstrates how to use BigDL to build and train a simple recurrent neural network  (RNN) for language model .  Auto-encoder : it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.", 
            "title": "Scala Models"
        }, 
        {
            "location": "/UserGuide/resources/#scala-examples", 
            "text": "BigDL ships plenty of Scala examples to show how to use BigDL to solve real problems. Some are listed blow. See all of them in  scala deep learning examples     text_classification : it demonstrates how to use BigDL to build a  text classifier  using a simple convolutional neural network (CNN) model.  image_classification : it demonstrates how to load a BigDL or  Torch  model trained on ImageNet data (e.g.,  Inception  or  ResNet ), and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.  load_model : it demonstrates how to use BigDL to load a pre-trained  Torch  or  Caffe  model into Spark program for prediction.", 
            "title": "Scala Examples"
        }, 
        {
            "location": "/UserGuide/resources/#python-modelsexamples", 
            "text": "BigDL also provides a lot of Python models and examples. Some are listed blow. See all in  python models  and  python examples   LeNet : it demonstrates how to use BigDL Python APIs to train and evaluate the  LeNet-5  network on MNIST data.  Text Classifier :  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.  Jupyter Notebook Tutorial : it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.", 
            "title": "Python Models/Examples"
        }, 
        {
            "location": "/UserGuide/resources/#python-tutorial-notebooks", 
            "text": "BigDL Tutorials Notebooks  - A series of notebooks that step-by- step introduce you how to do data science on Apache Spark and BigDL framework", 
            "title": "Python Tutorial Notebooks"
        }, 
        {
            "location": "/PythonSupport/python-api/", 
            "text": "Overview\n\n\nPython is one of the most widely used language in the big data and data science community, and BigDL provides full support for Python APIs (built on top of PySpark), which are similar to the \nScala interface\n. (Please note currently the Python support has been tested with Python 2.7 and Spark 1.6 / Spark 2.0). \n\n\nWith the full Python API support in BigDL, users can use deep learning models in BigDL together with existing Python libraries (e.g., Numpy and Pandas), which automatically runs in a distributed fashion to process large volumes of data on Spark.  \n\n\n\n\nTutorial: Text Classification using BigDL Python API\n\n\nThis tutorial describes the \ntextclassifier\n example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by \nthis Keras tutorial\n)\n\n\nThe example first creates the \nSparkContext\n using the SparkConf\nreturn by the\ncreate_spark_conf()` method, and then initialize the engine:\n\n\n  sc = SparkContext(appName=\ntext_classifier\n,\n                    conf=create_spark_conf())\n  init_engine()\n\n\n\n\nIt then loads the \n20 Newsgroup dataset\n into RDD, and transforms the input data into an RDD of \nSample\n. (Each \nSample\n in essence contains a tuple of two NumPy ndarray representing the feature and label).\n\n\n  texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])   \n\n\n\n\nAfter that, the example creates the neural network model as follows:\n\n\ndef build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \ncnn\n:\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \nlstm\n:\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \ngru\n:\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model\n\n\n\n\nFinally the example creates the \nOptimizer\n (which accepts both the model and the training Sample RDD) and trains the model by calling \nOptimizer.optimize()\n:\n\n\noptimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=\nAdagrad\n,\n    state=state)\n...\ntrain_model = optimizer.optimize()\n\n\n\n\nMore resources\n\n\nBigDL provides plenty of models, examples and tutorials. Please refer to \nPython Models/Examples\n and \nPython Tutorial Notebooks\n.", 
            "title": "API Usage"
        }, 
        {
            "location": "/PythonSupport/python-api/#overview", 
            "text": "Python is one of the most widely used language in the big data and data science community, and BigDL provides full support for Python APIs (built on top of PySpark), which are similar to the  Scala interface . (Please note currently the Python support has been tested with Python 2.7 and Spark 1.6 / Spark 2.0).   With the full Python API support in BigDL, users can use deep learning models in BigDL together with existing Python libraries (e.g., Numpy and Pandas), which automatically runs in a distributed fashion to process large volumes of data on Spark.", 
            "title": "Overview"
        }, 
        {
            "location": "/PythonSupport/python-api/#tutorial-text-classification-using-bigdl-python-api", 
            "text": "This tutorial describes the  textclassifier  example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by  this Keras tutorial )  The example first creates the  SparkContext  using the SparkConf return by the create_spark_conf()` method, and then initialize the engine:    sc = SparkContext(appName= text_classifier ,\n                    conf=create_spark_conf())\n  init_engine()  It then loads the  20 Newsgroup dataset  into RDD, and transforms the input data into an RDD of  Sample . (Each  Sample  in essence contains a tuple of two NumPy ndarray representing the feature and label).    texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])     After that, the example creates the neural network model as follows:  def build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() ==  cnn :\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() ==  lstm :\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() ==  gru :\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model  Finally the example creates the  Optimizer  (which accepts both the model and the training Sample RDD) and trains the model by calling  Optimizer.optimize() :  optimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method= Adagrad ,\n    state=state)\n...\ntrain_model = optimizer.optimize()", 
            "title": "Tutorial: Text Classification using BigDL Python API"
        }, 
        {
            "location": "/PythonSupport/python-api/#more-resources", 
            "text": "BigDL provides plenty of models, examples and tutorials. Please refer to  Python Models/Examples  and  Python Tutorial Notebooks .", 
            "title": "More resources"
        }, 
        {
            "location": "/PythonSupport/install-via-pip/", 
            "text": "Usage\n\n\n\n\nLaunch with Python REPL\n\n\n\n\n from bigdl.util.common import *\n\n init_engine()\n\n import bigdl.version\n\n bigdl.version.__version__\n'0.1.1rc0'\n\n from bigdl.nn.layer import *\n\n linear = Linear(2, 3)\ncreating: createLinear\n\n continue your experiment....\n\n\n\n\n\n\nLaunch with jupyter:\n\n\njupyter notebook --notebook-dir=./ --ip=* --no-browser\n\n\nYou need to create SparkContext in this way as we start jupyter without pyspark scripts:\n\n\n\n\n      from bigdl.util.common import *\n      sc = get_spark_context()\n\n\n\n\n\n\nInstall BigDL-0.1.1\n\n\n\n\nDownload Spark2.x:\n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\n\n\nExtract the tar ball and set SPARK_HOME\n\n\n\n\ntar -zxvf spark-2.1.0-bin-hadoop2.7.tgz\nexport SPARK_HOME=path to spark-2.1.0-bin-hadoop2.7\n\n\n\n\n\n\nInstall BigDL release via pip (we tested this on pip 9.0.1)\n\n\n\n\npip install --upgrade pip\npip install BigDL==0.1.1rc0     # for Python 2.7\npip3 install BigDL==0.1.1rc0  # for Python 3.n\n\n\n\n\n\n\nInstall BigDL-0.2.0-snapshot\n\n\n\n\nDownload Spark2.x:\n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\n\n\nExtract the tar ball and set SPARK_HOME\n\n\n\n\ntar -zxvf spark-2.1.0-bin-hadoop2.7.tgz\nexport SPARK_HOME=path to spark-2.1.0-bin-hadoop2.7\n\n\n\n\n\n\nInstall BigDL release via pip (we tested this on pip 9.0.1)\n\n\n\n\npip install --upgrade pip\npip install BigDL==0.2.0.dev2     # for Python 2.7\npip3 install BigDL==0.2.0.dev2  # for Python 3.n", 
            "title": "Install via pip"
        }, 
        {
            "location": "/PythonSupport/install-via-pip/#usage", 
            "text": "Launch with Python REPL    from bigdl.util.common import *  init_engine()  import bigdl.version  bigdl.version.__version__\n'0.1.1rc0'  from bigdl.nn.layer import *  linear = Linear(2, 3)\ncreating: createLinear  continue your experiment....   Launch with jupyter:  jupyter notebook --notebook-dir=./ --ip=* --no-browser  You need to create SparkContext in this way as we start jupyter without pyspark scripts:         from bigdl.util.common import *\n      sc = get_spark_context()", 
            "title": "Usage"
        }, 
        {
            "location": "/PythonSupport/install-via-pip/#install-bigdl-011", 
            "text": "Download Spark2.x:  wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz  Extract the tar ball and set SPARK_HOME   tar -zxvf spark-2.1.0-bin-hadoop2.7.tgz\nexport SPARK_HOME=path to spark-2.1.0-bin-hadoop2.7   Install BigDL release via pip (we tested this on pip 9.0.1)   pip install --upgrade pip\npip install BigDL==0.1.1rc0     # for Python 2.7\npip3 install BigDL==0.1.1rc0  # for Python 3.n", 
            "title": "Install BigDL-0.1.1"
        }, 
        {
            "location": "/PythonSupport/install-via-pip/#install-bigdl-020-snapshot", 
            "text": "Download Spark2.x:  wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz  Extract the tar ball and set SPARK_HOME   tar -zxvf spark-2.1.0-bin-hadoop2.7.tgz\nexport SPARK_HOME=path to spark-2.1.0-bin-hadoop2.7   Install BigDL release via pip (we tested this on pip 9.0.1)   pip install --upgrade pip\npip install BigDL==0.2.0.dev2     # for Python 2.7\npip3 install BigDL==0.2.0.dev2  # for Python 3.n", 
            "title": "Install BigDL-0.2.0-snapshot"
        }, 
        {
            "location": "/PythonSupport/python-no-pip/", 
            "text": "Run Python Program in Command Line\n\n\nA BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. One can run the BigDL \nlenet Python example\n using \nspark-submit\n as follows:\n\n\nPYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-jar-with-dependencies.jar\nPYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\n${SPARK_HOME}/bin/spark-submit \\\n    --master ${MASTER} \\\n    --driver-memory 10g  \\\n    --driver-cores 4  \\\n    --executor-memory 20g \\\n    --total-executor-cores ${TOTAL_CORES}\\\n    --executor-cores 10 ${EXECUTOR_CORES} \\\n    --py-files ${PYTHON_API_PATH},${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py  \\\n    --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n    --jars ${BigDL_JAR_PATH} \\\n    --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n    --conf spark.executor.extraClassPath=bigdl-VERSION-jar-with-dependencies.jar \\\n    ${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py\n\n\n\n\n\n\nRun Python Program in Notebook\n\n\nWith the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.\n\n\nFirst, install all the necessary libraries on the local node where you will run Jupyter, e.g., \n\n\nsudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud\n\n\n\n\nThen, you can launch the Jupyter notebook as follows:\n\n\nPYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-jar-with-dependencies.jar\n\nexport PYTHONPATH=${PYTHON_API_PATH}:$PYTHONPATH\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS=\nnotebook --notebook-dir=./ --ip=* --no-browser\n\n\n${SPARK_HOME}/bin/pyspark \\\n  --master ${MASTER} \\\n  --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n  --driver-memory 10g  \\\n  --driver-cores 4  \\\n  --executor-memory 20g \\\n  --total-executor-cores {TOTAL_CORES} \\\n  --executor-cores {EXECUTOR_CORES} \\\n  --py-files ${PYTHON_API_PATH} \\\n  --jars ${BigDL_JAR_PATH} \\\n  --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n  --conf spark.executor.extraClassPath=bigdl-0.1.0-jar-with-dependencies.jar\n\n\n\n\nAfter successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/\n\n\n\n\nUse Python on YARN cluster\n\n\nYou can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the      Python dependency). You  can first package all the required Python dependency into a virtual environment on the local    node (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program   on the YARN cluster (using that virtual environment). Please refer to this \npatch\n for more details.", 
            "title": "Use Python without Pip"
        }, 
        {
            "location": "/PythonSupport/python-no-pip/#run-python-program-in-command-line", 
            "text": "A BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. One can run the BigDL  lenet Python example  using  spark-submit  as follows:  PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-jar-with-dependencies.jar\nPYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\n${SPARK_HOME}/bin/spark-submit \\\n    --master ${MASTER} \\\n    --driver-memory 10g  \\\n    --driver-cores 4  \\\n    --executor-memory 20g \\\n    --total-executor-cores ${TOTAL_CORES}\\\n    --executor-cores 10 ${EXECUTOR_CORES} \\\n    --py-files ${PYTHON_API_PATH},${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py  \\\n    --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n    --jars ${BigDL_JAR_PATH} \\\n    --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n    --conf spark.executor.extraClassPath=bigdl-VERSION-jar-with-dependencies.jar \\\n    ${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py", 
            "title": "Run Python Program in Command Line"
        }, 
        {
            "location": "/PythonSupport/python-no-pip/#run-python-program-in-notebook", 
            "text": "With the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.  First, install all the necessary libraries on the local node where you will run Jupyter, e.g.,   sudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud  Then, you can launch the Jupyter notebook as follows:  PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-jar-with-dependencies.jar\n\nexport PYTHONPATH=${PYTHON_API_PATH}:$PYTHONPATH\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS= notebook --notebook-dir=./ --ip=* --no-browser \n\n${SPARK_HOME}/bin/pyspark \\\n  --master ${MASTER} \\\n  --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n  --driver-memory 10g  \\\n  --driver-cores 4  \\\n  --executor-memory 20g \\\n  --total-executor-cores {TOTAL_CORES} \\\n  --executor-cores {EXECUTOR_CORES} \\\n  --py-files ${PYTHON_API_PATH} \\\n  --jars ${BigDL_JAR_PATH} \\\n  --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n  --conf spark.executor.extraClassPath=bigdl-0.1.0-jar-with-dependencies.jar  After successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/", 
            "title": "Run Python Program in Notebook"
        }, 
        {
            "location": "/PythonSupport/python-no-pip/#use-python-on-yarn-cluster", 
            "text": "You can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the      Python dependency). You  can first package all the required Python dependency into a virtual environment on the local    node (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program   on the YARN cluster (using that virtual environment). Please refer to this  patch  for more details.", 
            "title": "Use Python on YARN cluster"
        }, 
        {
            "location": "/APIdocs/Model/SequentialModel/", 
            "text": "Sequential Model", 
            "title": "Sequential Model"
        }, 
        {
            "location": "/APIdocs/Model/SequentialModel/#sequential-model", 
            "text": "", 
            "title": "Sequential Model"
        }, 
        {
            "location": "/APIdocs/Model/FunctionalAPI/", 
            "text": "Functional API", 
            "title": "Functional API"
        }, 
        {
            "location": "/APIdocs/Model/FunctionalAPI/#functional-api", 
            "text": "", 
            "title": "Functional API"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/", 
            "text": "Graph\n\n\nScala:\n\n\nval graph = Graph[Float](Array(Node), Array(Node))\n\n\n\n\nPython:\n\n\nmodel = Model([Node], [Node])\n\n\n\n\nA graph container. Each node can have multiple inputs. The output of the node should be a tensor.\n The output tensor can be connected to multiple nodes. So the module in each node can have a\n tensor or table input, and should have a tensor output.\n\n\nThe graph container can have multiple inputs and multiple outputs. If there's one input, the\n input data fed to the graph module should be a tensor. If there're multiple inputs, the input\n data fed to the graph module should be a table, which is actually an sequence of tensor. The\n order of the input tensors should be same with the order of the input nodes. This is also\n applied to the gradient from the module in the back propagation.\n\n\nAll of the input modules must accept a tensor input. If your input module accept multiple\n tensors as input, you should add some Input module before it as input nodes and connect the\n output of the Input modules to that module.\n\n\nIf there's one output, the module output is a tensor. If there're multiple outputs, the module\n output is a table, which is actually an sequence of tensor. The order of the output tensors is\n same with the order of the output modules. This is also applied to the gradient passed to the\n module in the back propagation.\n\n\nAll inputs should be able to connect to outputs through some paths in the graph. It is\n allowed that some successors of the inputs node are not connect to outputs. If so, these nodes\n will be excluded in the computation.\n\n\nScala example:\n\n\n\nval input1 = Input[Float]()\nval input2 = Input[Float]()\nval cadd = CAddTable[Float]().apply(input1, input2)\nval graph = Graph[Float](Array(input1, input2), cadd)\n\nval output = graph.forward(T(Tensor[Float](T(0.1f, 0.2f, -0.3f, -0.4f)),\n                             Tensor[Float](T(0.5f, 0.4f, -0.2f, -0.1f))))\nval gradInput = graph.backward(T(Tensor[Float](T(0.1f, 0.2f, -0.3f, -0.4f)),\n                                 Tensor[Float](T(0.5f, 0.4f, -0.2f, -0.1f))),\n                               Tensor[Float](T(0.1f, 0.2f, 0.3f, 0.4f)))\n\n\n println(output)\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.6\n0.6\n-0.5\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n {\n        2: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n        1: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n }\n\n\n\n\n\n\n\nPython example:\n\n\n\nfc1 = Linear(4, 2)()\nfc2 = Linear(4, 2)()\ncadd = CAddTable()([fc1, fc2])\noutput1 = ReLU()(cadd)\noutput2 = Threshold(10.0)(cadd)\nmodel = Model([fc1, fc2], [output1, output2])\nfc1.element().set_weights([np.ones((4, 2)), np.ones((2, ))])\nfc2.element().set_weights([np.ones((4, 2)) * 2, np.ones((2, )) * 2])\n\noutput = model.forward([np.array([0.1, 0.2, -0.3, -0.4]),\n                        np.array([0.5, 0.4, -0.2, -0.1])])\n\ngradInput = model.backward([np.array([0.1, 0.2, -0.3, -0.4]),\n                            np.array([0.5, 0.4, -0.2, -0.1])],\n                           [np.array([1.0, 2.0]),\n                                    np.array([3.0, 4.0])])\nweights = fc1.element().get_weights()[0]\n\n\n output\n[array([ 3.79999971,  3.79999971], dtype=float32),\n array([ 0.,  0.], dtype=float32)]\n\n gradInput\n[array([ 3.,  3.,  3.,  3.], dtype=float32),\n array([ 6.,  6.,  6.,  6.], dtype=float32)]\n\n\n\n\n\nBottle\n\n\nScala:\n\n\nval model = Bottle(module, nInputDim, nOutputDim)\n\n\n\n\nPython:\n\n\nmodel = Bottle(module, nInputDim, nOutputDim)\n\n\n\n\nBottle allows varying dimensionality input to be forwarded through any module that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Bottle(Linear[Float](3, 2), 2, 2)\nval input = Tensor(2, 3, 3).rand()\n\nscala\n print(input)\n(1,.,.) =\n0.7843752   0.17286697  0.20767091  \n0.8594811   0.9100018   0.8448141   \n0.7683892   0.36661968  0.76637685  \n\n(2,.,.) =\n0.7163263   0.083962396 0.81222403  \n0.7947034   0.09976136  0.114404656 \n0.14890474  0.43289232  0.1489096   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3x3] \n\nval output = model.forward(input)\n\nscala\n print(output)\n(1,.,.) =\n-0.31146684 0.40719786  \n-0.51778656 0.58715886  \n-0.51676923 0.4027511   \n\n(2,.,.) =\n-0.5498678  0.29658738  \n-0.280177   0.39901164  \n-0.2387946  0.24809375  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2]\n\n\n\n\nPython example:\n\n\nmodel = Bottle(Linear(3, 2), 2, 2)\n\ninput = np.random.randn(2, 3, 3)\noutput = model.forward(input)\n\n\n print(input)\n[[[ 0.42370589 -1.7938942   0.56666373]\n  [-1.78501381  0.55676471 -0.50150367]\n  [-1.59262182  0.82079469  1.1873599 ]]\n\n [[ 0.95799792 -0.71447244  1.05344083]\n  [-0.07838376 -0.88780484 -1.80491177]\n  [ 0.99996222  1.39876002 -0.16326094]]]\n\n print(output)\n[[[ 0.26298434  0.74947536]\n  [-1.24375117 -0.33148435]\n  [-1.35218966  0.17042145]]\n\n [[ 0.08041853  0.91245329]\n  [-0.08317742 -0.13909879]\n  [-0.52287608  0.3667658 ]]]\n\n\n\n\nContainer\n\n\nContainer is a subclass of abstract class AbstractModule, which\ndeclares methods defined in all containers. A container usually\ncontains some other modules in the \nmodules\n variable. It overrides\nmany module methods such that calls are propogated to the contained\nmodules.\n\n\nTimeDistributed\n\n\nScala:\n\n\nval layer = TimeDistributed[T](layer)\n\n\n\n\nPython:\n\n\nlayer = TimeDistributed(layer)\n\n\n\n\nThis layer is intended to apply contained layer to each temporal time slice\nof input tensor.\n\n\nThe input data format is [Batch, Time, Other dims]. For the contained layer, it must not change\nthe Other dims length.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = TimeDistributed[Float](Sum[Float](1, squeeze = false, nInputDims = 2))\nval input = Tensor[Float](T(T(\n  T(\n    T(1.0f, 2.0f),\n    T(3.0f, 4.0f)\n  ),\n  T(\n    T(2.0f, 3.0f),\n    T(4.0f, 5.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor[Float](T(T(\n  T(\n    T(0.1f, 0.2f)\n  ),\n  T(\n    T(0.3f, 0.4f)\n  )\n))))\n\n\n\n\nIts output should be\n\n\n(1,1,.,.) =\n4.0     6.0\n\n(1,2,.,.) =\n6.0     8.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1x2]\n\n(1,1,.,.) =\n0.1     0.2\n0.1     0.2\n\n(1,2,.,.) =\n0.3     0.4\n0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import TimeDistributed,Sum\nimport numpy as np\n\nlayer = TimeDistributed(Sum(1, squeeze = False, n_input_dims = 2))\n\ninput = np.array([[\n  [\n    [1.0, 2.0],\n    [3.0, 4.0]\n  ],\n  [\n    [2.0, 3.0],\n    [4.0, 5.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[\n  [\n    [0.1, 0.2]\n  ],\n  [\n    [0.3, 0.4]\n  ]\n]]))\n\n\n\n\nIts output should be\n\n\narray([[[[ 4.,  6.]],\n\n        [[ 6.,  8.]]]], dtype=float32)\n\narray([[[[ 0.1       ,  0.2       ],\n         [ 0.1       ,  0.2       ]],\n\n        [[ 0.30000001,  0.40000001],\n         [ 0.30000001,  0.40000001]]]], dtype=float32)\n\n\n\n\nMapTable\n\n\nScala:\n\n\nval mod = MapTable(module=null)\n\n\n\n\nPython:\n\n\nmod = MapTable(module=None)\n\n\n\n\nThis class is a container for a single module which will be applied\nto all input elements. The member module is cloned as necessary to\nprocess all input elements.\n\n\nmodule\n a member module.  \n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \n\nval map = MapTable()\nmap.add(Linear(10, 3))\nval input = T(\n      Tensor(10).randn(),\n      Tensor(10).randn())\n\n print(map.forward(input))\n{\n    2: 0.2444828\n       -1.1700082\n       0.15887381\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    1: 0.06696482\n       0.18692614\n       -1.432079\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nmap = MapTable()\nmap.add(Linear(10, 3))\ninput = [np.random.rand(10), np.random.rand(10)]\n\nmap.forward(input)\n[array([ 0.69586945, -0.70547599, -0.05802459], dtype=float32),\n array([ 0.47995114, -0.67459631, -0.52500772], dtype=float32)]\n\n\n\n\nConcatTable\n\n\nScala:\n\n\nval module = ConcatTable()\n\n\n\n\nPython:\n\n\nmodule = ConcatTable()\n\n\n\n\nConcateTable is a container module like Concate. Applies an input\nto each member module, input can be a tensor or a table.\n\n\nConcateTable usually works with CAddTable and CMulTable to\n implement element wise add/multiply on outputs of two modules.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = ConcatTable()\nmlp.add(Linear(3, 2))\nmlp.add(Linear(3, 4))\n\n\n print(mlp.forward(Tensor(2, 3).rand()))\n\n{\n    2: -0.37111914  0.8542446   -0.362602   -0.75522065 \n       -0.28713673  0.6021913   -0.16525984 -0.44689763 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n    1: -0.79941726  0.8303885   \n       -0.8342782   0.89961016  \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nmlp = ConcatTable()\nmlp.add(Linear(3, 2))   \nmlp.add(Linear(3, 4))\n\n mlp.forward(np.array([[1, 2, 3], [1, 2, 3]]))\nout: [array([[ 1.16408789, -0.1508013 ],\n             [ 1.16408789, -0.1508013 ]], dtype=float32),\n      array([[-0.24672163, -0.56958938, -0.51390374,  0.64546645],\n             [-0.24672163, -0.56958938, -0.51390374,  0.64546645]], dtype=float32)]\n\n\n\n\n\nParallelTable\n\n\nScala:\n\n\nval module = ParallelTable()\n\n\n\n\nPython:\n\n\nmodule = ParallelTable()\n\n\n\n\nIt is a container module that applies the i-th member module to the i-th\n input, and outputs an output in the form of Table\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = ParallelTable()\nval log = Log()\nval exp = Exp()\nmodule.add(log)\nmodule.add(exp)\nval input1 = Tensor(3, 3).rand(0, 1)\nval input2 = Tensor(3).rand(0, 1)\nval input = T(1 -\n input1, 2 -\n input2)\n\n print(module.forward(input))\n {\n        2: 2.6996834\n           2.0741253\n           1.0625387\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: -1.073425    -0.6672964      -1.8160943\n           -0.54094607  -1.3029919      -1.7064717\n           -0.66175103  -0.08288143     -1.1840979\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nmodule = ParallelTable()\nlog = Log()\nexp = Exp()\nmodule.add(log)\nmodule.add(exp)\ninput1 = np.random.rand(3,3)\ninput2 = np.random.rand(3)\n\nmodule.forward([input1, input2])\n[array([[-1.27472472, -2.18216252, -0.60752904],\n        [-2.76213861, -1.77966928, -0.13652121],\n        [-1.47725129, -0.03578046, -1.37736678]], dtype=float32),\n array([ 1.10634041,  1.46384597,  1.96525407], dtype=float32)]\n\n\n\n\nConcat\n\n\nScala:\n\n\nval module = Concat(dimension)\n\n\n\n\nPython:\n\n\nmodule = Concat(dimension)\n\n\n\n\nConcat is a container who concatenates the output of it's submodules along the\nprovided \ndimension\n: all submodules take the same inputs, and their output is\nconcatenated.\n\n\n                 +----Concat----+\n            +----\n  submodule1  -----+\n            |    |              |    |\n input -----+----\n  submodule2  -----+----\n output\n            |    |              |    |\n            +----\n  submodule3  -----+\n                 +--------------+\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\n\nprintln(mlp.forward(Tensor(2, 3).rand()))\n\n\n\n\nOutput is\n\n\n-0.17087375 0.12954286  0.15685591  -0.027277306    0.38549712  -0.20375136\n-0.9473443  0.030516684 0.23380546  0.625985    -0.031360716    0.40449825\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\nprint(mlp.forward(np.array([[1, 2, 3], [1, 2, 3]])))\n\n\n\n\nOutput is\n\n\n[array([\n[-0.71994132,  2.17439198, -1.46522939,  0.64588934,  2.61534023, -2.39528942],\n[-0.89125222,  5.49583197, -2.8865242 ,  1.44914722,  5.26639175, -6.26586771]]\n      dtype=float32)]\n\n\n\n\n\nSequential\n\n\nScala:\n\n\nval module = Sequential()\n\n\n\n\nPython:\n\n\nseq = Sequential()\n\n\n\n\nSequential provides a means to plug layers together\nin a feed-forward fully connected manner.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.nn.{Sequential, Linear}\n\nval module = Sequential()\nmodule.add(Linear(10, 25))\nmodule.add(Linear(25, 10))\n\nval input = Tensor(10).range(1, 10, 1)\nval gradOutput = Tensor(10).range(1, 10, 1)\n\nval output = module.forward(input).toTensor\nval gradInput = module.backward(input, gradOutput).toTensor\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\nThe output is,\n\n\n-2.3750305\n2.4512818\n1.6998017\n-0.47432393\n4.3048754\n-0.044168986\n-1.1643536\n0.60341483\n2.0216258\n2.1190155\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]\n\n\n\n\nThe gradInput is,\n\n\n2.593382\n-1.4137214\n-1.8271983\n1.229643\n0.51384985\n1.509845\n2.9537349\n1.088281\n0.2618509\n1.4840821\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nseq = Sequential()\nseq.add(Linear(10, 25))\nseq.add(Linear(25, 10))\n\ninput = np.arange(1, 11, 1).astype(\nfloat32\n)\ninput = input.reshape(1, 10)\n\noutput = seq.forward(input)\nprint output\n\ngradOutput = np.arange(1, 11, 1).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(1, 10)\n\ngradInput = seq.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nThe output is,\n\n\n[array([[ 1.08462083, -2.03257799, -0.5400058 ,  0.27452484,  1.85562158,\n         1.64338267,  2.45694995,  1.70170391, -2.12998056, -1.28924525]], dtype=float32)]\n\n\n\n\nThe gradInput is,\n\n\n\n[array([[ 1.72007763,  1.64403224,  2.52977395, -1.00021958,  0.1134415 ,\n         2.06711197,  2.29631734, -3.39587498,  1.01093054, -0.54482007]], dtype=float32)]", 
            "title": "Containers"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#graph", 
            "text": "Scala:  val graph = Graph[Float](Array(Node), Array(Node))  Python:  model = Model([Node], [Node])  A graph container. Each node can have multiple inputs. The output of the node should be a tensor.\n The output tensor can be connected to multiple nodes. So the module in each node can have a\n tensor or table input, and should have a tensor output.  The graph container can have multiple inputs and multiple outputs. If there's one input, the\n input data fed to the graph module should be a tensor. If there're multiple inputs, the input\n data fed to the graph module should be a table, which is actually an sequence of tensor. The\n order of the input tensors should be same with the order of the input nodes. This is also\n applied to the gradient from the module in the back propagation.  All of the input modules must accept a tensor input. If your input module accept multiple\n tensors as input, you should add some Input module before it as input nodes and connect the\n output of the Input modules to that module.  If there's one output, the module output is a tensor. If there're multiple outputs, the module\n output is a table, which is actually an sequence of tensor. The order of the output tensors is\n same with the order of the output modules. This is also applied to the gradient passed to the\n module in the back propagation.  All inputs should be able to connect to outputs through some paths in the graph. It is\n allowed that some successors of the inputs node are not connect to outputs. If so, these nodes\n will be excluded in the computation.  Scala example:  \nval input1 = Input[Float]()\nval input2 = Input[Float]()\nval cadd = CAddTable[Float]().apply(input1, input2)\nval graph = Graph[Float](Array(input1, input2), cadd)\n\nval output = graph.forward(T(Tensor[Float](T(0.1f, 0.2f, -0.3f, -0.4f)),\n                             Tensor[Float](T(0.5f, 0.4f, -0.2f, -0.1f))))\nval gradInput = graph.backward(T(Tensor[Float](T(0.1f, 0.2f, -0.3f, -0.4f)),\n                                 Tensor[Float](T(0.5f, 0.4f, -0.2f, -0.1f))),\n                               Tensor[Float](T(0.1f, 0.2f, 0.3f, 0.4f)))  println(output)\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.6\n0.6\n-0.5\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  println(gradInput)\ngradInput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n {\n        2: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n        1: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n }  Python example:  \nfc1 = Linear(4, 2)()\nfc2 = Linear(4, 2)()\ncadd = CAddTable()([fc1, fc2])\noutput1 = ReLU()(cadd)\noutput2 = Threshold(10.0)(cadd)\nmodel = Model([fc1, fc2], [output1, output2])\nfc1.element().set_weights([np.ones((4, 2)), np.ones((2, ))])\nfc2.element().set_weights([np.ones((4, 2)) * 2, np.ones((2, )) * 2])\n\noutput = model.forward([np.array([0.1, 0.2, -0.3, -0.4]),\n                        np.array([0.5, 0.4, -0.2, -0.1])])\n\ngradInput = model.backward([np.array([0.1, 0.2, -0.3, -0.4]),\n                            np.array([0.5, 0.4, -0.2, -0.1])],\n                           [np.array([1.0, 2.0]),\n                                    np.array([3.0, 4.0])])\nweights = fc1.element().get_weights()[0]  output\n[array([ 3.79999971,  3.79999971], dtype=float32),\n array([ 0.,  0.], dtype=float32)]  gradInput\n[array([ 3.,  3.,  3.,  3.], dtype=float32),\n array([ 6.,  6.,  6.,  6.], dtype=float32)]", 
            "title": "Graph"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#bottle", 
            "text": "Scala:  val model = Bottle(module, nInputDim, nOutputDim)  Python:  model = Bottle(module, nInputDim, nOutputDim)  Bottle allows varying dimensionality input to be forwarded through any module that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Bottle(Linear[Float](3, 2), 2, 2)\nval input = Tensor(2, 3, 3).rand()\n\nscala  print(input)\n(1,.,.) =\n0.7843752   0.17286697  0.20767091  \n0.8594811   0.9100018   0.8448141   \n0.7683892   0.36661968  0.76637685  \n\n(2,.,.) =\n0.7163263   0.083962396 0.81222403  \n0.7947034   0.09976136  0.114404656 \n0.14890474  0.43289232  0.1489096   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3x3] \n\nval output = model.forward(input)\n\nscala  print(output)\n(1,.,.) =\n-0.31146684 0.40719786  \n-0.51778656 0.58715886  \n-0.51676923 0.4027511   \n\n(2,.,.) =\n-0.5498678  0.29658738  \n-0.280177   0.39901164  \n-0.2387946  0.24809375  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2]  Python example:  model = Bottle(Linear(3, 2), 2, 2)\n\ninput = np.random.randn(2, 3, 3)\noutput = model.forward(input)  print(input)\n[[[ 0.42370589 -1.7938942   0.56666373]\n  [-1.78501381  0.55676471 -0.50150367]\n  [-1.59262182  0.82079469  1.1873599 ]]\n\n [[ 0.95799792 -0.71447244  1.05344083]\n  [-0.07838376 -0.88780484 -1.80491177]\n  [ 0.99996222  1.39876002 -0.16326094]]]  print(output)\n[[[ 0.26298434  0.74947536]\n  [-1.24375117 -0.33148435]\n  [-1.35218966  0.17042145]]\n\n [[ 0.08041853  0.91245329]\n  [-0.08317742 -0.13909879]\n  [-0.52287608  0.3667658 ]]]", 
            "title": "Bottle"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#container", 
            "text": "Container is a subclass of abstract class AbstractModule, which\ndeclares methods defined in all containers. A container usually\ncontains some other modules in the  modules  variable. It overrides\nmany module methods such that calls are propogated to the contained\nmodules.", 
            "title": "Container"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#timedistributed", 
            "text": "Scala:  val layer = TimeDistributed[T](layer)  Python:  layer = TimeDistributed(layer)  This layer is intended to apply contained layer to each temporal time slice\nof input tensor.  The input data format is [Batch, Time, Other dims]. For the contained layer, it must not change\nthe Other dims length.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = TimeDistributed[Float](Sum[Float](1, squeeze = false, nInputDims = 2))\nval input = Tensor[Float](T(T(\n  T(\n    T(1.0f, 2.0f),\n    T(3.0f, 4.0f)\n  ),\n  T(\n    T(2.0f, 3.0f),\n    T(4.0f, 5.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor[Float](T(T(\n  T(\n    T(0.1f, 0.2f)\n  ),\n  T(\n    T(0.3f, 0.4f)\n  )\n))))  Its output should be  (1,1,.,.) =\n4.0     6.0\n\n(1,2,.,.) =\n6.0     8.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1x2]\n\n(1,1,.,.) =\n0.1     0.2\n0.1     0.2\n\n(1,2,.,.) =\n0.3     0.4\n0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]  Python example:  from bigdl.nn.layer import TimeDistributed,Sum\nimport numpy as np\n\nlayer = TimeDistributed(Sum(1, squeeze = False, n_input_dims = 2))\n\ninput = np.array([[\n  [\n    [1.0, 2.0],\n    [3.0, 4.0]\n  ],\n  [\n    [2.0, 3.0],\n    [4.0, 5.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[\n  [\n    [0.1, 0.2]\n  ],\n  [\n    [0.3, 0.4]\n  ]\n]]))  Its output should be  array([[[[ 4.,  6.]],\n\n        [[ 6.,  8.]]]], dtype=float32)\n\narray([[[[ 0.1       ,  0.2       ],\n         [ 0.1       ,  0.2       ]],\n\n        [[ 0.30000001,  0.40000001],\n         [ 0.30000001,  0.40000001]]]], dtype=float32)", 
            "title": "TimeDistributed"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#maptable", 
            "text": "Scala:  val mod = MapTable(module=null)  Python:  mod = MapTable(module=None)  This class is a container for a single module which will be applied\nto all input elements. The member module is cloned as necessary to\nprocess all input elements.  module  a member module.    Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \n\nval map = MapTable()\nmap.add(Linear(10, 3))\nval input = T(\n      Tensor(10).randn(),\n      Tensor(10).randn())  print(map.forward(input))\n{\n    2: 0.2444828\n       -1.1700082\n       0.15887381\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    1: 0.06696482\n       0.18692614\n       -1.432079\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }  Python example:  from bigdl.nn.layer import *\n\nmap = MapTable()\nmap.add(Linear(10, 3))\ninput = [np.random.rand(10), np.random.rand(10)] map.forward(input)\n[array([ 0.69586945, -0.70547599, -0.05802459], dtype=float32),\n array([ 0.47995114, -0.67459631, -0.52500772], dtype=float32)]", 
            "title": "MapTable"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#concattable", 
            "text": "Scala:  val module = ConcatTable()  Python:  module = ConcatTable()  ConcateTable is a container module like Concate. Applies an input\nto each member module, input can be a tensor or a table.  ConcateTable usually works with CAddTable and CMulTable to\n implement element wise add/multiply on outputs of two modules.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = ConcatTable()\nmlp.add(Linear(3, 2))\nmlp.add(Linear(3, 4))  print(mlp.forward(Tensor(2, 3).rand()))\n\n{\n    2: -0.37111914  0.8542446   -0.362602   -0.75522065 \n       -0.28713673  0.6021913   -0.16525984 -0.44689763 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n    1: -0.79941726  0.8303885   \n       -0.8342782   0.89961016  \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n }  Python example:  from bigdl.nn.layer import *\n\nmlp = ConcatTable()\nmlp.add(Linear(3, 2))   \nmlp.add(Linear(3, 4))  mlp.forward(np.array([[1, 2, 3], [1, 2, 3]]))\nout: [array([[ 1.16408789, -0.1508013 ],\n             [ 1.16408789, -0.1508013 ]], dtype=float32),\n      array([[-0.24672163, -0.56958938, -0.51390374,  0.64546645],\n             [-0.24672163, -0.56958938, -0.51390374,  0.64546645]], dtype=float32)]", 
            "title": "ConcatTable"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#paralleltable", 
            "text": "Scala:  val module = ParallelTable()  Python:  module = ParallelTable()  It is a container module that applies the i-th member module to the i-th\n input, and outputs an output in the form of Table  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = ParallelTable()\nval log = Log()\nval exp = Exp()\nmodule.add(log)\nmodule.add(exp)\nval input1 = Tensor(3, 3).rand(0, 1)\nval input2 = Tensor(3).rand(0, 1)\nval input = T(1 -  input1, 2 -  input2)  print(module.forward(input))\n {\n        2: 2.6996834\n           2.0741253\n           1.0625387\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: -1.073425    -0.6672964      -1.8160943\n           -0.54094607  -1.3029919      -1.7064717\n           -0.66175103  -0.08288143     -1.1840979\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }  Python example:  from bigdl.nn.layer import *\n\nmodule = ParallelTable()\nlog = Log()\nexp = Exp()\nmodule.add(log)\nmodule.add(exp)\ninput1 = np.random.rand(3,3)\ninput2 = np.random.rand(3) module.forward([input1, input2])\n[array([[-1.27472472, -2.18216252, -0.60752904],\n        [-2.76213861, -1.77966928, -0.13652121],\n        [-1.47725129, -0.03578046, -1.37736678]], dtype=float32),\n array([ 1.10634041,  1.46384597,  1.96525407], dtype=float32)]", 
            "title": "ParallelTable"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#concat", 
            "text": "Scala:  val module = Concat(dimension)  Python:  module = Concat(dimension)  Concat is a container who concatenates the output of it's submodules along the\nprovided  dimension : all submodules take the same inputs, and their output is\nconcatenated.                   +----Concat----+\n            +----   submodule1  -----+\n            |    |              |    |\n input -----+----   submodule2  -----+----  output\n            |    |              |    |\n            +----   submodule3  -----+\n                 +--------------+  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\n\nprintln(mlp.forward(Tensor(2, 3).rand()))  Output is  -0.17087375 0.12954286  0.15685591  -0.027277306    0.38549712  -0.20375136\n-0.9473443  0.030516684 0.23380546  0.625985    -0.031360716    0.40449825\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\nprint(mlp.forward(np.array([[1, 2, 3], [1, 2, 3]])))  Output is  [array([\n[-0.71994132,  2.17439198, -1.46522939,  0.64588934,  2.61534023, -2.39528942],\n[-0.89125222,  5.49583197, -2.8865242 ,  1.44914722,  5.26639175, -6.26586771]]\n      dtype=float32)]", 
            "title": "Concat"
        }, 
        {
            "location": "/APIdocs/Layers/Containers/merged-Containers/#sequential", 
            "text": "Scala:  val module = Sequential()  Python:  seq = Sequential()  Sequential provides a means to plug layers together\nin a feed-forward fully connected manner.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.nn.{Sequential, Linear}\n\nval module = Sequential()\nmodule.add(Linear(10, 25))\nmodule.add(Linear(25, 10))\n\nval input = Tensor(10).range(1, 10, 1)\nval gradOutput = Tensor(10).range(1, 10, 1)\n\nval output = module.forward(input).toTensor\nval gradInput = module.backward(input, gradOutput).toTensor\n\nprintln(output)\nprintln(gradInput)  The output is,  -2.3750305\n2.4512818\n1.6998017\n-0.47432393\n4.3048754\n-0.044168986\n-1.1643536\n0.60341483\n2.0216258\n2.1190155\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]  The gradInput is,  2.593382\n-1.4137214\n-1.8271983\n1.229643\n0.51384985\n1.509845\n2.9537349\n1.088281\n0.2618509\n1.4840821\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nseq = Sequential()\nseq.add(Linear(10, 25))\nseq.add(Linear(25, 10))\n\ninput = np.arange(1, 11, 1).astype( float32 )\ninput = input.reshape(1, 10)\n\noutput = seq.forward(input)\nprint output\n\ngradOutput = np.arange(1, 11, 1).astype( float32 )\ngradOutput = gradOutput.reshape(1, 10)\n\ngradInput = seq.backward(input, gradOutput)\nprint gradInput  The output is,  [array([[ 1.08462083, -2.03257799, -0.5400058 ,  0.27452484,  1.85562158,\n         1.64338267,  2.45694995,  1.70170391, -2.12998056, -1.28924525]], dtype=float32)]  The gradInput is,  \n[array([[ 1.72007763,  1.64403224,  2.52977395, -1.00021958,  0.1134415 ,\n         2.06711197,  2.29631734, -3.39587498,  1.01093054, -0.54482007]], dtype=float32)]", 
            "title": "Sequential"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/", 
            "text": "Reverse\n\n\nScala:\n\n\nval m = Reverse(dim = 1, isInplace = false)\n\n\n\n\nPython:\n\n\nm = Reverse(dimension=1)\n\n\n\n\nReverse the input w.r.t given dimension.\n The input can be a Tensor or Table. \nDimension is one-based index\n \n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(0, 1)\nval input = Tensor(2, 3)\ninput.apply1(x =\n randomn().toFloat)\nprintln(\ninput:\n)\nprintln(input)\nval layer = new Reverse(1)\nprintln(\noutput:\n)\nprintln(layer.forward(input))\n\n\n\n\ninput:\n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\noutput:\n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\n\nPython example:\n\n\ninput = np.random.random((2,3))\nlayer = Reverse(1)\nprint(\ninput:\n)\nprint(input)\nprint(\noutput:\n)\nprint(layer.forward(input))\n\n\n\n\ncreating: createReverse\ninput:\n[[ 0.89089717  0.07629756  0.30863782]\n [ 0.16066851  0.06421963  0.96719367]]\noutput:\n[[ 0.16066851  0.06421963  0.96719366]\n [ 0.89089715  0.07629756  0.30863783]]\n\n\n\n\n\n\nReshape\n\n\nScala:\n\n\nval reshape = Reshape(size, batchMode)\n\n\n\n\nPython:\n\n\nreshape = Reshape(size, batch_mode)\n\n\n\n\nThe \nforward(input)\n reshape the input tensor into \nsize(0) * size(1) * ...\n tensor,\ntaking the elements row-wise.\n\n\nparameters\n\n\n \nsize\n - the size after reshape\n\n \nbatchMode\n - It is a optional argument. If it is set to \nSome(true)\n,\n                  the first dimension of input is considered as batch dimension,\n                  and thus keep this dimension size fixed. This is necessary\n                  when dealing with batch sizes of one. When set to \nSome(false)\n,\n                  it forces the entire input (including the first dimension) to be reshaped\n                  to the input size. Default is \nNone\n, which means the module considers\n                  inputs with more elements than the product of provided sizes (size(0) *\n                  size(1) * ..) to be batches, otherwise in no batch mode.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval reshape = Reshape(Array(3, 2))\nval input = Tensor(2, 2, 3).rand()\nval output = reshape.forward(input)\n-\n print(output.size().toList)      \nList(2, 3, 2)\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nreshape =  Reshape([3, 2])\ninput = np.random.rand(2, 2, 3)\noutput = reshape.forward(input)\n-\n print output[0].shape\n(2, 3, 2)\n\n\n\n\nIndex\n\n\nScala:\n\n\nval model = Index(dimension)\n\n\n\n\nPython:\n\n\nmodel = Index(dimension)\n\n\n\n\nApplies the Tensor index operation along the given dimension.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).rand()\nval input2 = Tensor(4)\ninput2(Array(1)) = 1.0f\ninput2(Array(2)) = 2.0f\ninput2(Array(3)) = 2.0f\ninput2(Array(4)) = 3.0f\n\nval input = T(input1, input2)\nval model = Index(1)\nval output = model.forward(input)\n\nscala\n print(input)\n {\n    2: 1.0\n       2.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4]\n    1: 0.124325536\n       0.8768922\n       0.6378146\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\nscala\n print(output)\n0.124325536\n0.8768922\n0.8768922\n0.6378146\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.randn(3)\ninput2 = np.array([1, 2, 2, 3])\ninput = [input1, input2]\n\nmodel = Index(1)\noutput = model.forward(input)\n\n\n print(input)\n[array([-0.45804847, -0.20176707,  0.50963248]), array([1, 2, 2, 3])]\n\n\n print(output)\n[-0.45804846 -0.20176707 -0.20176707  0.50963247]\n\n\n\n\nIdentity\n\n\nScala:\n\n\nval identity = Identity()\n\n\n\n\nPython:\n\n\nidentity = Identity()\n\n\n\n\nIdentity just return input as the output which is useful in same parallel container to get an origin input\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval identity = Identity()\n\nval input = Tensor(3, 3).rand()\n\n print(input)\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n print(identity.forward(input))\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nidentity = Identity()\n\n  identity.forward(np.array([[1, 2, 3], [4, 5, 6]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]], dtype=float32)]\n\n\n\n\n\nNarrow\n\n\nScala:\n\n\nval layer = Narrow(dimension, offset, length = 1)\n\n\n\n\nPython:\n\n\nlayer = Narrow(dimension, offset, length=1)\n\n\n\n\nNarrow is an application of narrow operation in a module.\nThe module further supports a negative length in order to handle inputs with an unknown size.\n\n\nParameters:\n\n\ndimension\n -narrow along this dimension\n\n\noffset\n -the start index on the given dimension\n\n\nlength\n -length to narrow, default value is 1\n\n\nScala Example\n\n\nimport com.intel.analytics.bigdl.nn.Narrow\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Narrow(2, 2)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\n2.0\n3.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]\n\nval grad = layer.backward(input, gradOutput)\n0.0 3.0 0.0\n0.0 4.0 0.0\n0.0 5.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython Example\n\n\nlayer = Narrow(2, 2)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[ 2.]\n [ 3.]\n [ 4.]]\n\nprint grad\n[[ 0.  3.  0.]\n [ 0.  4.  0.]\n [ 0.  5.  0.]]\n\n\n\n\nUnsqueeze\n\n\nScala:\n\n\nval layer = Unsqueeze[Float](dim)\n\n\n\n\nPython:\n\n\nlayer = Unsqueeze(dim)\n\n\n\n\nInsert singleton dim (i.e., dimension 1) at position pos. For an input with dim = input.dim(),\nthere are dim + 1 possible positions to insert the singleton dimension. The dim starts from 1.\n\n\nScala example:\n\n\n\nval layer = Unsqueeze[Float](2)\nval input = Tensor[Float](2, 2, 2).rand\nval gradOutput = Tensor[Float](2, 1, 2, 2).rand\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)\n\n\n println(input.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n println(gradOutput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]\n\n\n println(output.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]\n\n\n println(gradInput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nlayer = Unsqueeze(2)\ninput = np.random.uniform(0, 1, (2, 2, 2)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (2, 1, 2, 2)).astype(\nfloat32\n)\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)\n\n\n output\n[array([[[[ 0.97488612,  0.43463323],\n          [ 0.39069486,  0.0949123 ]]],\n\n\n        [[[ 0.19310953,  0.73574477],\n          [ 0.95347691,  0.37380624]]]], dtype=float32)]\n\n gradInput\n[array([[[ 0.9995622 ,  0.69787127],\n         [ 0.65975296,  0.87002522]],\n\n        [[ 0.76349133,  0.96734989],\n         [ 0.88068211,  0.07284366]]], dtype=float32)]\n\n\n\n\nSqueeze\n\n\nScala:\n\n\nval module = Squeeze(dims=null, numInputDims=Int.MinValue)\n\n\n\n\nPython:\n\n\nmodule = Squeeze(dims, numInputDims=-2147483648)\n\n\n\n\nDelete all singleton dimensions or a specific singleton dimension.\n\n\ndims\n Optional. If this dimension is singleton dimension, it will be deleted.\n           The first index starts from 1. Default: delete all dimensions.\n\n\nnum_input_dims\n Optional. If in a batch model, set to the inputDims.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = Squeeze(2)\n\n print(layer.forward(Tensor(2, 1, 3).rand()))\n0.43709445  0.42752415  0.43069172  \n0.67029667  0.95641375  0.28823504  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = Squeeze(2)\n\nlayer.forward(np.array([[[1, 2, 3]], [[1, 2, 3]]]))\nout: array([[ 1.,  2.,  3.],\n            [ 1.,  2.,  3.]], dtype=float32)\n\n\n\n\n\nSelect\n\n\nScala:\n\n\nval layer = Select[T](dim, index)\n\n\n\n\nPython:\n\n\nlayer = Select(dim, index)\n\n\n\n\nA Simple layer selecting an index of the input tensor in the given dimension.\nPlease note that the index and dimension start from 1. In collaborative filtering, it can used together with LookupTable to create embeddings for users or items.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = Select[Float](1, 2)\nlayer.forward(Tensor[Float](T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\n\nlayer.backward(Tensor[Float](T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), Tensor[Float](T(0.1f, 0.2f, 0.3f)))\n\n\n\n\nIts output should be\n\n\n4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n0.0     0.0     0.0\n0.1     0.2     0.3\n0.0     0.0     0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import Select\nimport numpy as np\n\nlayer = Select(1, 2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), np.array([0.1, 0.2, 0.3]))\n\n\n\n\nIts output should be\n\n\narray([ 4.,  5.,  6.], dtype=float32)\n\narray([[ 0.        ,  0.        ,  0.        ],\n       [ 0.1       ,  0.2       ,  0.30000001],\n       [ 0.        ,  0.        ,  0.        ]], dtype=float32)\n\n\n\n\nMaskedSelect\n\n\nScala:\n\n\nval module = MaskedSelect()\n\n\n\n\nPython:\n\n\nmodule = MaskedSelect()\n\n\n\n\nPerforms a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument\n with the input on the forward and backward passes.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport scala.util.Random\n\n\nval layer = MaskedSelect()\nval input1 = Tensor(2, 2).apply1(e =\n Random.nextFloat())\nval mask = Tensor(2, 2)\nmask(Array(1, 1)) = 1\nmask(Array(1, 2)) = 0\nmask(Array(2, 1)) = 0\nmask(Array(2, 2)) = 1\nval input = T()\ninput(1.0) = input1\ninput(2.0) = mask\n\n print(layer.forward(input))\n0.2577119\n0.5061479\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = MaskedSelect()\ninput1 = np.random.rand(2,2)\nmask = np.array([[1,0], [0, 1]])\n\nlayer.forward([input1, mask])\narray([ 0.1525335 ,  0.05474588], dtype=float32)\n\n\n\n\nTranspose\n\n\nScala:\n\n\nval module = Transpose(permutations)\n\n\n\n\nPython:\n\n\nmodule = Transpose(permutations)\n\n\n\n\nConcat is a layer who transpose input along specified dimensions.\npermutations are dimension pairs that need to swap.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).rand()\nval layer = Transpose(Array((1, 2)))\nval output = layer.forward(input)\n\n\n input\n0.6653826   0.25350887  0.33434764  \n0.9618287   0.5484164   0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\n\n output\n0.6653826   0.9618287   \n0.25350887  0.5484164   \n0.33434764  0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nlayer = Transpose([(1,2)])\ninput = np.array([[0.6653826, 0.25350887, 0.33434764], [0.9618287, 0.5484164, 0.64844745]])\noutput = layer.forward(input)\n\n\n output\n[array([[ 0.66538262,  0.96182871],\n       [ 0.25350887,  0.54841638],\n       [ 0.33434764,  0.64844745]], dtype=float32)]\n\n\n\n\n\nInferReshape\n\n\nScala:\n\n\nval layer = InferReshape(size, batchMode = false)\n\n\n\n\nPython:\n\n\nlayer = InferReshape(size, batch_mode=False)\n\n\n\n\nReshape the input tensor with automatic size inference support.\nPositive numbers in the \nsize\n argument are used to reshape the input to the\ncorresponding dimension size.\n\n\nThere are also two special values allowed in \nsize\n:\n\n\n\n\n0\n means keep the corresponding dimension size of the input unchanged.\n      i.e., if the 1st dimension size of the input is 2,\n      the 1st dimension size of output will be set as 2 as well.\n\n\n-1\n means infer this dimension size from other dimensions.\n      This dimension size is calculated by keeping the amount of output elements\n      consistent with the input.\n      Only one \n-1\n is allowable in \nsize\n.\n\n\n\n\nFor example,\n\n\n   Input tensor with size: (4, 5, 6, 7)\n   -\n InferReshape(Array(4, 0, 3, -1))\n   Output tensor with size: (4, 5, 3, 14)\n\n\n\n\nThe 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,\nand inferred the last dim as 14.\n\n\nParameters:\n\n\nsize\n      -the target tensor size\n\n\nbatchMode\n -whether in batch mode\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.InferReshape\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval layer = InferReshape(Array(0, 3, -1))\nval input = Tensor(1, 2, 3).rand()\nval gradOutput = Tensor(1, 3, 2).rand()\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n0.8170822   0.40073588\n0.49389255  0.3782435\n0.42660004  0.5917206\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x2]\n\nprintln(grad)\n(1,.,.) =\n0.8294597   0.57101834  0.90910035\n0.32783163  0.30494633  0.7339092\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3]\n\n\n\n\nPython example:\n\n\nlayer = InferReshape([0, 3, -1])\ninput = np.random.rand(1, 2, 3)\n\ngradOutput = np.random.rand(1, 3, 2)\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 0.68635464  0.21277553]\n  [ 0.13390459  0.65662414]\n  [ 0.1021723   0.92319047]]]\n\nprint grad\n[[[ 0.84927064  0.55205333  0.25077972]\n  [ 0.76105869  0.30828172  0.1237276 ]]]\n\n\n\n\nReplicate\n\n\nScala:\n\n\nval module = Replicate(\n  nFeatures,\n  dim = 1,\n  nDim = Int.MaxValue)\n\n\n\n\nPython:\n\n\nmodule = Replicate(\n  n_features,\n  dim=1,\n  n_dim=INTMAX)\n\n\n\n\nReplicate repeats input \nnFeatures\n times along its \ndim\n dimension\n\n\nNotice: No memory copy, it set the stride along the \ndim\n-th dimension to zero.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Replicate(4, 1, 2)\n\nprintln(module.forward(Tensor.range(1, 6, 1).resize(1, 2, 3)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,2,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,3,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,4,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Replicate(4, 1, 2)\n\nprint(module.forward(np.arange(1, 7, 1).reshape(1, 2, 3)))\n\n\n\n\nOutput is \n\n\n[array([[[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]]]], dtype=float32)]\n\n\n\n\nView\n\n\nScala:\n\n\nval view = View(2, 8)\n\n\n\n\nor\n\n\nval view = View(Array(2, 8))\n\n\n\n\nPython:\n\n\nview = View([2, 8])\n\n\n\n\nThis module creates a new view of the input tensor using the sizes passed to the constructor.\nThe method setNumInputDims() allows to specify the expected number of dimensions of the inputs\nof the modules. This makes it possible to use minibatch inputs\nwhen using a size -1 for one of the dimensions.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.View\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval view = View(2, 8)\n\nval input = Tensor(4, 4).randn()\nval gradOutput = Tensor(2, 8).randn()\n\nval output = view.forward(input)\nval gradInput = view.backward(input, gradOutput)\n\n\n\n\nThe output is,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.43037438     1.2982363       -1.4723133      -0.2602826      0.7178128       -1.8763185      0.88629466      0.8346704\n0.20963766      -0.9349786      1.0376515       1.3153045       1.5450214       1.084113        -0.29929757     -0.18356979\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]\n\n\n\n\nThe gradInput is,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7360089       0.9133299       0.40443268      -0.94965595\n0.80520976      -0.09671917     -0.5498001      -0.098691925\n-2.3119886      -0.8455147      0.75891125      1.2985301\n0.5023749       1.4983269       0.42038065      -1.7002305\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nview = View([2, 8])\n\ninput = np.random.uniform(0, 1, [4, 4]).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, [2, 8]).astype(\nfloat32\n)\n\noutput = view.forward(input)\ngradInput = view.backward(input, gradOutput)\n\nprint output\nprint gradInput\n\n\n\n\nContiguous\n\n\nBe used to make input, gradOutput both contiguous\n\n\nScala:\n\n\nval contiguous = Contiguous()\n\n\n\n\nPython:\n\n\ncontiguous = Contiguous()\n\n\n\n\nUsed to make input, gradOutput both contiguous\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Contiguous\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(5).range(1, 5, 1)\nval contiguous = new Contiguous()\nval output = contiguous.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(5).range(2, 6, 1)\nval gradInput = contiguous.backward(input, gradOutput)\nprintln(gradOutput)\n\n\n\n\nThe output will be,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n2.0\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\n\n\n\nThe gradInput will be,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.0\n3.0\n4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ncontiguous = Contiguous()\n\ninput = np.arange(1, 6, 1).astype(\nfloat32\n)\ninput = input.reshape(1, 5)\n\noutput = contiguous.forward(input)\nprint output\n\ngradOutput = np.arange(2, 7, 1).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(1, 5)\n\ngradInput = contiguous.backward(input, gradOutput)\nprint gradInput\n\n\n\n\n\nThe output will be,\n\n\n[array([[ 1.,  2.,  3.,  4.,  5.]], dtype=float32)]\n\n\n\n\nThe gradInput will be,\n\n\n[array([[ 2.,  3.,  4.,  5.,  6.]], dtype=float32)]", 
            "title": "Simple Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#reverse", 
            "text": "Scala:  val m = Reverse(dim = 1, isInplace = false)  Python:  m = Reverse(dimension=1)  Reverse the input w.r.t given dimension.\n The input can be a Tensor or Table.  Dimension is one-based index    Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(0, 1)\nval input = Tensor(2, 3)\ninput.apply1(x =  randomn().toFloat)\nprintln( input: )\nprintln(input)\nval layer = new Reverse(1)\nprintln( output: )\nprintln(layer.forward(input))  input:\n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\noutput:\n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  input = np.random.random((2,3))\nlayer = Reverse(1)\nprint( input: )\nprint(input)\nprint( output: )\nprint(layer.forward(input))  creating: createReverse\ninput:\n[[ 0.89089717  0.07629756  0.30863782]\n [ 0.16066851  0.06421963  0.96719367]]\noutput:\n[[ 0.16066851  0.06421963  0.96719366]\n [ 0.89089715  0.07629756  0.30863783]]", 
            "title": "Reverse"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#reshape", 
            "text": "Scala:  val reshape = Reshape(size, batchMode)  Python:  reshape = Reshape(size, batch_mode)  The  forward(input)  reshape the input tensor into  size(0) * size(1) * ...  tensor,\ntaking the elements row-wise.  parameters    size  - the size after reshape   batchMode  - It is a optional argument. If it is set to  Some(true) ,\n                  the first dimension of input is considered as batch dimension,\n                  and thus keep this dimension size fixed. This is necessary\n                  when dealing with batch sizes of one. When set to  Some(false) ,\n                  it forces the entire input (including the first dimension) to be reshaped\n                  to the input size. Default is  None , which means the module considers\n                  inputs with more elements than the product of provided sizes (size(0) *\n                  size(1) * ..) to be batches, otherwise in no batch mode.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval reshape = Reshape(Array(3, 2))\nval input = Tensor(2, 2, 3).rand()\nval output = reshape.forward(input)\n-  print(output.size().toList)      \nList(2, 3, 2)  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nreshape =  Reshape([3, 2])\ninput = np.random.rand(2, 2, 3)\noutput = reshape.forward(input)\n-  print output[0].shape\n(2, 3, 2)", 
            "title": "Reshape"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#index", 
            "text": "Scala:  val model = Index(dimension)  Python:  model = Index(dimension)  Applies the Tensor index operation along the given dimension.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).rand()\nval input2 = Tensor(4)\ninput2(Array(1)) = 1.0f\ninput2(Array(2)) = 2.0f\ninput2(Array(3)) = 2.0f\ninput2(Array(4)) = 3.0f\n\nval input = T(input1, input2)\nval model = Index(1)\nval output = model.forward(input)\n\nscala  print(input)\n {\n    2: 1.0\n       2.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4]\n    1: 0.124325536\n       0.8768922\n       0.6378146\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\nscala  print(output)\n0.124325536\n0.8768922\n0.8768922\n0.6378146\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.randn(3)\ninput2 = np.array([1, 2, 2, 3])\ninput = [input1, input2]\n\nmodel = Index(1)\noutput = model.forward(input)  print(input)\n[array([-0.45804847, -0.20176707,  0.50963248]), array([1, 2, 2, 3])]  print(output)\n[-0.45804846 -0.20176707 -0.20176707  0.50963247]", 
            "title": "Index"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#identity", 
            "text": "Scala:  val identity = Identity()  Python:  identity = Identity()  Identity just return input as the output which is useful in same parallel container to get an origin input  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval identity = Identity()\n\nval input = Tensor(3, 3).rand()  print(input)\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(identity.forward(input))\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  Python example:  from bigdl.nn.layer import *\nidentity = Identity()   identity.forward(np.array([[1, 2, 3], [4, 5, 6]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]], dtype=float32)]", 
            "title": "Identity"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#narrow", 
            "text": "Scala:  val layer = Narrow(dimension, offset, length = 1)  Python:  layer = Narrow(dimension, offset, length=1)  Narrow is an application of narrow operation in a module.\nThe module further supports a negative length in order to handle inputs with an unknown size.  Parameters:  dimension  -narrow along this dimension  offset  -the start index on the given dimension  length  -length to narrow, default value is 1  Scala Example  import com.intel.analytics.bigdl.nn.Narrow\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Narrow(2, 2)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\n2.0\n3.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]\n\nval grad = layer.backward(input, gradOutput)\n0.0 3.0 0.0\n0.0 4.0 0.0\n0.0 5.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python Example  layer = Narrow(2, 2)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[ 2.]\n [ 3.]\n [ 4.]]\n\nprint grad\n[[ 0.  3.  0.]\n [ 0.  4.  0.]\n [ 0.  5.  0.]]", 
            "title": "Narrow"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#unsqueeze", 
            "text": "Scala:  val layer = Unsqueeze[Float](dim)  Python:  layer = Unsqueeze(dim)  Insert singleton dim (i.e., dimension 1) at position pos. For an input with dim = input.dim(),\nthere are dim + 1 possible positions to insert the singleton dimension. The dim starts from 1.  Scala example:  \nval layer = Unsqueeze[Float](2)\nval input = Tensor[Float](2, 2, 2).rand\nval gradOutput = Tensor[Float](2, 1, 2, 2).rand\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)  println(input.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  println(gradOutput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]  println(output.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]  println(gradInput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  layer = Unsqueeze(2)\ninput = np.random.uniform(0, 1, (2, 2, 2)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (2, 1, 2, 2)).astype( float32 )\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)  output\n[array([[[[ 0.97488612,  0.43463323],\n          [ 0.39069486,  0.0949123 ]]],\n\n\n        [[[ 0.19310953,  0.73574477],\n          [ 0.95347691,  0.37380624]]]], dtype=float32)]  gradInput\n[array([[[ 0.9995622 ,  0.69787127],\n         [ 0.65975296,  0.87002522]],\n\n        [[ 0.76349133,  0.96734989],\n         [ 0.88068211,  0.07284366]]], dtype=float32)]", 
            "title": "Unsqueeze"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#squeeze", 
            "text": "Scala:  val module = Squeeze(dims=null, numInputDims=Int.MinValue)  Python:  module = Squeeze(dims, numInputDims=-2147483648)  Delete all singleton dimensions or a specific singleton dimension.  dims  Optional. If this dimension is singleton dimension, it will be deleted.\n           The first index starts from 1. Default: delete all dimensions.  num_input_dims  Optional. If in a batch model, set to the inputDims.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = Squeeze(2)  print(layer.forward(Tensor(2, 1, 3).rand()))\n0.43709445  0.42752415  0.43069172  \n0.67029667  0.95641375  0.28823504  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\nlayer = Squeeze(2) layer.forward(np.array([[[1, 2, 3]], [[1, 2, 3]]]))\nout: array([[ 1.,  2.,  3.],\n            [ 1.,  2.,  3.]], dtype=float32)", 
            "title": "Squeeze"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#select", 
            "text": "Scala:  val layer = Select[T](dim, index)  Python:  layer = Select(dim, index)  A Simple layer selecting an index of the input tensor in the given dimension.\nPlease note that the index and dimension start from 1. In collaborative filtering, it can used together with LookupTable to create embeddings for users or items.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = Select[Float](1, 2)\nlayer.forward(Tensor[Float](T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\n\nlayer.backward(Tensor[Float](T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), Tensor[Float](T(0.1f, 0.2f, 0.3f)))  Its output should be  4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n0.0     0.0     0.0\n0.1     0.2     0.3\n0.0     0.0     0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import Select\nimport numpy as np\n\nlayer = Select(1, 2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), np.array([0.1, 0.2, 0.3]))  Its output should be  array([ 4.,  5.,  6.], dtype=float32)\n\narray([[ 0.        ,  0.        ,  0.        ],\n       [ 0.1       ,  0.2       ,  0.30000001],\n       [ 0.        ,  0.        ,  0.        ]], dtype=float32)", 
            "title": "Select"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#maskedselect", 
            "text": "Scala:  val module = MaskedSelect()  Python:  module = MaskedSelect()  Performs a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument\n with the input on the forward and backward passes.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport scala.util.Random\n\n\nval layer = MaskedSelect()\nval input1 = Tensor(2, 2).apply1(e =  Random.nextFloat())\nval mask = Tensor(2, 2)\nmask(Array(1, 1)) = 1\nmask(Array(1, 2)) = 0\nmask(Array(2, 1)) = 0\nmask(Array(2, 2)) = 1\nval input = T()\ninput(1.0) = input1\ninput(2.0) = mask  print(layer.forward(input))\n0.2577119\n0.5061479\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]  Python example:  from bigdl.nn.layer import *\n\nlayer = MaskedSelect()\ninput1 = np.random.rand(2,2)\nmask = np.array([[1,0], [0, 1]]) layer.forward([input1, mask])\narray([ 0.1525335 ,  0.05474588], dtype=float32)", 
            "title": "MaskedSelect"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#transpose", 
            "text": "Scala:  val module = Transpose(permutations)  Python:  module = Transpose(permutations)  Concat is a layer who transpose input along specified dimensions.\npermutations are dimension pairs that need to swap.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).rand()\nval layer = Transpose(Array((1, 2)))\nval output = layer.forward(input)  input\n0.6653826   0.25350887  0.33434764  \n0.9618287   0.5484164   0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]  output\n0.6653826   0.9618287   \n0.25350887  0.5484164   \n0.33434764  0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nlayer = Transpose([(1,2)])\ninput = np.array([[0.6653826, 0.25350887, 0.33434764], [0.9618287, 0.5484164, 0.64844745]])\noutput = layer.forward(input)  output\n[array([[ 0.66538262,  0.96182871],\n       [ 0.25350887,  0.54841638],\n       [ 0.33434764,  0.64844745]], dtype=float32)]", 
            "title": "Transpose"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#inferreshape", 
            "text": "Scala:  val layer = InferReshape(size, batchMode = false)  Python:  layer = InferReshape(size, batch_mode=False)  Reshape the input tensor with automatic size inference support.\nPositive numbers in the  size  argument are used to reshape the input to the\ncorresponding dimension size.  There are also two special values allowed in  size :   0  means keep the corresponding dimension size of the input unchanged.\n      i.e., if the 1st dimension size of the input is 2,\n      the 1st dimension size of output will be set as 2 as well.  -1  means infer this dimension size from other dimensions.\n      This dimension size is calculated by keeping the amount of output elements\n      consistent with the input.\n      Only one  -1  is allowable in  size .   For example,     Input tensor with size: (4, 5, 6, 7)\n   -  InferReshape(Array(4, 0, 3, -1))\n   Output tensor with size: (4, 5, 3, 14)  The 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,\nand inferred the last dim as 14.  Parameters:  size       -the target tensor size  batchMode  -whether in batch mode  Scala example:  import com.intel.analytics.bigdl.nn.InferReshape\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval layer = InferReshape(Array(0, 3, -1))\nval input = Tensor(1, 2, 3).rand()\nval gradOutput = Tensor(1, 3, 2).rand()\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n0.8170822   0.40073588\n0.49389255  0.3782435\n0.42660004  0.5917206\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x2]\n\nprintln(grad)\n(1,.,.) =\n0.8294597   0.57101834  0.90910035\n0.32783163  0.30494633  0.7339092\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3]  Python example:  layer = InferReshape([0, 3, -1])\ninput = np.random.rand(1, 2, 3)\n\ngradOutput = np.random.rand(1, 3, 2)\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 0.68635464  0.21277553]\n  [ 0.13390459  0.65662414]\n  [ 0.1021723   0.92319047]]]\n\nprint grad\n[[[ 0.84927064  0.55205333  0.25077972]\n  [ 0.76105869  0.30828172  0.1237276 ]]]", 
            "title": "InferReshape"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#replicate", 
            "text": "Scala:  val module = Replicate(\n  nFeatures,\n  dim = 1,\n  nDim = Int.MaxValue)  Python:  module = Replicate(\n  n_features,\n  dim=1,\n  n_dim=INTMAX)  Replicate repeats input  nFeatures  times along its  dim  dimension  Notice: No memory copy, it set the stride along the  dim -th dimension to zero.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Replicate(4, 1, 2)\n\nprintln(module.forward(Tensor.range(1, 6, 1).resize(1, 2, 3)))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,2,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,3,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,4,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x2x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Replicate(4, 1, 2)\n\nprint(module.forward(np.arange(1, 7, 1).reshape(1, 2, 3)))  Output is   [array([[[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]]]], dtype=float32)]", 
            "title": "Replicate"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#view", 
            "text": "Scala:  val view = View(2, 8)  or  val view = View(Array(2, 8))  Python:  view = View([2, 8])  This module creates a new view of the input tensor using the sizes passed to the constructor.\nThe method setNumInputDims() allows to specify the expected number of dimensions of the inputs\nof the modules. This makes it possible to use minibatch inputs\nwhen using a size -1 for one of the dimensions.  Scala example:  import com.intel.analytics.bigdl.nn.View\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval view = View(2, 8)\n\nval input = Tensor(4, 4).randn()\nval gradOutput = Tensor(2, 8).randn()\n\nval output = view.forward(input)\nval gradInput = view.backward(input, gradOutput)  The output is,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.43037438     1.2982363       -1.4723133      -0.2602826      0.7178128       -1.8763185      0.88629466      0.8346704\n0.20963766      -0.9349786      1.0376515       1.3153045       1.5450214       1.084113        -0.29929757     -0.18356979\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]  The gradInput is,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7360089       0.9133299       0.40443268      -0.94965595\n0.80520976      -0.09671917     -0.5498001      -0.098691925\n-2.3119886      -0.8455147      0.75891125      1.2985301\n0.5023749       1.4983269       0.42038065      -1.7002305  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nview = View([2, 8])\n\ninput = np.random.uniform(0, 1, [4, 4]).astype( float32 )\ngradOutput = np.random.uniform(0, 1, [2, 8]).astype( float32 )\n\noutput = view.forward(input)\ngradInput = view.backward(input, gradOutput)\n\nprint output\nprint gradInput", 
            "title": "View"
        }, 
        {
            "location": "/APIdocs/Layers/Simple_Layers/merged-Simple_Layers/#contiguous", 
            "text": "Be used to make input, gradOutput both contiguous  Scala:  val contiguous = Contiguous()  Python:  contiguous = Contiguous()  Used to make input, gradOutput both contiguous  Scala example:  import com.intel.analytics.bigdl.nn.Contiguous\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(5).range(1, 5, 1)\nval contiguous = new Contiguous()\nval output = contiguous.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(5).range(2, 6, 1)\nval gradInput = contiguous.backward(input, gradOutput)\nprintln(gradOutput)  The output will be,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n2.0\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]  The gradInput will be,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.0\n3.0\n4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ncontiguous = Contiguous()\n\ninput = np.arange(1, 6, 1).astype( float32 )\ninput = input.reshape(1, 5)\n\noutput = contiguous.forward(input)\nprint output\n\ngradOutput = np.arange(2, 7, 1).astype( float32 )\ngradOutput = gradOutput.reshape(1, 5)\n\ngradInput = contiguous.backward(input, gradOutput)\nprint gradInput  The output will be,  [array([[ 1.,  2.,  3.,  4.,  5.]], dtype=float32)]  The gradInput will be,  [array([[ 2.,  3.,  4.,  5.,  6.]], dtype=float32)]", 
            "title": "Contiguous"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/", 
            "text": "SpatialShareConvolution\n\n\nScala:\n\n\nval layer = SpatialShareConvolution[Float](nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)\n\n\n\n\nPython:\n\n\nlayer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n\n\n\n\nApplies a 2D convolution over an input image composed of several input planes.\n The input tensor in forward(input) is expected to be\n a 3D tensor (nInputPlane x height x width).\n\n\nThis layer has been optimized to save memory. If using this layer to construct multiple convolution\n layers, please add sharing script for the fInput and fGradInput. Please refer to the ResNet example.\n\n\nScala example:\n\n\n    val nInputPlane = 1\n    val nOutputPlane = 1\n    val kW = 2\n    val kH = 2\n    val dW = 1\n    val dH = 1\n    val padW = 0\n    val padH = 0\n    val layer = SpatialShareConvolution[Float](nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)\n\n    val inputData = Array(\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1\n    )\n\n    val kernelData = Array(\n      2.0, 3,\n      4, 5\n    )\n\n    val biasData = Array(0.0)\n\n    layer.weight.copy(Tensor[Float](Storage(kernelData), 1,\n      Array(nOutputPlane, nInputPlane, kH, kW)))\n    layer.bias.copy(Tensor[Float](Storage(biasData), 1, Array(nOutputPlane)))\n\n    val input = Tensor[Float](Storage(inputData), 1, Array(3, 1, 3, 4))\n    val output = layer.updateOutput(input)\n\n    \n output\nres2: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(2,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(3,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n\n\n\nPython example:\n\n\nnInputPlane = 1\nnOutputPlane = 1\nkW = 2\nkH = 2\ndW = 1\ndH = 1\npadW = 0\npadH = 0\nlayer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n\ninput = np.array([\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1]\n    ).astype(\nfloat32\n).reshape(3, 1, 3, 4)\nlayer.forward(input)\n\n\n print (output)\narray([[[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]]], dtype=float32)\n\n\n\n\nSpatialConvolution\n\n\nScala:\n\n\nval m = SpatialConvolution(nInputPlane,nOutputPlane,kernelW,kernelH,strideW=1,strideH=1,padW=0,padH=0,nGroup=1,propagateBack=true,wRegularizer=null,bRegularizer=null,initWeight=null, initBias=null, initGradWeight=null, initGradBias=null)\n\n\n\n\nPython:\n\n\nm = SpatialConvolution(n_input_plane,n_output_plane,kernel_w,kernel_h,stride_w=1,stride_h=1,pad_w=0,pad_h=0,n_group=1,propagate_back=True,wRegularizer=None,bRegularizer=None,init_weight=None,init_bias=None,init_grad_weight=None,init_grad_bias=None)\n\n\n\n\nSpatialConvolution is a module that applies a 2D convolution over an input image.\n\n\nThe input tensor in \nforward(input)\n is expected to be\neither a 4D tensor (\nbatch x nInputPlane x height x width\n) or a 3D tensor (\nnInputPlane x height x width\n). The convolution is performed on the last two dimensions.\n\n\nDetailed paramter explaination for the constructor.\n\n\n\n\nparam nInputPlane: The number of expected input planes in the image given into forward()\n\n\nparam nOutputPlane: The number of output planes the convolution layer will produce.\n\n\nparam kernelW: The kernel width of the convolution\n\n\nparam kernelH: The kernel height of the convolution\n\n\nparam strideW: The step of the convolution in the width dimension.\n\n\nparam strideH: The step of the convolution in the height dimension\n\n\nparam padW:  padding to be added to width to the input.\n\n\nparam padH: padding to be added to height to the input.\n\n\nparam nGroup: Kernel group number\n\n\nparam propagateBack: whether to propagate gradient back\n\n\nparam wRegularizer: regularizer on weight. an instance of [[Regularizer]] (e.g. L1 or L2)\n\n\nparam bRegularizer: regularizer on bias. an instance of [[Regularizer]] (e.g. L1 or L2).\n\n\nparam initWeight: weight initializer\n\n\nparam initBias:  bias initializer\n\n\nparam initGradWeight: weight gradient initializer\n\n\nparam initGradBias: bias gradient initializer\n\n\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialConvolution(2,1,2,2,1,1,0,0)\nm.setInitMethod(weightInitMethod = BilinearFiller, biasInitMethod = Zeros)\nval params = m.getParameters()\n\nscala\n print(params)\n(1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9],0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9])\n\nscala\n\nval input = Tensor(1,2,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,1,2,2).fill(0.2f)\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n(1,1,.,.) =\n-0.37011376     0.13565119      -0.73574775\n-0.19486316     -0.4430604      -0.62543416\n0.7017611       -0.6441595      -1.2953792\n\n(1,2,.,.) =\n-0.9903588      0.5669722       0.2630131\n0.03392942      -0.6984676      -0.12389368\n0.78704715      0.5411976       -1.3877676\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3]\n\nscala\n print(output)\n(1,1,.,.) =\n-1.3604726      0.70262337\n-0.16093373     -1.141528\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\nscala\n print(gradOut)\n(1,1,.,.) =\n0.2     0.2\n0.2     0.2\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x2x2]\n\nscala\n print(gradIn)\n(1,1,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n(1,2,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(1,3,3,3)\nprint \ninput is :\n,input\n\nm = SpatialConvolution(3,1,2,2,1,1,0,0)\nout = m.forward(input)\nprint \noutput m is :\n,out\n\ngrad_out = np.random.rand(1,1,2,2)\nprint \ngrad out of m is :\n,grad_out\ngrad_in = m.backward(input,grad_out)\nprint \ngrad input of m is :\n,grad_in\n\n\n\n\nproduces output:\n\n\ninput is : [[[[ 0.75276617  0.44212513  0.90275949]\n   [ 0.78205279  0.77864714  0.83647254]\n   [ 0.76220944  0.22106036  0.68762202]]\n\n  [[ 0.37346971  0.31532213  0.33276243]\n   [ 0.69872884  0.07262236  0.66372462]\n   [ 0.47803013  0.80194459  0.53313873]]\n\n  [[ 0.56196833  0.20599878  0.47575818]\n   [ 0.35454298  0.96910557  0.36234704]\n   [ 0.64017738  0.95762579  0.50073035]]]]\ncreating: createSpatialConvolution\noutput m is : [[[[-1.08398974 -0.67615652]\n   [-0.77027249 -0.82885492]]]]\ngrad out of m is : [[[[ 0.38295452  0.77048361]\n   [ 0.11671955  0.76357513]]]]\ngrad input of m is : [[[[-0.02344826 -0.06515953 -0.03618064]\n   [-0.06770924 -0.22586647 -0.14004168]\n   [-0.01845866 -0.13653883 -0.10325129]]\n\n  [[-0.09294108 -0.14361492  0.08727306]\n   [-0.09885897 -0.21209857  0.29151234]\n   [-0.02149716 -0.10957514  0.20318349]]\n\n  [[-0.05926216 -0.04542646  0.14849319]\n   [-0.09506465 -0.34244278 -0.03763583]\n   [-0.02346931 -0.1815301  -0.18314059]]]]\n\n\n\n\nSpatialFullConvolution\n\n\nScala:\n\n\nval m  = SpatialFullConvolution[Tensor[T], T](nInputPlane, nOutputPlane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0,nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)\n\n\n\n\nor\n\n\nval m = SpatialFullConvolution[Table, T](InputPlane, nOutputPlane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0,nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)\n\n\n\n\nPython:\n\n\nm = SpatialFullConvolution(n_input_plane,n_output_plane,kw,kh,dw=1,dh=1,pad_w=0,pad_h=0,adj_w=0,adj_h=0,n_group=1,no_bias=False,init_method='default',wRegularizer=None,bRegularizer=None)\n\n\n\n\nSpatialFullConvolution is a module that applies a 2D full convolution over an input image. \n\n\nThe input tensor in \nforward(input)\n is expected to be\neither a 4D tensor (\nbatch x nInputPlane x height x width\n) or a 3D tensor (\nnInputPlane x height x width\n). The convolution is performed on the last two dimensions. \nadjW\n and \nadjH\n are used to adjust the size of the output image. The size of output tensor of \nforward\n will be :\n\n\n  output width  = (width  - 1) * dW - 2*padW + kW + adjW\n  output height = (height - 1) * dH - 2*padH + kH + adjH\n\n\n\n\nNote, scala API also accepts a table input with two tensors: \nT(convInput, sizeTensor)\n where \nconvInput\n is the standard input tensor, and the size of \nsizeTensor\n is used to set the size of the output (will ignore the \nadjW\n and \nadjH\n values used to construct the module). Use \nSpatialFullConvolution[Table, T](...)\n instead of \nSpatialFullConvolution[Tensor,T](...)\n) for table input.\n\n\nThis module can also be used without a bias by setting parameter \nnoBias = true\n while constructing the module.\n\n\nOther frameworks may call this operation \"In-network Upsampling\", \"Fractionally-strided convolution\", \"Backwards Convolution,\" \"Deconvolution\", or \"Upconvolution.\"\n\n\nReference: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3431-3440.\n\n\nDetailed explaination of arguments in constructor. \n\n\n\n\nparam nInputPlane The number of expected input planes in the image given into forward()\n\n\nparam nOutputPlane The number of output planes the convolution layer will produce.\n\n\nparam kW The kernel width of the convolution.\n\n\nparam kH The kernel height of the convolution.\n\n\nparam dW The step of the convolution in the width dimension. Default is 1.\n\n\nparam dH The step of the convolution in the height dimension. Default is 1.\n\n\nparam padW The additional zeros added per width to the input planes. Default is 0.\n\n\nparam padH The additional zeros added per height to the input planes. Default is 0.\n\n\nparam adjW Extra width to add to the output image. Default is 0.\n\n\nparam adjH Extra height to add to the output image. Default is 0.\n\n\nparam nGroup Kernel group number.\n\n\nparam noBias If bias is needed.\n\n\nparam wRegularizer: instance of [[Regularizer]]\n\n\n(eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nparam bRegularizer: instance of [[Regularizer]]\n\n\napplied to the bias.\n\n\n\n\nScala example:\n\n\nTensor Input example: \n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialFullConvolution[Tensor[Float],Float](1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input = Tensor(1,1,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n(1,1,.,.) =\n0.18219171      1.3252861       -1.3991559\n0.82611334      1.0313315       0.6075537\n-0.7336061      0.3156875       -0.70616096\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x3x3]\n\nscala\n print(output)\n(1,1,.,.) =\n-0.49278542     -0.5823938      -0.8304068      -0.077556044\n-0.5028842      -0.7281958      -1.1927067      -0.34262076\n-0.41680115     -0.41400516     -0.7599415      -0.42024887\n-0.5286566      -0.30015367     -0.5997892      -0.32439864\n\n(1,2,.,.) =\n-0.13131973     -0.5770084      1.1069719       -0.6003375\n-0.40302444     -0.07293816     -0.2654545      0.39749345\n0.37311426      -0.49090374     0.3088816       -0.41700447\n-0.12861171     0.09394867      -0.17229918     0.05556257\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x4]\n\nscala\n print(gradOut)\n(1,1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(1,2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x4x4]\n\nscala\n print(gradIn)\n(1,1,.,.) =\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x3x3]\n\n\n\n\n\n\nTable input Example\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.{T, Table}\n\nval m = SpatialFullConvolution[Table, Float](1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input1 = Tensor(1, 3, 3).randn()\nval input2 = Tensor(3, 3).fill(2.0f)\nval input = T(input1, input2)\nval output = m.forward(input)\nval gradOut = Tensor(2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n {\n        2: 2.0  2.0     2.0\n           2.0  2.0     2.0\n           2.0  2.0     2.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: (1,.,.) =\n           1.276177     0.62761325      0.2715257\n           -0.030832397 0.5046206       0.6835176\n           -0.5832693   0.17266633      0.7461992\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n }\n\nscala\n print(output)\n(1,.,.) =\n-0.18339296     0.04208675      -0.17708774     -0.30901802\n-0.1484881      0.23592418      0.115615785     -0.11288056\n-0.47266048     -0.41772115     0.07501307      0.041751802\n-0.4851033      -0.5427048      -0.18293871     -0.12682784\n\n(2,.,.) =\n0.6391188       0.845774        0.41208875      0.13754106\n-0.45785713     0.31221163      0.6006259       0.36563575\n-0.24076991     -0.31931365     0.31651747      0.4836449\n0.24247466      -0.16731171     -0.20887817     0.19513035\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x4]\n\nscala\n print(gradOut)\n(1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x4x4]\n\nscala\n print(gradIn)\n {\n        2: 0.0  0.0     0.0\n           0.0  0.0     0.0\n           0.0  0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: (1,.,.) =\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nm = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, False)\n\nprint \n--------- tensor input---------\n\ntensor_input = np.random.rand(1,3,3)\nprint \ninput is :\n,tensor_input\nout = m.forward(tensor_input)\nprint \noutput m is :\n,out\n\nprint \n----------- table input --------\n\nadj_input=np.empty([3,3])\nadj_input.fill(2.0)\ntable_input = [tensor_input,adj_input]\nprint \ninput is :\n,table_input\nout = m.forward(table_input)\nprint \noutput m is :\n,out\n\n\n\n\nproduces output:\n\n\ncreating: createSpatialFullConvolution\n--------- tensor input---------\ninput is : [[[  9.03998497e-01   4.43054896e-01   6.19571211e-01]\n  [  4.24573060e-01   3.29886286e-04   5.48427154e-02]\n  [  8.99004782e-01   3.25514441e-01   6.85294650e-01]]]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]\n----------- table input --------\ninput is : [array([[[  9.03998497e-01,   4.43054896e-01,   6.19571211e-01],\n        [  4.24573060e-01,   3.29886286e-04,   5.48427154e-02],\n        [  8.99004782e-01,   3.25514441e-01,   6.85294650e-01]]]), array([[ 2.,  2.,  2.],\n       [ 2.,  2.,  2.],\n       [ 2.,  2.,  2.]])]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]\n\n\n\n\nSpatialDilatedConvolution\n\n\nScala:\n\n\nval layer = SpatialDilatedConvolution[T](\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)\n\n\n\n\nPython:\n\n\nlayer = SpatialDilatedConvolution(\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)\n\n\n\n\nApply a 2D dilated convolution over an input image.\n\n\nThe input tensor is expected to be a 3D or 4D(with batch) tensor.\n\n\nFor a normal SpatialConvolution, the kernel will multiply with input\nimage element-by-element contiguous. In dilated convolution, it\u2019s possible\nto have filters that have spaces between each cell. For example, filter w and\nimage x, when dilatiionW and dilationH both = 1, this is normal 2D convolution\n\n\nw(0, 0) * x(0, 0), w(0, 1) * x(0, 1)\nw(1, 0) * x(1, 0), w(1, 1) * x(1, 1)\n\n\n\n\nwhen dilationW and dilationH both = 2\n\n\nw(0, 0) * x(0, 0), w(0, 1) * x(0, 2)\nw(1, 0) * x(2, 0), w(1, 1) * x(2, 2)\n\n\n\n\nwhen dilationW and dilationH both = 3\n\n\nw(0, 0) * x(0, 0), w(0, 1) * x(0, 3)\nw(1, 0) * x(3, 0), w(1, 1) * x(3, 3)\n\n\n\n\nIf input is a 3D tensor nInputPlane x height x width,\n * owidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1\n * oheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1\n\n\nReference Paper:\n\n\n\n\nYu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].\narXiv preprint arXiv:1511.07122, 2015.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = SpatialDilatedConvolution[Float](1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\nval input = Tensor[Float](T(T(\n  T(1.0f, 2.0f, 3.0f, 4.0f),\n  T(5.0f, 6.0f, 7.0f, 8.0f),\n  T(9.0f, 1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f, 7.0f)\n)))\nval filter = Tensor[Float](T(T(T(\n  T(1.0f, 1.0f),\n  T(1.0f, 1.0f)\n))))\nlayer.weight.copy(filter)\nlayer.bias.zero()\nlayer.forward(input)\nlayer.backward(input, Tensor[Float](T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n))))\n\n\n\n\nIts output should be\n\n\n(1,.,.) =\n15.0    10.0\n22.0    26.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n(1,.,.) =\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import SpatialDilatedConvolution\nimport numpy as np\n\nlayer = SpatialDilatedConvolution(1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\ninput = np.array([[\n  [1.0, 2.0, 3.0, 4.0],\n  [5.0, 6.0, 7.0, 8.0],\n  [9.0, 1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0, 7.0]\n]])\nfilter = np.array([[[\n  [1.0, 1.0],\n  [1.0, 1.0]\n]]])\nbias = np.array([0.0])\nlayer.set_weights([filter, bias])\nlayer.forward(input)\nlayer.backward(input, np.array([[[0.1, 0.2], [0.3, 0.4]]]))\n\n\n\n\nIts output should be\n\n\narray([[[ 15.,  10.],\n        [ 22.,  26.]]], dtype=float32)\n\narray([[[ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001],\n        [ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001]]], dtype=float32)\n\n\n\n\n\nVolumetricConvolution\n\n\nScala:\n\n\nval module = VolumetricConvolution(nInputPlane, nOutputPlane, kT, kW, kH,\n  dT=1, dW=1, dH=1, padT=0, padW=0, padH=0, withBias=true)\n\n\n\n\nPython:\n\n\nmodule = VolumetricConvolution(n_input_plane, n_output_plane, k_t, k_w, k_h,\n  d_t=1, d_w=1, d_h=1, pad_t=0, pad_w=0, pad_h=0, with_bias=true)\n\n\n\n\nApplies a 3D convolution over an input image composed of several input planes. The input tensor\nin forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).\n * @param nInputPlane The number of expected input planes in the image given into forward()\n * @param nOutputPlane The number of output planes the convolution layer will produce.\n * @param kT The kernel size of the convolution in time\n * @param kW The kernel width of the convolution\n * @param kH The kernel height of the convolution\n * @param dT The step of the convolution in the time dimension. Default is 1\n * @param dW The step of the convolution in the width dimension. Default is 1\n * @param dH The step of the convolution in the height dimension. Default is 1\n * @param padT Additional zeros added to the input plane data on both sides of time axis.\n * Default is 0. (kT-1)/2 is often used here.\n * @param padW The additional zeros added per width to the input planes.\n * @param padH The additional zeros added per height to the input planes.\n * @param withBias whether with bias.\n\n\nScala example:\n\n\nval layer = VolumetricConvolution(2, 3, 2, 2, 2, dT=1, dW=1, dH=1,\n  padT=0, padW=0, padH=0, withBias=true)\nval input = Tensor(2, 2, 2, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.54846555      0.5549177\n0.43748873      0.6596535\n\n(1,2,.,.) =\n0.87915933      0.5955469\n0.67464 0.40921077\n\n(2,1,.,.) =\n0.24127467      0.49356017\n0.6707502       0.5421975\n\n(2,2,.,.) =\n0.007834963     0.08188637\n0.51387626      0.7376101\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\nlayer.forward(input)\nres16: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.6680023\n\n(2,1,.,.) =\n0.41926455\n\n(3,1,.,.) =\n-0.029196609\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x1x1]\n\n\n\n\nPython example:\n\n\nlayer = VolumetricConvolution(2, 3, 2, 2, 2, d_t=1, d_w=1, d_h=1,\n          pad_t=0, pad_w=0, pad_h=0, with_bias=True, init_method=\ndefault\n,\n          bigdl_type=\nfloat\n)\ninput = np.random.rand(2,2,2,2)\n array([[[[ 0.47639062,  0.76800312],\n         [ 0.28834351,  0.21883535]],\n\n        [[ 0.86097919,  0.89812597],\n         [ 0.43632181,  0.58004824]]],\n\n\n       [[[ 0.65784027,  0.34700039],\n         [ 0.64511955,  0.1660241 ]],\n\n        [[ 0.36060054,  0.71265665],\n         [ 0.51755249,  0.6508298 ]]]])\n\nlayer.forward(input)\narray([[[[ 0.54268712]]],\n\n\n       [[[ 0.17670505]]],\n\n\n       [[[ 0.40953237]]]], dtype=float32)\n\n\n\n\n\nSpatialConvolutionMap\n\n\nScala:\n\n\nval layer = SpatialConvolutionMap(\n  connTable,\n  kW,\n  kH,\n  dW = 1,\n  dH = 1,\n  padW = 0,\n  padH = 0,\n  wRegularizer = null,\n  bRegularizer = null)\n\n\n\n\nPython:\n\n\nlayer = SpatialConvolutionMap(\n  conn_table,\n  kw,\n  kh,\n  dw=1,\n  dh=1,\n  pad_w=0,\n  pad_h=0,\n  wRegularizer=None,\n  bRegularizer=None)\n\n\n\n\nThis class is a generalization of SpatialConvolution.\nIt uses a generic connection table between input and output features.\nThe SpatialConvolution is equivalent to using a full connection table.\n\nA Connection Table is the mapping of input/output feature map, stored in a 2D Tensor. The first column is the input feature maps. The second column is output feature maps.\n\n\nFull Connection table:\n\n\nval conn = SpatialConvolutionMap.full(nin: Int, nout: In)\n\n\n\n\nOne to One connection table:\n\n\nval conn = SpatialConvolutionMap.oneToOne(nfeat: Int)\n\n\n\n\nRandom Connection table:\n\n\nval conn = SpatialConvolutionMap.random(nin: Int, nout: Int, nto: Int)\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval conn = SpatialConvolutionMap.oneToOne(3)\n\n\n\n\nconn\n is\n\n\nconn: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 1.0\n2.0 2.0\n3.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\n\n\n\nval module = SpatialConvolutionMap(SpatialConvolutionMap.oneToOne(3), 2, 2)\n\npritnln(module.forward(Tensor.range(1, 48, 1).resize(3, 4, 4)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n4.5230045   5.8323975   7.1417904\n9.760576    11.069969   12.379362\n14.998148   16.30754    17.616934\n\n(2,.,.) =\n-5.6122046  -5.9227824  -6.233361\n-6.8545156  -7.165093   -7.4756703\n-8.096827   -8.407404   -8.71798\n\n(3,.,.) =\n13.534529   13.908197   14.281864\n15.029203   15.402873   15.77654\n16.523876   16.897545   17.271214\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = SpatialConvolutionMap(np.array([(1, 1), (2, 2), (3, 3)]), 2, 2)\n\nprint(module.forward(np.arange(1, 49, 1).reshape(3, 4, 4)))\n\n\n\n\nOutput is\n\n\n[array([[[-1.24280548, -1.70889318, -2.17498088],\n        [-3.10715604, -3.57324386, -4.03933144],\n        [-4.97150755, -5.43759441, -5.90368223]],\n\n       [[-5.22062826, -5.54696751, -5.87330723],\n        [-6.52598572, -6.85232496, -7.17866373],\n        [-7.8313427 , -8.15768337, -8.48402214]],\n\n       [[ 0.5065825 ,  0.55170798,  0.59683061],\n        [ 0.68707776,  0.73219943,  0.77732348],\n        [ 0.86757064,  0.91269422,  0.95781779]]], dtype=float32)]", 
            "title": "Convolution Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/#spatialshareconvolution", 
            "text": "Scala:  val layer = SpatialShareConvolution[Float](nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)  Python:  layer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)  Applies a 2D convolution over an input image composed of several input planes.\n The input tensor in forward(input) is expected to be\n a 3D tensor (nInputPlane x height x width).  This layer has been optimized to save memory. If using this layer to construct multiple convolution\n layers, please add sharing script for the fInput and fGradInput. Please refer to the ResNet example.  Scala example:      val nInputPlane = 1\n    val nOutputPlane = 1\n    val kW = 2\n    val kH = 2\n    val dW = 1\n    val dH = 1\n    val padW = 0\n    val padH = 0\n    val layer = SpatialShareConvolution[Float](nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)\n\n    val inputData = Array(\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1\n    )\n\n    val kernelData = Array(\n      2.0, 3,\n      4, 5\n    )\n\n    val biasData = Array(0.0)\n\n    layer.weight.copy(Tensor[Float](Storage(kernelData), 1,\n      Array(nOutputPlane, nInputPlane, kH, kW)))\n    layer.bias.copy(Tensor[Float](Storage(biasData), 1, Array(nOutputPlane)))\n\n    val input = Tensor[Float](Storage(inputData), 1, Array(3, 1, 3, 4))\n    val output = layer.updateOutput(input)\n\n      output\nres2: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(2,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(3,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0  Python example:  nInputPlane = 1\nnOutputPlane = 1\nkW = 2\nkH = 2\ndW = 1\ndH = 1\npadW = 0\npadH = 0\nlayer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n\ninput = np.array([\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1]\n    ).astype( float32 ).reshape(3, 1, 3, 4)\nlayer.forward(input)  print (output)\narray([[[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]]], dtype=float32)", 
            "title": "SpatialShareConvolution"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/#spatialconvolution", 
            "text": "Scala:  val m = SpatialConvolution(nInputPlane,nOutputPlane,kernelW,kernelH,strideW=1,strideH=1,padW=0,padH=0,nGroup=1,propagateBack=true,wRegularizer=null,bRegularizer=null,initWeight=null, initBias=null, initGradWeight=null, initGradBias=null)  Python:  m = SpatialConvolution(n_input_plane,n_output_plane,kernel_w,kernel_h,stride_w=1,stride_h=1,pad_w=0,pad_h=0,n_group=1,propagate_back=True,wRegularizer=None,bRegularizer=None,init_weight=None,init_bias=None,init_grad_weight=None,init_grad_bias=None)  SpatialConvolution is a module that applies a 2D convolution over an input image.  The input tensor in  forward(input)  is expected to be\neither a 4D tensor ( batch x nInputPlane x height x width ) or a 3D tensor ( nInputPlane x height x width ). The convolution is performed on the last two dimensions.  Detailed paramter explaination for the constructor.   param nInputPlane: The number of expected input planes in the image given into forward()  param nOutputPlane: The number of output planes the convolution layer will produce.  param kernelW: The kernel width of the convolution  param kernelH: The kernel height of the convolution  param strideW: The step of the convolution in the width dimension.  param strideH: The step of the convolution in the height dimension  param padW:  padding to be added to width to the input.  param padH: padding to be added to height to the input.  param nGroup: Kernel group number  param propagateBack: whether to propagate gradient back  param wRegularizer: regularizer on weight. an instance of [[Regularizer]] (e.g. L1 or L2)  param bRegularizer: regularizer on bias. an instance of [[Regularizer]] (e.g. L1 or L2).  param initWeight: weight initializer  param initBias:  bias initializer  param initGradWeight: weight gradient initializer  param initGradBias: bias gradient initializer   Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialConvolution(2,1,2,2,1,1,0,0)\nm.setInitMethod(weightInitMethod = BilinearFiller, biasInitMethod = Zeros)\nval params = m.getParameters()\n\nscala  print(params)\n(1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9],0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9])\n\nscala \nval input = Tensor(1,2,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,1,2,2).fill(0.2f)\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n(1,1,.,.) =\n-0.37011376     0.13565119      -0.73574775\n-0.19486316     -0.4430604      -0.62543416\n0.7017611       -0.6441595      -1.2953792\n\n(1,2,.,.) =\n-0.9903588      0.5669722       0.2630131\n0.03392942      -0.6984676      -0.12389368\n0.78704715      0.5411976       -1.3877676\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3]\n\nscala  print(output)\n(1,1,.,.) =\n-1.3604726      0.70262337\n-0.16093373     -1.141528\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\nscala  print(gradOut)\n(1,1,.,.) =\n0.2     0.2\n0.2     0.2\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x2x2]\n\nscala  print(gradIn)\n(1,1,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n(1,2,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(1,3,3,3)\nprint  input is : ,input\n\nm = SpatialConvolution(3,1,2,2,1,1,0,0)\nout = m.forward(input)\nprint  output m is : ,out\n\ngrad_out = np.random.rand(1,1,2,2)\nprint  grad out of m is : ,grad_out\ngrad_in = m.backward(input,grad_out)\nprint  grad input of m is : ,grad_in  produces output:  input is : [[[[ 0.75276617  0.44212513  0.90275949]\n   [ 0.78205279  0.77864714  0.83647254]\n   [ 0.76220944  0.22106036  0.68762202]]\n\n  [[ 0.37346971  0.31532213  0.33276243]\n   [ 0.69872884  0.07262236  0.66372462]\n   [ 0.47803013  0.80194459  0.53313873]]\n\n  [[ 0.56196833  0.20599878  0.47575818]\n   [ 0.35454298  0.96910557  0.36234704]\n   [ 0.64017738  0.95762579  0.50073035]]]]\ncreating: createSpatialConvolution\noutput m is : [[[[-1.08398974 -0.67615652]\n   [-0.77027249 -0.82885492]]]]\ngrad out of m is : [[[[ 0.38295452  0.77048361]\n   [ 0.11671955  0.76357513]]]]\ngrad input of m is : [[[[-0.02344826 -0.06515953 -0.03618064]\n   [-0.06770924 -0.22586647 -0.14004168]\n   [-0.01845866 -0.13653883 -0.10325129]]\n\n  [[-0.09294108 -0.14361492  0.08727306]\n   [-0.09885897 -0.21209857  0.29151234]\n   [-0.02149716 -0.10957514  0.20318349]]\n\n  [[-0.05926216 -0.04542646  0.14849319]\n   [-0.09506465 -0.34244278 -0.03763583]\n   [-0.02346931 -0.1815301  -0.18314059]]]]", 
            "title": "SpatialConvolution"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/#spatialfullconvolution", 
            "text": "Scala:  val m  = SpatialFullConvolution[Tensor[T], T](nInputPlane, nOutputPlane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0,nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)  or  val m = SpatialFullConvolution[Table, T](InputPlane, nOutputPlane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0,nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)  Python:  m = SpatialFullConvolution(n_input_plane,n_output_plane,kw,kh,dw=1,dh=1,pad_w=0,pad_h=0,adj_w=0,adj_h=0,n_group=1,no_bias=False,init_method='default',wRegularizer=None,bRegularizer=None)  SpatialFullConvolution is a module that applies a 2D full convolution over an input image.   The input tensor in  forward(input)  is expected to be\neither a 4D tensor ( batch x nInputPlane x height x width ) or a 3D tensor ( nInputPlane x height x width ). The convolution is performed on the last two dimensions.  adjW  and  adjH  are used to adjust the size of the output image. The size of output tensor of  forward  will be :    output width  = (width  - 1) * dW - 2*padW + kW + adjW\n  output height = (height - 1) * dH - 2*padH + kH + adjH  Note, scala API also accepts a table input with two tensors:  T(convInput, sizeTensor)  where  convInput  is the standard input tensor, and the size of  sizeTensor  is used to set the size of the output (will ignore the  adjW  and  adjH  values used to construct the module). Use  SpatialFullConvolution[Table, T](...)  instead of  SpatialFullConvolution[Tensor,T](...) ) for table input.  This module can also be used without a bias by setting parameter  noBias = true  while constructing the module.  Other frameworks may call this operation \"In-network Upsampling\", \"Fractionally-strided convolution\", \"Backwards Convolution,\" \"Deconvolution\", or \"Upconvolution.\"  Reference: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3431-3440.  Detailed explaination of arguments in constructor.    param nInputPlane The number of expected input planes in the image given into forward()  param nOutputPlane The number of output planes the convolution layer will produce.  param kW The kernel width of the convolution.  param kH The kernel height of the convolution.  param dW The step of the convolution in the width dimension. Default is 1.  param dH The step of the convolution in the height dimension. Default is 1.  param padW The additional zeros added per width to the input planes. Default is 0.  param padH The additional zeros added per height to the input planes. Default is 0.  param adjW Extra width to add to the output image. Default is 0.  param adjH Extra height to add to the output image. Default is 0.  param nGroup Kernel group number.  param noBias If bias is needed.  param wRegularizer: instance of [[Regularizer]]  (eg. L1 or L2 regularization), applied to the input weights matrices.  param bRegularizer: instance of [[Regularizer]]  applied to the bias.   Scala example:  Tensor Input example:   \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialFullConvolution[Tensor[Float],Float](1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input = Tensor(1,1,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n(1,1,.,.) =\n0.18219171      1.3252861       -1.3991559\n0.82611334      1.0313315       0.6075537\n-0.7336061      0.3156875       -0.70616096\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x3x3]\n\nscala  print(output)\n(1,1,.,.) =\n-0.49278542     -0.5823938      -0.8304068      -0.077556044\n-0.5028842      -0.7281958      -1.1927067      -0.34262076\n-0.41680115     -0.41400516     -0.7599415      -0.42024887\n-0.5286566      -0.30015367     -0.5997892      -0.32439864\n\n(1,2,.,.) =\n-0.13131973     -0.5770084      1.1069719       -0.6003375\n-0.40302444     -0.07293816     -0.2654545      0.39749345\n0.37311426      -0.49090374     0.3088816       -0.41700447\n-0.12861171     0.09394867      -0.17229918     0.05556257\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x4]\n\nscala  print(gradOut)\n(1,1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(1,2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x4x4]\n\nscala  print(gradIn)\n(1,1,.,.) =\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x3x3]  Table input Example  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.{T, Table}\n\nval m = SpatialFullConvolution[Table, Float](1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input1 = Tensor(1, 3, 3).randn()\nval input2 = Tensor(3, 3).fill(2.0f)\nval input = T(input1, input2)\nval output = m.forward(input)\nval gradOut = Tensor(2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n {\n        2: 2.0  2.0     2.0\n           2.0  2.0     2.0\n           2.0  2.0     2.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: (1,.,.) =\n           1.276177     0.62761325      0.2715257\n           -0.030832397 0.5046206       0.6835176\n           -0.5832693   0.17266633      0.7461992\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n }\n\nscala  print(output)\n(1,.,.) =\n-0.18339296     0.04208675      -0.17708774     -0.30901802\n-0.1484881      0.23592418      0.115615785     -0.11288056\n-0.47266048     -0.41772115     0.07501307      0.041751802\n-0.4851033      -0.5427048      -0.18293871     -0.12682784\n\n(2,.,.) =\n0.6391188       0.845774        0.41208875      0.13754106\n-0.45785713     0.31221163      0.6006259       0.36563575\n-0.24076991     -0.31931365     0.31651747      0.4836449\n0.24247466      -0.16731171     -0.20887817     0.19513035\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x4]\n\nscala  print(gradOut)\n(1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x4x4]\n\nscala  print(gradIn)\n {\n        2: 0.0  0.0     0.0\n           0.0  0.0     0.0\n           0.0  0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: (1,.,.) =\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nm = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, False)\n\nprint  --------- tensor input--------- \ntensor_input = np.random.rand(1,3,3)\nprint  input is : ,tensor_input\nout = m.forward(tensor_input)\nprint  output m is : ,out\n\nprint  ----------- table input -------- \nadj_input=np.empty([3,3])\nadj_input.fill(2.0)\ntable_input = [tensor_input,adj_input]\nprint  input is : ,table_input\nout = m.forward(table_input)\nprint  output m is : ,out  produces output:  creating: createSpatialFullConvolution\n--------- tensor input---------\ninput is : [[[  9.03998497e-01   4.43054896e-01   6.19571211e-01]\n  [  4.24573060e-01   3.29886286e-04   5.48427154e-02]\n  [  8.99004782e-01   3.25514441e-01   6.85294650e-01]]]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]\n----------- table input --------\ninput is : [array([[[  9.03998497e-01,   4.43054896e-01,   6.19571211e-01],\n        [  4.24573060e-01,   3.29886286e-04,   5.48427154e-02],\n        [  8.99004782e-01,   3.25514441e-01,   6.85294650e-01]]]), array([[ 2.,  2.,  2.],\n       [ 2.,  2.,  2.],\n       [ 2.,  2.,  2.]])]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]", 
            "title": "SpatialFullConvolution"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/#spatialdilatedconvolution", 
            "text": "Scala:  val layer = SpatialDilatedConvolution[T](\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)  Python:  layer = SpatialDilatedConvolution(\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)  Apply a 2D dilated convolution over an input image.  The input tensor is expected to be a 3D or 4D(with batch) tensor.  For a normal SpatialConvolution, the kernel will multiply with input\nimage element-by-element contiguous. In dilated convolution, it\u2019s possible\nto have filters that have spaces between each cell. For example, filter w and\nimage x, when dilatiionW and dilationH both = 1, this is normal 2D convolution  w(0, 0) * x(0, 0), w(0, 1) * x(0, 1)\nw(1, 0) * x(1, 0), w(1, 1) * x(1, 1)  when dilationW and dilationH both = 2  w(0, 0) * x(0, 0), w(0, 1) * x(0, 2)\nw(1, 0) * x(2, 0), w(1, 1) * x(2, 2)  when dilationW and dilationH both = 3  w(0, 0) * x(0, 0), w(0, 1) * x(0, 3)\nw(1, 0) * x(3, 0), w(1, 1) * x(3, 3)  If input is a 3D tensor nInputPlane x height x width,\n * owidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1\n * oheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1  Reference Paper:   Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].\narXiv preprint arXiv:1511.07122, 2015.   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = SpatialDilatedConvolution[Float](1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\nval input = Tensor[Float](T(T(\n  T(1.0f, 2.0f, 3.0f, 4.0f),\n  T(5.0f, 6.0f, 7.0f, 8.0f),\n  T(9.0f, 1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f, 7.0f)\n)))\nval filter = Tensor[Float](T(T(T(\n  T(1.0f, 1.0f),\n  T(1.0f, 1.0f)\n))))\nlayer.weight.copy(filter)\nlayer.bias.zero()\nlayer.forward(input)\nlayer.backward(input, Tensor[Float](T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n))))  Its output should be  (1,.,.) =\n15.0    10.0\n22.0    26.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n(1,.,.) =\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x4]  Python example:  from bigdl.nn.layer import SpatialDilatedConvolution\nimport numpy as np\n\nlayer = SpatialDilatedConvolution(1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\ninput = np.array([[\n  [1.0, 2.0, 3.0, 4.0],\n  [5.0, 6.0, 7.0, 8.0],\n  [9.0, 1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0, 7.0]\n]])\nfilter = np.array([[[\n  [1.0, 1.0],\n  [1.0, 1.0]\n]]])\nbias = np.array([0.0])\nlayer.set_weights([filter, bias])\nlayer.forward(input)\nlayer.backward(input, np.array([[[0.1, 0.2], [0.3, 0.4]]]))  Its output should be  array([[[ 15.,  10.],\n        [ 22.,  26.]]], dtype=float32)\n\narray([[[ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001],\n        [ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001]]], dtype=float32)", 
            "title": "SpatialDilatedConvolution"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/#volumetricconvolution", 
            "text": "Scala:  val module = VolumetricConvolution(nInputPlane, nOutputPlane, kT, kW, kH,\n  dT=1, dW=1, dH=1, padT=0, padW=0, padH=0, withBias=true)  Python:  module = VolumetricConvolution(n_input_plane, n_output_plane, k_t, k_w, k_h,\n  d_t=1, d_w=1, d_h=1, pad_t=0, pad_w=0, pad_h=0, with_bias=true)  Applies a 3D convolution over an input image composed of several input planes. The input tensor\nin forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).\n * @param nInputPlane The number of expected input planes in the image given into forward()\n * @param nOutputPlane The number of output planes the convolution layer will produce.\n * @param kT The kernel size of the convolution in time\n * @param kW The kernel width of the convolution\n * @param kH The kernel height of the convolution\n * @param dT The step of the convolution in the time dimension. Default is 1\n * @param dW The step of the convolution in the width dimension. Default is 1\n * @param dH The step of the convolution in the height dimension. Default is 1\n * @param padT Additional zeros added to the input plane data on both sides of time axis.\n * Default is 0. (kT-1)/2 is often used here.\n * @param padW The additional zeros added per width to the input planes.\n * @param padH The additional zeros added per height to the input planes.\n * @param withBias whether with bias.  Scala example:  val layer = VolumetricConvolution(2, 3, 2, 2, 2, dT=1, dW=1, dH=1,\n  padT=0, padW=0, padH=0, withBias=true)\nval input = Tensor(2, 2, 2, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.54846555      0.5549177\n0.43748873      0.6596535\n\n(1,2,.,.) =\n0.87915933      0.5955469\n0.67464 0.40921077\n\n(2,1,.,.) =\n0.24127467      0.49356017\n0.6707502       0.5421975\n\n(2,2,.,.) =\n0.007834963     0.08188637\n0.51387626      0.7376101\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\nlayer.forward(input)\nres16: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.6680023\n\n(2,1,.,.) =\n0.41926455\n\n(3,1,.,.) =\n-0.029196609\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x1x1]  Python example:  layer = VolumetricConvolution(2, 3, 2, 2, 2, d_t=1, d_w=1, d_h=1,\n          pad_t=0, pad_w=0, pad_h=0, with_bias=True, init_method= default ,\n          bigdl_type= float )\ninput = np.random.rand(2,2,2,2)\n array([[[[ 0.47639062,  0.76800312],\n         [ 0.28834351,  0.21883535]],\n\n        [[ 0.86097919,  0.89812597],\n         [ 0.43632181,  0.58004824]]],\n\n\n       [[[ 0.65784027,  0.34700039],\n         [ 0.64511955,  0.1660241 ]],\n\n        [[ 0.36060054,  0.71265665],\n         [ 0.51755249,  0.6508298 ]]]])\n\nlayer.forward(input)\narray([[[[ 0.54268712]]],\n\n\n       [[[ 0.17670505]]],\n\n\n       [[[ 0.40953237]]]], dtype=float32)", 
            "title": "VolumetricConvolution"
        }, 
        {
            "location": "/APIdocs/Layers/Convolution_Layers/merged-Convolution_Layers/#spatialconvolutionmap", 
            "text": "Scala:  val layer = SpatialConvolutionMap(\n  connTable,\n  kW,\n  kH,\n  dW = 1,\n  dH = 1,\n  padW = 0,\n  padH = 0,\n  wRegularizer = null,\n  bRegularizer = null)  Python:  layer = SpatialConvolutionMap(\n  conn_table,\n  kw,\n  kh,\n  dw=1,\n  dh=1,\n  pad_w=0,\n  pad_h=0,\n  wRegularizer=None,\n  bRegularizer=None)  This class is a generalization of SpatialConvolution.\nIt uses a generic connection table between input and output features.\nThe SpatialConvolution is equivalent to using a full connection table. \nA Connection Table is the mapping of input/output feature map, stored in a 2D Tensor. The first column is the input feature maps. The second column is output feature maps.  Full Connection table:  val conn = SpatialConvolutionMap.full(nin: Int, nout: In)  One to One connection table:  val conn = SpatialConvolutionMap.oneToOne(nfeat: Int)  Random Connection table:  val conn = SpatialConvolutionMap.random(nin: Int, nout: Int, nto: Int)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval conn = SpatialConvolutionMap.oneToOne(3)  conn  is  conn: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 1.0\n2.0 2.0\n3.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]  val module = SpatialConvolutionMap(SpatialConvolutionMap.oneToOne(3), 2, 2)\n\npritnln(module.forward(Tensor.range(1, 48, 1).resize(3, 4, 4)))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n4.5230045   5.8323975   7.1417904\n9.760576    11.069969   12.379362\n14.998148   16.30754    17.616934\n\n(2,.,.) =\n-5.6122046  -5.9227824  -6.233361\n-6.8545156  -7.165093   -7.4756703\n-8.096827   -8.407404   -8.71798\n\n(3,.,.) =\n13.534529   13.908197   14.281864\n15.029203   15.402873   15.77654\n16.523876   16.897545   17.271214\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = SpatialConvolutionMap(np.array([(1, 1), (2, 2), (3, 3)]), 2, 2)\n\nprint(module.forward(np.arange(1, 49, 1).reshape(3, 4, 4)))  Output is  [array([[[-1.24280548, -1.70889318, -2.17498088],\n        [-3.10715604, -3.57324386, -4.03933144],\n        [-4.97150755, -5.43759441, -5.90368223]],\n\n       [[-5.22062826, -5.54696751, -5.87330723],\n        [-6.52598572, -6.85232496, -7.17866373],\n        [-7.8313427 , -8.15768337, -8.48402214]],\n\n       [[ 0.5065825 ,  0.55170798,  0.59683061],\n        [ 0.68707776,  0.73219943,  0.77732348],\n        [ 0.86757064,  0.91269422,  0.95781779]]], dtype=float32)]", 
            "title": "SpatialConvolutionMap"
        }, 
        {
            "location": "/APIdocs/Layers/Pooling_Layers/merged-Pooling_Layers/", 
            "text": "SpatialAveragePooling\n\n\nScala:\n\n\nval m = SpatialAveragePooling(kW, kH, dW=1, dH=1, padW=0, padH=0, ceilMode=false, countIncludePad=true, divide=true)\n\n\n\n\nPython:\n\n\nm = SpatialAveragePooling(kw, kh, dw=1, dh=1, pad_w=0, pad_h=0,ceil_mode=False, count_include_pad=True, divide=True)\n\n\n\n\nSpatialAveragePooling is a module that applies 2D average-pooling operation in \nkW\nx\nkH\n regions by step size \ndW\nx\ndH\n.\n\n\nThe number of output features is equal to the number of input planes.\n\n\nScala example:\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(1, 3, 3).randn()\nval m = SpatialAveragePooling(3, 2, 2, 1)\nval output = m.forward(input)\nval gradOut = Tensor(1, 2, 1).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n(1,.,.) =\n0.9916249       1.0299556       0.5608558\n-0.1664829      1.5031902       0.48598626\n0.37362042      -0.0966136      -1.4257964\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n\nscala\n print(output)\n(1,.,.) =\n0.7341883\n0.1123173\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1]\n\nscala\n print(gradOut)\n(1,.,.) =\n-0.42837557\n-1.5104272\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x1]\n\nscala\n print(gradIn)\n(1,.,.) =\n-0.071395926    -0.071395926    -0.071395926\n-0.3231338      -0.3231338      -0.3231338\n-0.25173786     -0.25173786     -0.25173786\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.randn(1,3,3)\nprint \ninput is :\n,input\n\nm = SpatialAveragePooling(3,2,2,1)\nout = m.forward(input)\nprint \noutput of m is :\n,out\n\ngrad_out = np.random.rand(1,3,1)\ngrad_in = m.backward(input,grad_out)\nprint \ngrad input of m is :\n,grad_in\n\n\n\n\nproduces output:\n\n\ninput is : [[[ 1.50602425 -0.92869054 -1.9393117 ]\n  [ 0.31447547  0.63450578 -0.92485516]\n  [-2.07858315 -0.05688643  0.73648798]]]\ncreating: createSpatialAveragePooling\noutput of m is : [array([[[-0.22297533],\n        [-0.22914261]]], dtype=float32)]\ngrad input of m is : [array([[[ 0.06282618,  0.06282618,  0.06282618],\n        [ 0.09333335,  0.09333335,  0.09333335],\n        [ 0.03050717,  0.03050717,  0.03050717]]], dtype=float32)]\n\n\n\n\n\nVolumetricMaxPooling\n\n\nScala:\n\n\nval layer = VolumetricMaxPooling[T](\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)\n\n\n\n\nPython:\n\n\nlayer = VolumetricMaxPooling(\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)\n\n\n\n\nApplies 3D max-pooling operation in kT x kW x kH regions by step size dT x dW x dH.\nThe number of output features is equal to the number of input planes / dT.\nThe input can optionally be padded with zeros. Padding should be smaller than\nhalf of kernel size. That is, padT \n kT/2, padW \n kW/2 and padH \n kH/2\n\n\nThe input layout should be [batch, plane, time, height, width] or [plane, time, height, width]\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = VolumetricMaxPooling[Float](\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\nval input = Tensor[Float](T(T(\n  T(\n    T(1.0f, 2.0f, 3.0f),\n    T(4.0f, 5.0f, 6.0f),\n    T(7.0f, 8.0f, 9.0f)\n  ),\n  T(\n    T(4.0f, 5.0f, 6.0f),\n    T(1.0f, 3.0f, 9.0f),\n    T(2.0f, 3.0f, 7.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor[Float](T(T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n)))))\n\n\n\n\nIts output should be\n\n\n(1,1,.,.) =\n5.0     9.0\n8.0     9.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\n(1,1,.,.) =\n0.0     0.0     0.0\n0.0     0.1     0.0\n0.0     0.3     0.4\n\n(1,2,.,.) =\n0.0     0.0     0.0\n0.0     0.0     0.2\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import VolumetricMaxPooling\nimport numpy as np\n\nlayer = VolumetricMaxPooling(\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\ninput = np.array([[\n  [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n  ],\n  [\n    [4.0, 5.0, 6.0],\n    [1.0, 3.0, 9.0],\n    [2.0, 3.0, 7.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[[\n  [0.1, 0.2],\n  [0.3, 0.4]\n]]]))\n\n\n\n\nIts output should be\n\n\narray([[[[ 5.,  9.],\n         [ 8.,  9.]]]], dtype=float32)\n\narray([[[[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.1       ,  0.        ],\n         [ 0.        ,  0.30000001,  0.40000001]],\n\n        [[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.        ,  0.2       ],\n         [ 0.        ,  0.        ,  0.        ]]]], dtype=float32)\n\n\n\n\nRoiPooling\n\n\nScala:\n\n\nval m =  RoiPooling(pooled_w, pooled_h, spatial_scale)\n\n\n\n\nPython:\n\n\nm = RoiPooling(pooled_w, pooled_h, spatial_scale)\n\n\n\n\nRoiPooling is a module that performs Region of Interest pooling. \n\n\nIt uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of pooledH \u00d7 pooledW (e.g., 7 \u00d7 7).\n\n\nAn RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its top-left corner (x1, y1) and its bottom-right corner (x2, y2).\n\n\nRoI max pooling works by dividing the h \u00d7 w RoI window into an pooledH \u00d7 pooledW grid of sub-windows of approximate size h/H \u00d7 w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel\n\n\nforward\n accepts a table containing 2 tensors as input, the first tensor is the input image, the second tensor is the ROI regions. The dimension of the second tensor should be (*,5) (5 are  \nbatch_num, x1, y1, x2, y2\n).  \n\n\nScala example:\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input_data = Tensor[Float](2,2,6,8).randn()\nval rois = Array(0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3)\nval input_rois = Tensor(Storage(rois.map(x =\n x.toFloat))).resize(4, 5)\nval input = T(input_data,input_rois)\nval m = RoiPooling[Float](3, 2, 1)\nval output = m.forward(input)\n\nscala\n print(input)\n {\n        2: 0.0  0.0     0.0     7.0     5.0\n           1.0  6.0     2.0     7.0     5.0\n           1.0  3.0     1.0     6.0     4.0\n           0.0  3.0     3.0     3.0     3.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4x5]\n        1: (1,1,.,.) =\n           0.48066297   1.0994664       0.32474303      2.3391871       -0.79605865     0.836963950.36107457      1.2622415\n           0.657079     0.12720469      0.39894578      -0.41185552     -0.53111094     -0.36016005       -0.9726423      -2.5785272\n           0.3091435    -0.03613516     0.2375721       -1.1920663      -0.6757661      1.10612681.5409279        -0.17411499\n           0.23274016   -0.7149633      0.5473105       -0.40570387     -1.7966263      0.2071798-1.1530842       -0.010083453\n           -1.5769979   0.17043112      -0.28578365     -0.90779626     0.61457515      -0.1553582-0.3912479      -0.15326484\n           -0.24283029  1.3215472       1.3795123       -0.36933053     0.7077386       -0.56398267       0.6159163       0.5802894\n\n           (1,2,.,.) =\n           -1.1817129   -0.20470592     -1.3201113      0.36523122      -0.18260211     1.30210171.214403 1.1019816\n           0.7186407    0.78731173      1.5452348       0.0396181       0.5927014       1.17697431.0501136        -0.58295316\n           -0.96753055  0.6427254       -1.1396345      0.8701054       -0.22860864     -1.18719451.3372624       0.8616691\n           0.796831     -0.16609778     0.2950535       0.4595303       0.192339        0.6086106-0.76351887      -0.65964174\n           -0.12746814  -0.036058053    0.8858275       0.9677718       -1.1074747      -1.36859390.8783633       -0.11723315\n           -0.6947403   -0.23226547     -1.8510057      -1.3695518      -0.22317407     -0.36249024       -0.24097045     1.5691053\n\n           (2,1,.,.) =\n           0.84056973   1.144949        -1.0660682      0.4416162       -0.94440234     -0.24461010.91145027      -0.88650835\n           -0.81542057  0.14578317      -0.6531974      0.60776395      -0.32058007     -1.80771481.7660322       1.0680646\n           1.1328241    0.43677545      -0.9402618      -1.3002211      0.26012567      1.69481340.37849447       0.39286092\n           1.9443163    0.5415504       1.0793099       1.3312546       0.48346 1.2019655       0.3718734 0.21091922\n           0.5499047    1.6418253       0.8064177       0.37626198      0.8736181       -0.40816033       -0.5806787      1.286581\n           -0.5904657   -0.21188398     -0.040509004    1.2989452       1.6827602       1.3229258-0.68433124      0.87974\n\n           (2,2,.,.) =\n           -0.09759476  -0.32767114     0.16223079      2.3114302       -0.48496276     1.19290720.8572289        0.43429425\n           -1.0245247   0.19002944      1.5659521       -1.3689835      -1.4437296      -0.38216656       0.6333655       -0.57124794\n           -0.31111157  1.5184602       -1.3835855      -0.9295573      2.244521        -1.11849820.5451996       -0.4441631\n           -1.534093    -0.5599659      1.1980947       -1.0140935      1.3288999       0.19487387-0.1261734      -1.2222558\n           -0.070535585 0.9047848       -0.6719811      -1.6532638      -0.5290511      -0.18300447       0.69385433      0.018756092\n           0.24767837   0.620484        -0.5346291      1.0685066       -0.36903372     -0.26955062       1.1042496       0.5944603\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x6x8]\n }\n\nscala\n print(output)\n(1,1,.,.) =\n1.0994664       2.3391871       1.5409279\n1.3795123       1.3795123       0.6159163\n\n(1,2,.,.) =\n1.5452348       1.5452348       1.3372624\n0.8858275       0.9677718       1.5691053\n\n(2,1,.,.) =\n0.37849447      0.39286092      0.39286092\n-0.5806787      1.286581        1.286581\n\n(2,2,.,.) =\n0.5451996       0.5451996       -0.4441631\n1.1042496       1.1042496       0.5944603\n\n(3,1,.,.) =\n0.60776395      1.6948134       1.7660322\n1.3312546       1.2019655       1.2019655\n\n(3,2,.,.) =\n2.244521        2.244521        0.6333655\n1.3288999       1.3288999       0.69385433\n\n(4,1,.,.) =\n-0.40570387     -0.40570387     -0.40570387\n-0.40570387     -0.40570387     -0.40570387\n\n(4,2,.,.) =\n0.4595303       0.4595303       0.4595303\n0.4595303       0.4595303       0.4595303\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x2x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput_data = np.random.randn(2,2,6,8)\ninput_rois = np.array([0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3],dtype='float64').reshape(4,5)\nprint \ninput is :\n,[input_data, input_rois]\n\nm = RoiPooling(3,2,1.0)\nout = m.forward([input_data,input_rois])\nprint \noutput of m is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [array([[[[ 0.08500103,  0.33421796,  0.29084699,  1.60344635, -0.24289341,\n          -0.4793888 ,  0.09452426,  0.16842477],\n         [-1.18575497, -0.53337542,  0.11661001,  0.9647904 , -0.25187936,\n           0.36516823, -0.16647209, -0.08095158],\n         [ 1.1982232 , -0.33549174,  0.11721347, -0.29319686, -0.01290122,\n           0.12344296,  0.30074829, -2.34951463],\n         [-0.60470899, -0.84657051,  0.1269276 , -0.06152321, -1.68838416,\n          -0.69808296, -2.06112892, -1.44790449],\n         [ 1.03944288,  0.13871728,  0.91478479,  0.47517105,  1.24638374,\n           0.98666841,  0.49403488,  1.26101127],\n         [-1.03949343, -0.39291108,  1.39107512,  1.73779253,  0.91656129,\n           0.103381  ,  0.956243  ,  0.44743548]],\n\n        [[ 0.79028054,  0.64244228, -0.37997334, -0.09130215, -2.3903429 ,\n           0.71919208, -0.14079786,  0.98304272],\n         [ 1.14678457,  1.58825227,  0.17137367, -0.62121819, -0.36103113,\n          -0.04452576, -0.0886136 , -1.32884721],\n         [ 0.06728957, -0.29701304, -0.52754207, -1.5785875 ,  1.47354834,\n          -0.28545156,  0.49874194,  0.10277613],\n         [-0.10117571, -1.34902427, -1.40789327,  0.09853599,  0.60420022,\n           0.54869115, -0.49067696,  0.26696793],\n         [ 1.11780279, -0.77929016,  1.13772094,  0.14374057,  0.33199688,\n          -0.54057374, -0.45718861,  1.1577623 ],\n         [-1.4005645 ,  1.15870496,  0.39292003,  0.88379515,  0.06440974,\n           0.65013063,  0.03759244,  0.18730126]]],\n\n\n       [[[-2.28272906,  0.06056305,  0.73632597,  0.10063274, -1.27497525,\n          -0.95597581, -0.22745785,  0.40146498],\n         [-1.37783475,  1.66000653, -1.80071745, -0.11805539, -0.27160583,\n           0.30691418,  2.62243232, -1.95274516],\n         [ 1.61364148,  1.91470546, -1.51984424,  2.13598224, -0.23156685,\n          -0.74203698,  0.65316888,  0.08018846],\n         [-1.8912854 , -0.50106158,  0.94937966, -0.10930541,  0.82136627,\n          -1.33209063,  1.43371302, -1.36370916],\n         [-0.52737928, -0.0681305 , -0.63472587,  0.41979229, -0.57093624,\n          -0.15968764, -1.005951  , -2.06873572],\n         [-2.34089346,  1.02593977,  0.90183415,  0.09504819,  0.53185448,\n           1.11305345,  1.290016  , -1.76216646]],\n\n        [[-0.10885459, -0.57089742, -0.55340708, -1.94445884,  1.30130049,\n           0.6333372 , -1.03100083,  0.0111167 ],\n         [ 0.59678149, -0.67601521, -1.25288718, -0.10922251,  3.06808996,\n          -1.46701513, -0.42140765,  1.12485412],\n         [ 1.21301567, -1.43304957, -0.56047239,  0.20716087,  1.40737646,\n          -0.08386437, -0.21916043,  0.85692906],\n         [ 1.59992399, -1.37044315, -0.71884386,  2.61830979, -0.74305496,\n          -0.32021174,  1.43275058, -0.3891857 ],\n         [-0.41355145,  0.22589689,  0.33154415,  0.86146815, -1.66326091,\n           0.37581697, -3.2250516 , -0.48807863],\n         [-2.52968957,  0.95801598, -1.20118154,  0.01141421, -0.11871498,\n           0.04555184,  1.3950473 ,  0.37887998]]]]), array([[ 0.,  0.,  0.,  7.,  5.],\n       [ 1.,  6.,  2.,  7.,  5.],\n       [ 1.,  3.,  1.,  6.,  4.],\n       [ 0.,  3.,  3.,  3.,  3.]])]\ncreating: createRoiPooling\noutput of m is : [[[[ 1.19822323  1.60344636  0.36516821]\n   [ 1.39107513  1.73779249  1.26101124]]\n\n  [[ 1.58825231  1.47354829  0.98304272]\n   [ 1.158705    1.13772094  1.15776229]]]\n\n\n [[[ 1.43371308  1.43371308  0.08018846]\n   [ 1.29001606  1.29001606 -1.7621665 ]]\n\n  [[ 1.43275058  1.43275058  0.85692906]\n   [ 1.39504731  1.39504731  0.37887999]]]\n\n\n [[[ 2.13598228  0.30691418  2.62243223]\n   [ 0.82136625  0.82136625  1.43371308]]\n\n  [[ 3.06808996  3.06808996 -0.08386437]\n   [ 2.61830974  0.37581697  1.43275058]]]\n\n\n [[[-0.06152321 -0.06152321 -0.06152321]\n   [-0.06152321 -0.06152321 -0.06152321]]\n\n  [[ 0.09853599  0.09853599  0.09853599]\n   [ 0.09853599  0.09853599  0.09853599]]]]\n\n\n\n\n\nSpatialMaxPooling\n\n\nScala:\n\n\nval mp = SpatialMaxPooling(2, 2, dW=2, dH=2, padW=0, padH=0)\n\n\n\n\nPython:\n\n\nmp = SpatialMaxPooling(2, 2, dw=2, dh=2, pad_w=0, pad_h=0, to_ceil=false)\n\n\n\n\nApplies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.\nThe number of output features is equal to the number of input planes.\nIf the input image is a 3D tensor nInputPlane x height x width,\nthe output image size will be nOutputPlane x oheight x owidth where\nowidth  = op((width  + 2\npadW - kW) / dW + 1)\noheight = op((height + 2\npadH - kH) / dH + 1)\nop is a rounding operator. By default, it is floor.\nIt can be changed by calling :ceil() or :floor() methods.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.SpatialMaxPooling\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mp = SpatialMaxPooling(2, 2, 2, 2)\nval input = Tensor(1, 3, 3)\n\ninput(Array(1, 1, 1)) = 0.5336726f\ninput(Array(1, 1, 2)) = 0.7963769f\ninput(Array(1, 1, 3)) = 0.5674766f\ninput(Array(1, 2, 1)) = 0.1803996f\ninput(Array(1, 2, 2)) = 0.2460861f\ninput(Array(1, 2, 3)) = 0.2295625f\ninput(Array(1, 3, 1)) = 0.3073633f\ninput(Array(1, 3, 2)) = 0.5973460f\ninput(Array(1, 3, 3)) = 0.4298954f\n\nval gradOutput = Tensor(1, 1, 1)\ngradOutput(Array(1, 1, 1)) = 0.023921491578221f\n\nval output = mp.forward(input)\nval gradInput = mp.backward(input, gradOutput)\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\nThe output is,\n\n\n(1,.,.) =\n0.7963769\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1]\n\n\n\n\nThe gradInput is,\n\n\n(1,.,.) =\n0.0     0.023921492     0.0\n0.0     0.0     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmp = SpatialMaxPooling(2, 2, 2, 2)\n\n\ninput = np.array([0.5336726, 0.7963769, 0.5674766, 0.1803996, 0.2460861, 0.2295625, 0.3073633, 0.5973460, 0.4298954]).astype(\nfloat32\n)\ninput = input.reshape(1, 3, 3)\n\noutput = mp.forward(input)\nprint output\n\ngradOutput = np.array([0.023921491578221]).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(1, 1, 1)\n\ngradInput = mp.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nThe output is,\n\n\n[array([[[ 0.79637688]]], dtype=float32)]\n\n\n\n\nThe gradInput is,\n\n\n[array([[[ 0.        ,  0.02392149,  0.        ],\n        [ 0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ]]], dtype=float32)]", 
            "title": "Pooling Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Pooling_Layers/merged-Pooling_Layers/#spatialaveragepooling", 
            "text": "Scala:  val m = SpatialAveragePooling(kW, kH, dW=1, dH=1, padW=0, padH=0, ceilMode=false, countIncludePad=true, divide=true)  Python:  m = SpatialAveragePooling(kw, kh, dw=1, dh=1, pad_w=0, pad_h=0,ceil_mode=False, count_include_pad=True, divide=True)  SpatialAveragePooling is a module that applies 2D average-pooling operation in  kW x kH  regions by step size  dW x dH .  The number of output features is equal to the number of input planes.  Scala example:  scala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(1, 3, 3).randn()\nval m = SpatialAveragePooling(3, 2, 2, 1)\nval output = m.forward(input)\nval gradOut = Tensor(1, 2, 1).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n(1,.,.) =\n0.9916249       1.0299556       0.5608558\n-0.1664829      1.5031902       0.48598626\n0.37362042      -0.0966136      -1.4257964\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n\nscala  print(output)\n(1,.,.) =\n0.7341883\n0.1123173\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1]\n\nscala  print(gradOut)\n(1,.,.) =\n-0.42837557\n-1.5104272\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x1]\n\nscala  print(gradIn)\n(1,.,.) =\n-0.071395926    -0.071395926    -0.071395926\n-0.3231338      -0.3231338      -0.3231338\n-0.25173786     -0.25173786     -0.25173786\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.randn(1,3,3)\nprint  input is : ,input\n\nm = SpatialAveragePooling(3,2,2,1)\nout = m.forward(input)\nprint  output of m is : ,out\n\ngrad_out = np.random.rand(1,3,1)\ngrad_in = m.backward(input,grad_out)\nprint  grad input of m is : ,grad_in  produces output:  input is : [[[ 1.50602425 -0.92869054 -1.9393117 ]\n  [ 0.31447547  0.63450578 -0.92485516]\n  [-2.07858315 -0.05688643  0.73648798]]]\ncreating: createSpatialAveragePooling\noutput of m is : [array([[[-0.22297533],\n        [-0.22914261]]], dtype=float32)]\ngrad input of m is : [array([[[ 0.06282618,  0.06282618,  0.06282618],\n        [ 0.09333335,  0.09333335,  0.09333335],\n        [ 0.03050717,  0.03050717,  0.03050717]]], dtype=float32)]", 
            "title": "SpatialAveragePooling"
        }, 
        {
            "location": "/APIdocs/Layers/Pooling_Layers/merged-Pooling_Layers/#volumetricmaxpooling", 
            "text": "Scala:  val layer = VolumetricMaxPooling[T](\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)  Python:  layer = VolumetricMaxPooling(\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)  Applies 3D max-pooling operation in kT x kW x kH regions by step size dT x dW x dH.\nThe number of output features is equal to the number of input planes / dT.\nThe input can optionally be padded with zeros. Padding should be smaller than\nhalf of kernel size. That is, padT   kT/2, padW   kW/2 and padH   kH/2  The input layout should be [batch, plane, time, height, width] or [plane, time, height, width]  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = VolumetricMaxPooling[Float](\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\nval input = Tensor[Float](T(T(\n  T(\n    T(1.0f, 2.0f, 3.0f),\n    T(4.0f, 5.0f, 6.0f),\n    T(7.0f, 8.0f, 9.0f)\n  ),\n  T(\n    T(4.0f, 5.0f, 6.0f),\n    T(1.0f, 3.0f, 9.0f),\n    T(2.0f, 3.0f, 7.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor[Float](T(T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n)))))  Its output should be  (1,1,.,.) =\n5.0     9.0\n8.0     9.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\n(1,1,.,.) =\n0.0     0.0     0.0\n0.0     0.1     0.0\n0.0     0.3     0.4\n\n(1,2,.,.) =\n0.0     0.0     0.0\n0.0     0.0     0.2\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]  Python example:  from bigdl.nn.layer import VolumetricMaxPooling\nimport numpy as np\n\nlayer = VolumetricMaxPooling(\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\ninput = np.array([[\n  [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n  ],\n  [\n    [4.0, 5.0, 6.0],\n    [1.0, 3.0, 9.0],\n    [2.0, 3.0, 7.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[[\n  [0.1, 0.2],\n  [0.3, 0.4]\n]]]))  Its output should be  array([[[[ 5.,  9.],\n         [ 8.,  9.]]]], dtype=float32)\n\narray([[[[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.1       ,  0.        ],\n         [ 0.        ,  0.30000001,  0.40000001]],\n\n        [[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.        ,  0.2       ],\n         [ 0.        ,  0.        ,  0.        ]]]], dtype=float32)", 
            "title": "VolumetricMaxPooling"
        }, 
        {
            "location": "/APIdocs/Layers/Pooling_Layers/merged-Pooling_Layers/#roipooling", 
            "text": "Scala:  val m =  RoiPooling(pooled_w, pooled_h, spatial_scale)  Python:  m = RoiPooling(pooled_w, pooled_h, spatial_scale)  RoiPooling is a module that performs Region of Interest pooling.   It uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of pooledH \u00d7 pooledW (e.g., 7 \u00d7 7).  An RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its top-left corner (x1, y1) and its bottom-right corner (x2, y2).  RoI max pooling works by dividing the h \u00d7 w RoI window into an pooledH \u00d7 pooledW grid of sub-windows of approximate size h/H \u00d7 w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel  forward  accepts a table containing 2 tensors as input, the first tensor is the input image, the second tensor is the ROI regions. The dimension of the second tensor should be (*,5) (5 are   batch_num, x1, y1, x2, y2 ).    Scala example:  scala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input_data = Tensor[Float](2,2,6,8).randn()\nval rois = Array(0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3)\nval input_rois = Tensor(Storage(rois.map(x =  x.toFloat))).resize(4, 5)\nval input = T(input_data,input_rois)\nval m = RoiPooling[Float](3, 2, 1)\nval output = m.forward(input)\n\nscala  print(input)\n {\n        2: 0.0  0.0     0.0     7.0     5.0\n           1.0  6.0     2.0     7.0     5.0\n           1.0  3.0     1.0     6.0     4.0\n           0.0  3.0     3.0     3.0     3.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4x5]\n        1: (1,1,.,.) =\n           0.48066297   1.0994664       0.32474303      2.3391871       -0.79605865     0.836963950.36107457      1.2622415\n           0.657079     0.12720469      0.39894578      -0.41185552     -0.53111094     -0.36016005       -0.9726423      -2.5785272\n           0.3091435    -0.03613516     0.2375721       -1.1920663      -0.6757661      1.10612681.5409279        -0.17411499\n           0.23274016   -0.7149633      0.5473105       -0.40570387     -1.7966263      0.2071798-1.1530842       -0.010083453\n           -1.5769979   0.17043112      -0.28578365     -0.90779626     0.61457515      -0.1553582-0.3912479      -0.15326484\n           -0.24283029  1.3215472       1.3795123       -0.36933053     0.7077386       -0.56398267       0.6159163       0.5802894\n\n           (1,2,.,.) =\n           -1.1817129   -0.20470592     -1.3201113      0.36523122      -0.18260211     1.30210171.214403 1.1019816\n           0.7186407    0.78731173      1.5452348       0.0396181       0.5927014       1.17697431.0501136        -0.58295316\n           -0.96753055  0.6427254       -1.1396345      0.8701054       -0.22860864     -1.18719451.3372624       0.8616691\n           0.796831     -0.16609778     0.2950535       0.4595303       0.192339        0.6086106-0.76351887      -0.65964174\n           -0.12746814  -0.036058053    0.8858275       0.9677718       -1.1074747      -1.36859390.8783633       -0.11723315\n           -0.6947403   -0.23226547     -1.8510057      -1.3695518      -0.22317407     -0.36249024       -0.24097045     1.5691053\n\n           (2,1,.,.) =\n           0.84056973   1.144949        -1.0660682      0.4416162       -0.94440234     -0.24461010.91145027      -0.88650835\n           -0.81542057  0.14578317      -0.6531974      0.60776395      -0.32058007     -1.80771481.7660322       1.0680646\n           1.1328241    0.43677545      -0.9402618      -1.3002211      0.26012567      1.69481340.37849447       0.39286092\n           1.9443163    0.5415504       1.0793099       1.3312546       0.48346 1.2019655       0.3718734 0.21091922\n           0.5499047    1.6418253       0.8064177       0.37626198      0.8736181       -0.40816033       -0.5806787      1.286581\n           -0.5904657   -0.21188398     -0.040509004    1.2989452       1.6827602       1.3229258-0.68433124      0.87974\n\n           (2,2,.,.) =\n           -0.09759476  -0.32767114     0.16223079      2.3114302       -0.48496276     1.19290720.8572289        0.43429425\n           -1.0245247   0.19002944      1.5659521       -1.3689835      -1.4437296      -0.38216656       0.6333655       -0.57124794\n           -0.31111157  1.5184602       -1.3835855      -0.9295573      2.244521        -1.11849820.5451996       -0.4441631\n           -1.534093    -0.5599659      1.1980947       -1.0140935      1.3288999       0.19487387-0.1261734      -1.2222558\n           -0.070535585 0.9047848       -0.6719811      -1.6532638      -0.5290511      -0.18300447       0.69385433      0.018756092\n           0.24767837   0.620484        -0.5346291      1.0685066       -0.36903372     -0.26955062       1.1042496       0.5944603\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x6x8]\n }\n\nscala  print(output)\n(1,1,.,.) =\n1.0994664       2.3391871       1.5409279\n1.3795123       1.3795123       0.6159163\n\n(1,2,.,.) =\n1.5452348       1.5452348       1.3372624\n0.8858275       0.9677718       1.5691053\n\n(2,1,.,.) =\n0.37849447      0.39286092      0.39286092\n-0.5806787      1.286581        1.286581\n\n(2,2,.,.) =\n0.5451996       0.5451996       -0.4441631\n1.1042496       1.1042496       0.5944603\n\n(3,1,.,.) =\n0.60776395      1.6948134       1.7660322\n1.3312546       1.2019655       1.2019655\n\n(3,2,.,.) =\n2.244521        2.244521        0.6333655\n1.3288999       1.3288999       0.69385433\n\n(4,1,.,.) =\n-0.40570387     -0.40570387     -0.40570387\n-0.40570387     -0.40570387     -0.40570387\n\n(4,2,.,.) =\n0.4595303       0.4595303       0.4595303\n0.4595303       0.4595303       0.4595303\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x2x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput_data = np.random.randn(2,2,6,8)\ninput_rois = np.array([0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3],dtype='float64').reshape(4,5)\nprint  input is : ,[input_data, input_rois]\n\nm = RoiPooling(3,2,1.0)\nout = m.forward([input_data,input_rois])\nprint  output of m is : ,out  produces output:  input is : [array([[[[ 0.08500103,  0.33421796,  0.29084699,  1.60344635, -0.24289341,\n          -0.4793888 ,  0.09452426,  0.16842477],\n         [-1.18575497, -0.53337542,  0.11661001,  0.9647904 , -0.25187936,\n           0.36516823, -0.16647209, -0.08095158],\n         [ 1.1982232 , -0.33549174,  0.11721347, -0.29319686, -0.01290122,\n           0.12344296,  0.30074829, -2.34951463],\n         [-0.60470899, -0.84657051,  0.1269276 , -0.06152321, -1.68838416,\n          -0.69808296, -2.06112892, -1.44790449],\n         [ 1.03944288,  0.13871728,  0.91478479,  0.47517105,  1.24638374,\n           0.98666841,  0.49403488,  1.26101127],\n         [-1.03949343, -0.39291108,  1.39107512,  1.73779253,  0.91656129,\n           0.103381  ,  0.956243  ,  0.44743548]],\n\n        [[ 0.79028054,  0.64244228, -0.37997334, -0.09130215, -2.3903429 ,\n           0.71919208, -0.14079786,  0.98304272],\n         [ 1.14678457,  1.58825227,  0.17137367, -0.62121819, -0.36103113,\n          -0.04452576, -0.0886136 , -1.32884721],\n         [ 0.06728957, -0.29701304, -0.52754207, -1.5785875 ,  1.47354834,\n          -0.28545156,  0.49874194,  0.10277613],\n         [-0.10117571, -1.34902427, -1.40789327,  0.09853599,  0.60420022,\n           0.54869115, -0.49067696,  0.26696793],\n         [ 1.11780279, -0.77929016,  1.13772094,  0.14374057,  0.33199688,\n          -0.54057374, -0.45718861,  1.1577623 ],\n         [-1.4005645 ,  1.15870496,  0.39292003,  0.88379515,  0.06440974,\n           0.65013063,  0.03759244,  0.18730126]]],\n\n\n       [[[-2.28272906,  0.06056305,  0.73632597,  0.10063274, -1.27497525,\n          -0.95597581, -0.22745785,  0.40146498],\n         [-1.37783475,  1.66000653, -1.80071745, -0.11805539, -0.27160583,\n           0.30691418,  2.62243232, -1.95274516],\n         [ 1.61364148,  1.91470546, -1.51984424,  2.13598224, -0.23156685,\n          -0.74203698,  0.65316888,  0.08018846],\n         [-1.8912854 , -0.50106158,  0.94937966, -0.10930541,  0.82136627,\n          -1.33209063,  1.43371302, -1.36370916],\n         [-0.52737928, -0.0681305 , -0.63472587,  0.41979229, -0.57093624,\n          -0.15968764, -1.005951  , -2.06873572],\n         [-2.34089346,  1.02593977,  0.90183415,  0.09504819,  0.53185448,\n           1.11305345,  1.290016  , -1.76216646]],\n\n        [[-0.10885459, -0.57089742, -0.55340708, -1.94445884,  1.30130049,\n           0.6333372 , -1.03100083,  0.0111167 ],\n         [ 0.59678149, -0.67601521, -1.25288718, -0.10922251,  3.06808996,\n          -1.46701513, -0.42140765,  1.12485412],\n         [ 1.21301567, -1.43304957, -0.56047239,  0.20716087,  1.40737646,\n          -0.08386437, -0.21916043,  0.85692906],\n         [ 1.59992399, -1.37044315, -0.71884386,  2.61830979, -0.74305496,\n          -0.32021174,  1.43275058, -0.3891857 ],\n         [-0.41355145,  0.22589689,  0.33154415,  0.86146815, -1.66326091,\n           0.37581697, -3.2250516 , -0.48807863],\n         [-2.52968957,  0.95801598, -1.20118154,  0.01141421, -0.11871498,\n           0.04555184,  1.3950473 ,  0.37887998]]]]), array([[ 0.,  0.,  0.,  7.,  5.],\n       [ 1.,  6.,  2.,  7.,  5.],\n       [ 1.,  3.,  1.,  6.,  4.],\n       [ 0.,  3.,  3.,  3.,  3.]])]\ncreating: createRoiPooling\noutput of m is : [[[[ 1.19822323  1.60344636  0.36516821]\n   [ 1.39107513  1.73779249  1.26101124]]\n\n  [[ 1.58825231  1.47354829  0.98304272]\n   [ 1.158705    1.13772094  1.15776229]]]\n\n\n [[[ 1.43371308  1.43371308  0.08018846]\n   [ 1.29001606  1.29001606 -1.7621665 ]]\n\n  [[ 1.43275058  1.43275058  0.85692906]\n   [ 1.39504731  1.39504731  0.37887999]]]\n\n\n [[[ 2.13598228  0.30691418  2.62243223]\n   [ 0.82136625  0.82136625  1.43371308]]\n\n  [[ 3.06808996  3.06808996 -0.08386437]\n   [ 2.61830974  0.37581697  1.43275058]]]\n\n\n [[[-0.06152321 -0.06152321 -0.06152321]\n   [-0.06152321 -0.06152321 -0.06152321]]\n\n  [[ 0.09853599  0.09853599  0.09853599]\n   [ 0.09853599  0.09853599  0.09853599]]]]", 
            "title": "RoiPooling"
        }, 
        {
            "location": "/APIdocs/Layers/Pooling_Layers/merged-Pooling_Layers/#spatialmaxpooling", 
            "text": "Scala:  val mp = SpatialMaxPooling(2, 2, dW=2, dH=2, padW=0, padH=0)  Python:  mp = SpatialMaxPooling(2, 2, dw=2, dh=2, pad_w=0, pad_h=0, to_ceil=false)  Applies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.\nThe number of output features is equal to the number of input planes.\nIf the input image is a 3D tensor nInputPlane x height x width,\nthe output image size will be nOutputPlane x oheight x owidth where\nowidth  = op((width  + 2 padW - kW) / dW + 1)\noheight = op((height + 2 padH - kH) / dH + 1)\nop is a rounding operator. By default, it is floor.\nIt can be changed by calling :ceil() or :floor() methods.  Scala example:  import com.intel.analytics.bigdl.nn.SpatialMaxPooling\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mp = SpatialMaxPooling(2, 2, 2, 2)\nval input = Tensor(1, 3, 3)\n\ninput(Array(1, 1, 1)) = 0.5336726f\ninput(Array(1, 1, 2)) = 0.7963769f\ninput(Array(1, 1, 3)) = 0.5674766f\ninput(Array(1, 2, 1)) = 0.1803996f\ninput(Array(1, 2, 2)) = 0.2460861f\ninput(Array(1, 2, 3)) = 0.2295625f\ninput(Array(1, 3, 1)) = 0.3073633f\ninput(Array(1, 3, 2)) = 0.5973460f\ninput(Array(1, 3, 3)) = 0.4298954f\n\nval gradOutput = Tensor(1, 1, 1)\ngradOutput(Array(1, 1, 1)) = 0.023921491578221f\n\nval output = mp.forward(input)\nval gradInput = mp.backward(input, gradOutput)\n\nprintln(output)\nprintln(gradInput)  The output is,  (1,.,.) =\n0.7963769\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1]  The gradInput is,  (1,.,.) =\n0.0     0.023921492     0.0\n0.0     0.0     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmp = SpatialMaxPooling(2, 2, 2, 2)\n\n\ninput = np.array([0.5336726, 0.7963769, 0.5674766, 0.1803996, 0.2460861, 0.2295625, 0.3073633, 0.5973460, 0.4298954]).astype( float32 )\ninput = input.reshape(1, 3, 3)\n\noutput = mp.forward(input)\nprint output\n\ngradOutput = np.array([0.023921491578221]).astype( float32 )\ngradOutput = gradOutput.reshape(1, 1, 1)\n\ngradInput = mp.backward(input, gradOutput)\nprint gradInput  The output is,  [array([[[ 0.79637688]]], dtype=float32)]  The gradInput is,  [array([[[ 0.        ,  0.02392149,  0.        ],\n        [ 0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ]]], dtype=float32)]", 
            "title": "SpatialMaxPooling"
        }, 
        {
            "location": "/APIdocs/Layers/Linear_Layers/merged-Linear_Layers/", 
            "text": "Linear\n\n\nScala:\n\n\nval module = Linear(\n  inputSize,\n  outputSize,\n  withBias = true,\n  wRegularizer = null,\n  bRegularizer = null,\n  initWeight = null,\n  initBias = null,\n  initGradWeight = null,\n  initGradBias = null)\n\n\n\n\nPython:\n\n\nmodule = Linear(\n  input_size,\n  output_size,\n  init_method=\ndefault\n,\n  with_bias=True,\n  wRegularizer=None,\n  bRegularizer=None,\n  init_weight=None,\n  init_bias=None,\n  init_grad_weight=None,\n  init_grad_bias=None)\n\n\n\n\nThe \nLinear\n module applies a linear transformation to the input data,\ni.e. \ny = Wx + b\n. The \ninput\n given in \nforward(input)\n must be either\na vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must\nhave the size of \ninputSize\n. If it is a matrix, then each row is assumed to be\nan input sample of given batch (the number of rows means the batch size and\nthe number of columns should be equal to the \ninputSize\n).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Linear(3, 5)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))\n\n\n\n\nOutput is\n```com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.79338956\n-2.3417668\n-2.7557678\n-0.07507719\n-1.009765\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n**Python example:**\n```python\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Linear(3, 5)\n\nprint(module.forward(np.arange(1, 4, 1)))\n\n\n\n\nOutput is\n\n\n[array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]", 
            "title": "Linear Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Linear_Layers/merged-Linear_Layers/#linear", 
            "text": "Scala:  val module = Linear(\n  inputSize,\n  outputSize,\n  withBias = true,\n  wRegularizer = null,\n  bRegularizer = null,\n  initWeight = null,\n  initBias = null,\n  initGradWeight = null,\n  initGradBias = null)  Python:  module = Linear(\n  input_size,\n  output_size,\n  init_method= default ,\n  with_bias=True,\n  wRegularizer=None,\n  bRegularizer=None,\n  init_weight=None,\n  init_bias=None,\n  init_grad_weight=None,\n  init_grad_bias=None)  The  Linear  module applies a linear transformation to the input data,\ni.e.  y = Wx + b . The  input  given in  forward(input)  must be either\na vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must\nhave the size of  inputSize . If it is a matrix, then each row is assumed to be\nan input sample of given batch (the number of rows means the batch size and\nthe number of columns should be equal to the  inputSize ).  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Linear(3, 5)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))  Output is\n```com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.79338956\n-2.3417668\n-2.7557678\n-0.07507719\n-1.009765\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  \n**Python example:**\n```python\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Linear(3, 5)\n\nprint(module.forward(np.arange(1, 4, 1)))  Output is  [array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]", 
            "title": "Linear"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/", 
            "text": "SoftSign\n\n\nScala:\n\n\nval softSign = SoftSign()\n\n\n\n\nPython:\n\n\nsoftSign = SoftSign()\n\n\n\n\nSoftSign applies SoftSign function to the input tensor\n\n\nSoftSign function: \nf_i(x) = x_i / (1+|x_i|)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval softSign = SoftSign()\nval input = Tensor(3, 3).rand()\n\n\n print(input)\n0.6733504   0.7566517   0.43793806  \n0.09683273  0.05829774  0.4567967   \n0.20021072  0.11158377  0.31668025\n\n\n print(softSign.forward(input))\n0.40239656  0.4307352   0.30455974  \n0.08828395  0.05508633  0.31356242  \n0.16681297  0.10038269  0.24051417  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nsoftSign=SoftSign()\n\n softSign.forward(np.array([[1, 2, 4],[-1, -2, -4]]))\n[array([[ 0.5       ,  0.66666669,  0.80000001],\n       [-0.5       , -0.66666669, -0.80000001]], dtype=float32)]\n\n\n\n\n\nReLU6\n\n\nScala:\n\n\nval module = ReLU6(inplace = false)\n\n\n\n\nPython:\n\n\nmodule = ReLU6(inplace=False)\n\n\n\n\nSame as ReLU except that the rectifying function f(x) saturates at x = 6 \nReLU6 is defined as:\n\nf(x) = min(max(0, x), 6)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = ReLU6()\n\nprintln(module.forward(Tensor.range(-2, 8, 1)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n6.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 11]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = ReLU6()\n\nprint(module.forward(np.arange(-2, 9, 1)))\n\n\n\n\nOutput is\n\n\n[array([ 0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  6.,  6.], dtype=float32)]\n\n\n\n\nTanhShrink\n\n\nScala:\n\n\nval tanhShrink = TanhShrink()\n\n\n\n\nPython:\n\n\ntanhShrink = TanhShrink()\n\n\n\n\nTanhShrink applies element-wise Tanh and Shrink function to the input\n\n\nTanhShrink function : \nf(x) = scala.math.tanh(x) - 1\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval tanhShrink = TanhShrink()\nval input = Tensor(3, 3).rand()\n\n\n print(input)\n0.7056571   0.25239098  0.75746965  \n0.89736927  0.31193605  0.23842576  \n0.69492024  0.7512544   0.8386124   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n print(tanhShrink.forward(input))\n0.09771085  0.0052260756    0.11788553  \n0.18235475  0.009738684 0.004417494 \n0.09378672  0.1153577   0.153539    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\ntanhShrink = TanhShrink()\n\n\n  tanhShrink.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 0.23840582,  1.03597236,  2.00494528],\n       [ 3.00067067,  4.0000906 ,  5.0000124 ],\n       [ 6.00000191,  7.        ,  8.        ]], dtype=float32)]\n\n\n\n\n\nSoftMax\n\n\nScala:\n\n\nval layer = SoftMax()\n\n\n\n\nPython:\n\n\nlayer = SoftMax()\n\n\n\n\nApplies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.\nSoftmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)\nwhere shift = max_i(x_i).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SoftMax()\nval input = Tensor(3)\ninput.apply1(_ =\n 1.0f * 10)\nval gradOutput = Tensor(T(\n1.0f,\n0.0f,\n0.0f\n))\nval output = layer.forward(input)\nval gradient = layer.backward(input, gradOutput)\n-\n print(output)\n0.33333334\n0.33333334\n0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n-\n print(gradient)\n0.22222221\n-0.11111112\n-0.11111112\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nlayer = SoftMax()\ninput = np.ones(3)*10\ngrad_output = np.array([1.0, 0.0, 0.0])\noutput = layer.forward(input)\ngradient = layer.backward(input, grad_output)\n-\n print output\n[ 0.33333334  0.33333334  0.33333334]\n-\n print gradient\n[ 0.22222221 -0.11111112 -0.11111112]\n\n\n\n\nPReLU\n\n\nScala:\n\n\nval module = PReLU(nOutputPlane = 0)\n\n\n\n\nPython:\n\n\nmodule = PReLU(nOutputPlane=0)\n\n\n\n\nApplies parametric ReLU, which parameter varies the slope of the negative part.\n\n\nPReLU: f(x) = max(0, x) + a * min(0, x)\n\n\n\n\nnOutputPlane's default value is 0, that means using PReLU in shared version and has\nonly one parameters. nOutputPlane is the input map number(Default is 0).\n\n\nNotice: Please don't use weight decay on this.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = PReLU(2)\nval input = Tensor(2, 2, 3).randn()\nval output = module.forward(input)\n\n\n input\n(1,.,.) =\n-0.17810068 -0.69607687 0.25582042\n-1.2140307  -1.5410945  1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.16203058 -0.5643519  0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x3]\n\n\n output\n(1,.,.) =\n-0.04452517 -0.17401922 0.25582042\n-0.3035077  -0.38527364 1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.040507644    -0.14108798 0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = PReLU(2)\ninput = np.random.randn(2, 2, 3)\noutput = module.forward(input)\n\n\n input\n[[[ 2.50596953 -0.06593339 -1.90273409]\n  [ 0.2464341   0.45941315 -0.41977094]]\n\n [[-0.8584367   2.19389229  0.93136755]\n  [-0.39209027  0.16507514 -0.35850447]]]\n\n\n output\n[array([[[ 2.50596952, -0.01648335, -0.47568351],\n         [ 0.24643411,  0.45941314, -0.10494273]],\n\n        [[-0.21460918,  2.19389224,  0.93136758],\n         [-0.09802257,  0.16507514, -0.08962612]]], dtype=float32)]\n\n\n\n\nReLU\n\n\nScala:\n\n\nval relu = ReLU(ip = false)\n\n\n\n\nPython:\n\n\nrelu = ReLU(ip)\n\n\n\n\nReLU applies the element-wise rectified linear unit (ReLU) function to the input\n\n\nip\n illustrate if the ReLU fuction is done on the origin input\n\n\nReLU function : `f(x) = max(0, x)`\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval relu = ReLU(false)\n\nval input = Tensor(3, 3).rand()\n\n print(input)\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n print(relu.forward(input))\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nrelu = ReLU(False)\n\n relu.forward(np.array([[-1, -2, -3], [0, 0, 0], [1, 2, 3]]))\n[array([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 1.,  2.,  3.]], dtype=float32)]\n\n\n\n\n\nSoftMin\n\n\nScala:\n\n\nval sm = SoftMin()\n\n\n\n\nPython:\n\n\nsm = SoftMin()\n\n\n\n\nApplies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.\nSoftmin is defined as: f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)\nwhere shift = max_i(-x_i).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.SoftMin\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval sm = SoftMin()\nval input = Tensor(3, 3).range(1, 3 * 3)\n\nval output = sm.forward(input)\n\nval gradOutput = Tensor(3, 3).range(1, 3 * 3).apply1(x =\n (x / 10.0).toFloat)\nval gradInput = sm.backward(input, gradOutput)\n\n\n\n\n\nThe output will be,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nThe gradInput will be,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.02825874      -0.014077038    -0.014181711\n0.028258756     -0.01407703     -0.01418171\n0.028258756     -0.014077038    -0.014181707\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsm = SoftMin()\n\ninput = np.arange(1, 10, 1).astype(\nfloat32\n)\ninput = input.reshape(3, 3)\n\noutput = sm.forward(input)\nprint output\n\ngradOutput = np.arange(1, 10, 1).astype(\nfloat32\n)\ngradOutput = np.vectorize(lambda t: t / 10)(gradOutput)\ngradOutput = gradOutput.reshape(3, 3)\n\ngradInput = sm.backward(input, gradOutput)\nprint gradInput\n\n\n\n\n\nELU\n\n\nScala:\n\n\nval m = ELU(alpha = 1.0, inplace = false)\n\n\n\n\nPython:\n\n\nm = ELU(alpha=1.0, inplace=False)\n\n\n\n\nApplies exponential linear unit (\nELU\n), which parameter a varies the convergence value of the exponential function below zero:\n\n\nELU\n is defined as:\n\n\nf(x) = max(0, x) + min(0, alpha * (exp(x) - 1))\n\n\n\n\nThe output dimension is always equal to input dimension.\n\n\nFor reference see \nFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval xs = Tensor(4).randn()\nprintln(xs)\nprintln(ELU(4).forward(xs))\n\n\n\n\n1.0217569\n-0.17189966\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n1.0217569\n-0.63174534\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\nxs = np.linspace(-3, 3, num=200)\ngo = np.ones(200)\n\ndef f(a):\n    return ELU(a).forward(xs)[0]\ndef df(a):\n    m = ELU(a)\n    m.forward(xs)\n    return m.backward(xs, go)[0]\n\nplt.plot(xs, f(0.1), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, f(1.0), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, df(0.1), '-', label='dw ELU, alpha = 0.1')\nplt.plot(xs, df(1.0), '-', label='dw ELU, alpha = 0.1')\n\nplt.legend(loc='best', shadow=True, fancybox=True)\nplt.show()\n\n\n\n\n\n\n\nSoftShrink\n\n\nScala:\n\n\nval layer = SoftShrink(lambda = 0.5)\n\n\n\n\nPython:\n\n\nlayer = SoftShrink(the_lambda=0.5)\n\n\n\n\nApply the soft shrinkage function element-wise to the input Tensor\n\n\nSoftShrinkage operator:\n\n\n       \u23a7 x - lambda, if x \n  lambda\nf(x) = \u23a8 x + lambda, if x \n -lambda\n       \u23a9 0, otherwise\n\n\n\n\nParameters:\n\n\nlambda\n     - a factor, default is 0.5\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.SoftShrink\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = SoftShrink()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.5    1.5 2.5\n-1.5    2.5 3.5\n-2.5    3.5 4.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n3.0 4.0 5.0\n2.0 3.0 4.0\n1.0 2.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nScala example:\n\n\nactivation = SoftShrink()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n\nprint grad\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]\n\n\n\n\nSigmoid\n\n\nScala:\n\n\nval module = Sigmoid()\n\n\n\n\nPython:\n\n\nmodule = Sigmoid()\n\n\n\n\nApplies the Sigmoid function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.\n\n\nSigmoid is defined as: f(x) = 1 / (1 + exp(-x))\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = new Sigmoid()\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =\n {i += 1; i})\n\n print(layer.forward(input))\n0.7310586   0.880797    0.95257413  \n0.98201376  0.9933072   0.9975274   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = Sigmoid()\ninput = np.array([[1, 2, 3], [4, 5, 6]])\n\nlayer.forward(input)\narray([[ 0.7310586 ,  0.88079703,  0.95257413],\n       [ 0.98201376,  0.99330717,  0.99752742]], dtype=float32)\n\n\n\n\nTanh\n\n\nScala:\n\n\nval activation = Tanh()\n\n\n\n\nPython:\n\n\nactivation = Tanh()\n\n\n\n\nApplies the Tanh function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.\nTanh is defined as\n\n\nf(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Tanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = Tanh()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n0.7615942   0.9640276   0.9950548\n0.9640276   0.9950548   0.9993293\n0.9950548   0.9993293   0.9999092\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n1.259923    0.28260326  0.049329996\n0.14130163  0.029597998 0.0053634644\n0.009865999 0.0026817322    5.4466724E-4\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nactivation = Tanh()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[ 0.76159418  0.96402758  0.99505478]\n [ 0.96402758  0.99505478  0.99932933]\n [ 0.99505478  0.99932933  0.99990922]]\n\nprint grad\n[[  1.25992298e+00   2.82603264e-01   4.93299961e-02]\n [  1.41301632e-01   2.95979977e-02   5.36346436e-03]\n [  9.86599922e-03   2.68173218e-03   9.07778740e-04]]\n\n\n\n\nSoftPlus\n\n\nScala:\n\n\nval model = SoftPlus(beta = 1.0)\n\n\n\n\nPython:\n\n\nmodel = SoftPlus(beta = 1.0)\n\n\n\n\nApply the SoftPlus function to an n-dimensional input tensor.\nSoftPlus function: \n\n\nf_i(x) = 1/beta * log(1 + exp(beta * x_i))\n\n\n\n\n\n\nparam beta Controls sharpness of transfer function\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = SoftPlus()\nval input = Tensor(2, 3, 4).rand()\nval output = model.forward(input)\n\nscala\n println(input)\n(1,.,.) =\n0.9812126   0.7044107   0.0657767   0.9173636   \n0.20853543  0.76482195  0.60774535  0.47837523  \n0.62954164  0.56440496  0.28893307  0.40742245  \n\n(2,.,.) =\n0.18701692  0.7700966   0.98496467  0.8958407   \n0.037015386 0.34626052  0.36459026  0.8460807   \n0.051016055 0.6742781   0.14469075  0.07565566  \n\nscala\n println(output)\n(1,.,.) =\n1.2995617   1.1061354   0.7265762   1.2535294   \n0.80284095  1.1469617   1.0424956   0.9606715   \n1.0566612   1.0146512   0.8480129   0.91746557  \n\n(2,.,.) =\n0.7910212   1.1505641   1.3022922   1.2381986   \n0.71182615  0.88119024  0.8919668   1.203121    \n0.7189805   1.0860726   0.7681072   0.7316903   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodel = SoftPlus()\ninput = np.random.randn(2, 3, 4)\noutput = model.forward(input)\n\n\n print(input)\n[[[ 0.82634972 -0.09853824  0.97570235  1.84464617]\n  [ 0.38466503  0.08963732  1.29438774  1.25204527]\n  [-0.01910449 -0.19560752 -0.81769143 -1.06365733]]\n\n [[-0.56284365 -0.28473239 -0.58206869 -1.97350909]\n  [-0.28303919 -0.59735361  0.73282102  0.0176838 ]\n  [ 0.63439133  1.84904987 -1.24073643  2.13275833]]]\n\n print(output)\n[[[ 1.18935537  0.6450913   1.2955569   1.99141073]\n  [ 0.90386271  0.73896986  1.53660071  1.50351918]\n  [ 0.68364054  0.60011864  0.36564925  0.29653603]]\n\n [[ 0.45081255  0.56088102  0.44387865  0.1301229 ]\n  [ 0.56160825  0.43842646  1.12523568  0.70202816]\n  [ 1.0598278   1.99521446  0.2539995   2.24475574]]]\n\n\n\n\nL1Penalty\n\n\nScala:\n\n\nval l1Penalty = L1Penalty(l1weight, sizeAverage = false, provideOutput = true)\n\n\n\n\nPython:\n\n\nl1Penalty = L1Penalty( l1weight, size_average=False, provide_output=True)\n\n\n\n\nL1Penalty adds an L1 penalty to an input \nFor forward, the output is the same as input and a L1 loss of the latent state will be calculated each time\nFor backward, gradInput = gradOutput + gradLoss\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval l1Penalty = L1Penalty(1, true, true)\nval input = Tensor(3, 3).rand()\n\n\n print(input)\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n\n print(l1Penalty.forward(input))\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]   \n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nl1Penalty = L1Penalty(1, True, True)\n\n\n l1Penalty.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.],\n       [ 7.,  8.,  9.]], dtype=float32)]\n\n\n\n\n\nHardShrink\n\n\nScala:\n\n\nval m = HardShrink(lambda = 0.5)\n\n\n\n\nPython:\n\n\nm = HardShrink(the_lambda=0.5)\n\n\n\n\nApplies the hard shrinkage function element-wise to the input Tensor. lambda is set to 0.5 by default.\n\n\nHardShrinkage operator is defined as:\n\n\n       \u23a7 x, if x \n  lambda\nf(x) = \u23a8 x, if x \n -lambda\n       \u23a9 0, otherwise\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.utils._\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(-10, 10)\nval input = Tensor(3, 4)\ninput.apply1(x =\n randomn().toFloat)\n\nval layer = new HardShrink(8)\nprintln(\ninput:\n)\nprintln(input)\nprintln(\noutput:\n)\nprintln(layer.forward(input))\n\n\n\n\ninput:\n8.53746839798987    -2.25314284209162   2.838596091605723   0.7181660132482648  \n0.8278933027759194  8.986027473583817   -3.6885232804343104 -2.4018199276179075 \n-9.51015486381948   2.6402589259669185  5.438693333417177   -6.577442386187613  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\noutput:\n8.53746839798987    0.0 0.0 0.0 \n0.0 8.986027473583817   0.0 0.0 \n-9.51015486381948   0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\ninput = np.linspace(-5, 5, num=10)\nlayer = HardShrink(the_lambda=3.0)\nprint(\ninput:\n)\nprint(input)\nprint(\noutput: \n)\nprint(layer.forward(input))\n\n\n\n\ncreating: createHardShrink\ninput:\n[-5.         -3.88888889 -2.77777778 -1.66666667 -0.55555556  0.55555556\n  1.66666667  2.77777778  3.88888889  5.        ]\noutput: \n[-5.         -3.88888884  0.          0.          0.          0.          0.\n  0.          3.88888884  5.        ]\n\n\n\n\n\nRReLU\n\n\nScala:\n\n\nval layer = RReLU[T](lower, upper, inPlace)\n\n\n\n\nPython:\n\n\nlayer = RReLU(lower, upper, inPlace)\n\n\n\n\nApplies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension. Informally the RReLU is also known as 'insanity' layer.\n\n\nRReLU is defined as: f(x) = max(0,x) + a * min(0, x) where a ~ U(l, u).\n\n\nIn training mode negative inputs are multiplied by a factor drawn from a uniform random\ndistribution U(l, u). In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean\nfactor a = (l + u) / 2.\n\n\nBy default, l = 1/8 and u = 1/3. If l == u a RReLU effectively becomes a LeakyReLU.\n\n\nRegardless of operating in in-place mode a RReLU will internally allocate an input-sized noise tensor to store random factors for negative inputs.\n\n\nThe backward() operation assumes that forward() has been called before.\n\n\nFor reference see \nEmpirical Evaluation of Rectified Activations in Convolutional Network\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = RReLU[Float]()\nlayer.forward(Tensor[Float](T(1.0f, 2.0f, -1.0f, -2.0f)))\nlayer.backward(Tensor[Float](T(1.0f, 2.0f, -1.0f, -2.0f)),\nTensor[Float](T(0.1f, 0.2f, -0.1f, -0.2f)))\n\n\n\n\nThere's random factor. An output is like\n\n\n1.0\n2.0\n-0.24342789\n-0.43175703\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n0.1\n0.2\n-0.024342788\n-0.043175705\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import RReLU\nimport numpy as np\n\nlayer = RReLU()\nlayer.forward(np.array([1.0, 2.0, -1.0, -2.0]))\nlayer.backward(np.array([1.0, 2.0, -1.0, -2.0]),\n  np.array([0.1, 0.2, -0.1, -0.2]))\n\n\n\n\nThere's random factor. An output is like\n\n\narray([ 1.,  2., -0.15329693, -0.40423378], dtype=float32)\n\narray([ 0.1, 0.2, -0.01532969, -0.04042338], dtype=float32)\n\n\n\n\nHardTanh\n\n\nScala:\n\n\nval activation = HardTanh(\n    minValue = -1,\n    maxValue = 1,\n    inplace = false)\n\n\n\n\nPython:\n\n\nactivation = HardTanh(\n    min_value=-1.0,\n    max_value=1.0,\n    inplace=False)\n\n\n\n\nApplies non-linear function HardTanh to each element of input, HardTanh is defined:\n\n\n           \u23a7  maxValue, if x \n maxValue\n    f(x) = \u23a8  minValue, if x \n minValue\n           \u23a9  x, otherwise\n\n\n\n\nParameters:\n\n\nminValue\n - minValue in f(x), default is -1.\n\n\nmaxValue\n - maxValue in f(x), default is 1.\n\n\ninplace\n  - weather inplace update output from input. default is false.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.HardTanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = HardTanh()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.0 0.0 0.0\n0.0 0.0 0.0\n0.0 0.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nactivation = HardTanh()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-1.  1.  1.]\n [-1.  1.  1.]\n [-1.  1.  1.]]\n\nprint grad\n[[ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]\n\n\n\n\nLeakyReLU\n\n\nScala:\n\n\nlayer = LeakyReLU(negval=0.01,inplace=false)\n\n\n\n\nPython:\n\n\nlayer = LeakyReLU(negval=0.01,inplace=False,bigdl_type=\nfloat\n)\n\n\n\n\nIt is a transfer module that applies LeakyReLU, which parameter\nnegval sets the slope of the negative part:\n LeakyReLU is defined as:\n  f(x) = max(0, x) + negval * min(0, x)\n\n\n\n\n@param negval sets the slope of the negative partl, default is 0.01\n\n\n@param inplace if it is true, doing the operation in-place without\n                using extra state memory, default is false\n\n\n\n\nScala example:\n\n\nval layer = LeakyReLU(negval=0.01,inplace=false)\nval input = Tensor(3, 2).rand(-1, 1)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.6923256      -0.14086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nlayer.forward(input)\nres7: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.006923256    -0.0014086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n\n\n\n\nPython example:\n\n\nlayer = LeakyReLU(negval=0.01,inplace=False,bigdl_type=\nfloat\n)\ninput = np.random.rand(3, 2)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056004,  0.35643192],\n       [ 0.25075111,  0.18904582]])\n\nlayer.forward(input)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056001,  0.35643193],\n       [ 0.25075111,  0.18904583]], dtype=float32)\n\n\n\n\nLogSigmoid\n\n\nScala:\n\n\nval activation = LogSigmoid()\n\n\n\n\nPython:\n\n\nactivation = LogSigmoid()\n\n\n\n\nThis class is a activation layer corresponding to the non-linear function sigmoid function:\n\n\nf(x) = Log(1 / (1 + e ^ (-x)))\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.LogSigmoid\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = LogSigmoid()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.3132617  -0.12692802 -0.04858735\n-0.12692802 -0.04858735 -0.01814993\n-0.04858735 -0.01814993 -0.0067153485\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.8068244   0.47681168  0.23712938\n0.23840584  0.14227761  0.07194484\n0.047425874 0.03597242  0.020078553\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nactivation = LogSigmoid()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.31326169 -0.12692802 -0.04858735]\n [-0.12692802 -0.04858735 -0.01814993]\n [-0.04858735 -0.01814993 -0.00671535]]\n\nprint grad\n[[ 0.80682439  0.47681168  0.23712938]\n [ 0.23840584  0.14227761  0.07194484]\n [ 0.04742587  0.03597242  0.03346425]]\n\n\n\n\nLogSoftMax\n\n\nScala:\n\n\nval model = LogSoftMax()\n\n\n\n\nPython:\n\n\nmodel = LogSoftMax()\n\n\n\n\nThe LogSoftMax module applies a LogSoftMax transformation to the input data\nwhich is defined as:\n\n\nf_i(x) = log(1 / a exp(x_i))\nwhere a = sum_j[exp(x_j)]\n\n\n\n\nThe input given in \nforward(input)\n must be either\na vector (1D tensor) or matrix (2D tensor).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = LogSoftMax()\nval input = Tensor(2, 5).rand()\nval output = model.forward(input)\n\nscala\n print(input)\n0.4434036   0.64535594  0.7516194   0.11752353  0.5216674   \n0.57294756  0.744955    0.62644184  0.0052207764    0.900162    \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x5]\n\nscala\n print(output)\n-1.6841899  -1.4822376  -1.3759742  -2.01007    -1.605926   \n-1.6479948  -1.4759872  -1.5945004  -2.2157214  -1.3207803  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]\n\n\n\n\nPython example:\n\n\nmodel = LogSoftMax()\ninput = np.random.randn(4, 10)\noutput = model.forward(input)\n\n\n print(input)\n[[ 0.10805365  0.11392282  1.31891713 -0.62910637 -0.80532589  0.57976863\n  -0.44454368  0.26292944  0.8338328   0.32305099]\n [-0.16443839  0.12010763  0.62978233 -1.57224143 -2.16133614 -0.60932395\n  -0.22722708  0.23268273  0.00313597  0.34585582]\n [ 0.55913444 -0.7560615   0.12170887  1.40628806  0.97614582  1.20417145\n  -1.60619173 -0.54483025  1.12227399 -0.79976189]\n [-0.05540945  0.86954458  0.34586427  2.52004267  0.6998163  -1.61315173\n  -0.76276874  0.38332142  0.66351792 -0.30111399]]\n\n\n print(output)\n[[-2.55674744 -2.55087829 -1.34588397 -3.2939074  -3.47012711 -2.08503246\n  -3.10934472 -2.40187168 -1.83096838 -2.34175014]\n [-2.38306785 -2.09852171 -1.58884704 -3.79087067 -4.37996578 -2.82795334\n  -2.44585633 -1.98594666 -2.21549344 -1.87277353]\n [-2.31549931 -3.63069534 -2.75292492 -1.46834576 -1.89848804 -1.67046237\n  -4.48082542 -3.41946411 -1.75235975 -3.67439556]\n [-3.23354769 -2.30859375 -2.83227396 -0.6580956  -2.47832203 -4.79128981\n  -3.940907   -2.79481697 -2.5146203  -3.47925234]]\n\n\n\n\nThreshold\n\n\nScala:\n\n\nval module = Threshold(threshold, value, ip)\n\n\n\n\nPython:\n\n\nmodule = Threshold(threshold, value, ip)\n\n\n\n\nThresholds each element of the input Tensor.\nThreshold is defined as:\n\n\n     \u23a7 x        if x \n= threshold\n y = \u23a8 \n     \u23a9 value    if x \n  threshold\n\n\n\n\n\n\nthreshold: The value to threshold at\n\n\nvalue: The value to replace with\n\n\nip: can optionally do the operation in-place\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Threshold(1, 0.8)\nval input = Tensor(2, 2, 2).randn()\nval output = module.forward(input)\n\n\n input\n(1,.,.) =\n2.0502799   -0.37522468\n-1.2704345  -0.22533786\n\n(2,.,.) =\n1.1959263   1.6670992\n-0.24333914 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n output\n(1,.,.) =\n(1,.,.) =\n2.0502799   0.8\n0.8 0.8\n\n(2,.,.) =\n1.1959263   1.6670992\n0.8 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Threshold(1.0, 0.8)\ninput = np.random.randn(2, 2, 2)\noutput = module.forward(input)\n\n\n input\n[[[-0.43226865 -1.09160093]\n  [-0.20280088  0.68196767]]\n\n [[ 2.32017942  1.00003307]\n  [-0.46618767  0.57057167]]]\n\n\n output\n[array([[[ 0.80000001,  0.80000001],\n        [ 0.80000001,  0.80000001]],\n\n       [[ 2.32017946,  1.00003302],\n        [ 0.80000001,  0.80000001]]], dtype=float32)]", 
            "title": "Non-linear Activations"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#softsign", 
            "text": "Scala:  val softSign = SoftSign()  Python:  softSign = SoftSign()  SoftSign applies SoftSign function to the input tensor  SoftSign function:  f_i(x) = x_i / (1+|x_i|)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval softSign = SoftSign()\nval input = Tensor(3, 3).rand()  print(input)\n0.6733504   0.7566517   0.43793806  \n0.09683273  0.05829774  0.4567967   \n0.20021072  0.11158377  0.31668025  print(softSign.forward(input))\n0.40239656  0.4307352   0.30455974  \n0.08828395  0.05508633  0.31356242  \n0.16681297  0.10038269  0.24051417  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nsoftSign=SoftSign()  softSign.forward(np.array([[1, 2, 4],[-1, -2, -4]]))\n[array([[ 0.5       ,  0.66666669,  0.80000001],\n       [-0.5       , -0.66666669, -0.80000001]], dtype=float32)]", 
            "title": "SoftSign"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#relu6", 
            "text": "Scala:  val module = ReLU6(inplace = false)  Python:  module = ReLU6(inplace=False)  Same as ReLU except that the rectifying function f(x) saturates at x = 6 \nReLU6 is defined as: f(x) = min(max(0, x), 6)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = ReLU6()\n\nprintln(module.forward(Tensor.range(-2, 8, 1)))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n6.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 11]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = ReLU6()\n\nprint(module.forward(np.arange(-2, 9, 1)))  Output is  [array([ 0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  6.,  6.], dtype=float32)]", 
            "title": "ReLU6"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#tanhshrink", 
            "text": "Scala:  val tanhShrink = TanhShrink()  Python:  tanhShrink = TanhShrink()  TanhShrink applies element-wise Tanh and Shrink function to the input  TanhShrink function :  f(x) = scala.math.tanh(x) - 1  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval tanhShrink = TanhShrink()\nval input = Tensor(3, 3).rand()  print(input)\n0.7056571   0.25239098  0.75746965  \n0.89736927  0.31193605  0.23842576  \n0.69492024  0.7512544   0.8386124   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(tanhShrink.forward(input))\n0.09771085  0.0052260756    0.11788553  \n0.18235475  0.009738684 0.004417494 \n0.09378672  0.1153577   0.153539    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\ntanhShrink = TanhShrink()   tanhShrink.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 0.23840582,  1.03597236,  2.00494528],\n       [ 3.00067067,  4.0000906 ,  5.0000124 ],\n       [ 6.00000191,  7.        ,  8.        ]], dtype=float32)]", 
            "title": "TanhShrink"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#softmax", 
            "text": "Scala:  val layer = SoftMax()  Python:  layer = SoftMax()  Applies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.\nSoftmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)\nwhere shift = max_i(x_i).  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SoftMax()\nval input = Tensor(3)\ninput.apply1(_ =  1.0f * 10)\nval gradOutput = Tensor(T(\n1.0f,\n0.0f,\n0.0f\n))\nval output = layer.forward(input)\nval gradient = layer.backward(input, gradOutput)\n-  print(output)\n0.33333334\n0.33333334\n0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n-  print(gradient)\n0.22222221\n-0.11111112\n-0.11111112\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nlayer = SoftMax()\ninput = np.ones(3)*10\ngrad_output = np.array([1.0, 0.0, 0.0])\noutput = layer.forward(input)\ngradient = layer.backward(input, grad_output)\n-  print output\n[ 0.33333334  0.33333334  0.33333334]\n-  print gradient\n[ 0.22222221 -0.11111112 -0.11111112]", 
            "title": "SoftMax"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#prelu", 
            "text": "Scala:  val module = PReLU(nOutputPlane = 0)  Python:  module = PReLU(nOutputPlane=0)  Applies parametric ReLU, which parameter varies the slope of the negative part.  PReLU: f(x) = max(0, x) + a * min(0, x)  nOutputPlane's default value is 0, that means using PReLU in shared version and has\nonly one parameters. nOutputPlane is the input map number(Default is 0).  Notice: Please don't use weight decay on this.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = PReLU(2)\nval input = Tensor(2, 2, 3).randn()\nval output = module.forward(input)  input\n(1,.,.) =\n-0.17810068 -0.69607687 0.25582042\n-1.2140307  -1.5410945  1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.16203058 -0.5643519  0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x3]  output\n(1,.,.) =\n-0.04452517 -0.17401922 0.25582042\n-0.3035077  -0.38527364 1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.040507644    -0.14108798 0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = PReLU(2)\ninput = np.random.randn(2, 2, 3)\noutput = module.forward(input)  input\n[[[ 2.50596953 -0.06593339 -1.90273409]\n  [ 0.2464341   0.45941315 -0.41977094]]\n\n [[-0.8584367   2.19389229  0.93136755]\n  [-0.39209027  0.16507514 -0.35850447]]]  output\n[array([[[ 2.50596952, -0.01648335, -0.47568351],\n         [ 0.24643411,  0.45941314, -0.10494273]],\n\n        [[-0.21460918,  2.19389224,  0.93136758],\n         [-0.09802257,  0.16507514, -0.08962612]]], dtype=float32)]", 
            "title": "PReLU"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#relu", 
            "text": "Scala:  val relu = ReLU(ip = false)  Python:  relu = ReLU(ip)  ReLU applies the element-wise rectified linear unit (ReLU) function to the input  ip  illustrate if the ReLU fuction is done on the origin input  ReLU function : `f(x) = max(0, x)`  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval relu = ReLU(false)\n\nval input = Tensor(3, 3).rand()  print(input)\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(relu.forward(input))\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nrelu = ReLU(False)  relu.forward(np.array([[-1, -2, -3], [0, 0, 0], [1, 2, 3]]))\n[array([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 1.,  2.,  3.]], dtype=float32)]", 
            "title": "ReLU"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#softmin", 
            "text": "Scala:  val sm = SoftMin()  Python:  sm = SoftMin()  Applies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.\nSoftmin is defined as: f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)\nwhere shift = max_i(-x_i).  Scala example:  import com.intel.analytics.bigdl.nn.SoftMin\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval sm = SoftMin()\nval input = Tensor(3, 3).range(1, 3 * 3)\n\nval output = sm.forward(input)\n\nval gradOutput = Tensor(3, 3).range(1, 3 * 3).apply1(x =  (x / 10.0).toFloat)\nval gradInput = sm.backward(input, gradOutput)  The output will be,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  The gradInput will be,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.02825874      -0.014077038    -0.014181711\n0.028258756     -0.01407703     -0.01418171\n0.028258756     -0.014077038    -0.014181707\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsm = SoftMin()\n\ninput = np.arange(1, 10, 1).astype( float32 )\ninput = input.reshape(3, 3)\n\noutput = sm.forward(input)\nprint output\n\ngradOutput = np.arange(1, 10, 1).astype( float32 )\ngradOutput = np.vectorize(lambda t: t / 10)(gradOutput)\ngradOutput = gradOutput.reshape(3, 3)\n\ngradInput = sm.backward(input, gradOutput)\nprint gradInput", 
            "title": "SoftMin"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#elu", 
            "text": "Scala:  val m = ELU(alpha = 1.0, inplace = false)  Python:  m = ELU(alpha=1.0, inplace=False)  Applies exponential linear unit ( ELU ), which parameter a varies the convergence value of the exponential function below zero:  ELU  is defined as:  f(x) = max(0, x) + min(0, alpha * (exp(x) - 1))  The output dimension is always equal to input dimension.  For reference see  Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) .  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval xs = Tensor(4).randn()\nprintln(xs)\nprintln(ELU(4).forward(xs))  1.0217569\n-0.17189966\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n1.0217569\n-0.63174534\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\nxs = np.linspace(-3, 3, num=200)\ngo = np.ones(200)\n\ndef f(a):\n    return ELU(a).forward(xs)[0]\ndef df(a):\n    m = ELU(a)\n    m.forward(xs)\n    return m.backward(xs, go)[0]\n\nplt.plot(xs, f(0.1), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, f(1.0), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, df(0.1), '-', label='dw ELU, alpha = 0.1')\nplt.plot(xs, df(1.0), '-', label='dw ELU, alpha = 0.1')\n\nplt.legend(loc='best', shadow=True, fancybox=True)\nplt.show()", 
            "title": "ELU"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#softshrink", 
            "text": "Scala:  val layer = SoftShrink(lambda = 0.5)  Python:  layer = SoftShrink(the_lambda=0.5)  Apply the soft shrinkage function element-wise to the input Tensor  SoftShrinkage operator:         \u23a7 x - lambda, if x    lambda\nf(x) = \u23a8 x + lambda, if x   -lambda\n       \u23a9 0, otherwise  Parameters:  lambda      - a factor, default is 0.5  Scala example:  import com.intel.analytics.bigdl.nn.SoftShrink\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = SoftShrink()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.5    1.5 2.5\n-1.5    2.5 3.5\n-2.5    3.5 4.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n3.0 4.0 5.0\n2.0 3.0 4.0\n1.0 2.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Scala example:  activation = SoftShrink()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n\nprint grad\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]", 
            "title": "SoftShrink"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#sigmoid", 
            "text": "Scala:  val module = Sigmoid()  Python:  module = Sigmoid()  Applies the Sigmoid function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.  Sigmoid is defined as: f(x) = 1 / (1 + exp(-x))  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = new Sigmoid()\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =  {i += 1; i})  print(layer.forward(input))\n0.7310586   0.880797    0.95257413  \n0.98201376  0.9933072   0.9975274   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\nlayer = Sigmoid()\ninput = np.array([[1, 2, 3], [4, 5, 6]]) layer.forward(input)\narray([[ 0.7310586 ,  0.88079703,  0.95257413],\n       [ 0.98201376,  0.99330717,  0.99752742]], dtype=float32)", 
            "title": "Sigmoid"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#tanh", 
            "text": "Scala:  val activation = Tanh()  Python:  activation = Tanh()  Applies the Tanh function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.\nTanh is defined as  f(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).  Scala example:  import com.intel.analytics.bigdl.nn.Tanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = Tanh()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n0.7615942   0.9640276   0.9950548\n0.9640276   0.9950548   0.9993293\n0.9950548   0.9993293   0.9999092\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n1.259923    0.28260326  0.049329996\n0.14130163  0.029597998 0.0053634644\n0.009865999 0.0026817322    5.4466724E-4\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  activation = Tanh()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[ 0.76159418  0.96402758  0.99505478]\n [ 0.96402758  0.99505478  0.99932933]\n [ 0.99505478  0.99932933  0.99990922]]\n\nprint grad\n[[  1.25992298e+00   2.82603264e-01   4.93299961e-02]\n [  1.41301632e-01   2.95979977e-02   5.36346436e-03]\n [  9.86599922e-03   2.68173218e-03   9.07778740e-04]]", 
            "title": "Tanh"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#softplus", 
            "text": "Scala:  val model = SoftPlus(beta = 1.0)  Python:  model = SoftPlus(beta = 1.0)  Apply the SoftPlus function to an n-dimensional input tensor.\nSoftPlus function:   f_i(x) = 1/beta * log(1 + exp(beta * x_i))   param beta Controls sharpness of transfer function   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = SoftPlus()\nval input = Tensor(2, 3, 4).rand()\nval output = model.forward(input)\n\nscala  println(input)\n(1,.,.) =\n0.9812126   0.7044107   0.0657767   0.9173636   \n0.20853543  0.76482195  0.60774535  0.47837523  \n0.62954164  0.56440496  0.28893307  0.40742245  \n\n(2,.,.) =\n0.18701692  0.7700966   0.98496467  0.8958407   \n0.037015386 0.34626052  0.36459026  0.8460807   \n0.051016055 0.6742781   0.14469075  0.07565566  \n\nscala  println(output)\n(1,.,.) =\n1.2995617   1.1061354   0.7265762   1.2535294   \n0.80284095  1.1469617   1.0424956   0.9606715   \n1.0566612   1.0146512   0.8480129   0.91746557  \n\n(2,.,.) =\n0.7910212   1.1505641   1.3022922   1.2381986   \n0.71182615  0.88119024  0.8919668   1.203121    \n0.7189805   1.0860726   0.7681072   0.7316903   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodel = SoftPlus()\ninput = np.random.randn(2, 3, 4)\noutput = model.forward(input)  print(input)\n[[[ 0.82634972 -0.09853824  0.97570235  1.84464617]\n  [ 0.38466503  0.08963732  1.29438774  1.25204527]\n  [-0.01910449 -0.19560752 -0.81769143 -1.06365733]]\n\n [[-0.56284365 -0.28473239 -0.58206869 -1.97350909]\n  [-0.28303919 -0.59735361  0.73282102  0.0176838 ]\n  [ 0.63439133  1.84904987 -1.24073643  2.13275833]]]  print(output)\n[[[ 1.18935537  0.6450913   1.2955569   1.99141073]\n  [ 0.90386271  0.73896986  1.53660071  1.50351918]\n  [ 0.68364054  0.60011864  0.36564925  0.29653603]]\n\n [[ 0.45081255  0.56088102  0.44387865  0.1301229 ]\n  [ 0.56160825  0.43842646  1.12523568  0.70202816]\n  [ 1.0598278   1.99521446  0.2539995   2.24475574]]]", 
            "title": "SoftPlus"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#l1penalty", 
            "text": "Scala:  val l1Penalty = L1Penalty(l1weight, sizeAverage = false, provideOutput = true)  Python:  l1Penalty = L1Penalty( l1weight, size_average=False, provide_output=True)  L1Penalty adds an L1 penalty to an input \nFor forward, the output is the same as input and a L1 loss of the latent state will be calculated each time\nFor backward, gradInput = gradOutput + gradLoss  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval l1Penalty = L1Penalty(1, true, true)\nval input = Tensor(3, 3).rand()  print(input)\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(l1Penalty.forward(input))\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]     Python example:  from bigdl.nn.layer import *\nl1Penalty = L1Penalty(1, True, True)  l1Penalty.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.],\n       [ 7.,  8.,  9.]], dtype=float32)]", 
            "title": "L1Penalty"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#hardshrink", 
            "text": "Scala:  val m = HardShrink(lambda = 0.5)  Python:  m = HardShrink(the_lambda=0.5)  Applies the hard shrinkage function element-wise to the input Tensor. lambda is set to 0.5 by default.  HardShrinkage operator is defined as:         \u23a7 x, if x    lambda\nf(x) = \u23a8 x, if x   -lambda\n       \u23a9 0, otherwise  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.utils._\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(-10, 10)\nval input = Tensor(3, 4)\ninput.apply1(x =  randomn().toFloat)\n\nval layer = new HardShrink(8)\nprintln( input: )\nprintln(input)\nprintln( output: )\nprintln(layer.forward(input))  input:\n8.53746839798987    -2.25314284209162   2.838596091605723   0.7181660132482648  \n0.8278933027759194  8.986027473583817   -3.6885232804343104 -2.4018199276179075 \n-9.51015486381948   2.6402589259669185  5.438693333417177   -6.577442386187613  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\noutput:\n8.53746839798987    0.0 0.0 0.0 \n0.0 8.986027473583817   0.0 0.0 \n-9.51015486381948   0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\ninput = np.linspace(-5, 5, num=10)\nlayer = HardShrink(the_lambda=3.0)\nprint( input: )\nprint(input)\nprint( output:  )\nprint(layer.forward(input))  creating: createHardShrink\ninput:\n[-5.         -3.88888889 -2.77777778 -1.66666667 -0.55555556  0.55555556\n  1.66666667  2.77777778  3.88888889  5.        ]\noutput: \n[-5.         -3.88888884  0.          0.          0.          0.          0.\n  0.          3.88888884  5.        ]", 
            "title": "HardShrink"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#rrelu", 
            "text": "Scala:  val layer = RReLU[T](lower, upper, inPlace)  Python:  layer = RReLU(lower, upper, inPlace)  Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension. Informally the RReLU is also known as 'insanity' layer.  RReLU is defined as: f(x) = max(0,x) + a * min(0, x) where a ~ U(l, u).  In training mode negative inputs are multiplied by a factor drawn from a uniform random\ndistribution U(l, u). In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean\nfactor a = (l + u) / 2.  By default, l = 1/8 and u = 1/3. If l == u a RReLU effectively becomes a LeakyReLU.  Regardless of operating in in-place mode a RReLU will internally allocate an input-sized noise tensor to store random factors for negative inputs.  The backward() operation assumes that forward() has been called before.  For reference see  Empirical Evaluation of Rectified Activations in Convolutional Network .  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = RReLU[Float]()\nlayer.forward(Tensor[Float](T(1.0f, 2.0f, -1.0f, -2.0f)))\nlayer.backward(Tensor[Float](T(1.0f, 2.0f, -1.0f, -2.0f)),\nTensor[Float](T(0.1f, 0.2f, -0.1f, -0.2f)))  There's random factor. An output is like  1.0\n2.0\n-0.24342789\n-0.43175703\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n0.1\n0.2\n-0.024342788\n-0.043175705\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  from bigdl.nn.layer import RReLU\nimport numpy as np\n\nlayer = RReLU()\nlayer.forward(np.array([1.0, 2.0, -1.0, -2.0]))\nlayer.backward(np.array([1.0, 2.0, -1.0, -2.0]),\n  np.array([0.1, 0.2, -0.1, -0.2]))  There's random factor. An output is like  array([ 1.,  2., -0.15329693, -0.40423378], dtype=float32)\n\narray([ 0.1, 0.2, -0.01532969, -0.04042338], dtype=float32)", 
            "title": "RReLU"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#hardtanh", 
            "text": "Scala:  val activation = HardTanh(\n    minValue = -1,\n    maxValue = 1,\n    inplace = false)  Python:  activation = HardTanh(\n    min_value=-1.0,\n    max_value=1.0,\n    inplace=False)  Applies non-linear function HardTanh to each element of input, HardTanh is defined:             \u23a7  maxValue, if x   maxValue\n    f(x) = \u23a8  minValue, if x   minValue\n           \u23a9  x, otherwise  Parameters:  minValue  - minValue in f(x), default is -1.  maxValue  - maxValue in f(x), default is 1.  inplace   - weather inplace update output from input. default is false.  Scala example:  import com.intel.analytics.bigdl.nn.HardTanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = HardTanh()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.0 0.0 0.0\n0.0 0.0 0.0\n0.0 0.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  activation = HardTanh()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-1.  1.  1.]\n [-1.  1.  1.]\n [-1.  1.  1.]]\n\nprint grad\n[[ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]", 
            "title": "HardTanh"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#leakyrelu", 
            "text": "Scala:  layer = LeakyReLU(negval=0.01,inplace=false)  Python:  layer = LeakyReLU(negval=0.01,inplace=False,bigdl_type= float )  It is a transfer module that applies LeakyReLU, which parameter\nnegval sets the slope of the negative part:\n LeakyReLU is defined as:\n  f(x) = max(0, x) + negval * min(0, x)   @param negval sets the slope of the negative partl, default is 0.01  @param inplace if it is true, doing the operation in-place without\n                using extra state memory, default is false   Scala example:  val layer = LeakyReLU(negval=0.01,inplace=false)\nval input = Tensor(3, 2).rand(-1, 1)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.6923256      -0.14086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nlayer.forward(input)\nres7: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.006923256    -0.0014086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]  Python example:  layer = LeakyReLU(negval=0.01,inplace=False,bigdl_type= float )\ninput = np.random.rand(3, 2)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056004,  0.35643192],\n       [ 0.25075111,  0.18904582]])\n\nlayer.forward(input)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056001,  0.35643193],\n       [ 0.25075111,  0.18904583]], dtype=float32)", 
            "title": "LeakyReLU"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#logsigmoid", 
            "text": "Scala:  val activation = LogSigmoid()  Python:  activation = LogSigmoid()  This class is a activation layer corresponding to the non-linear function sigmoid function:  f(x) = Log(1 / (1 + e ^ (-x)))  Scala example:  import com.intel.analytics.bigdl.nn.LogSigmoid\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = LogSigmoid()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.3132617  -0.12692802 -0.04858735\n-0.12692802 -0.04858735 -0.01814993\n-0.04858735 -0.01814993 -0.0067153485\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.8068244   0.47681168  0.23712938\n0.23840584  0.14227761  0.07194484\n0.047425874 0.03597242  0.020078553\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  activation = LogSigmoid()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.31326169 -0.12692802 -0.04858735]\n [-0.12692802 -0.04858735 -0.01814993]\n [-0.04858735 -0.01814993 -0.00671535]]\n\nprint grad\n[[ 0.80682439  0.47681168  0.23712938]\n [ 0.23840584  0.14227761  0.07194484]\n [ 0.04742587  0.03597242  0.03346425]]", 
            "title": "LogSigmoid"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#logsoftmax", 
            "text": "Scala:  val model = LogSoftMax()  Python:  model = LogSoftMax()  The LogSoftMax module applies a LogSoftMax transformation to the input data\nwhich is defined as:  f_i(x) = log(1 / a exp(x_i))\nwhere a = sum_j[exp(x_j)]  The input given in  forward(input)  must be either\na vector (1D tensor) or matrix (2D tensor).  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = LogSoftMax()\nval input = Tensor(2, 5).rand()\nval output = model.forward(input)\n\nscala  print(input)\n0.4434036   0.64535594  0.7516194   0.11752353  0.5216674   \n0.57294756  0.744955    0.62644184  0.0052207764    0.900162    \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x5]\n\nscala  print(output)\n-1.6841899  -1.4822376  -1.3759742  -2.01007    -1.605926   \n-1.6479948  -1.4759872  -1.5945004  -2.2157214  -1.3207803  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]  Python example:  model = LogSoftMax()\ninput = np.random.randn(4, 10)\noutput = model.forward(input)  print(input)\n[[ 0.10805365  0.11392282  1.31891713 -0.62910637 -0.80532589  0.57976863\n  -0.44454368  0.26292944  0.8338328   0.32305099]\n [-0.16443839  0.12010763  0.62978233 -1.57224143 -2.16133614 -0.60932395\n  -0.22722708  0.23268273  0.00313597  0.34585582]\n [ 0.55913444 -0.7560615   0.12170887  1.40628806  0.97614582  1.20417145\n  -1.60619173 -0.54483025  1.12227399 -0.79976189]\n [-0.05540945  0.86954458  0.34586427  2.52004267  0.6998163  -1.61315173\n  -0.76276874  0.38332142  0.66351792 -0.30111399]]  print(output)\n[[-2.55674744 -2.55087829 -1.34588397 -3.2939074  -3.47012711 -2.08503246\n  -3.10934472 -2.40187168 -1.83096838 -2.34175014]\n [-2.38306785 -2.09852171 -1.58884704 -3.79087067 -4.37996578 -2.82795334\n  -2.44585633 -1.98594666 -2.21549344 -1.87277353]\n [-2.31549931 -3.63069534 -2.75292492 -1.46834576 -1.89848804 -1.67046237\n  -4.48082542 -3.41946411 -1.75235975 -3.67439556]\n [-3.23354769 -2.30859375 -2.83227396 -0.6580956  -2.47832203 -4.79128981\n  -3.940907   -2.79481697 -2.5146203  -3.47925234]]", 
            "title": "LogSoftMax"
        }, 
        {
            "location": "/APIdocs/Layers/Non-linear_Activations/merged-Non-linear_Activations/#threshold", 
            "text": "Scala:  val module = Threshold(threshold, value, ip)  Python:  module = Threshold(threshold, value, ip)  Thresholds each element of the input Tensor.\nThreshold is defined as:       \u23a7 x        if x  = threshold\n y = \u23a8 \n     \u23a9 value    if x    threshold   threshold: The value to threshold at  value: The value to replace with  ip: can optionally do the operation in-place   Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Threshold(1, 0.8)\nval input = Tensor(2, 2, 2).randn()\nval output = module.forward(input)  input\n(1,.,.) =\n2.0502799   -0.37522468\n-1.2704345  -0.22533786\n\n(2,.,.) =\n1.1959263   1.6670992\n-0.24333914 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  output\n(1,.,.) =\n(1,.,.) =\n2.0502799   0.8\n0.8 0.8\n\n(2,.,.) =\n1.1959263   1.6670992\n0.8 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Threshold(1.0, 0.8)\ninput = np.random.randn(2, 2, 2)\noutput = module.forward(input)  input\n[[[-0.43226865 -1.09160093]\n  [-0.20280088  0.68196767]]\n\n [[ 2.32017942  1.00003307]\n  [-0.46618767  0.57057167]]]  output\n[array([[[ 0.80000001,  0.80000001],\n        [ 0.80000001,  0.80000001]],\n\n       [[ 2.32017946,  1.00003302],\n        [ 0.80000001,  0.80000001]]], dtype=float32)]", 
            "title": "Threshold"
        }, 
        {
            "location": "/APIdocs/Layers/Embedding_Layers/merged-Embedding_Layers/", 
            "text": "LookupTable\n\n\nScala:\n\n\nval layer = LookupTable[Float](nIndex: Int, nOutput: Int, paddingValue: Double = 0,\n                                 maxNorm: Double = Double.MaxValue,\n                                 normType: Double = 2.0,\n                                 shouldScaleGradByFreq: Boolean = false,\n                                 wRegularizer: Regularizer[T] = null)\n\n\n\n\nPython:\n\n\nlayer = LookupTable(nIndex, nOutput, paddingValue, maxNorm, normType, shouldScaleGradByFreq)\n\n\n\n\nThis layer is a particular case of a convolution, where the width of the convolution would be 1.\nInput should be a 1D or 2D tensor filled with indices. Indices are corresponding to the position\nin weight. For each index element of input, it outputs the selected index part of weight.\nThis layer is often used in word embedding. In collaborative filtering, it can be used together with Select to create embeddings for users or items. \n\n\nScala example:\n\n\n\nval layer = LookupTable[Float](9, 4, 2, 0.1, 2.0, true)\nval input = Tensor[Float](Storage(Array(5.0f, 2.0f, 6.0f, 9.0f, 4.0f)), 1, Array(5))\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, output)\n\n\n println(layer.weight)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.2949163      -0.8240777      -0.9440595      -0.8326071\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n-1.7525806      0.7305201       0.3349018       0.03952092\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n0.014426847     -0.09052222     0.019132217     -0.035093457\n-0.7002858      1.1149521       0.9869375       1.2580993\n0.36649692      -0.6583153      0.90005803      0.12671651\n0.048913725     0.033388995     -0.07938445     0.01381052\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9x4]\n\n\n println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n5.0\n2.0\n6.0\n9.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n println(output)\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n0.014426847     -0.09052222     0.019132217     -0.035093457\n0.048913725     0.033388995     -0.07938445     0.01381052\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x4]\n\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\n\nPython example:\n\n\nlayer = LookupTable(9, 4, 2.0, 0.1, 2.0, True)\ninput = np.array([5.0, 2.0, 6.0, 9.0, 4.0]).astype(\nfloat32\n)\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, output)\n\n\n output\n[array([[-0.00704637,  0.07495038,  0.06465427,  0.01235369],\n        [ 0.00350313,  0.02751033, -0.02163727,  0.0936095 ],\n        [ 0.02330465, -0.05696457,  0.0081728 ,  0.07839092],\n        [ 0.06580321, -0.0743262 , -0.00414508, -0.01133001],\n        [-0.00382435, -0.04677011,  0.02839171, -0.08361723]], dtype=float32)]\n\n\n gradInput\n[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)]", 
            "title": "Embedding Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Embedding_Layers/merged-Embedding_Layers/#lookuptable", 
            "text": "Scala:  val layer = LookupTable[Float](nIndex: Int, nOutput: Int, paddingValue: Double = 0,\n                                 maxNorm: Double = Double.MaxValue,\n                                 normType: Double = 2.0,\n                                 shouldScaleGradByFreq: Boolean = false,\n                                 wRegularizer: Regularizer[T] = null)  Python:  layer = LookupTable(nIndex, nOutput, paddingValue, maxNorm, normType, shouldScaleGradByFreq)  This layer is a particular case of a convolution, where the width of the convolution would be 1.\nInput should be a 1D or 2D tensor filled with indices. Indices are corresponding to the position\nin weight. For each index element of input, it outputs the selected index part of weight.\nThis layer is often used in word embedding. In collaborative filtering, it can be used together with Select to create embeddings for users or items.   Scala example:  \nval layer = LookupTable[Float](9, 4, 2, 0.1, 2.0, true)\nval input = Tensor[Float](Storage(Array(5.0f, 2.0f, 6.0f, 9.0f, 4.0f)), 1, Array(5))\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, output)  println(layer.weight)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.2949163      -0.8240777      -0.9440595      -0.8326071\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n-1.7525806      0.7305201       0.3349018       0.03952092\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n0.014426847     -0.09052222     0.019132217     -0.035093457\n-0.7002858      1.1149521       0.9869375       1.2580993\n0.36649692      -0.6583153      0.90005803      0.12671651\n0.048913725     0.033388995     -0.07938445     0.01381052\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9x4]  println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n5.0\n2.0\n6.0\n9.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  println(output)\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n0.014426847     -0.09052222     0.019132217     -0.035093457\n0.048913725     0.033388995     -0.07938445     0.01381052\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x4]  println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  layer = LookupTable(9, 4, 2.0, 0.1, 2.0, True)\ninput = np.array([5.0, 2.0, 6.0, 9.0, 4.0]).astype( float32 )\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, output)  output\n[array([[-0.00704637,  0.07495038,  0.06465427,  0.01235369],\n        [ 0.00350313,  0.02751033, -0.02163727,  0.0936095 ],\n        [ 0.02330465, -0.05696457,  0.0081728 ,  0.07839092],\n        [ 0.06580321, -0.0743262 , -0.00414508, -0.01133001],\n        [-0.00382435, -0.04677011,  0.02839171, -0.08361723]], dtype=float32)]  gradInput\n[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)]", 
            "title": "LookupTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/", 
            "text": "Pack\n\n\nScala:\n\n\nval module = Pack(dim)\n\n\n\n\nPython:\n\n\nmodule = Pack(dim)\n\n\n\n\nPack is used to stack a list of n-dimensional tensors into one (n+1)-dimensional tensor.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Pack(2)\nval input1 = Tensor(2, 2).randn()\nval input2 = Tensor(2, 2).randn()\nval input = T()\ninput(1) = input1\ninput(2) = input2\n\nval output = module.forward(input)\n\n\n input\n {\n    2: -0.8737048   -0.7337217\n       0.7268678    -0.53470045\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: -1.3062215   -0.58756566\n       0.8921608    -1.8087773\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n\n\n output\n(1,.,.) =\n-1.3062215  -0.58756566\n-0.8737048  -0.7337217\n\n(2,.,.) =\n0.8921608   -1.8087773\n0.7268678   -0.53470045\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Pack(2)\ninput1 = np.random.randn(2, 2)\ninput2 = np.random.randn(2, 2)\ninput = [input1, input2]\noutput = module.forward(input)\n\n\n input\n[array([[ 0.92741416, -3.29826586],\n       [-0.03147819, -0.10049306]]), array([[-0.27146461, -0.25729802],\n       [ 0.1316149 ,  1.27620145]])]\n\n\n output\narray([[[ 0.92741418, -3.29826593],\n        [-0.27146462, -0.25729802]],\n\n       [[-0.03147819, -0.10049306],\n        [ 0.13161489,  1.27620149]]], dtype=float32)\n\n\n\n\nMM\n\n\nScala:\n\n\nval m = MM(transA=false,transB=false)\n\n\n\n\nPython:\n\n\nm = MM(trans_a=False,trans_b=False)\n\n\n\n\nMM is a module that performs matrix multiplication on two mini-batch inputs, producing one mini-batch.\n\n\nScala example:\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input = T(1 -\n Tensor(3, 3).randn(), 2 -\n Tensor(3, 3).randn())\nval m1 = MM()\nval output1 = m1.forward(input)\nval m2 = MM(true,true)\nval output2 = m2.forward(input)\n\nscala\n print(input)\n {\n        2: -0.62020904  -0.18690863     0.34132162\n           -0.5359324   -0.09937895     0.86147165\n           -2.6607985   -1.426654       2.3428898\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: -1.3087689   0.048720464     0.69583243\n           -0.52055264  -1.5275089      -1.1569321\n           0.28093573   -0.29353273     -0.9505267\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n }\n\nscala\n print(output1)\n-1.0658705      -0.7529337      1.225519\n4.2198563       1.8996398       -4.204146\n2.512235        1.3327343       -2.38396\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nscala\n print(output2)\n1.0048954       0.99516183      4.8832207\n0.15509865      -0.12717877     1.3618765\n-0.5397563      -1.0767963      -2.4279075\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput1=np.random.rand(3,3)\ninput2=np.random.rand(3,3)\ninput = [input1,input2]\nprint \ninput is :\n,input\nout = MM().forward(input)\nprint \noutput is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [array([[ 0.13696046,  0.92653165,  0.73585328],\n       [ 0.28167852,  0.06431783,  0.15710073],\n       [ 0.21896166,  0.00780161,  0.25780671]]), array([[ 0.11232797,  0.17023931,  0.92430042],\n       [ 0.86629537,  0.07630215,  0.08584417],\n       [ 0.47087278,  0.22992833,  0.59257503]])]\ncreating: createMM\noutput is : [array([[ 1.16452789,  0.26320592,  0.64217824],\n       [ 0.16133308,  0.08898225,  0.35897085],\n       [ 0.15274818,  0.09714822,  0.3558259 ]], dtype=float32)]\n\n\n\n\nCMaxTable\n\n\nScala:\n\n\nval m = CMaxTable()\n\n\n\n\nPython:\n\n\nm = CMaxTable()\n\n\n\n\nCMaxTable is a module that takes a table of Tensors and outputs the max of all of them.\n\n\nScala example:\n\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).randn()\nval input2 =  Tensor(3).randn()\nval input = T(input1, input2)\nval m = CMaxTable()\nval output = m.forward(input)\nval gradOut = Tensor(3).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n {\n        2: -0.38613814\n           0.74074316\n           -1.753783\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        1: -1.6037064\n           -2.3297918\n           -0.7160026\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\nscala\n print(output)\n-0.38613814\n0.74074316\n-0.7160026\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nscala\n print(gradOut)\n-1.4526331\n0.7070323\n0.29294914\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala\n print(gradIn)\n {\n        2: -1.4526331\n           0.7070323\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 0.0\n           0.0\n           0.29294914\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.rand(3)\ninput2 = np.random.rand(3)\nprint \ninput is :\n,input1,input2\n\nm = CMaxTable()\nout = m.forward([input1,input2])\nprint \noutput of m is :\n,out\n\ngrad_out = np.random.rand(3)\ngrad_in = m.backward([input1, input2],grad_out)\nprint \ngrad input of m is :\n,grad_in\n\n\n\n\nproduces output:\n\n\ninput is : [ 0.48649797  0.22131348  0.45667796] [ 0.73207053  0.74290136  0.03169769]\ncreating: createCMaxTable\noutput of m is : [array([ 0.73207051,  0.74290138,  0.45667794], dtype=float32)]\ngrad input of m is : [array([ 0.        ,  0.        ,  0.86938971], dtype=float32), array([ 0.04140199,  0.4787094 ,  0.        ], dtype=float32)]\n\n\n\n\nSplitTable\n\n\nScala:\n\n\nval layer = SplitTable[T](dim)\n\n\n\n\nPython:\n\n\nlayer = SplitTable(dim)\n\n\n\n\nSplitTable takes a Tensor as input and outputs several tables,\nsplitting the Tensor along the specified dimension \ndimension\n. Please note\nthe dimension starts from 1.\n\n\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user needs to specify the number of dimensions of each sample tensor in a\nbatch using \nnInputDims\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = SplitTable[Float](2)\nlayer.forward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\nlayer.backward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), T(\n  Tensor[Float](T(0.1f, 0.2f, 0.3f)),\n  Tensor[Float](T(0.4f, 0.5f, 0.6f)),\n  Tensor[Float](T(0.7f, 0.8f, 0.9f))\n))\n\n\n\n\nIts output should be \n\n\n {\n        2: 2.0\n           5.0\n           8.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 1.0\n           4.0\n           7.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        3: 3.0\n           6.0\n           9.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n0.1     0.4     0.7\n0.2     0.5     0.8\n0.3     0.6     0.9\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import SplitTable\nimport numpy as np\n\nlayer = SplitTable(2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\n\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), [\n  np.array([0.1, 0.2, 0.3]),\n  np.array([0.4, 0.5, 0.6]),\n  np.array([0.7, 0.8, 0.9])\n])\n\n\n\n\nIts output should be\n\n\n[\n  array([ 1.,  4.,  7.], dtype=float32),\n  array([ 2.,  5.,  8.], dtype=float32),\n  array([ 3.,  6.,  9.], dtype=float32)\n]\n\narray([[ 0.1       ,  0.40000001,  0.69999999],\n       [ 0.2       ,  0.5       ,  0.80000001],\n       [ 0.30000001,  0.60000002,  0.89999998]], dtype=float32)\n\n\n\n\nDotProduct\n\n\nScala:\n\n\nval m = DotProduct()\n\n\n\n\nPython:\n\n\nm = DotProduct()\n\n\n\n\nOutputs the dot product (similarity) between inputs\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = DotProduct()\nval x = Tensor(3).fill(1f)\nval y = Tensor(3).fill(2f)\nprintln(\ninput:\n)\nprintln(x)\nprintln(y)\nprintln(\noutput:\n)\nprintln(mlp.forward(T(x, y)))\n\n\n\n\ninput:\n1.0\n1.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\noutput:\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\nmlp = DotProduct()\nx = np.array([1, 1, 1])\ny = np.array([2, 2, 2])\nprint(\ninput:\n)\nprint(x)\nprint(y)\nprint(\noutput:\n)\nprint(mlp.forward([x, y]))\n\n\n\n\n\ncreating: createDotProduct\ninput:\n[1 1 1]\n[2 2 2]\noutput:\n[ 6.]\n\n\n\n\nCSubTable\n\n\nScala:\n\n\nval model = CSubTable()\n\n\n\n\nPython:\n\n\nmodel = CSubTable()\n\n\n\n\nTakes a sequence with two Tensor and returns the component-wise subtraction between them.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CSubTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala\n print(input)\n {\n    2: 0.29122078\n       0.17347474\n       0.14127742\n       0.2249051\n       0.12171601\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.6202152\n       0.70417005\n       0.21334995\n       0.05191216\n       0.4209623\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\nscala\n print(output)\n0.3289944\n0.5306953\n0.072072536\n-0.17299294\n0.2992463\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\nPython example:\n\n\nmodel = CSubTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)\n\n\n\n\noutput is\n\n\narray([-1.15087152,  0.6169951 ,  2.41840839,  1.34374809,  1.39436531], dtype=float32)\n\n\n\n\nCDivTable\n\n\nScala:\n\n\nval module = CDivTable()\n\n\n\n\nPython:\n\n\nmodule = CDivTable()\n\n\n\n\nTakes a table with two Tensor and returns the component-wise division between them.\n\n\nScala example:\n\n\nval module = CDivTable()\nval input = T(1 -\n Tensor(2,3).rand(), 2 -\n Tensor(2,3).rand())\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.802295     0.7113872       0.29395157\n           0.6562403    0.06519115      0.20099664\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n        1: 0.7435388    0.59126955      0.10225375\n           0.46819785   0.10572237      0.9861797\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n }\n\nmodule.forward(input)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9267648       0.8311501       0.34785917\n0.7134549       1.6217289       4.906449\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nmodule = CDivTable()\ninput = [np.array([[1, 2, 3],[4, 5, 6]]), np.array([[1, 4, 9],[6, 10, 3]])]\nmodule.forward(input)\n[array([\n[ 1.,                   0.5     ,    0.33333334],\n[ 0.66666669, 0.5       ,  2.        ]], dtype=float32)]\n\n\n\n\nJoinTable\n\n\nScala:\n\n\nval layer = JoinTable(dimension, nInputDims)\n\n\n\n\nPython:\n\n\nlayer = JoinTable(dimension, n_input_dims)\n\n\n\n\nIt is a table module which takes a table of Tensors as input and\noutputs a Tensor by joining them together along the dimension \ndimension\n.\n\n\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using \nnInputDims\n.\n\n\nParameters:\n\n\ndimension\n  - to be join in this dimension\n\n\nnInputDims\n - specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.JoinTable\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = JoinTable(2, 2)\nval input1 = Tensor(T(\n  T(\n    T(1f, 2f, 3f),\n    T(2f, 3f, 4f),\n    T(3f, 4f, 5f))\n))\n\nval input2 = Tensor(T(\n  T(\n    T(3f, 4f, 5f),\n    T(2f, 3f, 4f),\n    T(1f, 2f, 3f))\n))\n\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(\n    T(1f, 2f, 3f, 3f, 4f, 5f),\n    T(2f, 3f, 4f, 2f, 3f, 4f),\n    T(3f, 4f, 5f, 1f, 2f, 3f)\n)))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n1.0 2.0 3.0 3.0 4.0 5.0\n2.0 3.0 4.0 2.0 3.0 4.0\n3.0 4.0 5.0 1.0 2.0 3.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x6]\n\nprintln(grad)\n {\n    2: (1,.,.) =\n       3.0  4.0 5.0\n       2.0  3.0 4.0\n       1.0  2.0 3.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n    1: (1,.,.) =\n       1.0  2.0 3.0\n       2.0  3.0 4.0\n       3.0  4.0 5.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }\n\n\n\n\nPython example:\n\n\nlayer = JoinTable(2, 2)\ninput1 = np.array([\n [\n    [1.0, 2.0, 3.0],\n    [2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0]\n  ]\n])\n\ninput2 = np.array([\n  [\n    [3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0],\n    [1.0, 2.0, 3.0]\n  ]\n])\n\ninput = [input1, input2]\n\ngradOutput = np.array([\n  [\n    [1.0, 2.0, 3.0, 3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0, 2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0, 1.0, 2.0, 3.0]\n  ]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 1.  2.  3.  3.  4.  5.]\n  [ 2.  3.  4.  2.  3.  4.]\n  [ 3.  4.  5.  1.  2.  3.]]]\n\nprint grad\n[array([[[ 1.,  2.,  3.],\n        [ 2.,  3.,  4.],\n        [ 3.,  4.,  5.]]], dtype=float32), array([[[ 3.,  4.,  5.],\n        [ 2.,  3.,  4.],\n        [ 1.,  2.,  3.]]], dtype=float32)]\n\n\n\n\nSelectTable\n\n\nScala:\n\n\nval m = SelectTable(index: Int)\n\n\n\n\nPython:\n\n\nm = SelectTable(dimension)\n\n\n\n\nSelect one element from a table by a given index.\nIn Scala API, table is kind of like HashMap with one-base index as the key.\nIn python, table is a just a list.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = T(Tensor(2,3).randn(), Tensor(2,3).randn())\n\nprintln(\ninput: \n)\nprintln(input)\nprintln(\noutput:\n)\nprintln(SelectTable(1).forward(input)) // Select and output the first element of the input which shape is (2, 3)\nprintln(SelectTable(2).forward(input)) // Select and output the second element of the input which shape is (2, 3)\n\n\n\n\n\ninput: \n {\n    2: 2.005436370849835    0.09670211785545313 1.186779895312918   \n       2.238415300857082    0.241626512721254   0.15765709974113828 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    1: 0.5668905654052705   -1.3205159007397167 -0.5431464848526197 \n       -0.11582559521074104 0.7671830693813515  -0.39992781407893574    \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\noutput:\n0.5668905654052705  -1.3205159007397167 -0.5431464848526197 \n-0.11582559521074104    0.7671830693813515  -0.39992781407893574    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n2.005436370849835   0.09670211785545313 1.186779895312918   \n2.238415300857082   0.241626512721254   0.15765709974113828 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\ninput = [np.random.random((2,3)), np.random.random((2, 1))]\nprint(\ninput:\n)\nprint(input)\nprint(\noutput:\n)\nprint(SelectTable(1).forward(input)) # Select and output the first element of the input which shape is (2, 3)\n\n\n\n\ninput:\n[array([[ 0.07185111,  0.26140439,  0.9437582 ],\n       [ 0.50278191,  0.83923974,  0.06396735]]), array([[ 0.84955122],\n       [ 0.16053703]])]\noutput:\ncreating: createSelectTable\n[[ 0.07185111  0.2614044   0.94375819]\n [ 0.50278193  0.83923972  0.06396735]]\n\n\n\n\n\nNarrowTable\n\n\nScala:\n\n\nval narrowTable = NarrowTable(offset, length = 1)\n\n\n\n\nPython:\n\n\nnarrowTable = NarrowTable(offset, length = 1)\n\n\n\n\nNarrowTable takes a table as input and returns a subtable starting from index \noffset\n having \nlength\n elements\n\n\nNegative \nlength\n means the last element is located at Abs|length| to the last element of input\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\nval narrowTable = NarrowTable(1, 1)\n\nval input = T()\ninput(1.0) = Tensor(2, 2).rand()\ninput(2.0) = Tensor(2, 2).rand()\ninput(3.0) = Tensor(2, 2).rand()\n\n print(input)\n {\n    2.0: 0.27686104 0.9040761   \n         0.75969505 0.8008061   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1.0: 0.94122535 0.46173728  \n         0.43302807 0.1670979   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    3.0: 0.43944374 0.49336782  \n         0.7274511  0.67777634  \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n  print(narrowTable.forward(input))\n {\n    1: 0.94122535   0.46173728  \n       0.43302807   0.1670979   \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nnarrowTable = NarrowTable(1, 1)\n\n narrowTable.forward([np.array([1, 2, 3]), np.array([4, 5, 6])])\n[array([ 1.,  2.,  3.], dtype=float32)]\n\n\n\n\n\nCAddTable\n\n\nScala:\n\n\nval module = CAddTable(inplace = false)\n\n\n\n\nPython:\n\n\nmodule = CAddTable(inplace=False)\n\n\n\n\nCAddTable merges the input tensors in the input table by element-wise adding. The input table is actually an array of tensor with same size.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprintln(mlp.forward(Tensor.range(1, 3, 1)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.nn.abstractnn.Activity =\n2.0\n4.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprint(mlp.forward(np.arange(1, 4, 1)))\n\n\n\n\nOutput is\n\n\n[array([ 2.,  4.,  6.], dtype=float32)]\n\n\n\n\nCMulTable\n\n\nScala:\n\n\nval model = CMulTable()\n\n\n\n\nPython:\n\n\nmodel = CMulTable()\n\n\n\n\nTakes a sequence of Tensors and outputs the multiplication of all of them.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CMulTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala\n print(input)\n {\n    2: 0.13224044\n       0.5460452\n       0.33032498\n       0.6317603\n       0.6665052\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.28694472\n       0.45169437\n       0.36891535\n       0.9126049\n       0.41318864\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }\n\nscala\n print(output)\n0.037945695\n0.24664554\n0.12186196\n0.57654756\n0.27539238\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\nPython example:\n\n\nmodel = CMulTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)\n\n\n print(input)\n[array([ 0.28183274, -0.6477487 , -0.21279841,  0.22725124,  0.54748552]), array([-0.78673028, -1.08337196, -0.62710066,  0.37332587, -1.40708162])]\n\n\n print(output)\n[-0.22172636  0.70175284  0.13344601  0.08483877 -0.77035683]\n\n\n\n\nMV\n\n\nScala:\n\n\nval module = MV(trans = false)\n\n\n\n\nPython:\n\n\nmodule = MV(trans=False)\n\n\n\n\nIt is a module to perform matrix vector multiplication on two mini-batch inputs, producing a mini-batch.\n\n\ntrans\n means whether make matrix transpose before multiplication.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval module = MV()\n\nprintln(module.forward(T(Tensor.range(1, 12, 1).resize(2, 2, 3), Tensor.range(1, 6, 1).resize(2, 3))))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n14.0    32.0\n122.0   167.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nmodule = MV()\n\nprint(module.forward([np.arange(1, 13, 1).reshape(2, 2, 3), np.arange(1, 7, 1).reshape(2, 3)]))\n\n\n\n\nOutput is\n\n\n[array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]\n\n\n\n\nFlattenTable\n\n\nScala:\n\n\nval module = FlattenTable()\n\n\n\n\nPython:\n\n\nmodule = FlattenTable()\n\n\n\n\nFlattenTable takes an arbitrarily deep table of Tensors (potentially nested) as input and a table of Tensors without any nested table will be produced\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = FlattenTable()\nval t1 = Tensor(3).randn()\nval t2 = Tensor(3).randn()\nval t3 = Tensor(3).randn()\nval input = T(t1, T(t2, T(t3)))\n\nval output = module.forward(input)\n\n\n input\n {\n    2:  {\n        2:  {\n            1: 0.5521984\n               -0.4160644\n               -0.698762\n               [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n            }\n        1: -1.7380241\n           0.60336906\n           -0.8751049\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        }\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\n\n\n output\n{\n    2: -1.7380241\n       0.60336906\n       -0.8751049\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    3: 0.5521984\n       -0.4160644\n       -0.698762\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Sequential()\n# this will create a nested table\nnested = ConcatTable().add(Identity()).add(Identity())\nmodule.add(nested).add(FlattenTable())\nt1 = np.random.randn(3)\nt2 = np.random.randn(3)\ninput = [t1, t2]\noutput = module.forward(input)\n\n\n input\n[array([-2.21080689, -0.48928043, -0.26122161]), array([-0.8499716 ,  1.63694575, -0.31109292])]\n\n\n output\n[array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32),\n array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32)]\n\n\n\n\n\nCMinTable\n\n\nScala:\n\n\nval layer = CMinTable[T]()\n\n\n\n\nPython:\n\n\nlayer = CMinTable()\n\n\n\n\nCMinTable takes a bunch of tensors as inputs. These tensors must have\nsame shape. This layer will merge them by doing an element-wise comparision\nand use the min value.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = CMinTable[Float]()\nlayer.forward(T(\n  Tensor[Float](T(1.0f, 5.0f, 2.0f)),\n  Tensor[Float](T(3.0f, 4.0f, -1.0f)),\n  Tensor[Float](T(5.0f, 7.0f, -5.0f))\n))\nlayer.backward(T(\n  Tensor[Float](T(1.0f, 5.0f, 2.0f)),\n  Tensor[Float](T(3.0f, 4.0f, -1.0f)),\n  Tensor[Float](T(5.0f, 7.0f, -5.0f))\n), Tensor[Float](T(0.1f, 0.2f, 0.3f)))\n\n\n\n\nIts output should be\n\n\n1.0\n4.0\n-5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n{\n  2: 0.0\n     0.2\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  1: 0.1\n     0.0\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  3: 0.0\n     0.0\n     0.3\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n}\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import CMinTable\nimport numpy as np\n\nlayer = CMinTable()\nlayer.forward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n])\n\nlayer.backward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n], np.array([0.1, 0.2, 0.3]))\n\n\n\n\n\nIts output should be\n\n\narray([ 1.,  4., -5.], dtype=float32)\n\n[array([ 0.1, 0., 0.], dtype=float32),\narray([ 0., 0.2, 0.], dtype=float32),\narray([ 0., 0., 0.30000001], dtype=float32)]", 
            "title": "Merge/Split Layers"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#pack", 
            "text": "Scala:  val module = Pack(dim)  Python:  module = Pack(dim)  Pack is used to stack a list of n-dimensional tensors into one (n+1)-dimensional tensor.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Pack(2)\nval input1 = Tensor(2, 2).randn()\nval input2 = Tensor(2, 2).randn()\nval input = T()\ninput(1) = input1\ninput(2) = input2\n\nval output = module.forward(input)  input\n {\n    2: -0.8737048   -0.7337217\n       0.7268678    -0.53470045\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: -1.3062215   -0.58756566\n       0.8921608    -1.8087773\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }  output\n(1,.,.) =\n-1.3062215  -0.58756566\n-0.8737048  -0.7337217\n\n(2,.,.) =\n0.8921608   -1.8087773\n0.7268678   -0.53470045\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Pack(2)\ninput1 = np.random.randn(2, 2)\ninput2 = np.random.randn(2, 2)\ninput = [input1, input2]\noutput = module.forward(input)  input\n[array([[ 0.92741416, -3.29826586],\n       [-0.03147819, -0.10049306]]), array([[-0.27146461, -0.25729802],\n       [ 0.1316149 ,  1.27620145]])]  output\narray([[[ 0.92741418, -3.29826593],\n        [-0.27146462, -0.25729802]],\n\n       [[-0.03147819, -0.10049306],\n        [ 0.13161489,  1.27620149]]], dtype=float32)", 
            "title": "Pack"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#mm", 
            "text": "Scala:  val m = MM(transA=false,transB=false)  Python:  m = MM(trans_a=False,trans_b=False)  MM is a module that performs matrix multiplication on two mini-batch inputs, producing one mini-batch.  Scala example:  scala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input = T(1 -  Tensor(3, 3).randn(), 2 -  Tensor(3, 3).randn())\nval m1 = MM()\nval output1 = m1.forward(input)\nval m2 = MM(true,true)\nval output2 = m2.forward(input)\n\nscala  print(input)\n {\n        2: -0.62020904  -0.18690863     0.34132162\n           -0.5359324   -0.09937895     0.86147165\n           -2.6607985   -1.426654       2.3428898\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: -1.3087689   0.048720464     0.69583243\n           -0.52055264  -1.5275089      -1.1569321\n           0.28093573   -0.29353273     -0.9505267\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n }\n\nscala  print(output1)\n-1.0658705      -0.7529337      1.225519\n4.2198563       1.8996398       -4.204146\n2.512235        1.3327343       -2.38396\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nscala  print(output2)\n1.0048954       0.99516183      4.8832207\n0.15509865      -0.12717877     1.3618765\n-0.5397563      -1.0767963      -2.4279075\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput1=np.random.rand(3,3)\ninput2=np.random.rand(3,3)\ninput = [input1,input2]\nprint  input is : ,input\nout = MM().forward(input)\nprint  output is : ,out  produces output:  input is : [array([[ 0.13696046,  0.92653165,  0.73585328],\n       [ 0.28167852,  0.06431783,  0.15710073],\n       [ 0.21896166,  0.00780161,  0.25780671]]), array([[ 0.11232797,  0.17023931,  0.92430042],\n       [ 0.86629537,  0.07630215,  0.08584417],\n       [ 0.47087278,  0.22992833,  0.59257503]])]\ncreating: createMM\noutput is : [array([[ 1.16452789,  0.26320592,  0.64217824],\n       [ 0.16133308,  0.08898225,  0.35897085],\n       [ 0.15274818,  0.09714822,  0.3558259 ]], dtype=float32)]", 
            "title": "MM"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#cmaxtable", 
            "text": "Scala:  val m = CMaxTable()  Python:  m = CMaxTable()  CMaxTable is a module that takes a table of Tensors and outputs the max of all of them.  Scala example:  \nscala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).randn()\nval input2 =  Tensor(3).randn()\nval input = T(input1, input2)\nval m = CMaxTable()\nval output = m.forward(input)\nval gradOut = Tensor(3).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n {\n        2: -0.38613814\n           0.74074316\n           -1.753783\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        1: -1.6037064\n           -2.3297918\n           -0.7160026\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\nscala  print(output)\n-0.38613814\n0.74074316\n-0.7160026\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nscala  print(gradOut)\n-1.4526331\n0.7070323\n0.29294914\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala  print(gradIn)\n {\n        2: -1.4526331\n           0.7070323\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 0.0\n           0.0\n           0.29294914\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.rand(3)\ninput2 = np.random.rand(3)\nprint  input is : ,input1,input2\n\nm = CMaxTable()\nout = m.forward([input1,input2])\nprint  output of m is : ,out\n\ngrad_out = np.random.rand(3)\ngrad_in = m.backward([input1, input2],grad_out)\nprint  grad input of m is : ,grad_in  produces output:  input is : [ 0.48649797  0.22131348  0.45667796] [ 0.73207053  0.74290136  0.03169769]\ncreating: createCMaxTable\noutput of m is : [array([ 0.73207051,  0.74290138,  0.45667794], dtype=float32)]\ngrad input of m is : [array([ 0.        ,  0.        ,  0.86938971], dtype=float32), array([ 0.04140199,  0.4787094 ,  0.        ], dtype=float32)]", 
            "title": "CMaxTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#splittable", 
            "text": "Scala:  val layer = SplitTable[T](dim)  Python:  layer = SplitTable(dim)  SplitTable takes a Tensor as input and outputs several tables,\nsplitting the Tensor along the specified dimension  dimension . Please note\nthe dimension starts from 1.  The input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user needs to specify the number of dimensions of each sample tensor in a\nbatch using  nInputDims .  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = SplitTable[Float](2)\nlayer.forward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\nlayer.backward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), T(\n  Tensor[Float](T(0.1f, 0.2f, 0.3f)),\n  Tensor[Float](T(0.4f, 0.5f, 0.6f)),\n  Tensor[Float](T(0.7f, 0.8f, 0.9f))\n))  Its output should be    {\n        2: 2.0\n           5.0\n           8.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 1.0\n           4.0\n           7.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        3: 3.0\n           6.0\n           9.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n0.1     0.4     0.7\n0.2     0.5     0.8\n0.3     0.6     0.9\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import SplitTable\nimport numpy as np\n\nlayer = SplitTable(2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\n\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), [\n  np.array([0.1, 0.2, 0.3]),\n  np.array([0.4, 0.5, 0.6]),\n  np.array([0.7, 0.8, 0.9])\n])  Its output should be  [\n  array([ 1.,  4.,  7.], dtype=float32),\n  array([ 2.,  5.,  8.], dtype=float32),\n  array([ 3.,  6.,  9.], dtype=float32)\n]\n\narray([[ 0.1       ,  0.40000001,  0.69999999],\n       [ 0.2       ,  0.5       ,  0.80000001],\n       [ 0.30000001,  0.60000002,  0.89999998]], dtype=float32)", 
            "title": "SplitTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#dotproduct", 
            "text": "Scala:  val m = DotProduct()  Python:  m = DotProduct()  Outputs the dot product (similarity) between inputs  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = DotProduct()\nval x = Tensor(3).fill(1f)\nval y = Tensor(3).fill(2f)\nprintln( input: )\nprintln(x)\nprintln(y)\nprintln( output: )\nprintln(mlp.forward(T(x, y)))  input:\n1.0\n1.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\noutput:\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\nmlp = DotProduct()\nx = np.array([1, 1, 1])\ny = np.array([2, 2, 2])\nprint( input: )\nprint(x)\nprint(y)\nprint( output: )\nprint(mlp.forward([x, y]))  creating: createDotProduct\ninput:\n[1 1 1]\n[2 2 2]\noutput:\n[ 6.]", 
            "title": "DotProduct"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#csubtable", 
            "text": "Scala:  val model = CSubTable()  Python:  model = CSubTable()  Takes a sequence with two Tensor and returns the component-wise subtraction between them.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CSubTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala  print(input)\n {\n    2: 0.29122078\n       0.17347474\n       0.14127742\n       0.2249051\n       0.12171601\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.6202152\n       0.70417005\n       0.21334995\n       0.05191216\n       0.4209623\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\nscala  print(output)\n0.3289944\n0.5306953\n0.072072536\n-0.17299294\n0.2992463\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  model = CSubTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)  output is  array([-1.15087152,  0.6169951 ,  2.41840839,  1.34374809,  1.39436531], dtype=float32)", 
            "title": "CSubTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#cdivtable", 
            "text": "Scala:  val module = CDivTable()  Python:  module = CDivTable()  Takes a table with two Tensor and returns the component-wise division between them.  Scala example:  val module = CDivTable()\nval input = T(1 -  Tensor(2,3).rand(), 2 -  Tensor(2,3).rand())\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.802295     0.7113872       0.29395157\n           0.6562403    0.06519115      0.20099664\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n        1: 0.7435388    0.59126955      0.10225375\n           0.46819785   0.10572237      0.9861797\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n }\n\nmodule.forward(input)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9267648       0.8311501       0.34785917\n0.7134549       1.6217289       4.906449\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = CDivTable()\ninput = [np.array([[1, 2, 3],[4, 5, 6]]), np.array([[1, 4, 9],[6, 10, 3]])]\nmodule.forward(input)\n[array([\n[ 1.,                   0.5     ,    0.33333334],\n[ 0.66666669, 0.5       ,  2.        ]], dtype=float32)]", 
            "title": "CDivTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#jointable", 
            "text": "Scala:  val layer = JoinTable(dimension, nInputDims)  Python:  layer = JoinTable(dimension, n_input_dims)  It is a table module which takes a table of Tensors as input and\noutputs a Tensor by joining them together along the dimension  dimension .  The input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using  nInputDims .  Parameters:  dimension   - to be join in this dimension  nInputDims  - specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size  Scala example:  import com.intel.analytics.bigdl.nn.JoinTable\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = JoinTable(2, 2)\nval input1 = Tensor(T(\n  T(\n    T(1f, 2f, 3f),\n    T(2f, 3f, 4f),\n    T(3f, 4f, 5f))\n))\n\nval input2 = Tensor(T(\n  T(\n    T(3f, 4f, 5f),\n    T(2f, 3f, 4f),\n    T(1f, 2f, 3f))\n))\n\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(\n    T(1f, 2f, 3f, 3f, 4f, 5f),\n    T(2f, 3f, 4f, 2f, 3f, 4f),\n    T(3f, 4f, 5f, 1f, 2f, 3f)\n)))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n1.0 2.0 3.0 3.0 4.0 5.0\n2.0 3.0 4.0 2.0 3.0 4.0\n3.0 4.0 5.0 1.0 2.0 3.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x6]\n\nprintln(grad)\n {\n    2: (1,.,.) =\n       3.0  4.0 5.0\n       2.0  3.0 4.0\n       1.0  2.0 3.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n    1: (1,.,.) =\n       1.0  2.0 3.0\n       2.0  3.0 4.0\n       3.0  4.0 5.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }  Python example:  layer = JoinTable(2, 2)\ninput1 = np.array([\n [\n    [1.0, 2.0, 3.0],\n    [2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0]\n  ]\n])\n\ninput2 = np.array([\n  [\n    [3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0],\n    [1.0, 2.0, 3.0]\n  ]\n])\n\ninput = [input1, input2]\n\ngradOutput = np.array([\n  [\n    [1.0, 2.0, 3.0, 3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0, 2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0, 1.0, 2.0, 3.0]\n  ]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 1.  2.  3.  3.  4.  5.]\n  [ 2.  3.  4.  2.  3.  4.]\n  [ 3.  4.  5.  1.  2.  3.]]]\n\nprint grad\n[array([[[ 1.,  2.,  3.],\n        [ 2.,  3.,  4.],\n        [ 3.,  4.,  5.]]], dtype=float32), array([[[ 3.,  4.,  5.],\n        [ 2.,  3.,  4.],\n        [ 1.,  2.,  3.]]], dtype=float32)]", 
            "title": "JoinTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#selecttable", 
            "text": "Scala:  val m = SelectTable(index: Int)  Python:  m = SelectTable(dimension)  Select one element from a table by a given index.\nIn Scala API, table is kind of like HashMap with one-base index as the key.\nIn python, table is a just a list.  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = T(Tensor(2,3).randn(), Tensor(2,3).randn())\n\nprintln( input:  )\nprintln(input)\nprintln( output: )\nprintln(SelectTable(1).forward(input)) // Select and output the first element of the input which shape is (2, 3)\nprintln(SelectTable(2).forward(input)) // Select and output the second element of the input which shape is (2, 3)  input: \n {\n    2: 2.005436370849835    0.09670211785545313 1.186779895312918   \n       2.238415300857082    0.241626512721254   0.15765709974113828 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    1: 0.5668905654052705   -1.3205159007397167 -0.5431464848526197 \n       -0.11582559521074104 0.7671830693813515  -0.39992781407893574    \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\noutput:\n0.5668905654052705  -1.3205159007397167 -0.5431464848526197 \n-0.11582559521074104    0.7671830693813515  -0.39992781407893574    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n2.005436370849835   0.09670211785545313 1.186779895312918   \n2.238415300857082   0.241626512721254   0.15765709974113828 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\ninput = [np.random.random((2,3)), np.random.random((2, 1))]\nprint( input: )\nprint(input)\nprint( output: )\nprint(SelectTable(1).forward(input)) # Select and output the first element of the input which shape is (2, 3)  input:\n[array([[ 0.07185111,  0.26140439,  0.9437582 ],\n       [ 0.50278191,  0.83923974,  0.06396735]]), array([[ 0.84955122],\n       [ 0.16053703]])]\noutput:\ncreating: createSelectTable\n[[ 0.07185111  0.2614044   0.94375819]\n [ 0.50278193  0.83923972  0.06396735]]", 
            "title": "SelectTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#narrowtable", 
            "text": "Scala:  val narrowTable = NarrowTable(offset, length = 1)  Python:  narrowTable = NarrowTable(offset, length = 1)  NarrowTable takes a table as input and returns a subtable starting from index  offset  having  length  elements  Negative  length  means the last element is located at Abs|length| to the last element of input  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\nval narrowTable = NarrowTable(1, 1)\n\nval input = T()\ninput(1.0) = Tensor(2, 2).rand()\ninput(2.0) = Tensor(2, 2).rand()\ninput(3.0) = Tensor(2, 2).rand()  print(input)\n {\n    2.0: 0.27686104 0.9040761   \n         0.75969505 0.8008061   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1.0: 0.94122535 0.46173728  \n         0.43302807 0.1670979   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    3.0: 0.43944374 0.49336782  \n         0.7274511  0.67777634  \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }   print(narrowTable.forward(input))\n {\n    1: 0.94122535   0.46173728  \n       0.43302807   0.1670979   \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }  Python example:  from bigdl.nn.layer import *\nnarrowTable = NarrowTable(1, 1)  narrowTable.forward([np.array([1, 2, 3]), np.array([4, 5, 6])])\n[array([ 1.,  2.,  3.], dtype=float32)]", 
            "title": "NarrowTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#caddtable", 
            "text": "Scala:  val module = CAddTable(inplace = false)  Python:  module = CAddTable(inplace=False)  CAddTable merges the input tensors in the input table by element-wise adding. The input table is actually an array of tensor with same size.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprintln(mlp.forward(Tensor.range(1, 3, 1)))  Output is  com.intel.analytics.bigdl.nn.abstractnn.Activity =\n2.0\n4.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprint(mlp.forward(np.arange(1, 4, 1)))  Output is  [array([ 2.,  4.,  6.], dtype=float32)]", 
            "title": "CAddTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#cmultable", 
            "text": "Scala:  val model = CMulTable()  Python:  model = CMulTable()  Takes a sequence of Tensors and outputs the multiplication of all of them.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CMulTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala  print(input)\n {\n    2: 0.13224044\n       0.5460452\n       0.33032498\n       0.6317603\n       0.6665052\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.28694472\n       0.45169437\n       0.36891535\n       0.9126049\n       0.41318864\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }\n\nscala  print(output)\n0.037945695\n0.24664554\n0.12186196\n0.57654756\n0.27539238\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  model = CMulTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)  print(input)\n[array([ 0.28183274, -0.6477487 , -0.21279841,  0.22725124,  0.54748552]), array([-0.78673028, -1.08337196, -0.62710066,  0.37332587, -1.40708162])]  print(output)\n[-0.22172636  0.70175284  0.13344601  0.08483877 -0.77035683]", 
            "title": "CMulTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#mv", 
            "text": "Scala:  val module = MV(trans = false)  Python:  module = MV(trans=False)  It is a module to perform matrix vector multiplication on two mini-batch inputs, producing a mini-batch.  trans  means whether make matrix transpose before multiplication.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval module = MV()\n\nprintln(module.forward(T(Tensor.range(1, 12, 1).resize(2, 2, 3), Tensor.range(1, 6, 1).resize(2, 3))))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n14.0    32.0\n122.0   167.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  module = MV()\n\nprint(module.forward([np.arange(1, 13, 1).reshape(2, 2, 3), np.arange(1, 7, 1).reshape(2, 3)]))  Output is  [array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]", 
            "title": "MV"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#flattentable", 
            "text": "Scala:  val module = FlattenTable()  Python:  module = FlattenTable()  FlattenTable takes an arbitrarily deep table of Tensors (potentially nested) as input and a table of Tensors without any nested table will be produced  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = FlattenTable()\nval t1 = Tensor(3).randn()\nval t2 = Tensor(3).randn()\nval t3 = Tensor(3).randn()\nval input = T(t1, T(t2, T(t3)))\n\nval output = module.forward(input)  input\n {\n    2:  {\n        2:  {\n            1: 0.5521984\n               -0.4160644\n               -0.698762\n               [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n            }\n        1: -1.7380241\n           0.60336906\n           -0.8751049\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        }\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }  output\n{\n    2: -1.7380241\n       0.60336906\n       -0.8751049\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    3: 0.5521984\n       -0.4160644\n       -0.698762\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Sequential()\n# this will create a nested table\nnested = ConcatTable().add(Identity()).add(Identity())\nmodule.add(nested).add(FlattenTable())\nt1 = np.random.randn(3)\nt2 = np.random.randn(3)\ninput = [t1, t2]\noutput = module.forward(input)  input\n[array([-2.21080689, -0.48928043, -0.26122161]), array([-0.8499716 ,  1.63694575, -0.31109292])]  output\n[array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32),\n array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32)]", 
            "title": "FlattenTable"
        }, 
        {
            "location": "/APIdocs/Layers/MergeSplit_Layers/merged-MergeSplit_Layers/#cmintable", 
            "text": "Scala:  val layer = CMinTable[T]()  Python:  layer = CMinTable()  CMinTable takes a bunch of tensors as inputs. These tensors must have\nsame shape. This layer will merge them by doing an element-wise comparision\nand use the min value.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval layer = CMinTable[Float]()\nlayer.forward(T(\n  Tensor[Float](T(1.0f, 5.0f, 2.0f)),\n  Tensor[Float](T(3.0f, 4.0f, -1.0f)),\n  Tensor[Float](T(5.0f, 7.0f, -5.0f))\n))\nlayer.backward(T(\n  Tensor[Float](T(1.0f, 5.0f, 2.0f)),\n  Tensor[Float](T(3.0f, 4.0f, -1.0f)),\n  Tensor[Float](T(5.0f, 7.0f, -5.0f))\n), Tensor[Float](T(0.1f, 0.2f, 0.3f)))  Its output should be  1.0\n4.0\n-5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n{\n  2: 0.0\n     0.2\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  1: 0.1\n     0.0\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  3: 0.0\n     0.0\n     0.3\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n}  Python example:  from bigdl.nn.layer import CMinTable\nimport numpy as np\n\nlayer = CMinTable()\nlayer.forward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n])\n\nlayer.backward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n], np.array([0.1, 0.2, 0.3]))  Its output should be  array([ 1.,  4., -5.], dtype=float32)\n\n[array([ 0.1, 0., 0.], dtype=float32),\narray([ 0., 0.2, 0.], dtype=float32),\narray([ 0., 0., 0.30000001], dtype=float32)]", 
            "title": "CMinTable"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/", 
            "text": "Scale\n\n\nScala:\n\n\nval m = Scale(Array(2, 1))\n\n\n\n\nPython:\n\n\nm = scale = Scale([2, 1])\n\n\n\n\nScale is the combination of cmul and cadd. \nScale(size).forward(input) == CAdd(size).forward(CMul(size).forward(input))\n\nComputes the elementwise product of input and weight, with the shape of the weight \"expand\" to\nmatch the shape of the input.Similarly, perform a expand cdd bias and perform an elementwise add.\n\noutput = input .* weight .+ bias (element wise)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).fill(1f)\nprintln(\ninput:\n)\nprintln(input)\nval scale = Scale(Array(2, 1))\nval weight = Tensor(2, 1).fill(2f)\nval bias = Tensor(2, 1).fill(3f)\nscale.setWeightsBias(Array(weight, bias))\nprintln(\nWeight:\n)\nprintln(weight)\nprintln(\nbias:\n)\nprintln(bias)\nprintln(\noutput:\n)\nprint(scale.forward(input))\n\n\n\n\ninput:\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\nWeight:\n2.0 \n2.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\nbias:\n3.0 \n3.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\noutput:\n5.0 5.0 5.0 \n5.0 5.0 5.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\ninput = np.ones([2, 3])\nprint(\ninput:\n)\nprint(input)\nscale = Scale([2, 1])\nweight = np.full([2, 1], 2)\nbias = np.full([2, 1], 3)\nprint(\nweight: \n)\nprint(weight)\nprint(\nbias: \n)\nprint(bias)\nscale.set_weights([weight, bias])\nprint(\noutput: \n)\nprint(scale.forward(input))\n\n\n\n\n\ninput:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\ncreating: createScale\nweight: \n[[2]\n [2]]\nbias: \n[[3]\n [3]]\noutput: \n[[ 5.  5.  5.]\n [ 5.  5.  5.]]\n\n\n\n\nMin\n\n\nScala:\n\n\nval min = Min(dim, numInputDims)\n\n\n\n\nPython:\n\n\nmin = Min(dim, num_input_dims)\n\n\n\n\nApplies a min operation over dimension \ndim\n.\n\n\nParameters:\n\n\n \ndim\n - A integer. The dimension to min along.\n\n \nnumInputDims\n - An optional integer indicating the number of input dimensions.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval min = Min(2)\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n 1.0f,\n 1.0f\n))\nval output = min.forward(input)\nval gradient = min.backward(input, gradOutput)\n-\n print(output)\n1.0\n3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-\n print(gradient)\n1.0     0.0     \n1.0     0.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nmin = Min(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 4.0]\n])\n\ngrad_output = np.array([1.0, 1.0])\noutput = min.forward(input)\ngradient = min.backward(input, grad_output)\n-\n print output\n[ 1.  3.]\n-\n print gradient\n[[ 1.  0.]\n [ 1.  0.]]\n\n\n\n\nAdd\n\n\nScala:\n\n\nval addLayer = Add[T](inputSize)\n\n\n\n\nPython:\n\n\nadd_layer = Add(input_size)\n\n\n\n\nA.K.A BiasAdd. This layer adds input tensor with a parameter tensor and output the result.\nIf the input is 1D, this layer just do a element-wise add. If the input has multiple dimentions,\nthis layer will treat the first dimension as batch dimension, resize the input tensor to a 2D \ntensor(batch-dimension x input_size) and do a broadcast add between the 2D tensor and the \nparameter.\n\n\nPlease note that the parameter will be trained in the back propagation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval addLayer = Add[Float](4)\naddLayer.bias.set(Tensor[Float](T(1.0f, 2.0f, 3.0f, 4.0f)))\naddLayer.forward(Tensor[Float](T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))))\naddLayer.backward(Tensor[Float](T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))),\n    Tensor[Float](T(T(0.1f, 0.1f, 0.1f, 0.1f), T(0.3f, 0.3f, 0.3f, 0.3f))))\n\n\n\n\nIts output should be\n\n\n2.0     3.0     4.0     5.0\n4.0     5.0     6.0     7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n0.1     0.1     0.1     0.1\n0.3     0.3     0.3     0.3\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import Add\nimport numpy as np\n\nadd_layer = Add(4)\nadd_layer.set_weights([np.array([1.0, 2.0, 3.0, 4.0])])\nadd_layer.forward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]))\nadd_layer.backward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]),\n    np.array([[0.1, 0.1, 0.1, 0.1], [0.3, 0.3, 0.3, 0.3]]))\n\n\n\n\nIts output should be\n\n\narray([[ 2.,  3.,  4.,  5.],\n       [ 4.,  5.,  6.,  7.]], dtype=float32)\n\narray([[ 0.1       ,  0.1       ,  0.1       ,  0.1       ],\n       [ 0.30000001,  0.30000001,  0.30000001,  0.30000001]], dtype=float32)   \n\n\n\n\nBiLinear\n\n\nScala:\n\n\nval layer = BiLinear(\n  inputSize1,\n  inputSize2,\n  outputSize,\n  biasRes = true,\n  wRegularizer = null,\n  bRegularizer = null)\n\n\n\n\nPython:\n\n\nlayer = BiLinear(\n    input_size1,\n    input_size2,\n    output_size,\n    bias_res=True,\n    wRegularizer=None,\n    bRegularizer=None)\n\n\n\n\nA bilinear transformation with sparse inputs.\nThe input tensor given in forward(input) is a table containing both inputs x_1 and x_2,\nwhich are tensors of size N x inputDimension1 and N x inputDimension2, respectively.\n\n\nParameters:\n\n\ninputSize1\n   dimension of input x_1\n\n\ninputSize2\n   dimension of input x_2\n\n\noutputSize\n   output dimension\n\n\nbiasRes\n  The layer can be trained without biases by setting bias = false. otherwise true\n\n\nwRegularizer\n : instance of \nRegularizer\n\n             (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nbRegularizer\n : instance of \nRegularizer\n\n             applied to the bias.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Bilinear\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Bilinear(3, 2, 3)\nval input1 = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\nval input2 = Tensor(T(\n  T(-2f, 3f),\n  T(-1f, 2f),\n  T(-3f, 4f)\n))\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n-0.14168167 -8.697224   -10.097688\n-0.20962894 -7.114827   -8.568602\n0.16706467  -19.751905  -24.516418\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n {\n    2: 13.411718    -18.695072\n       14.674414    -19.503393\n       13.9599  -17.271534\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n    1: -5.3747015   -17.803686  -17.558662\n       -2.413877    -8.373887   -8.346823\n       -2.239298    -11.249412  -14.537216\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }\n\n\n\n\nPython example:\n\n\nlayer = Bilinear(3, 2, 3)\ninput_1 = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ninput_2 = np.array([\n  [-3.0, 4.0],\n  [-2.0, 3.0],\n  [-1.0, 2.0]\n])\n\ninput = [input_1, input_2]\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]\n\nprint grad\n[array([[ 11.86168194, -14.02727222,  -6.16624403],\n       [  6.72984409,  -7.96572971,  -2.89302039],\n       [  5.52902842,  -5.76724434,  -1.46646953]], dtype=float32), array([[ 13.22105694,  -4.6879468 ],\n       [ 14.39296341,  -6.71434498],\n       [ 20.93929482, -13.02455521]], dtype=float32)]\n\n\n\n\nClamp\n\n\nScala:\n\n\nval model = Clamp(min, max)\n\n\n\n\nPython:\n\n\nmodel = Clamp(min, max)\n\n\n\n\nA kind of hard tanh activition function with integer min and max\n- param min min value\n- param max max value\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Clamp(-10, 10)\nval input = Tensor(2, 2, 2).rand()\nval output = model.forward(input)\n\nscala\n print(input)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala\n print(output)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nmodel = Clamp(-10, 10)\ninput = np.random.randn(2, 2, 2)\noutput = model.forward(input)\n\n\n print(input)\n[[[-0.66763755  1.15392566]\n  [-2.10846048  0.46931736]]\n\n [[ 1.74174638 -1.04323311]\n  [-1.91858729  0.12624046]]]\n\n\n print(output)\n[[[-0.66763753  1.15392566]\n  [-2.10846043  0.46931735]]\n\n [[ 1.74174643 -1.04323316]\n  [-1.91858733  0.12624046]]\n\n\n\n\nSquare\n\n\nScala:\n\n\nval module = Square()\n\n\n\n\nPython:\n\n\nmodule = Square()\n\n\n\n\nSquare apply an element-wise square operation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Square()\n\nprintln(module.forward(Tensor.range(1, 6, 1)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n4.0\n9.0\n16.0\n25.0\n36.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 6]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Square()\nprint(module.forward(np.arange(1, 7, 1)))\n\n\n\n\nOutput is\n\n\n[array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)]\n\n\n\n\nMean\n\n\nScala:\n\n\nval m = Mean(dimension=1, nInputDims=-1, squeeze=true)\n\n\n\n\nPython:\n\n\nm = Mean(dimension=1,n_input_dims=-1, squeeze=True)\n\n\n\n\nMean is a module that simply applies a mean operation over the given dimension - specified by \ndimension\n (starting from 1).\n\n\nThe input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using \nnInputDims\n.  When input is one tensor, do not specify \nnInputDims\n or set it = -1, otherwise input will be interpreted as batch of tensors. \n\n\nScala example:\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Mean()\nval output1 = m1.forward(input)\nval m2 = Mean(2,1,true)\nval output2 = m2.forward(input)\n\nscala\n print(input)\n(1,.,.) =\n-0.52021635     -1.8250599\n-0.2321481      -2.5672712\n\n(2,.,.) =\n4.007425        -0.8705412\n1.6506456       -0.2470611\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala\n print(output1)\n1.7436042       -1.3478005\n0.7092488       -1.4071661\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala\n print(output2)\n-0.37618223     -2.1961656\n2.8290353       -0.5588012\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(2,2,2)\nprint \ninput is :\n,input\n\nm1 = Mean()\nout = m1.forward(input)\nprint \noutput m1 is :\n,out\n\nm2 = Mean(2,1,True)\nout = m2.forward(input)\nprint \noutput m2 is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [[[ 0.01990713  0.37740696]\n  [ 0.67689963  0.67715705]]\n\n [[ 0.45685026  0.58995121]\n  [ 0.33405769  0.86351324]]]\ncreating: createMean\noutput m1 is : [array([[ 0.23837869,  0.48367909],\n       [ 0.50547862,  0.77033514]], dtype=float32)]\ncreating: createMean\noutput m2 is : [array([[ 0.34840336,  0.527282  ],\n       [ 0.39545399,  0.72673225]], dtype=float32)]\n\n\n\n\nPower\n\n\nScala:\n\n\nval module = Power(power, scale=1, shift=0)\n\n\n\n\nPython:\n\n\nmodule = Power(power, scale=1.0, shift=0.0)\n\n\n\n\nApply an element-wise power operation with scale and shift.\n\n\nf(x) = (shift + scale * x)^power^\n\n\npower\n the exponent.\n \nscale\n Default is 1.\n \nshift\n Default is 0.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval power = Power(2, 1, 1)\nval input = Tensor(Storage(Array(0.0, 1, 2, 3, 4, 5)), 1, Array(2, 3))\n\n print(power.forward(input))\n1.0     4.0      9.0    \n16.0        25.0     36.0   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\npower = Power(2.0, 1.0, 1.0)\ninput = np.array([[0.0, 1, 2], [3, 4, 5]])\n\npower.forward(input)\narray([[  1.,   4.,   9.],\n       [ 16.,  25.,  36.]], dtype=float32)\n\n\n\n\n\nCMul\n\n\nScala:\n\n\nval module = CMul(size, wRegularizer = null)\n\n\n\n\nPython:\n\n\nmodule = CMul(size, wRegularizer=None)\n\n\n\n\nThis layer has a weight tensor with given size. The weight will be multiplied element wise to\nthe input tensor. If the element number of the weight tensor match the input tensor, a simply\nelement wise multiply will be done. Or the bias will be expanded to the same size of the input.\nThe expand means repeat on unmatched singleton dimension(if some unmatched dimension isn't\nsingleton dimension, it will report an error). If the input is a batch, a singleton dimension\nwill be add to the first dimension before the expand.\n\n\nsize\n the size of the bias, which is an array of bias shape\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = CMul(Array(2, 1))\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =\n {i += 1; i})\n\n print(layer.forward(input))\n-0.29362988     -0.58725977     -0.88088965\n1.9482219       2.4352775       2.9223328\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = CMul([2,1])\ninput = np.array([[1, 2, 3], [4, 5, 6]])\n\nlayer.forward(input)\narray([[-0.17618844, -0.35237688, -0.52856529],\n       [ 0.85603124,  1.07003903,  1.28404689]], dtype=float32)\n\n\n\n\nAddConstant\n\n\nScala:\n\n\nval module = AddConstant(constant_scalar,inplace= false)\n\n\n\n\nPython:\n\n\nmodule = AddConstant(constant_scalar,inplace=False,bigdl_type=\nfloat\n)\n\n\n\n\nElement wise add a constant scalar to input tensor\n\n @param constant_scalar constant value\n\n @param inplace Can optionally do its operation in-place without using extra state memory\n\n\nScala example:\n\n\nval module = AddConstant(3.0)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.40684703      0.077655114     0.42314094\n0.55392265      0.8650696       0.3621729\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres11: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n3.406847        3.077655        3.423141\n3.5539227       3.8650696       3.3621728\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\n\nPython example:\n\n\nmodule = AddConstant(3.0,inplace=False,bigdl_type=\nfloat\n)\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 4.,  5.,  6.],\n[ 7.,  8.,  9.]], dtype=float32)]\n\n\n\n\nAbs\n\n\nScala:\n\n\nval m = Abs()\n\n\n\n\nPython:\n\n\nm = Abs()\n\n\n\n\nAn element-wise abs operation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval abs = new Abs\nval input = Tensor(2)\ninput(1) = 21f\ninput(2) = -29f\nprint(abs.forward(input))\n\n\n\n\noutput is:\u300021.0\u300029.0\n\n\nPython example:\n\n\nabs = Abs()\ninput = np.array([21, -29, 30])\nprint(abs.forward(input))\n\n\n\n\noutput is: [array([ 21.,  29.,  30.], dtype=float32)]\n\n\nLog\n\n\nScala:\n\n\nval log = Log()\n\n\n\n\nPython:\n\n\nlog = Log()\n\n\n\n\nThe Log module applies a log transformation to the input data\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval log = Log()\nval input = Tensor(T(1.0f, Math.E.toFloat))\nval gradOutput = Tensor(T(1.0f, 1.0f))\nval output = log.forward(input)\nval gradient = log.backward(input, gradOutput)\n-\n print(output)\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-\n print(gradient)\n1.0\n0.36787945\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nimport math\nlog = Log()\ninput = np.array([1.0, math.e])\ngrad_output = np.array([1.0, 1.0])\noutput = log.forward(input)\ngradient = log.backward(input, grad_output)\n\n-\n print output\n[ 0.  1.]\n\n-\n print gradient\n[ 1.          0.36787945]\n\n\n\n\nSum\n\n\nScala:\n\n\nval m = Sum(dimension=1,nInputDims=-1,sizeAverage=false,squeeze=true)\n\n\n\n\nPython:\n\n\nm = Sum(dimension=1,n_input_dims=-1,size_average=False,squeeze=True)\n\n\n\n\nSum is a module that simply applies a sum operation over the given dimension - specified by the argument \ndimension\n (starting from 1). \n\n\nThe input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using \nnInputDims\n.  When input is one tensor, do not specify \nnInputDims\n or set it = -1, otherwise input will be interpreted as batch of tensors. \n\n\nScala example:\n\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Sum(2)\nval output1 = m1.forward(input)\nval m2 = Sum(2, 1, true)\nval output2 = m2.forward(input)\n\nscala\n print(input)\n(1,.,.) =\n-0.003314678    0.96401167\n0.79000163      0.78624517\n\n(2,.,.) =\n-0.29975495     0.24742787\n0.8709072       0.4381108\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala\n print(output1)\n0.78668696      1.7502568\n0.5711522       0.68553865\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala\n print(output2)\n0.39334348      0.8751284\n0.2855761       0.34276932\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,2,2)\nprint \ninput is :\n,input\nmodule = Sum(2)\nout = module.forward(input)\nprint \noutput 1 is :\n,out\nmodule = Sum(2,1,True)\nout = module.forward(input)\nprint \noutput 2 is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [[[ 0.7194801   0.99120677]\n  [ 0.07446639  0.056318  ]]\n\n [[ 0.08639016  0.17173268]\n  [ 0.71686986  0.30503663]]]\ncreating: createSum\noutput 1 is : [array([[ 0.7939465 ,  1.04752481],\n       [ 0.80325997,  0.47676933]], dtype=float32)]\ncreating: createSum\noutput 2 is : [array([[ 0.39697325,  0.5237624 ],\n       [ 0.40162998,  0.23838466]], dtype=float32)]\n\n\n\n\nSqrt\n\n\nApply an element-wise sqrt operation.\n\n\nScala:\n\n\nval sqrt = new Sqrt\n\n\n\n\nPython:\n\n\nsqrt = Sqrt()\n\n\n\n\nApply an element-wise sqrt operation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Sqrt\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(3, 5).range(1, 15, 1)\nval sqrt = new Sqrt\nval output = sqrt.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(3, 5).range(2, 16, 1)\nval gradInput = sqrt.backward(input, gradOutput)\nprintln(gradOutput\n\n\n\n\nThe output will be,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.4142135       1.7320508       2.0     2.236068\n2.4494898       2.6457512       2.828427        3.0     3.1622777\n3.3166249       3.4641016       3.6055512       3.7416575       3.8729835\n\n\n\n\nThe gradInput will be,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0606601       1.1547005       1.25    1.3416407\n1.428869        1.5118579       1.5909902       1.6666667       1.7392527\n1.8090681       1.8763883       1.9414507       2.0044594       2.065591\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsqrt = Sqrt()\n\ninput = np.arange(1, 16, 1).astype(\nfloat32\n)\ninput = input.reshape(3, 5)\n\noutput = sqrt.forward(input)\nprint output\n\ngradOutput = np.arange(2, 17, 1).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(3, 5)\n\ngradInput = sqrt.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nThe output will be:\n\n\n[array([[ 1.        ,  1.41421354,  1.73205078,  2.        ,  2.23606801],\n       [ 2.44948983,  2.64575124,  2.82842708,  3.        ,  3.1622777 ],\n       [ 3.31662488,  3.46410155,  3.60555124,  3.7416575 ,  3.87298346]], dtype=float32)]\n\n\n\n\nThe gradInput will be:\n\n\n[array([[ 1.        ,  1.06066012,  1.15470052,  1.25      ,  1.34164071],\n       [ 1.42886901,  1.51185787,  1.59099019,  1.66666675,  1.73925269],\n       [ 1.80906808,  1.87638831,  1.94145072,  2.00445938,  2.0655911 ]], dtype=float32)]\n\n\n\n\nExp\n\n\nScala:\n\n\nval exp = Exp()\n\n\n\n\nPython:\n\n\nexp = Exp()\n\n\n\n\nExp applies element-wise exp operation to input tensor\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval exp = Exp()\nval input = Tensor(3, 3).rand()\n\n print(input)\n0.0858663   0.28117087  0.85724664  \n0.62026995  0.29137492  0.07581586  \n0.22099794  0.45131826  0.78286386  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n print(exp.forward(input))\n1.0896606   1.32468     2.356663    \n1.85943     1.3382663   1.078764    \n1.2473209   1.5703809   2.1877286   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nexp = Exp()\n\n exp.forward(np.array([[1, 2, 3],[1, 2, 3]]))\n[array([[  2.71828175,   7.38905621,  20.08553696],\n       [  2.71828175,   7.38905621,  20.08553696]], dtype=float32)]\n\n\n\n\n\nMax\n\n\nScala:\n\n\nval layer = Max(dim = 1, numInputDims = Int.MinValue)\n\n\n\n\nPython:\n\n\nlayer = Max(dim, num_input_dims=INTMIN)\n\n\n\n\nApplies a max operation over dimension \ndim\n.\n\n\nParameters:\n\n\ndim\n max along this dimension\n\n\nnumInputDims\n Optional. If in a batch model, set to the inputDims.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Max\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Max(1, 1)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nprintln(grad)\n0.0 0.0 3.0\n0.0 0.0 4.0\n0.0 0.0 5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nlayer = Max(1, 1)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[ 3.  4.  5.]\n\nprint grad\n[[ 0.  0.  3.]\n [ 0.  0.  4.]\n [ 0.  0.  5.]]\n``\n## CAdd ##\n\n**Scala:**\n```scala\nval module = CAdd(size,bRegularizer=null)\n\n\n\n\nPython:\n\n\nmodule = CAdd(size,bRegularizer=None,bigdl_type=\nfloat\n)\n\n\n\n\nThis layer has a bias tensor with given size. The bias will be added element wise to the input\ntensor. If the element number of the bias tensor match the input tensor, a simply element wise\nwill be done. Or the bias will be expanded to the same size of the input. The expand means\nrepeat on unmatched singleton dimension(if some unmatched dimension isn't singleton dimension,\nit will report an error). If the input is a batch, a singleton dimension will be add to the first\ndimension before the expand.\n\n\n\n\n@param size the size of the bias \n\n\n\n\nScala example:\n\n\nval module = CAdd(Array(2, 1),bRegularizer=null)\nval input = Tensor(2, 3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.52146345      0.86262375      0.74210143\n0.15882674      0.026310394     0.28394955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres12: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.97027373      1.311434        1.1909117\n-0.047433108    -0.17994945     0.07768971\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nmodule = CAdd([2, 1],bRegularizer=None,bigdl_type=\nfloat\n)\ninput = np.random.rand(2, 3)\narray([[ 0.71239789,  0.65869477,  0.50425182],\n       [ 0.40333312,  0.64843273,  0.07286636]])\n\nmodule.forward(input)\narray([[ 0.89537328,  0.84167016,  0.68722725],\n       [ 0.1290929 ,  0.37419251, -0.20137388]], dtype=float32)\n\n\n\n\nCosine\n\n\nScala:\n\n\nval m = Cosine(inputSize, outputSize)\n\n\n\n\nPython:\n\n\nm = Cosine(input_size, output_size)\n\n\n\n\nCosine is a module used to  calculate the \ncosine similarity\n of the input to \noutputSize\n centers, i.e. this layer has the weights \nw_j\n, for \nj = 1,..,outputSize\n, where \nw_j\n are vectors of dimension \ninputSize\n.\n\n\nThe distance \ny_j\n between center \nj\n and input \nx\n is formulated as \ny_j = (x \u00b7 w_j) / ( || w_j || * || x || )\n.\n\n\nThe input given in \nforward(input)\n must be either a vector (1D tensor) or matrix (2D tensor). If the input is a\nvector, it must have the size of \ninputSize\n. If it is a matrix, then each row is assumed to be an input sample of given batch (the number of rows means the batch size and the number of columns should be equal to the \ninputSize\n).\n\n\nScala example:\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval m = Cosine(2, 3)\nval input = Tensor(3, 2).rand()\nval output = m.forward(input)\n\nscala\n print(input)\n0.48958543      0.38529378\n0.28814933      0.66979927\n0.3581584       0.67365724\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala\n print(output)\n0.998335        0.9098057       -0.71862763\n0.8496431       0.99756527      -0.2993874\n0.8901594       0.9999207       -0.37689084\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,3)\nprint \ninput is :\n,input\nmodule = Cosine(3,3)\nmodule.forward(input)\nprint \noutput is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [[ 0.31156943  0.85577626  0.4274042 ]\n [ 0.79744055  0.66431136  0.05657437]]\ncreating: createCosine\noutput is : [array([[-0.73284394, -0.28076306, -0.51965958],\n       [-0.9563939 , -0.42036989, -0.08060561]], dtype=float32)]\n\n\n\n\n\n\nMul\n\n\nScala:\n\n\nval module = Mul[Float]()\n\n\n\n\nPython:\n\n\nmodule = Mul()\n\n\n\n\nMultiply a singla scalar factor to the incoming data\n\n\n                 +----Mul----+\n input -----+---\n input * weight -----+----\n output\n\n\n\n\nScala example:\n\n\nval mul = Mul[Float]()\n\n\n print(mul.forward(Tensor(1, 5).rand()))\n-0.03212923     -0.019040342    -9.136753E-4    -0.014459004    -0.04096878\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5]\n\n\n\n\nPython example:\n\n\nmul = Mul()\ninput = np.random.uniform(0, 1, (1, 5)).astype(\nfloat32\n)\n\n\n mul.forward(input)\n[array([[ 0.72429317,  0.7377845 ,  0.09136307,  0.40439236,  0.29011244]], dtype=float32)]\n\n\n\n\n\nMulConstant\n\n\nScala:\n\n\nval layer = MulConstant(scalar, inplace)\n\n\n\n\nPython:\n\n\nlayer = MulConstant(const, inplace)\n\n\n\n\nMultiplies input Tensor by a (non-learnable) scalar constant.\nThis module is sometimes useful for debugging purposes.\n\n\nParameters:\n\n\n \nconstant\n - scalar constant\n\n \ninplace\n - Can optionally do its operation in-place without using extra state memory. Default: false\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n T(1.0f, 1.0f),\n T(1.0f, 1.0f))\n)\nval scalar = 2.0\nval module = MulConstant(scalar)\nval output = module.forward(input)\nval gradient = module.backward(input, gradOutput)\n-\n print(output)\n2.0     4.0     \n6.0     8.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n-\n print(gradient)\n2.0     2.0     \n2.0     2.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ngrad_output = np.array([\n           [1.0, 1.0],\n           [1.0, 1.0]\n         ])\nscalar = 2.0\nmodule = MulConstant(scalar)\noutput = module.forward(input)\ngradient = module.backward(input, grad_output)\n-\n print output\n[[ 2.  4.]\n [ 6.  8.]]\n-\n print gradient\n[[ 2.  2.]\n [ 2.  2.]]", 
            "title": "Math Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#scale", 
            "text": "Scala:  val m = Scale(Array(2, 1))  Python:  m = scale = Scale([2, 1])  Scale is the combination of cmul and cadd.  Scale(size).forward(input) == CAdd(size).forward(CMul(size).forward(input)) \nComputes the elementwise product of input and weight, with the shape of the weight \"expand\" to\nmatch the shape of the input.Similarly, perform a expand cdd bias and perform an elementwise add. output = input .* weight .+ bias (element wise)  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).fill(1f)\nprintln( input: )\nprintln(input)\nval scale = Scale(Array(2, 1))\nval weight = Tensor(2, 1).fill(2f)\nval bias = Tensor(2, 1).fill(3f)\nscale.setWeightsBias(Array(weight, bias))\nprintln( Weight: )\nprintln(weight)\nprintln( bias: )\nprintln(bias)\nprintln( output: )\nprint(scale.forward(input))  input:\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\nWeight:\n2.0 \n2.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\nbias:\n3.0 \n3.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\noutput:\n5.0 5.0 5.0 \n5.0 5.0 5.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\ninput = np.ones([2, 3])\nprint( input: )\nprint(input)\nscale = Scale([2, 1])\nweight = np.full([2, 1], 2)\nbias = np.full([2, 1], 3)\nprint( weight:  )\nprint(weight)\nprint( bias:  )\nprint(bias)\nscale.set_weights([weight, bias])\nprint( output:  )\nprint(scale.forward(input))  input:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\ncreating: createScale\nweight: \n[[2]\n [2]]\nbias: \n[[3]\n [3]]\noutput: \n[[ 5.  5.  5.]\n [ 5.  5.  5.]]", 
            "title": "Scale"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#min", 
            "text": "Scala:  val min = Min(dim, numInputDims)  Python:  min = Min(dim, num_input_dims)  Applies a min operation over dimension  dim .  Parameters:    dim  - A integer. The dimension to min along.   numInputDims  - An optional integer indicating the number of input dimensions.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval min = Min(2)\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n 1.0f,\n 1.0f\n))\nval output = min.forward(input)\nval gradient = min.backward(input, gradOutput)\n-  print(output)\n1.0\n3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-  print(gradient)\n1.0     0.0     \n1.0     0.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nmin = Min(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 4.0]\n])\n\ngrad_output = np.array([1.0, 1.0])\noutput = min.forward(input)\ngradient = min.backward(input, grad_output)\n-  print output\n[ 1.  3.]\n-  print gradient\n[[ 1.  0.]\n [ 1.  0.]]", 
            "title": "Min"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#add", 
            "text": "Scala:  val addLayer = Add[T](inputSize)  Python:  add_layer = Add(input_size)  A.K.A BiasAdd. This layer adds input tensor with a parameter tensor and output the result.\nIf the input is 1D, this layer just do a element-wise add. If the input has multiple dimentions,\nthis layer will treat the first dimension as batch dimension, resize the input tensor to a 2D \ntensor(batch-dimension x input_size) and do a broadcast add between the 2D tensor and the \nparameter.  Please note that the parameter will be trained in the back propagation.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval addLayer = Add[Float](4)\naddLayer.bias.set(Tensor[Float](T(1.0f, 2.0f, 3.0f, 4.0f)))\naddLayer.forward(Tensor[Float](T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))))\naddLayer.backward(Tensor[Float](T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))),\n    Tensor[Float](T(T(0.1f, 0.1f, 0.1f, 0.1f), T(0.3f, 0.3f, 0.3f, 0.3f))))  Its output should be  2.0     3.0     4.0     5.0\n4.0     5.0     6.0     7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n0.1     0.1     0.1     0.1\n0.3     0.3     0.3     0.3\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]  Python example:  from bigdl.nn.layer import Add\nimport numpy as np\n\nadd_layer = Add(4)\nadd_layer.set_weights([np.array([1.0, 2.0, 3.0, 4.0])])\nadd_layer.forward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]))\nadd_layer.backward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]),\n    np.array([[0.1, 0.1, 0.1, 0.1], [0.3, 0.3, 0.3, 0.3]]))  Its output should be  array([[ 2.,  3.,  4.,  5.],\n       [ 4.,  5.,  6.,  7.]], dtype=float32)\n\narray([[ 0.1       ,  0.1       ,  0.1       ,  0.1       ],\n       [ 0.30000001,  0.30000001,  0.30000001,  0.30000001]], dtype=float32)", 
            "title": "Add"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#bilinear", 
            "text": "Scala:  val layer = BiLinear(\n  inputSize1,\n  inputSize2,\n  outputSize,\n  biasRes = true,\n  wRegularizer = null,\n  bRegularizer = null)  Python:  layer = BiLinear(\n    input_size1,\n    input_size2,\n    output_size,\n    bias_res=True,\n    wRegularizer=None,\n    bRegularizer=None)  A bilinear transformation with sparse inputs.\nThe input tensor given in forward(input) is a table containing both inputs x_1 and x_2,\nwhich are tensors of size N x inputDimension1 and N x inputDimension2, respectively.  Parameters:  inputSize1    dimension of input x_1  inputSize2    dimension of input x_2  outputSize    output dimension  biasRes   The layer can be trained without biases by setting bias = false. otherwise true  wRegularizer  : instance of  Regularizer \n             (eg. L1 or L2 regularization), applied to the input weights matrices.  bRegularizer  : instance of  Regularizer \n             applied to the bias.  Scala example:  import com.intel.analytics.bigdl.nn.Bilinear\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Bilinear(3, 2, 3)\nval input1 = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\nval input2 = Tensor(T(\n  T(-2f, 3f),\n  T(-1f, 2f),\n  T(-3f, 4f)\n))\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n-0.14168167 -8.697224   -10.097688\n-0.20962894 -7.114827   -8.568602\n0.16706467  -19.751905  -24.516418\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n {\n    2: 13.411718    -18.695072\n       14.674414    -19.503393\n       13.9599  -17.271534\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n    1: -5.3747015   -17.803686  -17.558662\n       -2.413877    -8.373887   -8.346823\n       -2.239298    -11.249412  -14.537216\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }  Python example:  layer = Bilinear(3, 2, 3)\ninput_1 = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ninput_2 = np.array([\n  [-3.0, 4.0],\n  [-2.0, 3.0],\n  [-1.0, 2.0]\n])\n\ninput = [input_1, input_2]\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]\n\nprint grad\n[array([[ 11.86168194, -14.02727222,  -6.16624403],\n       [  6.72984409,  -7.96572971,  -2.89302039],\n       [  5.52902842,  -5.76724434,  -1.46646953]], dtype=float32), array([[ 13.22105694,  -4.6879468 ],\n       [ 14.39296341,  -6.71434498],\n       [ 20.93929482, -13.02455521]], dtype=float32)]", 
            "title": "BiLinear"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#clamp", 
            "text": "Scala:  val model = Clamp(min, max)  Python:  model = Clamp(min, max)  A kind of hard tanh activition function with integer min and max\n- param min min value\n- param max max value  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Clamp(-10, 10)\nval input = Tensor(2, 2, 2).rand()\nval output = model.forward(input)\n\nscala  print(input)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala  print(output)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  model = Clamp(-10, 10)\ninput = np.random.randn(2, 2, 2)\noutput = model.forward(input)  print(input)\n[[[-0.66763755  1.15392566]\n  [-2.10846048  0.46931736]]\n\n [[ 1.74174638 -1.04323311]\n  [-1.91858729  0.12624046]]]  print(output)\n[[[-0.66763753  1.15392566]\n  [-2.10846043  0.46931735]]\n\n [[ 1.74174643 -1.04323316]\n  [-1.91858733  0.12624046]]", 
            "title": "Clamp"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#square", 
            "text": "Scala:  val module = Square()  Python:  module = Square()  Square apply an element-wise square operation.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Square()\n\nprintln(module.forward(Tensor.range(1, 6, 1)))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n4.0\n9.0\n16.0\n25.0\n36.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 6]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Square()\nprint(module.forward(np.arange(1, 7, 1)))  Output is  [array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)]", 
            "title": "Square"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#mean", 
            "text": "Scala:  val m = Mean(dimension=1, nInputDims=-1, squeeze=true)  Python:  m = Mean(dimension=1,n_input_dims=-1, squeeze=True)  Mean is a module that simply applies a mean operation over the given dimension - specified by  dimension  (starting from 1).  The input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using  nInputDims .  When input is one tensor, do not specify  nInputDims  or set it = -1, otherwise input will be interpreted as batch of tensors.   Scala example:  scala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Mean()\nval output1 = m1.forward(input)\nval m2 = Mean(2,1,true)\nval output2 = m2.forward(input)\n\nscala  print(input)\n(1,.,.) =\n-0.52021635     -1.8250599\n-0.2321481      -2.5672712\n\n(2,.,.) =\n4.007425        -0.8705412\n1.6506456       -0.2470611\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala  print(output1)\n1.7436042       -1.3478005\n0.7092488       -1.4071661\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala  print(output2)\n-0.37618223     -2.1961656\n2.8290353       -0.5588012\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(2,2,2)\nprint  input is : ,input\n\nm1 = Mean()\nout = m1.forward(input)\nprint  output m1 is : ,out\n\nm2 = Mean(2,1,True)\nout = m2.forward(input)\nprint  output m2 is : ,out  produces output:  input is : [[[ 0.01990713  0.37740696]\n  [ 0.67689963  0.67715705]]\n\n [[ 0.45685026  0.58995121]\n  [ 0.33405769  0.86351324]]]\ncreating: createMean\noutput m1 is : [array([[ 0.23837869,  0.48367909],\n       [ 0.50547862,  0.77033514]], dtype=float32)]\ncreating: createMean\noutput m2 is : [array([[ 0.34840336,  0.527282  ],\n       [ 0.39545399,  0.72673225]], dtype=float32)]", 
            "title": "Mean"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#power", 
            "text": "Scala:  val module = Power(power, scale=1, shift=0)  Python:  module = Power(power, scale=1.0, shift=0.0)  Apply an element-wise power operation with scale and shift.  f(x) = (shift + scale * x)^power^  power  the exponent.\n  scale  Default is 1.\n  shift  Default is 0.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval power = Power(2, 1, 1)\nval input = Tensor(Storage(Array(0.0, 1, 2, 3, 4, 5)), 1, Array(2, 3))  print(power.forward(input))\n1.0     4.0      9.0    \n16.0        25.0     36.0   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\npower = Power(2.0, 1.0, 1.0)\ninput = np.array([[0.0, 1, 2], [3, 4, 5]]) power.forward(input)\narray([[  1.,   4.,   9.],\n       [ 16.,  25.,  36.]], dtype=float32)", 
            "title": "Power"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#cmul", 
            "text": "Scala:  val module = CMul(size, wRegularizer = null)  Python:  module = CMul(size, wRegularizer=None)  This layer has a weight tensor with given size. The weight will be multiplied element wise to\nthe input tensor. If the element number of the weight tensor match the input tensor, a simply\nelement wise multiply will be done. Or the bias will be expanded to the same size of the input.\nThe expand means repeat on unmatched singleton dimension(if some unmatched dimension isn't\nsingleton dimension, it will report an error). If the input is a batch, a singleton dimension\nwill be add to the first dimension before the expand.  size  the size of the bias, which is an array of bias shape  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = CMul(Array(2, 1))\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =  {i += 1; i})  print(layer.forward(input))\n-0.29362988     -0.58725977     -0.88088965\n1.9482219       2.4352775       2.9223328\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\nlayer = CMul([2,1])\ninput = np.array([[1, 2, 3], [4, 5, 6]]) layer.forward(input)\narray([[-0.17618844, -0.35237688, -0.52856529],\n       [ 0.85603124,  1.07003903,  1.28404689]], dtype=float32)", 
            "title": "CMul"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#addconstant", 
            "text": "Scala:  val module = AddConstant(constant_scalar,inplace= false)  Python:  module = AddConstant(constant_scalar,inplace=False,bigdl_type= float )  Element wise add a constant scalar to input tensor  @param constant_scalar constant value  @param inplace Can optionally do its operation in-place without using extra state memory  Scala example:  val module = AddConstant(3.0)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.40684703      0.077655114     0.42314094\n0.55392265      0.8650696       0.3621729\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres11: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n3.406847        3.077655        3.423141\n3.5539227       3.8650696       3.3621728\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = AddConstant(3.0,inplace=False,bigdl_type= float )\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 4.,  5.,  6.],\n[ 7.,  8.,  9.]], dtype=float32)]", 
            "title": "AddConstant"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#abs", 
            "text": "Scala:  val m = Abs()  Python:  m = Abs()  An element-wise abs operation.  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval abs = new Abs\nval input = Tensor(2)\ninput(1) = 21f\ninput(2) = -29f\nprint(abs.forward(input))  output is:\u300021.0\u300029.0  Python example:  abs = Abs()\ninput = np.array([21, -29, 30])\nprint(abs.forward(input))  output is: [array([ 21.,  29.,  30.], dtype=float32)]", 
            "title": "Abs"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#log", 
            "text": "Scala:  val log = Log()  Python:  log = Log()  The Log module applies a log transformation to the input data  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval log = Log()\nval input = Tensor(T(1.0f, Math.E.toFloat))\nval gradOutput = Tensor(T(1.0f, 1.0f))\nval output = log.forward(input)\nval gradient = log.backward(input, gradOutput)\n-  print(output)\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-  print(gradient)\n1.0\n0.36787945\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nimport math\nlog = Log()\ninput = np.array([1.0, math.e])\ngrad_output = np.array([1.0, 1.0])\noutput = log.forward(input)\ngradient = log.backward(input, grad_output)\n\n-  print output\n[ 0.  1.]\n\n-  print gradient\n[ 1.          0.36787945]", 
            "title": "Log"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#sum", 
            "text": "Scala:  val m = Sum(dimension=1,nInputDims=-1,sizeAverage=false,squeeze=true)  Python:  m = Sum(dimension=1,n_input_dims=-1,size_average=False,squeeze=True)  Sum is a module that simply applies a sum operation over the given dimension - specified by the argument  dimension  (starting from 1).   The input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using  nInputDims .  When input is one tensor, do not specify  nInputDims  or set it = -1, otherwise input will be interpreted as batch of tensors.   Scala example:  \nscala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Sum(2)\nval output1 = m1.forward(input)\nval m2 = Sum(2, 1, true)\nval output2 = m2.forward(input)\n\nscala  print(input)\n(1,.,.) =\n-0.003314678    0.96401167\n0.79000163      0.78624517\n\n(2,.,.) =\n-0.29975495     0.24742787\n0.8709072       0.4381108\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala  print(output1)\n0.78668696      1.7502568\n0.5711522       0.68553865\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala  print(output2)\n0.39334348      0.8751284\n0.2855761       0.34276932\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,2,2)\nprint  input is : ,input\nmodule = Sum(2)\nout = module.forward(input)\nprint  output 1 is : ,out\nmodule = Sum(2,1,True)\nout = module.forward(input)\nprint  output 2 is : ,out  produces output:  input is : [[[ 0.7194801   0.99120677]\n  [ 0.07446639  0.056318  ]]\n\n [[ 0.08639016  0.17173268]\n  [ 0.71686986  0.30503663]]]\ncreating: createSum\noutput 1 is : [array([[ 0.7939465 ,  1.04752481],\n       [ 0.80325997,  0.47676933]], dtype=float32)]\ncreating: createSum\noutput 2 is : [array([[ 0.39697325,  0.5237624 ],\n       [ 0.40162998,  0.23838466]], dtype=float32)]", 
            "title": "Sum"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#sqrt", 
            "text": "Apply an element-wise sqrt operation.  Scala:  val sqrt = new Sqrt  Python:  sqrt = Sqrt()  Apply an element-wise sqrt operation.  Scala example:  import com.intel.analytics.bigdl.nn.Sqrt\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(3, 5).range(1, 15, 1)\nval sqrt = new Sqrt\nval output = sqrt.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(3, 5).range(2, 16, 1)\nval gradInput = sqrt.backward(input, gradOutput)\nprintln(gradOutput  The output will be,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.4142135       1.7320508       2.0     2.236068\n2.4494898       2.6457512       2.828427        3.0     3.1622777\n3.3166249       3.4641016       3.6055512       3.7416575       3.8729835  The gradInput will be,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0606601       1.1547005       1.25    1.3416407\n1.428869        1.5118579       1.5909902       1.6666667       1.7392527\n1.8090681       1.8763883       1.9414507       2.0044594       2.065591  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsqrt = Sqrt()\n\ninput = np.arange(1, 16, 1).astype( float32 )\ninput = input.reshape(3, 5)\n\noutput = sqrt.forward(input)\nprint output\n\ngradOutput = np.arange(2, 17, 1).astype( float32 )\ngradOutput = gradOutput.reshape(3, 5)\n\ngradInput = sqrt.backward(input, gradOutput)\nprint gradInput  The output will be:  [array([[ 1.        ,  1.41421354,  1.73205078,  2.        ,  2.23606801],\n       [ 2.44948983,  2.64575124,  2.82842708,  3.        ,  3.1622777 ],\n       [ 3.31662488,  3.46410155,  3.60555124,  3.7416575 ,  3.87298346]], dtype=float32)]  The gradInput will be:  [array([[ 1.        ,  1.06066012,  1.15470052,  1.25      ,  1.34164071],\n       [ 1.42886901,  1.51185787,  1.59099019,  1.66666675,  1.73925269],\n       [ 1.80906808,  1.87638831,  1.94145072,  2.00445938,  2.0655911 ]], dtype=float32)]", 
            "title": "Sqrt"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#exp", 
            "text": "Scala:  val exp = Exp()  Python:  exp = Exp()  Exp applies element-wise exp operation to input tensor  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval exp = Exp()\nval input = Tensor(3, 3).rand()  print(input)\n0.0858663   0.28117087  0.85724664  \n0.62026995  0.29137492  0.07581586  \n0.22099794  0.45131826  0.78286386  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(exp.forward(input))\n1.0896606   1.32468     2.356663    \n1.85943     1.3382663   1.078764    \n1.2473209   1.5703809   2.1877286   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nexp = Exp()  exp.forward(np.array([[1, 2, 3],[1, 2, 3]]))\n[array([[  2.71828175,   7.38905621,  20.08553696],\n       [  2.71828175,   7.38905621,  20.08553696]], dtype=float32)]", 
            "title": "Exp"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#max", 
            "text": "Scala:  val layer = Max(dim = 1, numInputDims = Int.MinValue)  Python:  layer = Max(dim, num_input_dims=INTMIN)  Applies a max operation over dimension  dim .  Parameters:  dim  max along this dimension  numInputDims  Optional. If in a batch model, set to the inputDims.  Scala example:  import com.intel.analytics.bigdl.nn.Max\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Max(1, 1)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nprintln(grad)\n0.0 0.0 3.0\n0.0 0.0 4.0\n0.0 0.0 5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  layer = Max(1, 1)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[ 3.  4.  5.]\n\nprint grad\n[[ 0.  0.  3.]\n [ 0.  0.  4.]\n [ 0.  0.  5.]]\n``\n## CAdd ##\n\n**Scala:**\n```scala\nval module = CAdd(size,bRegularizer=null)  Python:  module = CAdd(size,bRegularizer=None,bigdl_type= float )  This layer has a bias tensor with given size. The bias will be added element wise to the input\ntensor. If the element number of the bias tensor match the input tensor, a simply element wise\nwill be done. Or the bias will be expanded to the same size of the input. The expand means\nrepeat on unmatched singleton dimension(if some unmatched dimension isn't singleton dimension,\nit will report an error). If the input is a batch, a singleton dimension will be add to the first\ndimension before the expand.   @param size the size of the bias    Scala example:  val module = CAdd(Array(2, 1),bRegularizer=null)\nval input = Tensor(2, 3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.52146345      0.86262375      0.74210143\n0.15882674      0.026310394     0.28394955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres12: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.97027373      1.311434        1.1909117\n-0.047433108    -0.17994945     0.07768971\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = CAdd([2, 1],bRegularizer=None,bigdl_type= float )\ninput = np.random.rand(2, 3)\narray([[ 0.71239789,  0.65869477,  0.50425182],\n       [ 0.40333312,  0.64843273,  0.07286636]])\n\nmodule.forward(input)\narray([[ 0.89537328,  0.84167016,  0.68722725],\n       [ 0.1290929 ,  0.37419251, -0.20137388]], dtype=float32)", 
            "title": "Max"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#cosine", 
            "text": "Scala:  val m = Cosine(inputSize, outputSize)  Python:  m = Cosine(input_size, output_size)  Cosine is a module used to  calculate the  cosine similarity  of the input to  outputSize  centers, i.e. this layer has the weights  w_j , for  j = 1,..,outputSize , where  w_j  are vectors of dimension  inputSize .  The distance  y_j  between center  j  and input  x  is formulated as  y_j = (x \u00b7 w_j) / ( || w_j || * || x || ) .  The input given in  forward(input)  must be either a vector (1D tensor) or matrix (2D tensor). If the input is a\nvector, it must have the size of  inputSize . If it is a matrix, then each row is assumed to be an input sample of given batch (the number of rows means the batch size and the number of columns should be equal to the  inputSize ).  Scala example:  scala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval m = Cosine(2, 3)\nval input = Tensor(3, 2).rand()\nval output = m.forward(input)\n\nscala  print(input)\n0.48958543      0.38529378\n0.28814933      0.66979927\n0.3581584       0.67365724\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala  print(output)\n0.998335        0.9098057       -0.71862763\n0.8496431       0.99756527      -0.2993874\n0.8901594       0.9999207       -0.37689084\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,3)\nprint  input is : ,input\nmodule = Cosine(3,3)\nmodule.forward(input)\nprint  output is : ,out  produces output:  input is : [[ 0.31156943  0.85577626  0.4274042 ]\n [ 0.79744055  0.66431136  0.05657437]]\ncreating: createCosine\noutput is : [array([[-0.73284394, -0.28076306, -0.51965958],\n       [-0.9563939 , -0.42036989, -0.08060561]], dtype=float32)]", 
            "title": "Cosine"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#mul", 
            "text": "Scala:  val module = Mul[Float]()  Python:  module = Mul()  Multiply a singla scalar factor to the incoming data                   +----Mul----+\n input -----+---  input * weight -----+----  output  Scala example:  val mul = Mul[Float]()  print(mul.forward(Tensor(1, 5).rand()))\n-0.03212923     -0.019040342    -9.136753E-4    -0.014459004    -0.04096878\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5]  Python example:  mul = Mul()\ninput = np.random.uniform(0, 1, (1, 5)).astype( float32 )  mul.forward(input)\n[array([[ 0.72429317,  0.7377845 ,  0.09136307,  0.40439236,  0.29011244]], dtype=float32)]", 
            "title": "Mul"
        }, 
        {
            "location": "/APIdocs/Layers/Math-Layers/merged-Math-Layers/#mulconstant", 
            "text": "Scala:  val layer = MulConstant(scalar, inplace)  Python:  layer = MulConstant(const, inplace)  Multiplies input Tensor by a (non-learnable) scalar constant.\nThis module is sometimes useful for debugging purposes.  Parameters:    constant  - scalar constant   inplace  - Can optionally do its operation in-place without using extra state memory. Default: false  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n T(1.0f, 1.0f),\n T(1.0f, 1.0f))\n)\nval scalar = 2.0\nval module = MulConstant(scalar)\nval output = module.forward(input)\nval gradient = module.backward(input, gradOutput)\n-  print(output)\n2.0     4.0     \n6.0     8.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n-  print(gradient)\n2.0     2.0     \n2.0     2.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ngrad_output = np.array([\n           [1.0, 1.0],\n           [1.0, 1.0]\n         ])\nscalar = 2.0\nmodule = MulConstant(scalar)\noutput = module.forward(input)\ngradient = module.backward(input, grad_output)\n-  print output\n[[ 2.  4.]\n [ 6.  8.]]\n-  print gradient\n[[ 2.  2.]\n [ 2.  2.]]", 
            "title": "MulConstant"
        }, 
        {
            "location": "/APIdocs/Layers/Padding_Layers/merged-Padding_Layers/", 
            "text": "SpatialZeroPadding\n\n\nScala:\n\n\nval spatialZeroPadding = SpatialZeroPadding(padLeft, padRight, padTop, padBottom)\n\n\n\n\nPython:\n\n\nspatialZeroPadding = SpatialZeroPadding(pad_left, pad_right, pad_top, pad_bottom)\n\n\n\n\nEach feature map of a given input is padded with specified number of zeros.\n\n\nIf padding values are negative, then input will be cropped.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)\nval input = Tensor(3, 3, 3).rand()\n\n print(input)\n(1,.,.) =\n0.9494078   0.31556255  0.8432871   \n0.0064580487    0.6487367   0.151881    \n0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.32691675  0.07487922  0.08813124  \n0.4564806   0.37191486  0.05507739  \n0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.068939745 0.040364727 0.4893642   \n0.39481318  0.17923461  0.15748173  \n0.87117475  0.9933199   0.6097995\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3x3]\n\n\n  print(spatialZeroPadding.forward(input))\n(1,.,.) =\n0.0 0.0064580487    0.6487367   0.151881    \n0.0 0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.0 0.4564806   0.37191486  0.05507739  \n0.0 0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.0 0.39481318  0.17923461  0.15748173  \n0.0 0.87117475  0.9933199   0.6097995   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2x4]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nspatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)\n\n spatialZeroPadding.forward(np.array([[[1, 2],[3, 4]],[[1, 2],[3, 4]]]))\n[array([[[ 0.,  3.,  4.]],\n       [[ 0.,  3.,  4.]]], dtype=float32)]\n\n\n\n\n\nPadding\n\n\nScala:\n\n\nval module = Padding(dim,pad,nInputDim,value=0.0,nIndex=1)\n\n\n\n\nPython:\n\n\nmodule = Padding(dim,pad,n_input_dim,value=0.0,n_index=1,bigdl_type=\nfloat\n)\n\n\n\n\nThis module adds pad units of padding to dimension dim of the input. If pad is negative,\npadding is added to the left, otherwise, it is added to the right of the dimension.\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using nInputDims.\n\n\n\n\n@param dim the dimension to be applied padding operation\n\n\n@param pad num of the pad units\n\n\n@param nInputDim specify the number of dimensions that this module will receive\n                  If it is more than the dimension of input tensors, the first dimension\n                  would be considered as batch size\n\n\n@param value padding value, default is 0\n\n\n\n\nScala example:\n\n\nval module = Padding(1,-1,3,value=0.0,nIndex=1)\nval input = Tensor(3,2,1).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.673425\n0.9350421\n\n(2,.,.) =\n0.35407698\n0.52607465\n\n(3,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2x1]\n\nmodule.forward(input)\nres14: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.0\n0.0\n\n(2,.,.) =\n0.673425\n0.9350421\n\n(3,.,.) =\n0.35407698\n0.52607465\n\n(4,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x1]\n\n\n\n\n\nPython example:\n\n\nmodule = Padding(1, -1, 3, value=0.0,n_index=1,bigdl_type=\nfloat\n)\ninput = np.random.rand(3, 2, 1)\narray([[[ 0.81505274],\n        [ 0.55769512]],\n\n       [[ 0.13193961],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394154]]])\n\nmodule.forward(input)\narray([[[ 0.        ],\n        [ 0.        ]],\n\n       [[ 0.81505275],\n        [ 0.55769515]],\n\n       [[ 0.1319396 ],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394153]]], dtype=float32)", 
            "title": "Padding Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Padding_Layers/merged-Padding_Layers/#spatialzeropadding", 
            "text": "Scala:  val spatialZeroPadding = SpatialZeroPadding(padLeft, padRight, padTop, padBottom)  Python:  spatialZeroPadding = SpatialZeroPadding(pad_left, pad_right, pad_top, pad_bottom)  Each feature map of a given input is padded with specified number of zeros.  If padding values are negative, then input will be cropped.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)\nval input = Tensor(3, 3, 3).rand()  print(input)\n(1,.,.) =\n0.9494078   0.31556255  0.8432871   \n0.0064580487    0.6487367   0.151881    \n0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.32691675  0.07487922  0.08813124  \n0.4564806   0.37191486  0.05507739  \n0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.068939745 0.040364727 0.4893642   \n0.39481318  0.17923461  0.15748173  \n0.87117475  0.9933199   0.6097995\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3x3]   print(spatialZeroPadding.forward(input))\n(1,.,.) =\n0.0 0.0064580487    0.6487367   0.151881    \n0.0 0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.0 0.4564806   0.37191486  0.05507739  \n0.0 0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.0 0.39481318  0.17923461  0.15748173  \n0.0 0.87117475  0.9933199   0.6097995   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2x4]  Python example:  from bigdl.nn.layer import *\nspatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)  spatialZeroPadding.forward(np.array([[[1, 2],[3, 4]],[[1, 2],[3, 4]]]))\n[array([[[ 0.,  3.,  4.]],\n       [[ 0.,  3.,  4.]]], dtype=float32)]", 
            "title": "SpatialZeroPadding"
        }, 
        {
            "location": "/APIdocs/Layers/Padding_Layers/merged-Padding_Layers/#padding", 
            "text": "Scala:  val module = Padding(dim,pad,nInputDim,value=0.0,nIndex=1)  Python:  module = Padding(dim,pad,n_input_dim,value=0.0,n_index=1,bigdl_type= float )  This module adds pad units of padding to dimension dim of the input. If pad is negative,\npadding is added to the left, otherwise, it is added to the right of the dimension.\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using nInputDims.   @param dim the dimension to be applied padding operation  @param pad num of the pad units  @param nInputDim specify the number of dimensions that this module will receive\n                  If it is more than the dimension of input tensors, the first dimension\n                  would be considered as batch size  @param value padding value, default is 0   Scala example:  val module = Padding(1,-1,3,value=0.0,nIndex=1)\nval input = Tensor(3,2,1).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.673425\n0.9350421\n\n(2,.,.) =\n0.35407698\n0.52607465\n\n(3,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2x1]\n\nmodule.forward(input)\nres14: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.0\n0.0\n\n(2,.,.) =\n0.673425\n0.9350421\n\n(3,.,.) =\n0.35407698\n0.52607465\n\n(4,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x1]  Python example:  module = Padding(1, -1, 3, value=0.0,n_index=1,bigdl_type= float )\ninput = np.random.rand(3, 2, 1)\narray([[[ 0.81505274],\n        [ 0.55769512]],\n\n       [[ 0.13193961],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394154]]])\n\nmodule.forward(input)\narray([[[ 0.        ],\n        [ 0.        ]],\n\n       [[ 0.81505275],\n        [ 0.55769515]],\n\n       [[ 0.1319396 ],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394153]]], dtype=float32)", 
            "title": "Padding"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/", 
            "text": "SpatialSubtractiveNormalization\n\n\nScala:\n\n\nval spatialSubtractiveNormalization = SpatialSubtractiveNormalization(nInputPlane = 1, kernel = null)\n\n\n\n\nPython:\n\n\nspatialSubtractiveNormalization = SpatialSubtractiveNormalization(n_input_plane=1, kernel=None)\n\n\n\n\nSpatialSubtractiveNormalization applies a spatial subtraction operation on a series of 2D inputs using kernel for computing the weighted average in a neighborhood.The neighborhood is defined for a local spatial region that is the size as kernel and across all features. For an input image, since there is only one feature, the region is only spatial. For an RGB image, the weighted average is taken over RGB channels and a spatial region.\n\n\nIf the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.\n\n\nThe kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.\n\n\nnInputPlane : number of input plane, default is 1.\nkernel : kernel tensor, default is a 9 x 9 tensor.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval kernel = Tensor(3, 3).rand()\n\n\n print(kernel)\n0.56141114  0.76815456  0.29409808  \n0.3599753   0.17142025  0.5243272   \n0.62450963  0.28084084  0.17154165  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\nval spatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)\n\nval input = Tensor(1, 1, 1, 5).rand()\n\n\n print(input)\n(1,1,.,.) =\n0.122356184 0.44442436  0.6394927   0.9349956   0.8226007   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x1x5]\n\n\n print(spatialSubtractiveNormalization.forward(input))\n(1,1,.,.) =\n-0.2427161  0.012936085 -0.08024883 0.15658027  -0.07613802 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1x5]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nkernel=np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\nspatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)\n\n  spatialSubtractiveNormalization.forward(np.array([[[[1, 2, 3, 4, 5]]]]))\n[array([[[[ 0.,  0.,  0.,  0.,  0.]]]], dtype=float32)]\n\n\n\n\n\n\nNormalize\n\n\nScala:\n\n\nval module = Normalize(p,eps=1e-10)\n\n\n\n\nPython:\n\n\nmodule = Normalize(p,eps=1e-10,bigdl_type=\nfloat\n)\n\n\n\n\nNormalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents\ndivision by zero when the input contains all zero elements (default = 1e-10).\nThe input can be 1d, 2d or 4d. If the input is 4d, it should follow the format (n, c, h, w) where n is the batch number,\nc is the channel number, h is the height and w is the width\n * @param p L_p norm\n * @param eps smoothing parameter\n\n\nScala example:\n\n\nval module = Normalize(2.0,eps=1e-10)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7075603       0.084298864     0.91339105\n0.22373432      0.8704987       0.6936567\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres8: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.6107763       0.072768        0.7884524\n0.19706465      0.76673317      0.61097115\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nmodule = Normalize(2.0,eps=1e-10,bigdl_type=\nfloat\n)\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 0.26726124,  0.53452247,  0.80178368],\n[ 0.45584232,  0.56980288,  0.68376344]], dtype=float32)]\n\n\n\n\nSpatialBatchNormalization\n\n\nScala:\n\n\nval module = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=true,\n                                           initWeight=null, initBias=null, initGradWeight=null, initGradBias=null)\n\n\n\n\nPython:\n\n\nmodule = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=True)\n\n\n\n\n\nThis file implements Batch Normalization as described in the paper:\n\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"\nby Sergey Ioffe, Christian Szegedy.\n\n\nThis implementation is useful for inputs coming from convolution layers.\nFor non-convolutional layers, see \nBatchNormalization\n\nThe operation implemented is:\n ``` \n        ( x - mean(x) )\n  y = -------------------- * gamma + beta\n       standard-deviation(x)\n\n\nwhere gamma and beta are learnable parameters.\n  The learning of gamma and beta is optional.\n\n\n`nOutput` output feature map number\n\n`eps` avoid divide zero\n\n`momentum` momentum for weight update\n\n`affine` affine operation on output or not\n\n`initWeight` initial weight tensor\n\n`initBias`  initial bias tensor\n\n`initGradWeight` initial gradient weight \n\n`initGradBias` initial gradient bias\n\n\n**Scala example:**\n```scala\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SpatialBatchNormalization(3, 1e-3)\nval input = Tensor(2, 3, 2, 2).randn()\n\n print(layer.forward(input))\n(1,1,.,.) =\n-0.21939678 -0.64394164 \n-0.03280549 0.13889995  \n\n(1,2,.,.) =\n0.48519397  0.40222475  \n-0.9339038  0.4131121   \n\n(1,3,.,.) =\n0.39790314  -0.040012743    \n-0.009540742    0.21598668  \n\n(2,1,.,.) =\n0.32008895  -0.23125978 \n0.4053611   0.26305377  \n\n(2,2,.,.) =\n-0.3810518  -0.34581286 \n0.14797378  0.21226381  \n\n(2,3,.,.) =\n0.2558251   -0.2211882  \n-0.59388477 -0.00508846 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = SpatialBatchNormalization(3, 1e-3)\ninput = np.random.rand(2,3,2,2)\n\nlayer.forward(input)\narray([[[[  5.70826093e-03,   9.06338100e-05],\n         [ -3.49177676e-03,   1.10401707e-02]],\n\n        [[  1.80168569e-01,  -8.87815133e-02],\n         [  2.11335659e-01,   2.11817324e-01]],\n\n        [[ -1.02916014e+00,   4.02444333e-01],\n         [ -1.72453150e-01,   5.31806648e-01]]],\n\n\n       [[[ -3.46255396e-03,  -1.37512591e-02],\n         [  3.84721952e-03,   1.93112865e-05]],\n\n        [[  4.65962708e-01,  -5.29752195e-01],\n         [ -2.28064612e-01,  -2.22685724e-01]],\n\n        [[  8.49217057e-01,  -9.03094828e-01],\n         [  8.56826544e-01,  -5.35586655e-01]]]], dtype=float32)\n\n\n\n\nSpatialDivisiveNormalization\n\n\nScala:\n\n\nval layer = SpatialDivisiveNormalization[Float]()\n\n\n\n\nPython:\n\n\nlayer = SpatialDivisiveNormalization()\n\n\n\n\nApplies a spatial division operation on a series of 2D inputs using kernel for\ncomputing the weighted average in a neighborhood. The neighborhood is defined for\na local spatial region that is the size as kernel and across all features. For\nan input image, since there is only one feature, the region is only spatial. For\nan RGB image, the weighted average is taken over RGB channels and a spatial region.\n\n\nIf the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.\n\n\nThe kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.\n\n\nScala example:\n\n\n\nval layer = SpatialDivisiveNormalization[Float]()\nval input = Tensor[Float](1, 5, 5).rand\nval gradOutput = Tensor[Float](1, 5, 5).rand\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)\n\n\n println(input)\nres19: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.4022106       0.6872489       0.9712838       0.7769542       0.771034\n0.97930336      0.61022973      0.65092266      0.9507807       0.3158211\n0.12607759      0.320569        0.9267993       0.47579524      0.63989824\n0.713135        0.30836385      0.009723447     0.67723924      0.24405171\n0.51036286      0.115807846     0.123513035     0.28398398      0.271164\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n println(output)\nres20: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.37849638      0.6467289       0.91401714      0.73114514      0.725574\n0.9215639       0.57425076      0.6125444       0.89472294      0.29720038\n0.11864409      0.30166835      0.8721555       0.4477425       0.60217\n0.67108876      0.2901828       0.009150156     0.6373094       0.2296625\n0.480272        0.10897984      0.11623074      0.26724035      0.25517625\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n println(gradInput)\nres21: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.09343022     -0.25612304     0.25756648      -0.66132677     -0.44575396\n0.052990615     0.7899354       0.27205157      0.028260134     0.23150417\n-0.115425855    0.21133065      0.53093016      -0.36421964     -0.102551565\n0.7222408       0.46287358      0.0010696054    0.26336592      -0.050598443\n0.03733714      0.2775169       -0.21430963     0.3175013       0.6600435\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n\n\n\nPython example:\n\n\nlayer = SpatialDivisiveNormalization()\ninput = np.random.uniform(0, 1, (1, 5, 5)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (1, 5, 5)).astype(\nfloat32\n)\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)\n\n\n output\n[array([[[ 0.30657911,  0.75221181,  0.2318386 ,  0.84053135,  0.24818985],\n         [ 0.32852787,  0.43504578,  0.0219258 ,  0.47856906,  0.31112722],\n         [ 0.12381417,  0.61807972,  0.90043157,  0.57342309,  0.65450585],\n         [ 0.00401461,  0.33700454,  0.79859954,  0.64382601,  0.51768768],\n         [ 0.38087726,  0.8963666 ,  0.7982524 ,  0.78525543,  0.09658573]]], dtype=float32)]\n\n gradInput\n[array([[[ 0.08059166, -0.4616771 ,  0.11626807,  0.30253756,  0.7333734 ],\n         [ 0.2633073 , -0.01641282,  0.40653706,  0.07766753, -0.0237394 ],\n         [ 0.10733987,  0.23385212, -0.3291783 , -0.12808481,  0.4035565 ],\n         [ 0.56126803,  0.49945205, -0.40531909, -0.18559581,  0.27156472],\n         [ 0.28016835,  0.03791744, -0.17803842, -0.27817759,  0.42473239]]], dtype=float32)]\n\n\n\n\nSpatialCrossMapLRN\n\n\nScala:\n\n\nval spatialCrossMapLRN = SpatialCrossMapLRN(size = 5, alpha  = 1.0, beta = 0.75, k = 1.0)\n\n\n\n\nPython:\n\n\nspatialCrossMapLRN = SpatialCrossMapLRN(size=5, alpha=1.0, beta=0.75, k=1.0)\n\n\n\n\nSpatialCrossMapLRN applies Spatial Local Response Normalization between different feature maps\n\n\n                             x_f\n  y_f =  -------------------------------------------------\n          (k+(alpha/size)* sum_{l=l1 to l2} (x_l^2^))^beta^\n\nwhere  l1 corresponds to `max(0,f-ceil(size/2))` and l2 to `min(F, f-ceil(size/2) + size)`, `F` is the number  of feature maps       \n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)\n\nval input = Tensor(2, 2, 2, 2).rand()\n\n\n print(input)\n(1,1,.,.) =\n0.42596373  0.20075735  \n0.10307904  0.7486494   \n\n(1,2,.,.) =\n0.9887414   0.3554662   \n0.6291069   0.53952795  \n\n(2,1,.,.) =\n0.41220918  0.5463298   \n0.40766734  0.08064394  \n\n(2,2,.,.) =\n0.58255607  0.027811589 \n0.47811228  0.3082057   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\n\n print(spatialCrossMapLRN.forward(input))\n(1,1,.,.) =\n0.42522463  0.20070718  \n0.10301625  0.74769455  \n\n(1,2,.,.) =\n0.98702586  0.35537735  \n0.6287237   0.5388398   \n\n(2,1,.,.) =\n0.41189456  0.5460847   \n0.4074261   0.08063166  \n\n(2,2,.,.) =\n0.5821114   0.02779911  \n0.47782937  0.3081588   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nspatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)\n\n spatialCrossMapLRN.forward(np.array([[[[1, 2],[3, 4]],[[5, 6],[7, 8]]],[[[9, 10],[11, 12]],[[13, 14],[15, 16]]]]))\n[array([[[[  0.96269381,   1.88782692],\n         [  2.76295042,   3.57862759]],\n\n        [[  4.81346893,   5.66348076],\n         [  6.44688463,   7.15725517]]],\n\n\n       [[[  6.6400919 ,   7.05574226],\n         [  7.41468   ,   7.72194815]],\n\n        [[  9.59124374,   9.87803936],\n         [ 10.11092758,  10.29593086]]]], dtype=float32)]\n\n\n\n\n\n\nBatchNormalization\n\n\nScala:\n\n\nval bn = BatchNormalization(nOutput, eps, momentum, affine)\n\n\n\n\nPython:\n\n\nbn = BatchNormalization(n_output, eps, momentum, affine)\n\n\n\n\nThis layer implements Batch Normalization as described in the paper:\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n\nby Sergey Ioffe, Christian Szegedy\n\n\nThis implementation is useful for inputs NOT coming from convolution layers. For convolution layers, use nn.SpatialBatchNormalization.\n\n\nThe operation implemented is:\n\n\n              ( x - mean(x) )\n      y =  -------------------- * gamma + beta\n              standard-deviation(x)\n\n\n\n\nwhere gamma and beta are learnable parameters.The learning of gamma and beta is optional.\n\n\nParameters:\n\n\n \nnOutput\n - feature map number\n\n \neps\n - avoid divide zero. Default: 1e-5\n\n \nmomentum\n - momentum for weight update. Default: 0.1\n\n \naffine\n - affine operation on output or not. Default: true\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval bn = BatchNormalization(2)\nval input = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n            )\nval gradOutput = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n)\nval output = bn.forward(input)\nval gradient = bn.backward(input, gradOutput)\n-\n print(output) \n# There's random factor. An output could be\n-0.46433213     -0.2762179      \n0.46433213      0.2762179       \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n-\n print(gradient)\n# There's random factor. An output could be\n-4.649627E-6    -6.585548E-7    \n4.649627E-6     6.585548E-7     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nbn = BatchNormalization(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 6.0]\n])\ngrad_output = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = bn.forward(input)\ngradient = bn.backward(input, grad_output)\n-\n print output\n# There's random factor. An output could be\n[[-0.99583918 -0.13030811]\n [ 0.99583918  0.13030811]]\n-\n print gradient\n# There's random factor. An output could be\n[[ -9.97191637e-06  -1.55339364e-07]\n [  9.97191637e-06   1.55339364e-07]]", 
            "title": "Normalization Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/#spatialsubtractivenormalization", 
            "text": "Scala:  val spatialSubtractiveNormalization = SpatialSubtractiveNormalization(nInputPlane = 1, kernel = null)  Python:  spatialSubtractiveNormalization = SpatialSubtractiveNormalization(n_input_plane=1, kernel=None)  SpatialSubtractiveNormalization applies a spatial subtraction operation on a series of 2D inputs using kernel for computing the weighted average in a neighborhood.The neighborhood is defined for a local spatial region that is the size as kernel and across all features. For an input image, since there is only one feature, the region is only spatial. For an RGB image, the weighted average is taken over RGB channels and a spatial region.  If the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.  The kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.  nInputPlane : number of input plane, default is 1.\nkernel : kernel tensor, default is a 9 x 9 tensor.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval kernel = Tensor(3, 3).rand()  print(kernel)\n0.56141114  0.76815456  0.29409808  \n0.3599753   0.17142025  0.5243272   \n0.62450963  0.28084084  0.17154165  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\nval spatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)\n\nval input = Tensor(1, 1, 1, 5).rand()  print(input)\n(1,1,.,.) =\n0.122356184 0.44442436  0.6394927   0.9349956   0.8226007   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x1x5]  print(spatialSubtractiveNormalization.forward(input))\n(1,1,.,.) =\n-0.2427161  0.012936085 -0.08024883 0.15658027  -0.07613802 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1x5]  Python example:  from bigdl.nn.layer import *\nkernel=np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\nspatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)   spatialSubtractiveNormalization.forward(np.array([[[[1, 2, 3, 4, 5]]]]))\n[array([[[[ 0.,  0.,  0.,  0.,  0.]]]], dtype=float32)]", 
            "title": "SpatialSubtractiveNormalization"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/#normalize", 
            "text": "Scala:  val module = Normalize(p,eps=1e-10)  Python:  module = Normalize(p,eps=1e-10,bigdl_type= float )  Normalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents\ndivision by zero when the input contains all zero elements (default = 1e-10).\nThe input can be 1d, 2d or 4d. If the input is 4d, it should follow the format (n, c, h, w) where n is the batch number,\nc is the channel number, h is the height and w is the width\n * @param p L_p norm\n * @param eps smoothing parameter  Scala example:  val module = Normalize(2.0,eps=1e-10)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7075603       0.084298864     0.91339105\n0.22373432      0.8704987       0.6936567\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres8: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.6107763       0.072768        0.7884524\n0.19706465      0.76673317      0.61097115\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = Normalize(2.0,eps=1e-10,bigdl_type= float )\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 0.26726124,  0.53452247,  0.80178368],\n[ 0.45584232,  0.56980288,  0.68376344]], dtype=float32)]", 
            "title": "Normalize"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/#spatialbatchnormalization", 
            "text": "Scala:  val module = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=true,\n                                           initWeight=null, initBias=null, initGradWeight=null, initGradBias=null)  Python:  module = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=True)  This file implements Batch Normalization as described in the paper:\n\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"\nby Sergey Ioffe, Christian Szegedy.  This implementation is useful for inputs coming from convolution layers.\nFor non-convolutional layers, see  BatchNormalization \nThe operation implemented is:\n ``` \n        ( x - mean(x) )\n  y = -------------------- * gamma + beta\n       standard-deviation(x)  where gamma and beta are learnable parameters.\n  The learning of gamma and beta is optional.  `nOutput` output feature map number\n\n`eps` avoid divide zero\n\n`momentum` momentum for weight update\n\n`affine` affine operation on output or not\n\n`initWeight` initial weight tensor\n\n`initBias`  initial bias tensor\n\n`initGradWeight` initial gradient weight \n\n`initGradBias` initial gradient bias\n\n\n**Scala example:**\n```scala\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SpatialBatchNormalization(3, 1e-3)\nval input = Tensor(2, 3, 2, 2).randn()  print(layer.forward(input))\n(1,1,.,.) =\n-0.21939678 -0.64394164 \n-0.03280549 0.13889995  \n\n(1,2,.,.) =\n0.48519397  0.40222475  \n-0.9339038  0.4131121   \n\n(1,3,.,.) =\n0.39790314  -0.040012743    \n-0.009540742    0.21598668  \n\n(2,1,.,.) =\n0.32008895  -0.23125978 \n0.4053611   0.26305377  \n\n(2,2,.,.) =\n-0.3810518  -0.34581286 \n0.14797378  0.21226381  \n\n(2,3,.,.) =\n0.2558251   -0.2211882  \n-0.59388477 -0.00508846 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2x2]  Python example:  from bigdl.nn.layer import *\n\nlayer = SpatialBatchNormalization(3, 1e-3)\ninput = np.random.rand(2,3,2,2) layer.forward(input)\narray([[[[  5.70826093e-03,   9.06338100e-05],\n         [ -3.49177676e-03,   1.10401707e-02]],\n\n        [[  1.80168569e-01,  -8.87815133e-02],\n         [  2.11335659e-01,   2.11817324e-01]],\n\n        [[ -1.02916014e+00,   4.02444333e-01],\n         [ -1.72453150e-01,   5.31806648e-01]]],\n\n\n       [[[ -3.46255396e-03,  -1.37512591e-02],\n         [  3.84721952e-03,   1.93112865e-05]],\n\n        [[  4.65962708e-01,  -5.29752195e-01],\n         [ -2.28064612e-01,  -2.22685724e-01]],\n\n        [[  8.49217057e-01,  -9.03094828e-01],\n         [  8.56826544e-01,  -5.35586655e-01]]]], dtype=float32)", 
            "title": "SpatialBatchNormalization"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/#spatialdivisivenormalization", 
            "text": "Scala:  val layer = SpatialDivisiveNormalization[Float]()  Python:  layer = SpatialDivisiveNormalization()  Applies a spatial division operation on a series of 2D inputs using kernel for\ncomputing the weighted average in a neighborhood. The neighborhood is defined for\na local spatial region that is the size as kernel and across all features. For\nan input image, since there is only one feature, the region is only spatial. For\nan RGB image, the weighted average is taken over RGB channels and a spatial region.  If the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.  The kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.  Scala example:  \nval layer = SpatialDivisiveNormalization[Float]()\nval input = Tensor[Float](1, 5, 5).rand\nval gradOutput = Tensor[Float](1, 5, 5).rand\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)  println(input)\nres19: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.4022106       0.6872489       0.9712838       0.7769542       0.771034\n0.97930336      0.61022973      0.65092266      0.9507807       0.3158211\n0.12607759      0.320569        0.9267993       0.47579524      0.63989824\n0.713135        0.30836385      0.009723447     0.67723924      0.24405171\n0.51036286      0.115807846     0.123513035     0.28398398      0.271164\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  println(output)\nres20: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.37849638      0.6467289       0.91401714      0.73114514      0.725574\n0.9215639       0.57425076      0.6125444       0.89472294      0.29720038\n0.11864409      0.30166835      0.8721555       0.4477425       0.60217\n0.67108876      0.2901828       0.009150156     0.6373094       0.2296625\n0.480272        0.10897984      0.11623074      0.26724035      0.25517625\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  println(gradInput)\nres21: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.09343022     -0.25612304     0.25756648      -0.66132677     -0.44575396\n0.052990615     0.7899354       0.27205157      0.028260134     0.23150417\n-0.115425855    0.21133065      0.53093016      -0.36421964     -0.102551565\n0.7222408       0.46287358      0.0010696054    0.26336592      -0.050598443\n0.03733714      0.2775169       -0.21430963     0.3175013       0.6600435\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  Python example:  layer = SpatialDivisiveNormalization()\ninput = np.random.uniform(0, 1, (1, 5, 5)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (1, 5, 5)).astype( float32 )\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)  output\n[array([[[ 0.30657911,  0.75221181,  0.2318386 ,  0.84053135,  0.24818985],\n         [ 0.32852787,  0.43504578,  0.0219258 ,  0.47856906,  0.31112722],\n         [ 0.12381417,  0.61807972,  0.90043157,  0.57342309,  0.65450585],\n         [ 0.00401461,  0.33700454,  0.79859954,  0.64382601,  0.51768768],\n         [ 0.38087726,  0.8963666 ,  0.7982524 ,  0.78525543,  0.09658573]]], dtype=float32)]  gradInput\n[array([[[ 0.08059166, -0.4616771 ,  0.11626807,  0.30253756,  0.7333734 ],\n         [ 0.2633073 , -0.01641282,  0.40653706,  0.07766753, -0.0237394 ],\n         [ 0.10733987,  0.23385212, -0.3291783 , -0.12808481,  0.4035565 ],\n         [ 0.56126803,  0.49945205, -0.40531909, -0.18559581,  0.27156472],\n         [ 0.28016835,  0.03791744, -0.17803842, -0.27817759,  0.42473239]]], dtype=float32)]", 
            "title": "SpatialDivisiveNormalization"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/#spatialcrossmaplrn", 
            "text": "Scala:  val spatialCrossMapLRN = SpatialCrossMapLRN(size = 5, alpha  = 1.0, beta = 0.75, k = 1.0)  Python:  spatialCrossMapLRN = SpatialCrossMapLRN(size=5, alpha=1.0, beta=0.75, k=1.0)  SpatialCrossMapLRN applies Spatial Local Response Normalization between different feature maps                               x_f\n  y_f =  -------------------------------------------------\n          (k+(alpha/size)* sum_{l=l1 to l2} (x_l^2^))^beta^\n\nwhere  l1 corresponds to `max(0,f-ceil(size/2))` and l2 to `min(F, f-ceil(size/2) + size)`, `F` is the number  of feature maps         Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)\n\nval input = Tensor(2, 2, 2, 2).rand()  print(input)\n(1,1,.,.) =\n0.42596373  0.20075735  \n0.10307904  0.7486494   \n\n(1,2,.,.) =\n0.9887414   0.3554662   \n0.6291069   0.53952795  \n\n(2,1,.,.) =\n0.41220918  0.5463298   \n0.40766734  0.08064394  \n\n(2,2,.,.) =\n0.58255607  0.027811589 \n0.47811228  0.3082057   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]  print(spatialCrossMapLRN.forward(input))\n(1,1,.,.) =\n0.42522463  0.20070718  \n0.10301625  0.74769455  \n\n(1,2,.,.) =\n0.98702586  0.35537735  \n0.6287237   0.5388398   \n\n(2,1,.,.) =\n0.41189456  0.5460847   \n0.4074261   0.08063166  \n\n(2,2,.,.) =\n0.5821114   0.02779911  \n0.47782937  0.3081588   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2]  Python example:  from bigdl.nn.layer import *\nspatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)  spatialCrossMapLRN.forward(np.array([[[[1, 2],[3, 4]],[[5, 6],[7, 8]]],[[[9, 10],[11, 12]],[[13, 14],[15, 16]]]]))\n[array([[[[  0.96269381,   1.88782692],\n         [  2.76295042,   3.57862759]],\n\n        [[  4.81346893,   5.66348076],\n         [  6.44688463,   7.15725517]]],\n\n\n       [[[  6.6400919 ,   7.05574226],\n         [  7.41468   ,   7.72194815]],\n\n        [[  9.59124374,   9.87803936],\n         [ 10.11092758,  10.29593086]]]], dtype=float32)]", 
            "title": "SpatialCrossMapLRN"
        }, 
        {
            "location": "/APIdocs/Layers/Normalization_Layers/merged-Normalization_Layers/#batchnormalization", 
            "text": "Scala:  val bn = BatchNormalization(nOutput, eps, momentum, affine)  Python:  bn = BatchNormalization(n_output, eps, momentum, affine)  This layer implements Batch Normalization as described in the paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift \nby Sergey Ioffe, Christian Szegedy  This implementation is useful for inputs NOT coming from convolution layers. For convolution layers, use nn.SpatialBatchNormalization.  The operation implemented is:                ( x - mean(x) )\n      y =  -------------------- * gamma + beta\n              standard-deviation(x)  where gamma and beta are learnable parameters.The learning of gamma and beta is optional.  Parameters:    nOutput  - feature map number   eps  - avoid divide zero. Default: 1e-5   momentum  - momentum for weight update. Default: 0.1   affine  - affine operation on output or not. Default: true  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval bn = BatchNormalization(2)\nval input = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n            )\nval gradOutput = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n)\nval output = bn.forward(input)\nval gradient = bn.backward(input, gradOutput)\n-  print(output) \n# There's random factor. An output could be\n-0.46433213     -0.2762179      \n0.46433213      0.2762179       \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n-  print(gradient)\n# There's random factor. An output could be\n-4.649627E-6    -6.585548E-7    \n4.649627E-6     6.585548E-7     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nbn = BatchNormalization(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 6.0]\n])\ngrad_output = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = bn.forward(input)\ngradient = bn.backward(input, grad_output)\n-  print output\n# There's random factor. An output could be\n[[-0.99583918 -0.13030811]\n [ 0.99583918  0.13030811]]\n-  print gradient\n# There's random factor. An output could be\n[[ -9.97191637e-06  -1.55339364e-07]\n [  9.97191637e-06   1.55339364e-07]]", 
            "title": "BatchNormalization"
        }, 
        {
            "location": "/APIdocs/Layers/Dropout_Layers/merged-Dropout_Layers/", 
            "text": "Dropout\n\n\nScala:\n\n\nval module = Dropout(\n  initP = 0.5,\n  inplace = false,\n  scale = true)\n\n\n\n\nPython:\n\n\nmodule = Dropout(\n  init_p=0.5,\n  inplace=False,\n  scale=True)\n\n\n\n\nDropout masks(set to zero) parts of input using a bernoulli distribution.\nEach input element has a probability \ninitP\n of being dropped. If \nscale\n is\ntrue(true by default), the outputs are scaled by a factor of \n1/(1-initP)\n during training.\nDuring evaluating, output is the same as input.\n\n\nIt has been proven an effective approach for regularization and preventing\nco-adaptation of feature detectors. For more details, plese see\n[Improving neural networks by preventing co-adaptation of feature detectors]\n(https://arxiv.org/abs/1207.0580)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Dropout()\nval x = Tensor.range(1, 8, 1).resize(2, 4)\n\nprintln(module.forward(x))\nprintln(module.backward(x, x.clone().mul(0.5f))) // backward drops out the gradients at the same location.\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0     4.0     6.0     0.0\n10.0    12.0    0.0     16.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0    2.0    3.0    0.0\n5.0    6.0    0.0    8.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Dropout()\nx = np.arange(1, 9, 1).reshape(2, 4)\n\nprint(module.forward(x))\nprint(module.backward(x, x.copy() * 0.5)) # backward drops out the gradients at the same location.\n\n\n\n\nOutput is\n\n\n[array([[ 0.,  4.,  6.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]\n\n[array([[ 0.,  2.,  3.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]", 
            "title": "Dropout Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Dropout_Layers/merged-Dropout_Layers/#dropout", 
            "text": "Scala:  val module = Dropout(\n  initP = 0.5,\n  inplace = false,\n  scale = true)  Python:  module = Dropout(\n  init_p=0.5,\n  inplace=False,\n  scale=True)  Dropout masks(set to zero) parts of input using a bernoulli distribution.\nEach input element has a probability  initP  of being dropped. If  scale  is\ntrue(true by default), the outputs are scaled by a factor of  1/(1-initP)  during training.\nDuring evaluating, output is the same as input.  It has been proven an effective approach for regularization and preventing\nco-adaptation of feature detectors. For more details, plese see\n[Improving neural networks by preventing co-adaptation of feature detectors]\n(https://arxiv.org/abs/1207.0580)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Dropout()\nval x = Tensor.range(1, 8, 1).resize(2, 4)\n\nprintln(module.forward(x))\nprintln(module.backward(x, x.clone().mul(0.5f))) // backward drops out the gradients at the same location.  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0     4.0     6.0     0.0\n10.0    12.0    0.0     16.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0    2.0    3.0    0.0\n5.0    6.0    0.0    8.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Dropout()\nx = np.arange(1, 9, 1).reshape(2, 4)\n\nprint(module.forward(x))\nprint(module.backward(x, x.copy() * 0.5)) # backward drops out the gradients at the same location.  Output is  [array([[ 0.,  4.,  6.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]\n\n[array([[ 0.,  2.,  3.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]", 
            "title": "Dropout"
        }, 
        {
            "location": "/APIdocs/Layers/Distance_Layers/merged-Distance_Layers/", 
            "text": "PairwiseDistance\n\n\nScala:\n\n\nval pd = PairwiseDistance(norm=2)\n\n\n\n\nPython:\n\n\npd = PairwiseDistance(norm=2)\n\n\n\n\nIt is a module that takes a table of two vectors as input and outputs\nthe distance between them using the p-norm.\nThe input given in \nforward(input)\n is a [[Table]] that contains two tensors which\nmust be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,\nit must have the size of \ninputSize\n. If it is a matrix, then each row is assumed to be\nan input sample of the given batch (the number of rows means the batch size and\nthe number of columns should be equal to the \ninputSize\n).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.PairwiseDistance\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval pd = PairwiseDistance()\nval input1 = Tensor(3, 3).randn()\nval input2 = Tensor(3, 3).randn()\nval input = T(1 -\n input1, 2 -\n input2)\n\nval output = pd.forward(input)\n\nval gradOutput = Tensor(3).randn()\nval gradInput = pd.backward(input, gradOutput)\n\n\n\n\n\nThe ouotput is,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.155246\n1.1267666\n2.1415536\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.32565984\n-1.0108998\n-0.030873261\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\n\n\n\nThe gradInput is,\n\n\ngradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.012723052  0.31482473      0.08232752\n           0.7552968    -0.27292773     -0.6139655\n           0.0062761847 -0.018232936    -0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: -0.012723052 -0.31482473     -0.08232752\n           -0.7552968   0.27292773      0.6139655\n           -0.0062761847        0.018232936     0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npd = PairwiseDistance()\n\ninput1 = np.random.uniform(0, 1, [3, 3]).astype(\nfloat32\n)\ninput2 = np.random.uniform(0, 1, [3, 3]).astype(\nfloat32\n)\ninput1 = input1.reshape(3, 3)\ninput2 = input2.reshape(3, 3)\n\ninput = [input1, input2]\n\noutput = pd.forward(input)\nprint output\n\ngradOutput = np.random.uniform(0, 1, [3]).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(3)\n\ngradInput = pd.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nThe output is,\n\n\n[ 0.99588805  0.65620303  1.11735415]\n\n\n\n\nThe gradInput is,\n\n\n[array([[-0.27412388,  0.32756016, -0.02032043],\n       [-0.16920818,  0.60189474,  0.21347123],\n       [ 0.57771122,  0.28602061,  0.58044904]], dtype=float32), array([[ 0.27412388, -0.32756016,  0.02032043],\n       [ 0.16920818, -0.60189474, -0.21347123],\n       [-0.57771122, -0.28602061, -0.58044904]], dtype=float32)]\n\n\n\n\nCosineDistance\n\n\nScala:\n\n\nval module = CosineDistance()\n\n\n\n\nPython:\n\n\nmodule = CosineDistance()\n\n\n\n\nCosineDistance creates a module that takes a table of two vectors (or matrices if in batch mode) as input and outputs the cosine distance between them.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = CosineDistance()\nval t1 = Tensor().range(1, 3)\nval t2 = Tensor().range(4, 6)\nval input = T(t1, t2)\nval output = module.forward(input)\n\n\n input\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n    2: 4.0\n       5.0\n       6.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\n\n output\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9746319\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = CosineDistance()\nt1 = np.array([1.0, 2.0, 3.0])\nt2 = np.array([4.0, 5.0, 6.0])\ninput = [t1, t2]\noutput = module.forward(input)\n\n\n input\n[array([ 1.,  2.,  3.]), array([ 4.,  5.,  6.])]\n\n\n output\n[ 0.97463191]\n\n\n\n\nEuclidean\n\n\nScala:\n\n\nval module = Euclidean(\n  inputSize,\n  outputSize,\n  fastBackward = true)\n\n\n\n\nPython:\n\n\nmodule = Euclidean(\n  input_size,\n  output_size,\n  fast_backward=True)\n\n\n\n\nOutputs the Euclidean distance of the input to \noutputSize\n centers.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Euclidean(3, 3)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.0323668\n3.7177157\n3.8736997\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Euclidean(3, 3)\n\nprint(module.forward(np.arange(1, 4, 1)))\n\n\n\n\nOutput is\n\n\n[array([ 3.86203027,  4.02212906,  3.2648952 ], dtype=float32)]", 
            "title": "Distance Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Distance_Layers/merged-Distance_Layers/#pairwisedistance", 
            "text": "Scala:  val pd = PairwiseDistance(norm=2)  Python:  pd = PairwiseDistance(norm=2)  It is a module that takes a table of two vectors as input and outputs\nthe distance between them using the p-norm.\nThe input given in  forward(input)  is a [[Table]] that contains two tensors which\nmust be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,\nit must have the size of  inputSize . If it is a matrix, then each row is assumed to be\nan input sample of the given batch (the number of rows means the batch size and\nthe number of columns should be equal to the  inputSize ).  Scala example:  import com.intel.analytics.bigdl.nn.PairwiseDistance\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval pd = PairwiseDistance()\nval input1 = Tensor(3, 3).randn()\nval input2 = Tensor(3, 3).randn()\nval input = T(1 -  input1, 2 -  input2)\n\nval output = pd.forward(input)\n\nval gradOutput = Tensor(3).randn()\nval gradInput = pd.backward(input, gradOutput)  The ouotput is,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.155246\n1.1267666\n2.1415536\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.32565984\n-1.0108998\n-0.030873261\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]  The gradInput is,  gradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.012723052  0.31482473      0.08232752\n           0.7552968    -0.27292773     -0.6139655\n           0.0062761847 -0.018232936    -0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: -0.012723052 -0.31482473     -0.08232752\n           -0.7552968   0.27292773      0.6139655\n           -0.0062761847        0.018232936     0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npd = PairwiseDistance()\n\ninput1 = np.random.uniform(0, 1, [3, 3]).astype( float32 )\ninput2 = np.random.uniform(0, 1, [3, 3]).astype( float32 )\ninput1 = input1.reshape(3, 3)\ninput2 = input2.reshape(3, 3)\n\ninput = [input1, input2]\n\noutput = pd.forward(input)\nprint output\n\ngradOutput = np.random.uniform(0, 1, [3]).astype( float32 )\ngradOutput = gradOutput.reshape(3)\n\ngradInput = pd.backward(input, gradOutput)\nprint gradInput  The output is,  [ 0.99588805  0.65620303  1.11735415]  The gradInput is,  [array([[-0.27412388,  0.32756016, -0.02032043],\n       [-0.16920818,  0.60189474,  0.21347123],\n       [ 0.57771122,  0.28602061,  0.58044904]], dtype=float32), array([[ 0.27412388, -0.32756016,  0.02032043],\n       [ 0.16920818, -0.60189474, -0.21347123],\n       [-0.57771122, -0.28602061, -0.58044904]], dtype=float32)]", 
            "title": "PairwiseDistance"
        }, 
        {
            "location": "/APIdocs/Layers/Distance_Layers/merged-Distance_Layers/#cosinedistance", 
            "text": "Scala:  val module = CosineDistance()  Python:  module = CosineDistance()  CosineDistance creates a module that takes a table of two vectors (or matrices if in batch mode) as input and outputs the cosine distance between them.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = CosineDistance()\nval t1 = Tensor().range(1, 3)\nval t2 = Tensor().range(4, 6)\nval input = T(t1, t2)\nval output = module.forward(input)  input\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n    2: 4.0\n       5.0\n       6.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }  output\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9746319\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = CosineDistance()\nt1 = np.array([1.0, 2.0, 3.0])\nt2 = np.array([4.0, 5.0, 6.0])\ninput = [t1, t2]\noutput = module.forward(input)  input\n[array([ 1.,  2.,  3.]), array([ 4.,  5.,  6.])]  output\n[ 0.97463191]", 
            "title": "CosineDistance"
        }, 
        {
            "location": "/APIdocs/Layers/Distance_Layers/merged-Distance_Layers/#euclidean", 
            "text": "Scala:  val module = Euclidean(\n  inputSize,\n  outputSize,\n  fastBackward = true)  Python:  module = Euclidean(\n  input_size,\n  output_size,\n  fast_backward=True)  Outputs the Euclidean distance of the input to  outputSize  centers.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Euclidean(3, 3)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.0323668\n3.7177157\n3.8736997\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Euclidean(3, 3)\n\nprint(module.forward(np.arange(1, 4, 1)))  Output is  [array([ 3.86203027,  4.02212906,  3.2648952 ], dtype=float32)]", 
            "title": "Euclidean"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/", 
            "text": "RNN\n\n\nScala:\n\n\nval rnnCell = RnnCell[Double](inputSize, hiddenSize, activation, wRegularizer, uRegularizer, bRegularizer)\n\n\n\n\nPython:\n\n\nrnnCell = RnnCell(input_size, hidden_size, Tanh(), w_regularizer, u_regularizer, b_regularizer)\n\n\n\n\nImplementation of vanilla recurrent neural network cell\ni2h: weight matrix of input to hidden units\nh2h: weight matrix of hidden units to themselves through time\nThe updating is defined as:\nh_t = f(i2h * x_t + h2h * h_{t-1})\n\n\nParameters:\n\n\n \ninputSize\n - input size. Default: 4\n\n \nhiddenSize\n - hidden layer size. Default: 3\n\n \nactivation\n - activation function f for non-linearity\n\n \nwRegularizer\n - instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the input weights matrices. Default: null\n\n \nuRegularizer\n - instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null\n\n \nbRegularizer\n - instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the bias. Default: null\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(RnnCell(inputSize, hiddenSize, Tanh())))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n-\n print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.41442442      0.1663357       \n0.5339842       0.57332826      \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n-\n print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n1.1512008       2.181274        \n-0.4805725      1.6620052       \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(RnnCell(input_size, hidden_size, Tanh()))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-\n print output\n# There's random factor. An output could be\n[[[-0.67860311  0.80307233]\n  [-0.77462083  0.97191858]]]\n\n-\n print gradient\n# There's random factor. An output could be\n[[[-0.90771425  1.24791598]\n  [-0.70141178  0.97821164]]]\n\n\n\n\nRecurrent\n\n\nScala:\n\n\nval module = Recurrent()\n\n\n\n\nPython:\n\n\nmodule = Recurrent()\n\n\n\n\nRecurrent module is a container of rnn cells. Different types of rnn cells can be added using add() function.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 5\nval module = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\nval input = Tensor(Array(1, 5, inputSize))\nfor (i \n- 1 to 5) {\n  val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0)*inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n}\n\nval output = module.forward(input)\n\n\n input\n(1,.,.) =\n0.0 1.0 0.0 0.0 0.0\n0.0 1.0 0.0 0.0 0.0\n0.0 0.0 1.0 0.0 0.0\n0.0 1.0 0.0 0.0 0.0\n0.0 0.0 1.0 0.0 0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x5]\n\n\n output\n(1,.,.) =\n-0.44992247 -0.50529593 -0.033753205    -0.29562786\n-0.19734861 -0.5647412  0.07520321  -0.35515767\n-0.6771096  -0.4985356  -0.5806829  -0.47552463\n-0.06949129 -0.53153497 0.11510986  -0.34098053\n-0.71635246 -0.5226476  -0.5929389  -0.46533492\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 5\nmodule = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\ninput = np.zeros((1, 5, 5))\ninput[0][0][4] = 1\ninput[0][1][0] = 1\ninput[0][2][4] = 1\ninput[0][3][3] = 1\ninput[0][4][0] = 1\n\noutput = module.forward(input)\n\n\n output\n[array([[[ 0.7526533 ,  0.29162994, -0.28749418, -0.11243925],\n         [ 0.33291328, -0.07243762, -0.38017112,  0.53216213],\n         [ 0.83854133,  0.07213539, -0.34503224,  0.33690596],\n         [ 0.44095358,  0.27467242, -0.05471399,  0.46601957],\n         [ 0.451913  , -0.33519334, -0.61357468,  0.56650752]]], dtype=float32)]\n\n\n\n\nBiRecurrent\n\n\nScala:\n\n\nval module = BiRecurrent(merge=null)\n\n\n\n\nPython:\n\n\nmodule = BiRecurrent(merge=None,bigdl_type=\nfloat\n)\n\n\n\n\nThis layer implement a bidirectional recurrent neural network\n * @param merge concat or add the output tensor of the two RNNs. Default is add\n\n\nScala example:\n\n\nval module = BiRecurrent(CAddTable())\n.add(RnnCell(6, 4, Sigmoid()))\nval input = Tensor(Array(1, 2, 6)).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.55511624      0.44330198      0.9025551       0.26096714      0.3434667       0.20060952\n0.24903035      0.24026379      0.89252585      0.23025699      0.8131796       0.4013688\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x6]\n\nmodule.forward(input)\nres10: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.3577285       0.8861933       0.52908427      0.86278\n1.2850789       0.82549953      0.5560188       0.81468254\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4]\n\n\n\n\nPython example:\n\n\nmodule = BiRecurrent(CAddTable()).add(RnnCell(6, 4, Sigmoid()))\ninput = np.random.rand(1, 2, 6)\narray([[[ 0.75637438,  0.2642816 ,  0.61973312,  0.68565282,  0.73571443,\n          0.17167681],\n        [ 0.16439321,  0.06853251,  0.42257202,  0.42814042,  0.15706152,\n          0.57866659]]])\n\nmodule.forward(input)\narray([[[ 0.69091094,  0.97150528,  0.9562254 ,  1.14894259],\n        [ 0.83814102,  1.11358368,  0.96752423,  1.00913286]]], dtype=float32)\n\n\n\n\nLSTMPeephole\n\n\nScala:\n\n\nval model = LSTMPeephole(\n  inputSize = 4,\n  hiddenSize = 3,\n  p = 0.0,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null)\n\n\n\n\nPython:\n\n\nmodel = LSTMPeephole(\n  input_size,\n  hidden_size,\n  p=0.0,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None)\n\n\n\n\nLong Short Term Memory architecture with peephole.\nRef. A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)\nB. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf\nC. http://arxiv.org/pdf/1503.04069v1.pdf\nD. https://github.com/wojzaremba/lstm\n\n\n\n\nparam inputSize the size of each input vector\n\n\nparam hiddenSize Hidden unit size in the LSTM\n\n\nparam  p is used for [[Dropout]] probability. For more details about\n           RNN dropouts, please refer to\n           [RnnDrop: A Novel Dropout for RNNs in ASR]\n           (http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)\n           [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]\n           (https://arxiv.org/pdf/1512.05287.pdf)\n\n\nparam wRegularizer: instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nparam uRegularizer: instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n\nparam bRegularizer: instance of [[Regularizer]]\n          applied to the bias.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize))\nfor (b \n- 1 to batchSize) {\n  for (i \n- 1 to seqLength) {\n    val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0) * inputSize).toInt\n    input.setValue(b, i, rdmInput, 1.0f)\n  }\n}\n\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input).toTensor\n\nscala\n print(input)\n(1,.,.) =\n1.0 0.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n0.0 1.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n1.0 0.0 0.0 0.0 0.0 0.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x6]\n\nscala\n print(output)\n(1,.,.) =\n0.34764957  -0.31453514 -0.45646006 -0.42966008 -0.13651063 \n0.3624894   -0.2926056  -0.4347164  -0.40951455 -0.1775867  \n0.33391106  -0.29304913 -0.4748538  -0.45285955 -0.14919288 \n0.35499972  -0.29385415 -0.4419502  -0.42135617 -0.17544147 \n0.32911295  -0.30237123 -0.47175884 -0.4409852  -0.15733294 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 6\noutputSize = 5\nseqLength = 5\nbatchSize = 1\n\ninput = np.random.randn(batchSize, seqLength, inputSize)\nrec = Recurrent(hiddenSize)\nmodel = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\noutput = model.forward(input)\n\n\n print(input)\n[[[ 0.73624017 -0.91135209 -0.30627796 -1.07902111 -1.13549159  0.52868762]\n  [-0.07251559 -0.45596589  1.64020513  0.53218623  1.37993166 -0.47724947]\n  [-1.24958366 -1.22220259 -0.52454306  0.17382396  1.77666173 -1.2961758 ]\n  [ 0.45407533  0.82944329  0.02155243  1.82168093 -0.06022129  2.23823013]\n  [ 1.09100802  0.28555387 -0.94312648  0.55774033 -0.54895792  0.79885853]]]\n\n\n print(output)\n[[[ 0.4034881  -0.26156989  0.46799076  0.06283229  0.11794794]\n  [ 0.37359846 -0.17925361  0.31623816  0.06038529  0.10813089]\n  [ 0.34150451 -0.16565879  0.25264332  0.1187657   0.05118144]\n  [ 0.40773875 -0.2028828   0.24765283  0.0986848   0.12132661]\n  [ 0.40263647 -0.22403356  0.38489845  0.04720671  0.1686969 ]]]\n\n\n\n\nGRU\n\n\nScala:\n\n\nval gru = GRU(inputSize, outputSize, p, wRegularizer, uRegularizer, bRegularizer)\n\n\n\n\nPython:\n\n\ngru = GRU(inputSize, outputSize, p, w_regularizer, u_regularizer, b_regularizer)\n\n\n\n\nGated Recurrent Units architecture. The first input in sequence uses zero value for cell and hidden state.\n\n\nRef.\n 1. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n 2. https://github.com/Element-Research/rnn/blob/master/GRU.lua\n\n\nParameters:\n\n\n \ninputSize\n - the size of each input vector\n\n \noutputSize\n - hidden unit size in GRU\n\n \np\n - is used for [[Dropout]] probability. For more details about\n          RNN dropouts, please refer to\n           \nRnnDrop: A Novel Dropout for RNNs in ASR\n\n            and \nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n. Default: 0.0\n\n \nwRegularizer\n - instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the input weights matrices. Default: null\n\n \nuRegularizer\n - instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null\n\n \nbRegularizer\n - instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the bias. Default: null\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(GRU(inputSize, hiddenSize)))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n\n-\n print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.3833429       0.0082434565    \n-0.041063666    -0.08152798     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n-\n print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n-0.7684499      -0.49320614     \n-0.98002595     -0.47857404     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(GRU(input_size, hidden_size))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-\n print output\n# There's random factor. An output could be\n[[[ 0.27857888  0.20263115]\n  [ 0.29470384  0.22594413]]]\n-\n print gradient\n[[[-0.32956457  0.27405274]\n  [-0.32718879  0.32963118]]]\n\n\n\n\nLSTM\n\n\nScala:\n\n\nval lstm = LSTM(inputSize, hiddenSize)\n\n\n\n\nPython:\n\n\nlstm = LSTM(input_size, hidden_size)\n\n\n\n\nLong Short Term Memory architecture.\n\n\nRef:\n\n\n\n\nhttp://arxiv.org/pdf/1303.5778v1 (blueprint for this module)\n\n\nhttp://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf\n\n\nhttp://arxiv.org/pdf/1503.04069v1.pdf\n\n\nhttps://github.com/wojzaremba/lstm\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.optim.SGD\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval seed = 100\n\nRNG.setSeed(seed)\nval input = Tensor(Array(1, seqLength, inputSize))\nval labels = Tensor(Array(1, seqLength))\nfor (i \n- 1 to seqLength) {\n  val rdmLabel = Math.ceil(RNG.uniform(0, 1) * outputSize).toInt\n  val rdmInput = Math.ceil(RNG.uniform(0, 1) * inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n  labels.setValue(1, i, rdmLabel)\n}\n\nprintln(input)\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(\n  rec.add(\n      LSTM(inputSize, hiddenSize))).add(\n        TimeDistributed(Linear(hiddenSize, outputSize)))\n\nval criterion = TimeDistributedCriterion(\n  CrossEntropyCriterion(), false)\n\nval sgd = new SGD(learningRate=0.1, learningRateDecay=5e-7, weightDecay=0.1, momentum=0.002)\n\nval (weight, grad) = model.getParameters()\n\nval output = model.forward(input).toTensor\nval _loss = criterion.forward(output, labels)\nmodel.zeroGradParameters()\nval gradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)\n\ndef feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n  val output = model.forward(input).toTensor\n  val _loss = criterion.forward(output, labels)\n  model.zeroGradParameters()\n  val gradInput = criterion.backward(output, labels)\n  model.backward(input, gradInput)\n  (_loss, grad)\n}\n\nvar loss: Array[Float] = null\nfor (i \n- 1 to 100) {\n  loss = sgd.optimize(feval, weight)._2\n  println(s\n${i}-th loss = ${loss(0)}\n)\n}\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nhidden_size = 4\ninput_size = 6\noutput_size = 5\nseq_length = 5\n\ninput = np.random.uniform(0, 1, [1, seq_length, input_size]).astype(\nfloat32\n)\nlabels = np.random.uniform(1, 5, [1, seq_length]).astype(\nint\n)\n\nprint labels\nprint input\n\nrec = Recurrent()\nrec.add(LSTM(input_size, hidden_size))\n\nmodel = Sequential()\nmodel.add(rec)\nmodel.add(TimeDistributed(Linear(hidden_size, output_size)))\n\ncriterion = TimeDistributedCriterion(CrossEntropyCriterion(), False)\n\nsgd = SGD(learningrate=0.1, learningrate_decay=5e-7)\n\nweight, grad = model.parameters()\n\noutput = model.forward(input)\nloss = criterion.forward(input, labels)\ngradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)", 
            "title": "Recurrent Layers"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/#rnn", 
            "text": "Scala:  val rnnCell = RnnCell[Double](inputSize, hiddenSize, activation, wRegularizer, uRegularizer, bRegularizer)  Python:  rnnCell = RnnCell(input_size, hidden_size, Tanh(), w_regularizer, u_regularizer, b_regularizer)  Implementation of vanilla recurrent neural network cell\ni2h: weight matrix of input to hidden units\nh2h: weight matrix of hidden units to themselves through time\nThe updating is defined as:\nh_t = f(i2h * x_t + h2h * h_{t-1})  Parameters:    inputSize  - input size. Default: 4   hiddenSize  - hidden layer size. Default: 3   activation  - activation function f for non-linearity   wRegularizer  - instance of  Regularizer (eg. L1 or L2 regularization), applied to the input weights matrices. Default: null   uRegularizer  - instance of  Regularizer (eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null   bRegularizer  - instance of  Regularizer (eg. L1 or L2 regularization), applied to the bias. Default: null  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(RnnCell(inputSize, hiddenSize, Tanh())))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n-  print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.41442442      0.1663357       \n0.5339842       0.57332826      \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n-  print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n1.1512008       2.181274        \n-0.4805725      1.6620052       \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(RnnCell(input_size, hidden_size, Tanh()))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-  print output\n# There's random factor. An output could be\n[[[-0.67860311  0.80307233]\n  [-0.77462083  0.97191858]]]\n\n-  print gradient\n# There's random factor. An output could be\n[[[-0.90771425  1.24791598]\n  [-0.70141178  0.97821164]]]", 
            "title": "RNN"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/#recurrent", 
            "text": "Scala:  val module = Recurrent()  Python:  module = Recurrent()  Recurrent module is a container of rnn cells. Different types of rnn cells can be added using add() function.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 5\nval module = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\nval input = Tensor(Array(1, 5, inputSize))\nfor (i  - 1 to 5) {\n  val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0)*inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n}\n\nval output = module.forward(input)  input\n(1,.,.) =\n0.0 1.0 0.0 0.0 0.0\n0.0 1.0 0.0 0.0 0.0\n0.0 0.0 1.0 0.0 0.0\n0.0 1.0 0.0 0.0 0.0\n0.0 0.0 1.0 0.0 0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x5]  output\n(1,.,.) =\n-0.44992247 -0.50529593 -0.033753205    -0.29562786\n-0.19734861 -0.5647412  0.07520321  -0.35515767\n-0.6771096  -0.4985356  -0.5806829  -0.47552463\n-0.06949129 -0.53153497 0.11510986  -0.34098053\n-0.71635246 -0.5226476  -0.5929389  -0.46533492\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 5\nmodule = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\ninput = np.zeros((1, 5, 5))\ninput[0][0][4] = 1\ninput[0][1][0] = 1\ninput[0][2][4] = 1\ninput[0][3][3] = 1\ninput[0][4][0] = 1\n\noutput = module.forward(input)  output\n[array([[[ 0.7526533 ,  0.29162994, -0.28749418, -0.11243925],\n         [ 0.33291328, -0.07243762, -0.38017112,  0.53216213],\n         [ 0.83854133,  0.07213539, -0.34503224,  0.33690596],\n         [ 0.44095358,  0.27467242, -0.05471399,  0.46601957],\n         [ 0.451913  , -0.33519334, -0.61357468,  0.56650752]]], dtype=float32)]", 
            "title": "Recurrent"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/#birecurrent", 
            "text": "Scala:  val module = BiRecurrent(merge=null)  Python:  module = BiRecurrent(merge=None,bigdl_type= float )  This layer implement a bidirectional recurrent neural network\n * @param merge concat or add the output tensor of the two RNNs. Default is add  Scala example:  val module = BiRecurrent(CAddTable())\n.add(RnnCell(6, 4, Sigmoid()))\nval input = Tensor(Array(1, 2, 6)).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.55511624      0.44330198      0.9025551       0.26096714      0.3434667       0.20060952\n0.24903035      0.24026379      0.89252585      0.23025699      0.8131796       0.4013688\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x6]\n\nmodule.forward(input)\nres10: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.3577285       0.8861933       0.52908427      0.86278\n1.2850789       0.82549953      0.5560188       0.81468254\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4]  Python example:  module = BiRecurrent(CAddTable()).add(RnnCell(6, 4, Sigmoid()))\ninput = np.random.rand(1, 2, 6)\narray([[[ 0.75637438,  0.2642816 ,  0.61973312,  0.68565282,  0.73571443,\n          0.17167681],\n        [ 0.16439321,  0.06853251,  0.42257202,  0.42814042,  0.15706152,\n          0.57866659]]])\n\nmodule.forward(input)\narray([[[ 0.69091094,  0.97150528,  0.9562254 ,  1.14894259],\n        [ 0.83814102,  1.11358368,  0.96752423,  1.00913286]]], dtype=float32)", 
            "title": "BiRecurrent"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/#lstmpeephole", 
            "text": "Scala:  val model = LSTMPeephole(\n  inputSize = 4,\n  hiddenSize = 3,\n  p = 0.0,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null)  Python:  model = LSTMPeephole(\n  input_size,\n  hidden_size,\n  p=0.0,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None)  Long Short Term Memory architecture with peephole.\nRef. A.: http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)\nB. http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf\nC. http://arxiv.org/pdf/1503.04069v1.pdf\nD. https://github.com/wojzaremba/lstm   param inputSize the size of each input vector  param hiddenSize Hidden unit size in the LSTM  param  p is used for [[Dropout]] probability. For more details about\n           RNN dropouts, please refer to\n           [RnnDrop: A Novel Dropout for RNNs in ASR]\n           (http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)\n           [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]\n           (https://arxiv.org/pdf/1512.05287.pdf)  param wRegularizer: instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.  param uRegularizer: instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.  param bRegularizer: instance of [[Regularizer]]\n          applied to the bias.   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize))\nfor (b  - 1 to batchSize) {\n  for (i  - 1 to seqLength) {\n    val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0) * inputSize).toInt\n    input.setValue(b, i, rdmInput, 1.0f)\n  }\n}\n\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input).toTensor\n\nscala  print(input)\n(1,.,.) =\n1.0 0.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n0.0 1.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n1.0 0.0 0.0 0.0 0.0 0.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x6]\n\nscala  print(output)\n(1,.,.) =\n0.34764957  -0.31453514 -0.45646006 -0.42966008 -0.13651063 \n0.3624894   -0.2926056  -0.4347164  -0.40951455 -0.1775867  \n0.33391106  -0.29304913 -0.4748538  -0.45285955 -0.14919288 \n0.35499972  -0.29385415 -0.4419502  -0.42135617 -0.17544147 \n0.32911295  -0.30237123 -0.47175884 -0.4409852  -0.15733294 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 6\noutputSize = 5\nseqLength = 5\nbatchSize = 1\n\ninput = np.random.randn(batchSize, seqLength, inputSize)\nrec = Recurrent(hiddenSize)\nmodel = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\noutput = model.forward(input)  print(input)\n[[[ 0.73624017 -0.91135209 -0.30627796 -1.07902111 -1.13549159  0.52868762]\n  [-0.07251559 -0.45596589  1.64020513  0.53218623  1.37993166 -0.47724947]\n  [-1.24958366 -1.22220259 -0.52454306  0.17382396  1.77666173 -1.2961758 ]\n  [ 0.45407533  0.82944329  0.02155243  1.82168093 -0.06022129  2.23823013]\n  [ 1.09100802  0.28555387 -0.94312648  0.55774033 -0.54895792  0.79885853]]]  print(output)\n[[[ 0.4034881  -0.26156989  0.46799076  0.06283229  0.11794794]\n  [ 0.37359846 -0.17925361  0.31623816  0.06038529  0.10813089]\n  [ 0.34150451 -0.16565879  0.25264332  0.1187657   0.05118144]\n  [ 0.40773875 -0.2028828   0.24765283  0.0986848   0.12132661]\n  [ 0.40263647 -0.22403356  0.38489845  0.04720671  0.1686969 ]]]", 
            "title": "LSTMPeephole"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/#gru", 
            "text": "Scala:  val gru = GRU(inputSize, outputSize, p, wRegularizer, uRegularizer, bRegularizer)  Python:  gru = GRU(inputSize, outputSize, p, w_regularizer, u_regularizer, b_regularizer)  Gated Recurrent Units architecture. The first input in sequence uses zero value for cell and hidden state.  Ref.\n 1. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n 2. https://github.com/Element-Research/rnn/blob/master/GRU.lua  Parameters:    inputSize  - the size of each input vector   outputSize  - hidden unit size in GRU   p  - is used for [[Dropout]] probability. For more details about\n          RNN dropouts, please refer to\n            RnnDrop: A Novel Dropout for RNNs in ASR \n            and  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . Default: 0.0   wRegularizer  - instance of  Regularizer (eg. L1 or L2 regularization), applied to the input weights matrices. Default: null   uRegularizer  - instance of  Regularizer (eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null   bRegularizer  - instance of  Regularizer (eg. L1 or L2 regularization), applied to the bias. Default: null  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(GRU(inputSize, hiddenSize)))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n\n-  print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.3833429       0.0082434565    \n-0.041063666    -0.08152798     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n-  print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n-0.7684499      -0.49320614     \n-0.98002595     -0.47857404     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(GRU(input_size, hidden_size))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-  print output\n# There's random factor. An output could be\n[[[ 0.27857888  0.20263115]\n  [ 0.29470384  0.22594413]]]\n-  print gradient\n[[[-0.32956457  0.27405274]\n  [-0.32718879  0.32963118]]]", 
            "title": "GRU"
        }, 
        {
            "location": "/APIdocs/Layers/Recurrent_Layers/merged-Recurrent_Layers/#lstm", 
            "text": "Scala:  val lstm = LSTM(inputSize, hiddenSize)  Python:  lstm = LSTM(input_size, hidden_size)  Long Short Term Memory architecture.  Ref:   http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)  http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf  http://arxiv.org/pdf/1503.04069v1.pdf  https://github.com/wojzaremba/lstm   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.optim.SGD\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval seed = 100\n\nRNG.setSeed(seed)\nval input = Tensor(Array(1, seqLength, inputSize))\nval labels = Tensor(Array(1, seqLength))\nfor (i  - 1 to seqLength) {\n  val rdmLabel = Math.ceil(RNG.uniform(0, 1) * outputSize).toInt\n  val rdmInput = Math.ceil(RNG.uniform(0, 1) * inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n  labels.setValue(1, i, rdmLabel)\n}\n\nprintln(input)\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(\n  rec.add(\n      LSTM(inputSize, hiddenSize))).add(\n        TimeDistributed(Linear(hiddenSize, outputSize)))\n\nval criterion = TimeDistributedCriterion(\n  CrossEntropyCriterion(), false)\n\nval sgd = new SGD(learningRate=0.1, learningRateDecay=5e-7, weightDecay=0.1, momentum=0.002)\n\nval (weight, grad) = model.getParameters()\n\nval output = model.forward(input).toTensor\nval _loss = criterion.forward(output, labels)\nmodel.zeroGradParameters()\nval gradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)\n\ndef feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n  val output = model.forward(input).toTensor\n  val _loss = criterion.forward(output, labels)\n  model.zeroGradParameters()\n  val gradInput = criterion.backward(output, labels)\n  model.backward(input, gradInput)\n  (_loss, grad)\n}\n\nvar loss: Array[Float] = null\nfor (i  - 1 to 100) {\n  loss = sgd.optimize(feval, weight)._2\n  println(s ${i}-th loss = ${loss(0)} )\n}  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nhidden_size = 4\ninput_size = 6\noutput_size = 5\nseq_length = 5\n\ninput = np.random.uniform(0, 1, [1, seq_length, input_size]).astype( float32 )\nlabels = np.random.uniform(1, 5, [1, seq_length]).astype( int )\n\nprint labels\nprint input\n\nrec = Recurrent()\nrec.add(LSTM(input_size, hidden_size))\n\nmodel = Sequential()\nmodel.add(rec)\nmodel.add(TimeDistributed(Linear(hidden_size, output_size)))\n\ncriterion = TimeDistributedCriterion(CrossEntropyCriterion(), False)\n\nsgd = SGD(learningrate=0.1, learningrate_decay=5e-7)\n\nweight, grad = model.parameters()\n\noutput = model.forward(input)\nloss = criterion.forward(input, labels)\ngradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)", 
            "title": "LSTM"
        }, 
        {
            "location": "/APIdocs/Layers/Utilities/merged-Utilities/", 
            "text": "Echo\n\n\nScala:\n\n\nval module = Echo()\n\n\n\n\nPython:\n\n\nmodule = Echo()\n\n\n\n\nThis module is for debug purpose, which can print activation and gradient size in your model topology\n\n\nScala example:\n\n\nval module = Echo()\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nmodule.forward(input)\nres13: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\n\n\n\nPython example:\n\n\nmodule = Echo()\ninput = np.random.rand(3,2)\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]\n\nmodule.forward(input)\ncom.intel.analytics.bigdl.nn.Echo@535c681 : Activation size is 3x2\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]", 
            "title": "Utilities"
        }, 
        {
            "location": "/APIdocs/Layers/Utilities/merged-Utilities/#echo", 
            "text": "Scala:  val module = Echo()  Python:  module = Echo()  This module is for debug purpose, which can print activation and gradient size in your model topology  Scala example:  val module = Echo()\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nmodule.forward(input)\nres13: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]  Python example:  module = Echo()\ninput = np.random.rand(3,2)\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]\n\nmodule.forward(input)\ncom.intel.analytics.bigdl.nn.Echo@535c681 : Activation size is 3x2\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]", 
            "title": "Echo"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/", 
            "text": "L1Cost\n\n\nScala:\n\n\nval layer = L1Cost[Float]()\n\n\n\n\nPython:\n\n\nlayer = L1Cost()\n\n\n\n\nCompute L1 norm for input, and sign of input\n\n\nScala example:\n\n\nval layer = L1Cost[Float]()\nval input = Tensor[Float](2, 2).rand\nval target = Tensor[Float](2, 2).rand\n\nval output = layer.forward(input, target)\nval gradInput = layer.backward(input, target)\n\n\n println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.48145306      0.476887\n0.23729686      0.5169516\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n println(target)\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.42999148      0.22272833\n0.49723643      0.17884709\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n println(output)\noutput: Float = 1.7125885\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nlayer = L1Cost()\n\ninput = np.random.uniform(0, 1, (2, 2)).astype(\nfloat32\n)\ntarget = np.random.uniform(0, 1, (2, 2)).astype(\nfloat32\n)\n\noutput = layer.forward(input, target)\ngradInput = layer.backward(input, target)\n\n\n output\n2.522411\n\n gradInput\n[array([[ 1.,  1.],\n        [ 1.,  1.]], dtype=float32)]\n\n\n\n\nTimeDistributedCriterion\n\n\nScala:\n\n\nval module = TimeDistributedCriterion(critrn, sizeAverage)\n\n\n\n\nPython:\n\n\nmodule = TimeDistributedCriterion(critrn, sizeAverage)\n\n\n\n\nThis class is intended to support inputs with 3 or more dimensions.\nApply Any Provided Criterion to every temporal slice of an input.\n\n\ncritrn\n embedded criterion\n\n\nsizeAverage\n whether to divide the sequence length. Default is false.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval criterion = ClassNLLCriterion[Double]()\nval layer = TimeDistributedCriterion[Double](criterion, true)\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404))).resize(3, 2, 3)\nval target = Tensor[Double](3, 2)\n    target(Array(1, 1)) = 1\n    target(Array(1, 2)) = 1\n    target(Array(2, 1)) = 2\n    target(Array(2, 2)) = 2\n    target(Array(3, 1)) = 3\n    target(Array(3, 2)) = 3\n\n print(layer.forward(input, target))\n0.8793184268272332\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\nlayer = TimeDistributedCriterion(criterion, True)\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404]).reshape(3,2,3)\ntarget = np.array([[1,1],[2,2],[3,3]])                      \n\nlayer.forward(input, target)\n0.8793184\n\n\n\n\nMarginRankingCriterion\n\n\nScala:\n\n\nval mse = new MarginRankingCriterion(margin=1.0, sizeAverage=true)\n\n\n\n\nPython:\n\n\nmse = MarginRankingCriterion(margin=1.0, size_average=true)\n\n\n\n\nCreates a criterion that measures the loss given an input x = {x1, x2},\na table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).\nIn batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size\nbatchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.\nIf y == 1 then it assumed the first input should be ranked higher (have a larger value) than\nthe second input, and vice-versa for y == -1.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.MarginRankingCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nimport scala.util.Random\n\nval input1Arr = Array(1, 2, 3, 4, 5)\nval input2Arr = Array(5, 4, 3, 2, 1)\n\nval target1Arr = Array(-1, 1, -1, 1, 1)\n\nval input1 = Tensor(Storage(input1Arr.map(x =\n x.toFloat)))\nval input2 = Tensor(Storage(input2Arr.map(x =\n x.toFloat)))\n\nval input = T((1.toFloat, input1), (2.toFloat, input2))\n\nval target1 = Tensor(Storage(target1Arr.map(x =\n x.toFloat)))\nval target = T((1.toFloat, target1))\n\nval mse = new MarginRankingCriterion()\n\nval output = mse.forward(input, target)\nval gradInput = mse.backward(input, target)\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\nThe output will be,\n\n\noutput: Float = 0.8                                                                                                                                                                    [21/154]\n\n\n\n\nThe gradInput will be,\n\n\ngradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: -0.0\n           0.2\n           -0.2\n           0.0\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n        1: 0.0\n           -0.2\n           0.2\n           -0.0\n           -0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmse = MarginRankingCriterion()\n\ninput1 = np.array([1, 2, 3, 4, 5]).astype(\nfloat32\n)\ninput2 = np.array([5, 4, 3, 2, 1]).astype(\nfloat32\n)\ninput = [input1, input2]\n\ntarget1 = np.array([-1, 1, -1, 1, 1]).astype(\nfloat32\n)\ntarget = [target1, target1]\n\noutput = mse.forward(input, target)\ngradInput = mse.backward(input, target)\n\nprint output\nprint gradInput\n\n\n\n\nThe output will be,\n\n\n0.8\n\n\n\n\nThe gradInput will be,\n\n\n[array([ 0. , -0.2,  0.2, -0. , -0. ], dtype=float32), array([-0. ,  0.2, -0.2,  0. ,  0. ], dtype=float32)] \n\n\n\n\nClassNLLCriterion\n\n\nScala:\n\n\nval criterion = ClassNLLCriterion(weights = null, sizeAverage = true)\n\n\n\n\nPython:\n\n\ncriterion = ClassNLLCriterion(weights=None, size_average=True)\n\n\n\n\nThe negative log likelihood criterion. It is useful to train a classification problem with n\nclasses. If provided, the optional argument weights should be a 1D Tensor assigning weight to\neach of the classes. This is particularly useful when you have an unbalanced training set.\n\n\nThe input given through a \nforward()\n is expected to contain log-probabilities of each class:\ninput has to be a 1D Tensor of size \nn\n. Obtaining log-probabilities in a neural network is easily\nachieved by adding a \nLogSoftMax\n layer in the last layer of your neural network. You may use\n\nCrossEntropyCriterion\n instead, if you prefer not to add an extra layer to your network. This\ncriterion expects a class index (1 to the number of class) as target when calling\n\nforward(input, target)\n and \nbackward(input, target)\n.\n\n\nThe loss can be described as:\n     loss(x, class) = -x[class]\n or in the case of the weights argument it is specified as follows:\n     loss(x, class) = -weights[class] * x[class]\n Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when\n calculating losses in non-batch mode.\n\n\nBy default, the losses are averaged over observations for each minibatch. However, if the field\n \nsizeAverage\n is set to false, the losses are instead summed for each minibatch.\n\n\nParameters:\n\n\nweights\n     - weights of each element of the input\n\n\nsizeAverage\n - A boolean indicating whether normalizing by the number of elements in the input.\n                  Default: true\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.ClassNLLCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval criterion = ClassNLLCriterion()\nval input = Tensor(T(\n              T(1f, 2f, 3f),\n              T(2f, 3f, 4f),\n              T(3f, 4f, 5f)\n          ))\n\nval target = Tensor(T(1f, 2f, 3f))\n\nval loss = criterion.forward(input, target)\nval grad = criterion.backward(input, target)\n\nprint(loss)\n-3.0\nprintln(grad)\n-0.33333334 0.0 0.0\n0.0 -0.33333334 0.0\n0.0 0.0 -0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\ninput = np.array([\n              [1.0, 2.0, 3.0],\n              [2.0, 3.0, 4.0],\n              [3.0, 4.0, 5.0]\n          ])\n\ntarget = np.array([1.0, 2.0, 3.0])\n\nloss = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n\nprint loss\n-3.0\nprint gradient\n-3.0\n[[-0.33333334  0.          0.        ]\n [ 0.         -0.33333334  0.        ]\n [ 0.          0.         -0.33333334]]\n\n\n\n\nSoftmaxWithCriterion\n\n\nScala:\n\n\nval model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)\n\n\n\n\nPython:\n\n\nmodel = SoftmaxWithCriterion(ignoreLabel, normalizeMode)\n\n\n\n\nComputes the multinomial logistic loss for a one-of-many classification task, passing real-valued predictions through a softmax to\nget a probability distribution over classes. It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as \nits gradient computation is more numerically stable.\n\n\n\n\nparam ignoreLabel   (optional) Specify a label value that should be ignored when computing the loss.\n\n\nparam normalizeMode How to normalize the output loss.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\n\nval input = Tensor(1, 5, 2, 3).rand()\nval target = Tensor(Storage(Array(2.0f, 4.0f, 2.0f, 4.0f, 1.0f, 2.0f))).resize(1, 1, 2, 3)\n\nval model = SoftmaxWithCriterion[Float]()\nval output = model.forward(input, target)\n\nscala\n print(input)\n(1,1,.,.) =\n0.65131104  0.9332143   0.5618989   \n0.9965054   0.9370902   0.108070895 \n\n(1,2,.,.) =\n0.46066576  0.9636703   0.8123812   \n0.31076035  0.16386998  0.37894428  \n\n(1,3,.,.) =\n0.49111295  0.3704862   0.9938375   \n0.87996656  0.8695406   0.53354675  \n\n(1,4,.,.) =\n0.8502225   0.9033509   0.8518651   \n0.0692618   0.10121379  0.970959    \n\n(1,5,.,.) =\n0.9397213   0.49688303  0.75739735  \n0.25074655  0.11416598  0.6594504   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x2x3]\n\nscala\n print(output)\n1.6689054\n\n\n\n\nPython example:\n\n\ninput = np.random.randn(1, 5, 2, 3)\ntarget = np.array([[[[2.0, 4.0, 2.0], [4.0, 1.0, 2.0]]]])\n\nmodel = SoftmaxWithCriterion()\noutput = model.forward(input, target)\n\n\n print input\n[[[[ 0.78455689  0.01402084  0.82539628]\n   [-1.06448238  2.58168413  0.60053703]]\n\n  [[-0.48617618  0.44538094  0.46611658]\n   [-1.41509329  0.40038991 -0.63505732]]\n\n  [[ 0.91266769  1.68667933  0.92423611]\n   [ 0.1465411   0.84637557  0.14917515]]\n\n  [[-0.7060493  -2.02544114  0.89070726]\n   [ 0.14535539  0.73980064 -0.33130613]]\n\n  [[ 0.64538791 -0.44384233 -0.40112523]\n   [ 0.44346658 -2.22303621  0.35715986]]]]\n\n\n print output\n2.1002123\n\n\n\n\n\nSmoothL1Criterion\n\n\nScala:\n\n\nval slc = SmoothL1Criterion(sizeAverage=true)\n\n\n\n\nPython:\n\n\nslc = SmoothL1Criterion(size_average=True)\n\n\n\n\nCreates a criterion that can be thought of as a smooth version of the AbsCriterion.\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some\ncases prevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.SmoothL1Criterion\n\nval slc = SmoothL1Criterion()\n\nval inputArr = Array(\n  0.17503996845335,\n  0.83220188552514,\n  0.48450597329065,\n  0.64701424003579,\n  0.62694586534053,\n  0.34398410236463,\n  0.55356747563928,\n  0.20383032318205\n)\nval targetArr = Array(\n  0.69956525065936,\n  0.86074831243604,\n  0.54923197557218,\n  0.57388074393384,\n  0.63334444304928,\n  0.99680578662083,\n  0.49997645849362,\n  0.23869121982716\n)\n\nval input = Tensor(Storage(inputArr.map(x =\n x.toFloat))).reshape(Array(2, 2, 2))\nval target = Tensor(Storage(targetArr.map(x =\n x.toFloat))).reshape(Array(2, 2, 2))\n\nval output = slc.forward(input, target)\nval gradInput = slc.backward(input, target)\n\n\n\n\nThe output is,\n\n\noutput: Float = 0.0447365\n\n\n\n\nThe gradInput is,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.06556566     -0.003568299\n-0.008090746    0.009141691\n\n(2,.,.) =\n-7.998273E-4    -0.08160271\n0.0066988766    -0.0043576136\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nslc = SmoothL1Criterion()\n\ninput = np.array([\n    0.17503996845335,\n    0.83220188552514,\n    0.48450597329065,\n    0.64701424003579,\n    0.62694586534053,\n    0.34398410236463,\n    0.55356747563928,\n    0.20383032318205\n])\ninput.reshape(2, 2, 2)\n\ntarget = np.array([\n    0.69956525065936,\n    0.86074831243604,\n    0.54923197557218,\n    0.57388074393384,\n    0.63334444304928,\n    0.99680578662083,\n    0.49997645849362,\n    0.23869121982716\n])\n\ntarget.reshape(2, 2, 2)\n\noutput = slc.forward(input, target)\ngradInput = slc.backward(input, target)\n\nprint output\nprint gradInput\n\n\n\n\nSmoothL1CriterionWithWeights\n\n\nScala:\n\n\nval smcod = SmoothL1CriterionWithWeights[Float](sigma: Float = 2.4f, num: Int = 2)\n\n\n\n\nPython:\n\n\nsmcod = SmoothL1CriterionWithWeights(sigma, num)\n\n\n\n\na smooth version of the AbsCriterion\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some cases\nprevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).\n\n\n   d = (x - y) * w_in\n\n  loss(x, y, w_in, w_out)\n              | 0.5 * (sigma * d_i)^2 * w_out          if |d_i| \n 1 / sigma / sigma\n   = 1/n \\sum |\n              | (|d_i| - 0.5 / sigma / sigma) * w_out   otherwise\n\n\n\n\nScala example:\n\n\nval smcod = SmoothL1CriterionWithWeights[Float](2.4f, 2)\n\nval inputArr = Array(1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03)\nval targetArr = Array(0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58)\nval inWArr = Array(-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8)\nval outWArr = Array(-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1)\n\nval input = Tensor(Storage(inputArr.map(x =\n x.toFloat)))\nval target = T()\ntarget.insert(Tensor(Storage(targetArr.map(x =\n x.toFloat))))\ntarget.insert(Tensor(Storage(inWArr.map(x =\n x.toFloat))))\ntarget.insert(Tensor(Storage(outWArr.map(x =\n x.toFloat))))\n\nval output = smcod.forward(input, target)\nval gradInput = smcod.backward(input, target)\n\n\n println(output)\n  output: Float = -2.17488\n\n println(gradInput)\n-0.010944003\n0.425\n0.63037443\n-0.95\n-0.1\n0.07\n0.120000005\n-0.44000003\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 8]\n\n\n\n\nPython example:\n\n\nsmcod = SmoothL1CriterionWithWeights(2.4, 2)\n\ninput = np.array([1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03]).astype(\nfloat32\n)\ntargetArr = np.array([0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58]).astype(\nfloat32\n)\ninWArr = np.array([-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8]).astype(\nfloat32\n)\noutWArr = np.array([-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1]).astype(\nfloat32\n)\ntarget = [targetArr, inWArr, outWArr]\n\noutput = smcod.forward(input, target)\ngradInput = smcod.backward(input, target)\n\n\n output\n-2.17488\n\n gradInput\n[array([-0.010944  ,  0.42500001,  0.63037443, -0.94999999, -0.1       ,\n         0.07      ,  0.12      , -0.44000003], dtype=float32)]\n\n\n\n\nMultiMarginCriterion\n\n\nScala:\n\n\nval loss = MultiMarginCriterion(p=1,weights=null,margin=1.0,sizeAverage=true)\n\n\n\n\nPython:\n\n\nloss = MultiMarginCriterion(p=1,weights=None,margin=1.0,size_average=True)\n\n\n\n\nMultiMarginCriterion is a loss function that optimizes a multi-class classification hinge loss (margin-based loss) between input \nx\n and output \ny\n (\ny\n is the target class index).\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(3,2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f, 2.0f)))\nval loss = MultiMarginCriterion(1)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala\n print(input)\n-0.45896783     -0.80141246\n0.22560088      -0.13517438\n0.2601126       0.35492152\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala\n print(target)\n2.0\n1.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala\n print(output)\n0.4811434\n\nscala\n print(grad)\n0.16666667      -0.16666667\n-0.16666667     0.16666667\n0.16666667      -0.16666667\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(3,2)\ntarget = np.array([2,1,2])\nprint \ninput=\n,input\nprint \ntarget=\n,target\n\nloss = MultiMarginCriterion(1)\nout = loss.forward(input, target)\nprint \noutput of loss is : \n,out\n\ngrad_out = loss.backward(input,target)\nprint \ngrad out of loss is : \n,grad_out\n\n\n\n\nproduces output\n\n\ninput= [[ 0.46868305 -2.28562261]\n [ 0.8076243  -0.67809689]\n [-0.20342555 -0.66264743]]\ntarget= [2 1 2]\ncreating: createMultiMarginCriterion\noutput of loss is :  0.8689213\ngrad out of loss is :  [[ 0.16666667 -0.16666667]\n [ 0.          0.        ]\n [ 0.16666667 -0.16666667]]\n\n\n\n\n\n\nHingeEmbeddingCriterion\n\n\nScala:\n\n\nval m = HingeEmbeddingCriterion(margin = 1, sizeAverage = true)\n\n\n\n\nPython:\n\n\nm = HingeEmbeddingCriterion(margin=1, size_average=True)\n\n\n\n\nCreates a criterion that measures the loss given an input \nx\n which is a 1-dimensional vector and a label \ny\n (\n1\n or \n-1\n).\nThis is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.\n\n\n                 \u23a7 x_i,                  if y_i ==  1\nloss(x, y) = 1/n \u23a8\n                 \u23a9 max(0, margin - x_i), if y_i == -1\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval loss = HingeEmbeddingCriterion(1, sizeAverage = false)\nval input = Tensor(T(0.1f, 2.0f, 2.0f, 2.0f))\nprintln(\ninput: \\n\n + input)\nprintln(\nouput: \n)\n\nprintln(\nTarget=1: \n + loss.forward(input, Tensor(4, 1).fill(1f)))\n\nprintln(\nTarget=-1: \n + loss.forward(input, Tensor(4, 1).fill(-1f)))\n\n\n\n\ninput: \n0.1\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\nouput: \nTarget=1: 6.1\nTarget=-1: 0.9\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.criterion import *\ninput = np.array([0.1, 2.0, 2.0, 2.0])\ntarget = np.full(4, 1)\nprint(\ninput: \n )\nprint(input)\nprint(\ntarget: \n)\nprint(target)\nprint(\noutput: \n)\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, target))\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, np.full(4, -1)))\n\n\n\n\ninput: \n[ 0.1  2.   2.   2. ]\ntarget: \n[1 1 1 1]\noutput: \ncreating: createHingeEmbeddingCriterion\n6.1\ncreating: createHingeEmbeddingCriterion\n0.9\n\n\n\n\nMarginCriterion\n\n\nScala:\n\n\ncriterion = MarginCriterion(margin=1.0, sizeAverage=true)\n\n\n\n\nPython:\n\n\ncriterion = MarginCriterion(margin=1.0, sizeAverage=true, bigdl_type=\nfloat\n)\n\n\n\n\nCreates a criterion that optimizes a two-class classification hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.\n * @param margin if unspecified, is by default 1.\n * @param sizeAverage whether to average the loss, is by default true\n\n\nScala example:\n\n\nval criterion = MarginCriterion(margin=1.0, sizeAverage=true)\n\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.33753583      0.3575501\n0.23477706      0.7240361\n0.92835575      0.4737949\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nval target = Tensor(3, 2).rand()\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.27280563      0.7022703\n0.3348442       0.43332106\n0.08935371      0.17876455\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\ncriterion.forward(input, target)\nres5: Float = 0.84946966\n\n\n\n\nPython example:\n\n\ncriterion = MarginCriterion(margin=1.0,size_average=True,bigdl_type=\nfloat\n)\ninput = np.random.rand(3, 2)\narray([[ 0.20824672,  0.67299837],\n       [ 0.80561452,  0.19564743],\n       [ 0.42501441,  0.19408184]])\n\ntarget = np.random.rand(3, 2)\narray([[ 0.67882632,  0.61257846],\n       [ 0.10111138,  0.75225082],\n       [ 0.60404296,  0.31373273]])\n\ncriterion.forward(input, target)\n0.8166871\n\n\n\n\nCosineEmbeddingCriterion\n\n\nScala:\n\n\nval cosineEmbeddingCriterion = CosineEmbeddingCriterion(margin  = 0.0, sizeAverage = true)\n\n\n\n\nPython:\n\n\ncosineEmbeddingCriterion = CosineEmbeddingCriterion( margin=0.0,size_average=True)\n\n\n\n\nCosineEmbeddingCriterion creates a criterion that measures the loss given an input x = {x1, x2},\na table of two Tensors, and a Tensor label y with values 1 or -1.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimpot com.intel.analytics.bigdl.utils.T\nval cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, false)\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T()\ninput(1.0) = input1\ninput(2.0) = input2\nval target1 = Tensor(Storage(Array(-0.5f)))\nval target = T()\ntarget(1.0) = target1\n\n\n print(input)\n {\n    2.0: 0.4110882\n         0.57726574\n         0.1949834\n         0.67670715\n         0.16984987\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1.0: 0.16878392\n         0.24124223\n         0.8964794\n         0.11156334\n         0.5101486\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }\n\n\n print(cosineEmbeddingCriterion.forward(input, target))\n0.49919847\n\n\n print(cosineEmbeddingCriterion.backward(input, target))\n {\n    2: -0.045381278\n       -0.059856333\n       0.72547954\n       -0.2268434\n       0.3842142\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n    1: 0.30369008\n       0.42463788\n       -0.20637506\n       0.5712836\n       -0.06355385\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\ncosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, False)\n\n cosineEmbeddingCriterion.forward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n0.6363636\n\n cosineEmbeddingCriterion.backward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n[array([ 0.07933884,  0.04958678,  0.01983471, -0.00991735, -0.03966942], dtype=float32), array([-0.03966942, -0.00991735,  0.01983471,  0.04958678,  0.07933884], dtype=float32)]\n\n\n\n\n\nBCECriterion\n\n\nScala:\n\n\nval criterion = BCECriterion[Float]()\n\n\n\n\nPython:\n\n\ncriterion = BCECriterion()\n\n\n\n\nThis loss function measures the Binary Cross Entropy between the target and the output\n\n\n loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\n\n\n\n\nor in the case of the weights argument being specified:\n\n\n loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\n\n\n\n\nBy default, the losses are averaged for each mini-batch over observations as well as over\n dimensions. However, if the field sizeAverage is set to false, the losses are instead summed.\n\n\nScala example:\n\n\n\nval criterion = BCECriterion[Float]()\nval input = Tensor[Float](3, 1).rand\n\nval target = Tensor[Float](3)\ntarget(1) = 1\ntarget(2) = 0\ntarget(3) = 1\n\nval output = criterion.forward(input, target)\nval gradInput = criterion.backward(input, target)\n\n\n println(target)\nres25: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n println(output)\noutput: Float = 0.9009579\n\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-1.5277504\n1.0736246\n-0.336957\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]\n\n\n\n\n\nPython example:\n\n\n\ncriterion = BCECriterion()\ninput = np.random.uniform(0, 1, (3, 1)).astype(\nfloat32\n)\ntarget = np.array([1, 0, 1])\noutput = criterion.forward(input, target)\ngradInput = criterion.backward(input, target)\n\n\n output\n1.9218739\n\n gradInput\n[array([[-4.3074522 ],\n        [ 2.24244714],\n        [-1.22368968]], dtype=float32)]\n\n\n\n\n\nDiceCoefficientCriterion\n\n\nScala:\n\n\nval loss = DiceCoefficientCriterion(sizeAverage=true, epsilon=1.0f)\n\n\n\n\nPython:\n\n\nloss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)\n\n\n\n\nDiceCoefficientCriterion is the Dice-Coefficient objective function. \n\n\nBoth \nforward\n and \nbackward\n accept two tensors : input and target. The \nforward\n result is formulated as \n          \n1 - (2 * (input intersection target) / (input union target))\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DiceCoefficientCriterion(epsilon = 1.0f)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala\n print(input)\n-0.50278\n0.51387966\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(output)\n0.9958517\n\nscala\n print(grad)\n-0.99619853     -0.49758217\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1],dtype='float64')\n\nprint \ninput=\n, input\nprint \ntarget=\n, target\nloss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)\nout = loss.forward(input,target)\nprint \noutput of loss is :\n,out\n\ngrad_out = loss.backward(input,target)\nprint \ngrad out of loss is :\n,grad_out\n\n\n\n\nproduces output:\n\n\ninput= [ 0.4440505  2.9430301]\ntarget= [ 2.  1.]\ncreating: createDiceCoefficientCriterion\noutput of loss is : -0.17262316\ngrad out of loss is : [[-0.38274616 -0.11200322]]\n\n\n\n\nMSECriterion\n\n\nScala:\n\n\nval criterion = MSECriterion()\n\n\n\n\nPython:\n\n\ncriterion = MSECriterion()\n\n\n\n\nThe mean squared error criterion e.g. input: a, target: b, total elements: n\n\n\nloss(a, b) = 1/n * sum(|a_i - b_i|^2)\n\n\n\n\nParameters:\n\n\n\n\nsizeAverage\n - a boolean indicating whether to divide the sum of squared error by n.\n Default: true\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MSECriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(2.0f, 3.0f),\n T(4.0f, 5.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-\n print(output)\n1.0\n-\n print(gradient)\n-0.5    -0.5    \n-0.5    -0.5    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = MSECriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n-\n print output\n1.0\n-\n print gradient\n[[-0.5 -0.5]\n [-0.5 -0.5]]\n\n\n\n\nSoftMarginCriterion\n\n\nScala:\n\n\nval criterion = SoftMarginCriterion(sizeAverage)\n\n\n\n\nPython:\n\n\ncriterion = SoftMarginCriterion(size_average)\n\n\n\n\nCreates a criterion that optimizes a two-class classification logistic loss between\ninput x (a Tensor of dimension 1) and output y (which is a tensor containing either\n1s or -1s).\n\n\nloss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()\n\n\n\n\nParameters:\n\n* \nsizeAverage\n - A boolean indicating whether normalizing by the number of elements in the input.\n                    Default: true\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = SoftMarginCriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(1.0f, -1.0f),\n T(-1.0f, 1.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-\n print(output)\n1.3767318\n-\n print(gradient)\n-0.06723536     0.22019927      \n0.23814353      -0.0044965525   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = SoftMarginCriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-\n print output\n1.3767318\n-\n print gradient\n[[-0.06723536  0.22019927]\n [ 0.23814353 -0.00449655]]\n\n\n\n\nDistKLDivCriterion\n\n\nScala:\n\n\nval loss = DistKLDivCriterion[T](sizeAverage=true)\n\n\n\n\nPython:\n\n\nloss = DistKLDivCriterion(size_average=True)\n\n\n\n\nDistKLDivCriterion is the Kullback\u2013Leibler divergence loss.\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DistKLDivCriterion()\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala\n print(input)\n-0.3854126\n-0.7707398\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(output)\n1.4639297\n\nscala\n print(grad)\n-1.0\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1])\n\nprint \ninput=\n, input\nprint \ntarget=\n, target\nloss = DistKLDivCriterion()\nout = loss.forward(input,target)\nprint \noutput of loss is :\n,out\n\ngrad_out = loss.backward(input,target)\nprint \ngrad out of loss is :\n,grad_out\n\n\n\n\nproduces output:\n\n\ninput= [-1.14333924  0.97662296]\ntarget= [2 1]\ncreating: createDistKLDivCriterion\noutput of loss is : 1.348175\ngrad out of loss is : [-1.  -0.5]\n\n\n\n\nClassSimplexCriterion\n\n\nScala:\n\n\nval criterion = ClassSimplexCriterion(nClasses)\n\n\n\n\nPython:\n\n\ncriterion = ClassSimplexCriterion(nClasses)\n\n\n\n\nClassSimplexCriterion implements a criterion for classification.\nIt learns an embedding per class, where each class' embedding is a\npoint on an (N-1)-dimensional simplex, where N is the number of classes.\n\n\nParameters:\n\n* \nnClasses\n - An integer, the number of classes.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = ClassSimplexCriterion(5)\nval input = Tensor(T(\n T(1.0f, 2.0f, 3.0f, 4.0f, 5.0f),\n T(4.0f, 5.0f, 6.0f, 7.0f, 8.0f)\n))\nval target = Tensor(2)\ntarget(1) = 2.0f\ntarget(2) = 1.0f\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-\n print(output)\n23.562702\n-\n print(gradient)\n0.25    0.20635083      0.6     0.8     1.0     \n0.6     1.0     1.2     1.4     1.6     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = ClassSimplexCriterion(5)\ninput = np.array([\n   [1.0, 2.0, 3.0, 4.0, 5.0],\n   [4.0, 5.0, 6.0, 7.0, 8.0]\n])\ntarget = np.array([2.0, 1.0])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-\n print output\n23.562702\n-\n print gradient\n[[ 0.25        0.20635083  0.60000002  0.80000001  1.        ]\n [ 0.60000002  1.          1.20000005  1.39999998  1.60000002]]\n\n\n\n\nL1HingeEmbeddingCriterion\n\n\nScala:\n\n\nval model = L1HingeEmbeddingCriterion(margin)\n\n\n\n\nPython:\n\n\nmodel = L1HingeEmbeddingCriterion(margin)\n\n\n\n\nCreates a criterion that measures the loss given an input \nx = {x1, x2}\n, a table of two Tensors, and a label y (1 or -1).\nThis is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.\n\n\n             \u23a7 ||x1 - x2||_1,                  if y ==  1\nloss(x, y) = \u23a8\n             \u23a9 max(0, margin - ||x1 - x2||_1), if y == -1\n\n\n\n\nThe margin has a default value of 1, or can be set in the constructor.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = L1HingeEmbeddingCriterion(0.6)\nval input1 = Tensor(T(1.0f, -0.1f))\nval input2 = Tensor(T(2.0f, -0.2f))\nval input = T(input1, input2)\nval target = Tensor(1)\ntarget(Array(1)) = 1.0f\n\nval output = model.forward(input, target)\n\nscala\n print(output)\n1.1\n\n\n\n\nPython example:\n\n\nmodel = L1HingeEmbeddingCriterion(0.6)\ninput1 = np.array(1.0, -0.1)\ninput2 = np.array(2.0, -0.2)\ninput = [input1, input2]\ntarget = np.array([1.0])\n\noutput = model.forward(input, target)\n\n\n print output\n1.1\n\n\n\n\nCrossEntropyCriterion\n\n\nScala:\n\n\nval module = CrossEntropyCriterion(weights, sizeAverage)\n\n\n\n\nPython:\n\n\nmodule = CrossEntropyCriterion(weights, sizeAverage)\n\n\n\n\nThis criterion combines LogSoftMax and ClassNLLCriterion in one single class.\n\n\nweights\n A tensor assigning weight to each of the classes\n\n\nsizeAverage\n whether to divide the sequence length. Default is true.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval layer = CrossEntropyCriterion[Double]()\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404\n    ))).resize(3, 3)\nval target = Tensor[Double](3)\n    target(Array(1)) = 1\n    target(Array(2)) = 2\n    target(Array(3)) = 3\n\n print(layer.forward(input, target))\n0.9483051199107635\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\n\nlayer = CrossEntropyCriterion()\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404\n                      ]).reshape(3,3)\ntarget = np.array([1, 2, 3])                      \n\nlayer.forward(input, target)\n0.94830513\n\n\n\n\nParallelCriterion\n\n\nScala:\n\n\nval pc = ParallelCriterion(repeatTarget=false)\n\n\n\n\nPython:\n\n\npc = ParallelCriterion(repeat_target=False)\n\n\n\n\nParallelCriterion is a weighted sum of other criterions each applied to a different input\nand target. Set repeatTarget = true to share the target for criterions.\nUse add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.{ParallelCriterion, ClassNLLCriterion, MSECriterion}\n\nval pc = ParallelCriterion()\n\nval input = T(Tensor(2, 10), Tensor(2, 10))\nvar i = 0\ninput[Tensor](1).apply1(_ =\n {i += 1; i})\ninput[Tensor](2).apply1(_ =\n {i -= 1; i})\nval target = T(Tensor(Storage(Array(1.0f, 8.0f))), Tensor(2, 10).fill(1.0f))\n\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\npc.add(nll, 0.5).add(mse)\n\nval output = pc.forward(input, target)\nval gradInput = pc.backward(input, target)\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\n\nThe output is,\n\n\n100.75\n\n\n\n\n\nThe gradInput is,\n\n\n {\n        2: 1.8000001    1.7     1.6     1.5     1.4     1.3000001       1.2     1.1     1.0     0.90000004\n           0.8  0.7     0.6     0.5     0.4     0.3     0.2     0.1     0.0     -0.1\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n        1: -0.25        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n           0.0  0.0     0.0     0.0     0.0     0.0     0.0     -0.25   0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npc = ParallelCriterion()\n\ninput1 = np.arange(1, 21, 1).astype(\nfloat32\n)\ninput2 = np.arange(0, 20, 1).astype(\nfloat32\n)[::-1]\ninput1 = input1.reshape(2, 10)\ninput2 = input2.reshape(2, 10)\n\ninput = [input1, input2]\n\ntarget1 = np.array([1.0, 8.0]).astype(\nfloat32\n)\ntarget1 = target1.reshape(2)\ntarget2 = np.full([2, 10], 1).astype(\nfloat32\n)\ntarget2 = target2.reshape(2, 10)\ntarget = [target1, target2]\n\nnll = ClassNLLCriterion()\nmse = MSECriterion()\n\npc.add(nll, weight = 0.5).add(mse)\n\nprint \ninput = \\n %s \n % input\nprint \ntarget = \\n %s\n % target\n\noutput = pc.forward(input, target)\ngradInput = pc.backward(input, target)\n\nprint \noutput = %s \n % output\nprint \ngradInput = %s \n % gradInput\n\n\n\n\nThe console will output,\n\n\ninput = \n [array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n       [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]], dtype=float32), array([[ 19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,  10.],\n       [  9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.]], dtype=float32)] \ntarget = \n [array([ 1.,  8.], dtype=float32), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)]\noutput = 100.75 \ngradInput = [array([[-0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ]], dtype=float32), array([[ 1.80000007,  1.70000005,  1.60000002,  1.5       ,  1.39999998,\n         1.30000007,  1.20000005,  1.10000002,  1.        ,  0.90000004],\n       [ 0.80000001,  0.69999999,  0.60000002,  0.5       ,  0.40000001,\n         0.30000001,  0.2       ,  0.1       ,  0.        , -0.1       ]], dtype=float32)]\n\n\n\n\nMultiLabelMarginCriterion\n\n\nScala:\n\n\nval multiLabelMarginCriterion = MultiLabelMarginCriterion(sizeAverage = true)\n\n\n\n\nPython:\n\n\nmultiLabelMarginCriterion = MultiLabelMarginCriterion(size_average=True)\n\n\n\n\nMultiLabelMarginCriterion creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x and output y \n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval multiLabelMarginCriterion = MultiLabelMarginCriterion(false)\nval input = Tensor(4).rand()\nval target = Tensor(4)\ntarget(Array(1)) = 3\ntarget(Array(2)) = 2\ntarget(Array(3)) = 1\ntarget(Array(4)) = 0\n\n\n print(input)\n0.40267515\n0.5913795\n0.84936756\n0.05999674\n\n\n  print(multiLabelMarginCriterion.forward(input, target))\n0.33414197\n\n\n print(multiLabelMarginCriterion.backward(input, target))\n-0.25\n-0.25\n-0.25\n0.75\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nmultiLabelMarginCriterion = MultiLabelMarginCriterion(False)\n\n\n multiLabelMarginCriterion.forward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n0.975\n\n\n multiLabelMarginCriterion.backward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n[array([-0.25, -0.25, -0.25,  0.75], dtype=float32)]\n\n\n\n\n\nMultiLabelSoftMarginCriterion\n\n\nScala:\n\n\nval criterion = MultiLabelSoftMarginCriterion(weights = null, sizeAverage = true)\n\n\n\n\nPython:\n\n\ncriterion = MultiLabelSoftMarginCriterion(weights=None, size_average=True)\n\n\n\n\nMultiLabelSoftMarginCriterion is a multiLabel multiclass criterion based on sigmoid:\n\n\nl(x,y) = - sum_i y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i])\n\n\n\n\nwhere \np[i] = exp(x[i]) / (1 + exp(x[i]))\n\n\nIf with weights,\n \nl(x,y) = - sum_i weights[i] (y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i]))\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiLabelSoftMarginCriterion()\nval input = Tensor(3)\ninput(Array(1)) = 0.4f\ninput(Array(2)) = 0.5f\ninput(Array(3)) = 0.6f\nval target = Tensor(3)\ntarget(Array(1)) = 0\ntarget(Array(2)) = 1\ntarget(Array(3)) = 1\n\n\n criterion.forward(input, target)\nres0: Float = 0.6081934\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiLabelSoftMarginCriterion()\ninput = np.array([0.4, 0.5, 0.6])\ntarget = np.array([0, 1, 1])\n\n\n criterion.forward(input, target)\n0.6081934\n\n\n\n\nAbsCriterion\n\n\nScala:\n\n\nval criterion = AbsCriterion(sizeAverage)\n\n\n\n\nPython:\n\n\ncriterion = AbsCriterion(sizeAverage)\n\n\n\n\nMeasures the mean absolute value of the element-wise difference between input and target\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = AbsCriterion()\nval input = Tensor(T(1.0f, 2.0f, 3.0f))\nval target = Tensor(T(4.0f, 5.0f, 6.0f))\nval output = criterion.forward(input, target)\n\nscala\n print(output)\n3.0\n\n\n\n\nPython example:\n\n\ncriterion = AbsCriterion()\ninput = np.array([1.0, 2.0, 3.0])\ntarget = np.array([4.0, 5.0, 6.0])\noutput=criterion.forward(input, target)\n\n\n print output\n3.0\n\n\n\n\nMultiCriterion\n\n\nScala:\n\n\nval criterion = MultiCriterion()\n\n\n\n\nPython:\n\n\ncriterion = MultiCriterion()\n\n\n\n\nMultiCriterion is a weighted sum of other criterions each applied to the same input and target\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiCriterion()\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\nval input = Tensor(5).randn()\nval target = Tensor(5)\ntarget(Array(1)) = 1\ntarget(Array(2)) = 2\ntarget(Array(3)) = 3\ntarget(Array(4)) = 2\ntarget(Array(5)) = 1\n\nval output = criterion.forward(input, target)\n\n\n input\n1.0641425\n-0.33507252\n1.2345984\n0.08065767\n0.531199\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\n\n\n output\nres7: Float = 1.9633228\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiCriterion()\nnll = ClassNLLCriterion()\nmse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\ninput = np.array([0.9682213801388531,\n0.35258855644097503,\n0.04584479998452568,\n-0.21781499692588918,\n-1.02721844006879])\ntarget = np.array([1, 2, 3, 2, 1])\n\noutput = criterion.forward(input, target)\n\n\n output\n3.6099546", 
            "title": "Losses"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#l1cost", 
            "text": "Scala:  val layer = L1Cost[Float]()  Python:  layer = L1Cost()  Compute L1 norm for input, and sign of input  Scala example:  val layer = L1Cost[Float]()\nval input = Tensor[Float](2, 2).rand\nval target = Tensor[Float](2, 2).rand\n\nval output = layer.forward(input, target)\nval gradInput = layer.backward(input, target)  println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.48145306      0.476887\n0.23729686      0.5169516\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  println(target)\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.42999148      0.22272833\n0.49723643      0.17884709\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  println(output)\noutput: Float = 1.7125885  println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  layer = L1Cost()\n\ninput = np.random.uniform(0, 1, (2, 2)).astype( float32 )\ntarget = np.random.uniform(0, 1, (2, 2)).astype( float32 )\n\noutput = layer.forward(input, target)\ngradInput = layer.backward(input, target)  output\n2.522411  gradInput\n[array([[ 1.,  1.],\n        [ 1.,  1.]], dtype=float32)]", 
            "title": "L1Cost"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#timedistributedcriterion", 
            "text": "Scala:  val module = TimeDistributedCriterion(critrn, sizeAverage)  Python:  module = TimeDistributedCriterion(critrn, sizeAverage)  This class is intended to support inputs with 3 or more dimensions.\nApply Any Provided Criterion to every temporal slice of an input.  critrn  embedded criterion  sizeAverage  whether to divide the sequence length. Default is false.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval criterion = ClassNLLCriterion[Double]()\nval layer = TimeDistributedCriterion[Double](criterion, true)\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404))).resize(3, 2, 3)\nval target = Tensor[Double](3, 2)\n    target(Array(1, 1)) = 1\n    target(Array(1, 2)) = 1\n    target(Array(2, 1)) = 2\n    target(Array(2, 2)) = 2\n    target(Array(3, 1)) = 3\n    target(Array(3, 2)) = 3  print(layer.forward(input, target))\n0.8793184268272332  Python example:  from bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\nlayer = TimeDistributedCriterion(criterion, True)\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404]).reshape(3,2,3)\ntarget = np.array([[1,1],[2,2],[3,3]])                       layer.forward(input, target)\n0.8793184", 
            "title": "TimeDistributedCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#marginrankingcriterion", 
            "text": "Scala:  val mse = new MarginRankingCriterion(margin=1.0, sizeAverage=true)  Python:  mse = MarginRankingCriterion(margin=1.0, size_average=true)  Creates a criterion that measures the loss given an input x = {x1, x2},\na table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).\nIn batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size\nbatchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.\nIf y == 1 then it assumed the first input should be ranked higher (have a larger value) than\nthe second input, and vice-versa for y == -1.  Scala example:  import com.intel.analytics.bigdl.nn.MarginRankingCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nimport scala.util.Random\n\nval input1Arr = Array(1, 2, 3, 4, 5)\nval input2Arr = Array(5, 4, 3, 2, 1)\n\nval target1Arr = Array(-1, 1, -1, 1, 1)\n\nval input1 = Tensor(Storage(input1Arr.map(x =  x.toFloat)))\nval input2 = Tensor(Storage(input2Arr.map(x =  x.toFloat)))\n\nval input = T((1.toFloat, input1), (2.toFloat, input2))\n\nval target1 = Tensor(Storage(target1Arr.map(x =  x.toFloat)))\nval target = T((1.toFloat, target1))\n\nval mse = new MarginRankingCriterion()\n\nval output = mse.forward(input, target)\nval gradInput = mse.backward(input, target)\n\nprintln(output)\nprintln(gradInput)  The output will be,  output: Float = 0.8                                                                                                                                                                    [21/154]  The gradInput will be,  gradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: -0.0\n           0.2\n           -0.2\n           0.0\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n        1: 0.0\n           -0.2\n           0.2\n           -0.0\n           -0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmse = MarginRankingCriterion()\n\ninput1 = np.array([1, 2, 3, 4, 5]).astype( float32 )\ninput2 = np.array([5, 4, 3, 2, 1]).astype( float32 )\ninput = [input1, input2]\n\ntarget1 = np.array([-1, 1, -1, 1, 1]).astype( float32 )\ntarget = [target1, target1]\n\noutput = mse.forward(input, target)\ngradInput = mse.backward(input, target)\n\nprint output\nprint gradInput  The output will be,  0.8  The gradInput will be,  [array([ 0. , -0.2,  0.2, -0. , -0. ], dtype=float32), array([-0. ,  0.2, -0.2,  0. ,  0. ], dtype=float32)]", 
            "title": "MarginRankingCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#classnllcriterion", 
            "text": "Scala:  val criterion = ClassNLLCriterion(weights = null, sizeAverage = true)  Python:  criterion = ClassNLLCriterion(weights=None, size_average=True)  The negative log likelihood criterion. It is useful to train a classification problem with n\nclasses. If provided, the optional argument weights should be a 1D Tensor assigning weight to\neach of the classes. This is particularly useful when you have an unbalanced training set.  The input given through a  forward()  is expected to contain log-probabilities of each class:\ninput has to be a 1D Tensor of size  n . Obtaining log-probabilities in a neural network is easily\nachieved by adding a  LogSoftMax  layer in the last layer of your neural network. You may use CrossEntropyCriterion  instead, if you prefer not to add an extra layer to your network. This\ncriterion expects a class index (1 to the number of class) as target when calling forward(input, target)  and  backward(input, target) .  The loss can be described as:\n     loss(x, class) = -x[class]\n or in the case of the weights argument it is specified as follows:\n     loss(x, class) = -weights[class] * x[class]\n Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when\n calculating losses in non-batch mode.  By default, the losses are averaged over observations for each minibatch. However, if the field\n  sizeAverage  is set to false, the losses are instead summed for each minibatch.  Parameters:  weights      - weights of each element of the input  sizeAverage  - A boolean indicating whether normalizing by the number of elements in the input.\n                  Default: true  Scala example:  import com.intel.analytics.bigdl.nn.ClassNLLCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval criterion = ClassNLLCriterion()\nval input = Tensor(T(\n              T(1f, 2f, 3f),\n              T(2f, 3f, 4f),\n              T(3f, 4f, 5f)\n          ))\n\nval target = Tensor(T(1f, 2f, 3f))\n\nval loss = criterion.forward(input, target)\nval grad = criterion.backward(input, target)\n\nprint(loss)\n-3.0\nprintln(grad)\n-0.33333334 0.0 0.0\n0.0 -0.33333334 0.0\n0.0 0.0 -0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\ninput = np.array([\n              [1.0, 2.0, 3.0],\n              [2.0, 3.0, 4.0],\n              [3.0, 4.0, 5.0]\n          ])\n\ntarget = np.array([1.0, 2.0, 3.0])\n\nloss = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n\nprint loss\n-3.0\nprint gradient\n-3.0\n[[-0.33333334  0.          0.        ]\n [ 0.         -0.33333334  0.        ]\n [ 0.          0.         -0.33333334]]", 
            "title": "ClassNLLCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#softmaxwithcriterion", 
            "text": "Scala:  val model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)  Python:  model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)  Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued predictions through a softmax to\nget a probability distribution over classes. It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as \nits gradient computation is more numerically stable.   param ignoreLabel   (optional) Specify a label value that should be ignored when computing the loss.  param normalizeMode How to normalize the output loss.   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\n\nval input = Tensor(1, 5, 2, 3).rand()\nval target = Tensor(Storage(Array(2.0f, 4.0f, 2.0f, 4.0f, 1.0f, 2.0f))).resize(1, 1, 2, 3)\n\nval model = SoftmaxWithCriterion[Float]()\nval output = model.forward(input, target)\n\nscala  print(input)\n(1,1,.,.) =\n0.65131104  0.9332143   0.5618989   \n0.9965054   0.9370902   0.108070895 \n\n(1,2,.,.) =\n0.46066576  0.9636703   0.8123812   \n0.31076035  0.16386998  0.37894428  \n\n(1,3,.,.) =\n0.49111295  0.3704862   0.9938375   \n0.87996656  0.8695406   0.53354675  \n\n(1,4,.,.) =\n0.8502225   0.9033509   0.8518651   \n0.0692618   0.10121379  0.970959    \n\n(1,5,.,.) =\n0.9397213   0.49688303  0.75739735  \n0.25074655  0.11416598  0.6594504   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x2x3]\n\nscala  print(output)\n1.6689054  Python example:  input = np.random.randn(1, 5, 2, 3)\ntarget = np.array([[[[2.0, 4.0, 2.0], [4.0, 1.0, 2.0]]]])\n\nmodel = SoftmaxWithCriterion()\noutput = model.forward(input, target)  print input\n[[[[ 0.78455689  0.01402084  0.82539628]\n   [-1.06448238  2.58168413  0.60053703]]\n\n  [[-0.48617618  0.44538094  0.46611658]\n   [-1.41509329  0.40038991 -0.63505732]]\n\n  [[ 0.91266769  1.68667933  0.92423611]\n   [ 0.1465411   0.84637557  0.14917515]]\n\n  [[-0.7060493  -2.02544114  0.89070726]\n   [ 0.14535539  0.73980064 -0.33130613]]\n\n  [[ 0.64538791 -0.44384233 -0.40112523]\n   [ 0.44346658 -2.22303621  0.35715986]]]]  print output\n2.1002123", 
            "title": "SoftmaxWithCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#smoothl1criterion", 
            "text": "Scala:  val slc = SmoothL1Criterion(sizeAverage=true)  Python:  slc = SmoothL1Criterion(size_average=True)  Creates a criterion that can be thought of as a smooth version of the AbsCriterion.\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some\ncases prevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).  Scala example:  import com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.SmoothL1Criterion\n\nval slc = SmoothL1Criterion()\n\nval inputArr = Array(\n  0.17503996845335,\n  0.83220188552514,\n  0.48450597329065,\n  0.64701424003579,\n  0.62694586534053,\n  0.34398410236463,\n  0.55356747563928,\n  0.20383032318205\n)\nval targetArr = Array(\n  0.69956525065936,\n  0.86074831243604,\n  0.54923197557218,\n  0.57388074393384,\n  0.63334444304928,\n  0.99680578662083,\n  0.49997645849362,\n  0.23869121982716\n)\n\nval input = Tensor(Storage(inputArr.map(x =  x.toFloat))).reshape(Array(2, 2, 2))\nval target = Tensor(Storage(targetArr.map(x =  x.toFloat))).reshape(Array(2, 2, 2))\n\nval output = slc.forward(input, target)\nval gradInput = slc.backward(input, target)  The output is,  output: Float = 0.0447365  The gradInput is,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.06556566     -0.003568299\n-0.008090746    0.009141691\n\n(2,.,.) =\n-7.998273E-4    -0.08160271\n0.0066988766    -0.0043576136  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nslc = SmoothL1Criterion()\n\ninput = np.array([\n    0.17503996845335,\n    0.83220188552514,\n    0.48450597329065,\n    0.64701424003579,\n    0.62694586534053,\n    0.34398410236463,\n    0.55356747563928,\n    0.20383032318205\n])\ninput.reshape(2, 2, 2)\n\ntarget = np.array([\n    0.69956525065936,\n    0.86074831243604,\n    0.54923197557218,\n    0.57388074393384,\n    0.63334444304928,\n    0.99680578662083,\n    0.49997645849362,\n    0.23869121982716\n])\n\ntarget.reshape(2, 2, 2)\n\noutput = slc.forward(input, target)\ngradInput = slc.backward(input, target)\n\nprint output\nprint gradInput", 
            "title": "SmoothL1Criterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#smoothl1criterionwithweights", 
            "text": "Scala:  val smcod = SmoothL1CriterionWithWeights[Float](sigma: Float = 2.4f, num: Int = 2)  Python:  smcod = SmoothL1CriterionWithWeights(sigma, num)  a smooth version of the AbsCriterion\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some cases\nprevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).     d = (x - y) * w_in\n\n  loss(x, y, w_in, w_out)\n              | 0.5 * (sigma * d_i)^2 * w_out          if |d_i|   1 / sigma / sigma\n   = 1/n \\sum |\n              | (|d_i| - 0.5 / sigma / sigma) * w_out   otherwise  Scala example:  val smcod = SmoothL1CriterionWithWeights[Float](2.4f, 2)\n\nval inputArr = Array(1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03)\nval targetArr = Array(0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58)\nval inWArr = Array(-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8)\nval outWArr = Array(-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1)\n\nval input = Tensor(Storage(inputArr.map(x =  x.toFloat)))\nval target = T()\ntarget.insert(Tensor(Storage(targetArr.map(x =  x.toFloat))))\ntarget.insert(Tensor(Storage(inWArr.map(x =  x.toFloat))))\ntarget.insert(Tensor(Storage(outWArr.map(x =  x.toFloat))))\n\nval output = smcod.forward(input, target)\nval gradInput = smcod.backward(input, target)  println(output)\n  output: Float = -2.17488  println(gradInput)\n-0.010944003\n0.425\n0.63037443\n-0.95\n-0.1\n0.07\n0.120000005\n-0.44000003\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 8]  Python example:  smcod = SmoothL1CriterionWithWeights(2.4, 2)\n\ninput = np.array([1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03]).astype( float32 )\ntargetArr = np.array([0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58]).astype( float32 )\ninWArr = np.array([-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8]).astype( float32 )\noutWArr = np.array([-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1]).astype( float32 )\ntarget = [targetArr, inWArr, outWArr]\n\noutput = smcod.forward(input, target)\ngradInput = smcod.backward(input, target)  output\n-2.17488  gradInput\n[array([-0.010944  ,  0.42500001,  0.63037443, -0.94999999, -0.1       ,\n         0.07      ,  0.12      , -0.44000003], dtype=float32)]", 
            "title": "SmoothL1CriterionWithWeights"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#multimargincriterion", 
            "text": "Scala:  val loss = MultiMarginCriterion(p=1,weights=null,margin=1.0,sizeAverage=true)  Python:  loss = MultiMarginCriterion(p=1,weights=None,margin=1.0,size_average=True)  MultiMarginCriterion is a loss function that optimizes a multi-class classification hinge loss (margin-based loss) between input  x  and output  y  ( y  is the target class index).  Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(3,2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f, 2.0f)))\nval loss = MultiMarginCriterion(1)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala  print(input)\n-0.45896783     -0.80141246\n0.22560088      -0.13517438\n0.2601126       0.35492152\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala  print(target)\n2.0\n1.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala  print(output)\n0.4811434\n\nscala  print(grad)\n0.16666667      -0.16666667\n-0.16666667     0.16666667\n0.16666667      -0.16666667\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(3,2)\ntarget = np.array([2,1,2])\nprint  input= ,input\nprint  target= ,target\n\nloss = MultiMarginCriterion(1)\nout = loss.forward(input, target)\nprint  output of loss is :  ,out\n\ngrad_out = loss.backward(input,target)\nprint  grad out of loss is :  ,grad_out  produces output  input= [[ 0.46868305 -2.28562261]\n [ 0.8076243  -0.67809689]\n [-0.20342555 -0.66264743]]\ntarget= [2 1 2]\ncreating: createMultiMarginCriterion\noutput of loss is :  0.8689213\ngrad out of loss is :  [[ 0.16666667 -0.16666667]\n [ 0.          0.        ]\n [ 0.16666667 -0.16666667]]", 
            "title": "MultiMarginCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#hingeembeddingcriterion", 
            "text": "Scala:  val m = HingeEmbeddingCriterion(margin = 1, sizeAverage = true)  Python:  m = HingeEmbeddingCriterion(margin=1, size_average=True)  Creates a criterion that measures the loss given an input  x  which is a 1-dimensional vector and a label  y  ( 1  or  -1 ).\nThis is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.                   \u23a7 x_i,                  if y_i ==  1\nloss(x, y) = 1/n \u23a8\n                 \u23a9 max(0, margin - x_i), if y_i == -1  Scala example:  import com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval loss = HingeEmbeddingCriterion(1, sizeAverage = false)\nval input = Tensor(T(0.1f, 2.0f, 2.0f, 2.0f))\nprintln( input: \\n  + input)\nprintln( ouput:  )\n\nprintln( Target=1:   + loss.forward(input, Tensor(4, 1).fill(1f)))\n\nprintln( Target=-1:   + loss.forward(input, Tensor(4, 1).fill(-1f)))  input: \n0.1\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\nouput: \nTarget=1: 6.1\nTarget=-1: 0.9  Python example:  import numpy as np\nfrom bigdl.nn.criterion import *\ninput = np.array([0.1, 2.0, 2.0, 2.0])\ntarget = np.full(4, 1)\nprint( input:   )\nprint(input)\nprint( target:  )\nprint(target)\nprint( output:  )\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, target))\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, np.full(4, -1)))  input: \n[ 0.1  2.   2.   2. ]\ntarget: \n[1 1 1 1]\noutput: \ncreating: createHingeEmbeddingCriterion\n6.1\ncreating: createHingeEmbeddingCriterion\n0.9", 
            "title": "HingeEmbeddingCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#margincriterion", 
            "text": "Scala:  criterion = MarginCriterion(margin=1.0, sizeAverage=true)  Python:  criterion = MarginCriterion(margin=1.0, sizeAverage=true, bigdl_type= float )  Creates a criterion that optimizes a two-class classification hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.\n * @param margin if unspecified, is by default 1.\n * @param sizeAverage whether to average the loss, is by default true  Scala example:  val criterion = MarginCriterion(margin=1.0, sizeAverage=true)\n\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.33753583      0.3575501\n0.23477706      0.7240361\n0.92835575      0.4737949\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nval target = Tensor(3, 2).rand()\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.27280563      0.7022703\n0.3348442       0.43332106\n0.08935371      0.17876455\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\ncriterion.forward(input, target)\nres5: Float = 0.84946966  Python example:  criterion = MarginCriterion(margin=1.0,size_average=True,bigdl_type= float )\ninput = np.random.rand(3, 2)\narray([[ 0.20824672,  0.67299837],\n       [ 0.80561452,  0.19564743],\n       [ 0.42501441,  0.19408184]])\n\ntarget = np.random.rand(3, 2)\narray([[ 0.67882632,  0.61257846],\n       [ 0.10111138,  0.75225082],\n       [ 0.60404296,  0.31373273]])\n\ncriterion.forward(input, target)\n0.8166871", 
            "title": "MarginCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#cosineembeddingcriterion", 
            "text": "Scala:  val cosineEmbeddingCriterion = CosineEmbeddingCriterion(margin  = 0.0, sizeAverage = true)  Python:  cosineEmbeddingCriterion = CosineEmbeddingCriterion( margin=0.0,size_average=True)  CosineEmbeddingCriterion creates a criterion that measures the loss given an input x = {x1, x2},\na table of two Tensors, and a Tensor label y with values 1 or -1.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimpot com.intel.analytics.bigdl.utils.T\nval cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, false)\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T()\ninput(1.0) = input1\ninput(2.0) = input2\nval target1 = Tensor(Storage(Array(-0.5f)))\nval target = T()\ntarget(1.0) = target1  print(input)\n {\n    2.0: 0.4110882\n         0.57726574\n         0.1949834\n         0.67670715\n         0.16984987\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1.0: 0.16878392\n         0.24124223\n         0.8964794\n         0.11156334\n         0.5101486\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }  print(cosineEmbeddingCriterion.forward(input, target))\n0.49919847  print(cosineEmbeddingCriterion.backward(input, target))\n {\n    2: -0.045381278\n       -0.059856333\n       0.72547954\n       -0.2268434\n       0.3842142\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n    1: 0.30369008\n       0.42463788\n       -0.20637506\n       0.5712836\n       -0.06355385\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }  Python example:  from bigdl.nn.layer import *\ncosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, False)  cosineEmbeddingCriterion.forward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n0.6363636  cosineEmbeddingCriterion.backward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n[array([ 0.07933884,  0.04958678,  0.01983471, -0.00991735, -0.03966942], dtype=float32), array([-0.03966942, -0.00991735,  0.01983471,  0.04958678,  0.07933884], dtype=float32)]", 
            "title": "CosineEmbeddingCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#bcecriterion", 
            "text": "Scala:  val criterion = BCECriterion[Float]()  Python:  criterion = BCECriterion()  This loss function measures the Binary Cross Entropy between the target and the output   loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))  or in the case of the weights argument being specified:   loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))  By default, the losses are averaged for each mini-batch over observations as well as over\n dimensions. However, if the field sizeAverage is set to false, the losses are instead summed.  Scala example:  \nval criterion = BCECriterion[Float]()\nval input = Tensor[Float](3, 1).rand\n\nval target = Tensor[Float](3)\ntarget(1) = 1\ntarget(2) = 0\ntarget(3) = 1\n\nval output = criterion.forward(input, target)\nval gradInput = criterion.backward(input, target)  println(target)\nres25: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  println(output)\noutput: Float = 0.9009579  println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-1.5277504\n1.0736246\n-0.336957\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]  Python example:  \ncriterion = BCECriterion()\ninput = np.random.uniform(0, 1, (3, 1)).astype( float32 )\ntarget = np.array([1, 0, 1])\noutput = criterion.forward(input, target)\ngradInput = criterion.backward(input, target)  output\n1.9218739  gradInput\n[array([[-4.3074522 ],\n        [ 2.24244714],\n        [-1.22368968]], dtype=float32)]", 
            "title": "BCECriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#dicecoefficientcriterion", 
            "text": "Scala:  val loss = DiceCoefficientCriterion(sizeAverage=true, epsilon=1.0f)  Python:  loss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)  DiceCoefficientCriterion is the Dice-Coefficient objective function.   Both  forward  and  backward  accept two tensors : input and target. The  forward  result is formulated as \n           1 - (2 * (input intersection target) / (input union target))  Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DiceCoefficientCriterion(epsilon = 1.0f)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala  print(input)\n-0.50278\n0.51387966\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(output)\n0.9958517\n\nscala  print(grad)\n-0.99619853     -0.49758217\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1],dtype='float64')\n\nprint  input= , input\nprint  target= , target\nloss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)\nout = loss.forward(input,target)\nprint  output of loss is : ,out\n\ngrad_out = loss.backward(input,target)\nprint  grad out of loss is : ,grad_out  produces output:  input= [ 0.4440505  2.9430301]\ntarget= [ 2.  1.]\ncreating: createDiceCoefficientCriterion\noutput of loss is : -0.17262316\ngrad out of loss is : [[-0.38274616 -0.11200322]]", 
            "title": "DiceCoefficientCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#msecriterion", 
            "text": "Scala:  val criterion = MSECriterion()  Python:  criterion = MSECriterion()  The mean squared error criterion e.g. input: a, target: b, total elements: n  loss(a, b) = 1/n * sum(|a_i - b_i|^2)  Parameters:   sizeAverage  - a boolean indicating whether to divide the sum of squared error by n.\n Default: true   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MSECriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(2.0f, 3.0f),\n T(4.0f, 5.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-  print(output)\n1.0\n-  print(gradient)\n-0.5    -0.5    \n-0.5    -0.5    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = MSECriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n-  print output\n1.0\n-  print gradient\n[[-0.5 -0.5]\n [-0.5 -0.5]]", 
            "title": "MSECriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#softmargincriterion", 
            "text": "Scala:  val criterion = SoftMarginCriterion(sizeAverage)  Python:  criterion = SoftMarginCriterion(size_average)  Creates a criterion that optimizes a two-class classification logistic loss between\ninput x (a Tensor of dimension 1) and output y (which is a tensor containing either\n1s or -1s).  loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()  Parameters: \n*  sizeAverage  - A boolean indicating whether normalizing by the number of elements in the input.\n                    Default: true  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = SoftMarginCriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(1.0f, -1.0f),\n T(-1.0f, 1.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-  print(output)\n1.3767318\n-  print(gradient)\n-0.06723536     0.22019927      \n0.23814353      -0.0044965525   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = SoftMarginCriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-  print output\n1.3767318\n-  print gradient\n[[-0.06723536  0.22019927]\n [ 0.23814353 -0.00449655]]", 
            "title": "SoftMarginCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#distkldivcriterion", 
            "text": "Scala:  val loss = DistKLDivCriterion[T](sizeAverage=true)  Python:  loss = DistKLDivCriterion(size_average=True)  DistKLDivCriterion is the Kullback\u2013Leibler divergence loss.  Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DistKLDivCriterion()\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala  print(input)\n-0.3854126\n-0.7707398\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(output)\n1.4639297\n\nscala  print(grad)\n-1.0\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1])\n\nprint  input= , input\nprint  target= , target\nloss = DistKLDivCriterion()\nout = loss.forward(input,target)\nprint  output of loss is : ,out\n\ngrad_out = loss.backward(input,target)\nprint  grad out of loss is : ,grad_out  produces output:  input= [-1.14333924  0.97662296]\ntarget= [2 1]\ncreating: createDistKLDivCriterion\noutput of loss is : 1.348175\ngrad out of loss is : [-1.  -0.5]", 
            "title": "DistKLDivCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#classsimplexcriterion", 
            "text": "Scala:  val criterion = ClassSimplexCriterion(nClasses)  Python:  criterion = ClassSimplexCriterion(nClasses)  ClassSimplexCriterion implements a criterion for classification.\nIt learns an embedding per class, where each class' embedding is a\npoint on an (N-1)-dimensional simplex, where N is the number of classes.  Parameters: \n*  nClasses  - An integer, the number of classes.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = ClassSimplexCriterion(5)\nval input = Tensor(T(\n T(1.0f, 2.0f, 3.0f, 4.0f, 5.0f),\n T(4.0f, 5.0f, 6.0f, 7.0f, 8.0f)\n))\nval target = Tensor(2)\ntarget(1) = 2.0f\ntarget(2) = 1.0f\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-  print(output)\n23.562702\n-  print(gradient)\n0.25    0.20635083      0.6     0.8     1.0     \n0.6     1.0     1.2     1.4     1.6     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = ClassSimplexCriterion(5)\ninput = np.array([\n   [1.0, 2.0, 3.0, 4.0, 5.0],\n   [4.0, 5.0, 6.0, 7.0, 8.0]\n])\ntarget = np.array([2.0, 1.0])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-  print output\n23.562702\n-  print gradient\n[[ 0.25        0.20635083  0.60000002  0.80000001  1.        ]\n [ 0.60000002  1.          1.20000005  1.39999998  1.60000002]]", 
            "title": "ClassSimplexCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#l1hingeembeddingcriterion", 
            "text": "Scala:  val model = L1HingeEmbeddingCriterion(margin)  Python:  model = L1HingeEmbeddingCriterion(margin)  Creates a criterion that measures the loss given an input  x = {x1, x2} , a table of two Tensors, and a label y (1 or -1).\nThis is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.               \u23a7 ||x1 - x2||_1,                  if y ==  1\nloss(x, y) = \u23a8\n             \u23a9 max(0, margin - ||x1 - x2||_1), if y == -1  The margin has a default value of 1, or can be set in the constructor.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = L1HingeEmbeddingCriterion(0.6)\nval input1 = Tensor(T(1.0f, -0.1f))\nval input2 = Tensor(T(2.0f, -0.2f))\nval input = T(input1, input2)\nval target = Tensor(1)\ntarget(Array(1)) = 1.0f\n\nval output = model.forward(input, target)\n\nscala  print(output)\n1.1  Python example:  model = L1HingeEmbeddingCriterion(0.6)\ninput1 = np.array(1.0, -0.1)\ninput2 = np.array(2.0, -0.2)\ninput = [input1, input2]\ntarget = np.array([1.0])\n\noutput = model.forward(input, target)  print output\n1.1", 
            "title": "L1HingeEmbeddingCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#crossentropycriterion", 
            "text": "Scala:  val module = CrossEntropyCriterion(weights, sizeAverage)  Python:  module = CrossEntropyCriterion(weights, sizeAverage)  This criterion combines LogSoftMax and ClassNLLCriterion in one single class.  weights  A tensor assigning weight to each of the classes  sizeAverage  whether to divide the sequence length. Default is true.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval layer = CrossEntropyCriterion[Double]()\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404\n    ))).resize(3, 3)\nval target = Tensor[Double](3)\n    target(Array(1)) = 1\n    target(Array(2)) = 2\n    target(Array(3)) = 3  print(layer.forward(input, target))\n0.9483051199107635  Python example:  from bigdl.nn.criterion import *\n\nlayer = CrossEntropyCriterion()\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404\n                      ]).reshape(3,3)\ntarget = np.array([1, 2, 3])                       layer.forward(input, target)\n0.94830513", 
            "title": "CrossEntropyCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#parallelcriterion", 
            "text": "Scala:  val pc = ParallelCriterion(repeatTarget=false)  Python:  pc = ParallelCriterion(repeat_target=False)  ParallelCriterion is a weighted sum of other criterions each applied to a different input\nand target. Set repeatTarget = true to share the target for criterions.\nUse add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).  Scala example:  import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.{ParallelCriterion, ClassNLLCriterion, MSECriterion}\n\nval pc = ParallelCriterion()\n\nval input = T(Tensor(2, 10), Tensor(2, 10))\nvar i = 0\ninput[Tensor](1).apply1(_ =  {i += 1; i})\ninput[Tensor](2).apply1(_ =  {i -= 1; i})\nval target = T(Tensor(Storage(Array(1.0f, 8.0f))), Tensor(2, 10).fill(1.0f))\n\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\npc.add(nll, 0.5).add(mse)\n\nval output = pc.forward(input, target)\nval gradInput = pc.backward(input, target)\n\nprintln(output)\nprintln(gradInput)  The output is,  100.75  The gradInput is,   {\n        2: 1.8000001    1.7     1.6     1.5     1.4     1.3000001       1.2     1.1     1.0     0.90000004\n           0.8  0.7     0.6     0.5     0.4     0.3     0.2     0.1     0.0     -0.1\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n        1: -0.25        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n           0.0  0.0     0.0     0.0     0.0     0.0     0.0     -0.25   0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npc = ParallelCriterion()\n\ninput1 = np.arange(1, 21, 1).astype( float32 )\ninput2 = np.arange(0, 20, 1).astype( float32 )[::-1]\ninput1 = input1.reshape(2, 10)\ninput2 = input2.reshape(2, 10)\n\ninput = [input1, input2]\n\ntarget1 = np.array([1.0, 8.0]).astype( float32 )\ntarget1 = target1.reshape(2)\ntarget2 = np.full([2, 10], 1).astype( float32 )\ntarget2 = target2.reshape(2, 10)\ntarget = [target1, target2]\n\nnll = ClassNLLCriterion()\nmse = MSECriterion()\n\npc.add(nll, weight = 0.5).add(mse)\n\nprint  input = \\n %s   % input\nprint  target = \\n %s  % target\n\noutput = pc.forward(input, target)\ngradInput = pc.backward(input, target)\n\nprint  output = %s   % output\nprint  gradInput = %s   % gradInput  The console will output,  input = \n [array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n       [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]], dtype=float32), array([[ 19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,  10.],\n       [  9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.]], dtype=float32)] \ntarget = \n [array([ 1.,  8.], dtype=float32), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)]\noutput = 100.75 \ngradInput = [array([[-0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ]], dtype=float32), array([[ 1.80000007,  1.70000005,  1.60000002,  1.5       ,  1.39999998,\n         1.30000007,  1.20000005,  1.10000002,  1.        ,  0.90000004],\n       [ 0.80000001,  0.69999999,  0.60000002,  0.5       ,  0.40000001,\n         0.30000001,  0.2       ,  0.1       ,  0.        , -0.1       ]], dtype=float32)]", 
            "title": "ParallelCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#multilabelmargincriterion", 
            "text": "Scala:  val multiLabelMarginCriterion = MultiLabelMarginCriterion(sizeAverage = true)  Python:  multiLabelMarginCriterion = MultiLabelMarginCriterion(size_average=True)  MultiLabelMarginCriterion creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x and output y   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval multiLabelMarginCriterion = MultiLabelMarginCriterion(false)\nval input = Tensor(4).rand()\nval target = Tensor(4)\ntarget(Array(1)) = 3\ntarget(Array(2)) = 2\ntarget(Array(3)) = 1\ntarget(Array(4)) = 0  print(input)\n0.40267515\n0.5913795\n0.84936756\n0.05999674   print(multiLabelMarginCriterion.forward(input, target))\n0.33414197  print(multiLabelMarginCriterion.backward(input, target))\n-0.25\n-0.25\n-0.25\n0.75\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  from bigdl.nn.layer import *\nmultiLabelMarginCriterion = MultiLabelMarginCriterion(False)  multiLabelMarginCriterion.forward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n0.975  multiLabelMarginCriterion.backward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n[array([-0.25, -0.25, -0.25,  0.75], dtype=float32)]", 
            "title": "MultiLabelMarginCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#multilabelsoftmargincriterion", 
            "text": "Scala:  val criterion = MultiLabelSoftMarginCriterion(weights = null, sizeAverage = true)  Python:  criterion = MultiLabelSoftMarginCriterion(weights=None, size_average=True)  MultiLabelSoftMarginCriterion is a multiLabel multiclass criterion based on sigmoid:  l(x,y) = - sum_i y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i])  where  p[i] = exp(x[i]) / (1 + exp(x[i]))  If with weights,\n  l(x,y) = - sum_i weights[i] (y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i]))  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiLabelSoftMarginCriterion()\nval input = Tensor(3)\ninput(Array(1)) = 0.4f\ninput(Array(2)) = 0.5f\ninput(Array(3)) = 0.6f\nval target = Tensor(3)\ntarget(Array(1)) = 0\ntarget(Array(2)) = 1\ntarget(Array(3)) = 1  criterion.forward(input, target)\nres0: Float = 0.6081934  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiLabelSoftMarginCriterion()\ninput = np.array([0.4, 0.5, 0.6])\ntarget = np.array([0, 1, 1])  criterion.forward(input, target)\n0.6081934", 
            "title": "MultiLabelSoftMarginCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#abscriterion", 
            "text": "Scala:  val criterion = AbsCriterion(sizeAverage)  Python:  criterion = AbsCriterion(sizeAverage)  Measures the mean absolute value of the element-wise difference between input and target  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = AbsCriterion()\nval input = Tensor(T(1.0f, 2.0f, 3.0f))\nval target = Tensor(T(4.0f, 5.0f, 6.0f))\nval output = criterion.forward(input, target)\n\nscala  print(output)\n3.0  Python example:  criterion = AbsCriterion()\ninput = np.array([1.0, 2.0, 3.0])\ntarget = np.array([4.0, 5.0, 6.0])\noutput=criterion.forward(input, target)  print output\n3.0", 
            "title": "AbsCriterion"
        }, 
        {
            "location": "/APIdocs/Losses/merged-Losses/#multicriterion", 
            "text": "Scala:  val criterion = MultiCriterion()  Python:  criterion = MultiCriterion()  MultiCriterion is a weighted sum of other criterions each applied to the same input and target  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiCriterion()\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\nval input = Tensor(5).randn()\nval target = Tensor(5)\ntarget(Array(1)) = 1\ntarget(Array(2)) = 2\ntarget(Array(3)) = 3\ntarget(Array(4)) = 2\ntarget(Array(5)) = 1\n\nval output = criterion.forward(input, target)  input\n1.0641425\n-0.33507252\n1.2345984\n0.08065767\n0.531199\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]  output\nres7: Float = 1.9633228  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiCriterion()\nnll = ClassNLLCriterion()\nmse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\ninput = np.array([0.9682213801388531,\n0.35258855644097503,\n0.04584479998452568,\n-0.21781499692588918,\n-1.02721844006879])\ntarget = np.array([1, 2, 3, 2, 1])\n\noutput = criterion.forward(input, target)  output\n3.6099546", 
            "title": "MultiCriterion"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/", 
            "text": "Optimizer\n\n\nYou can use Optimizer to distributed train your model with\na spark cluster.\n\n\nHow to use Optimizer\n\n\nYou need at least provide model, data, loss function and batch size.\n\n\n\n\nmodel\n\n\n\n\nA neural network model. May be a layer, a sequence of layers or a\ngraph of layers.\n\n\n\n\ndata\n\n\n\n\nYour training data. As we train models on Spark, one of\nthe most common distributed data structures is RDD. Of course\nyou can use DataFrame. Please check the BigDL pipeline example.\n\n\nThe element in the RDD is Sample, which is actually a sequence of\nTensors. You need to convert your data record(image, audio, text)\nto Tensors before you feed them into Optimizer. We also provide\nmany utilities to do it.\n\n\n\n\nloss function\n\n\n\n\nIn supervised machine learning, loss function compares the output of\nthe model with the ground truth(the labels of the training data). It\noutputs a loss value to measure how good the model is(the lower the\nbetter). It also provides a gradient to indicate how to tune the model.\n\n\nIn BigDL, all loss functions are subclass of Criterion.\n\n\n\n\nbatch size\n\n\n\n\nTraining is an iterative process. In each iteration, only a batch of data\nis used for training the model. You need to specify the batch size. Please note, \nthe batch size should be divisible by the total cores number.\n\n\nHere's an example of how to train a Linear classification model\n\n\nscala\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n// Define the model\nval model = Linear[Float](2, 1)\nmodel.bias.zero()\n\n// Generate 2D dummy data, y = 0.1 * x[1] + 0.3 * x[2]\nval samples = Seq(\n  Sample[Float](Tensor[Float](T(5f, 5f)), Tensor[Float](T(2.0f))),\n  Sample[Float](Tensor[Float](T(-5f, -5f)), Tensor[Float](T(-2.0f))),\n  Sample[Float](Tensor[Float](T(-2f, 5f)), Tensor[Float](T(1.3f))),\n  Sample[Float](Tensor[Float](T(-5f, 2f)), Tensor[Float](T(0.1f))),\n  Sample[Float](Tensor[Float](T(5f, -2f)), Tensor[Float](T(-0.1f))),\n  Sample[Float](Tensor[Float](T(2f, -5f)), Tensor[Float](T(-1.3f)))\n)\nval trainData = sc.parallelize(samples, 1)\n\n// Define the model\nval optimizer = Optimizer[Float](model, trainData, MSECriterion[Float](), 4)\nEngine.init\noptimizer.optimize()\nprintln(model.weight)\n\n\n\n\nThe weight of linear is init randomly. But the output should be like\n\n\nscala\n println(model.weight)\n0.09316949      0.2887804\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]\n\n\n\n\npython\n\n\nfrom bigdl.nn.layer import Linear\nfrom bigdl.util.common import *\nfrom bigdl.nn.criterion import MSECriterion\nfrom bigdl.optim.optimizer import Optimizer, MaxIteration\nimport numpy as np\n\nmodel = Linear(2, 1)\nsamples = [\n  Sample.from_ndarray(np.array([5, 5]), np.array([2.0])),\n  Sample.from_ndarray(np.array([-5, -5]), np.array([-2.0])),\n  Sample.from_ndarray(np.array([-2, 5]), np.array([1.3])),\n  Sample.from_ndarray(np.array([-5, 2]), np.array([0.1])),\n  Sample.from_ndarray(np.array([5, -2]), np.array([-0.1])),\n  Sample.from_ndarray(np.array([2, -5]), np.array([-1.3]))\n]\ntrain_data = sc.parallelize(samples, 1)\ninit_engine()\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)\noptimizer.optimize()\nmodel.get_weights()[0]\n\n\n\n\nThe output should be like\n\n\narray([[ 0.11578175,  0.28315681]], dtype=float32)\n\n\n\n\nYou can see the model is trained.\n\n\nDefine when to end the training\n\n\nYou need define when to end the training. It can be several iterations, or how many round\ndata you want to process, a.k.a epoch.\n\n\nscala\n\n\n// The default endWhen in scala is 100 iterations\noptimizer.setEndWhen(Trigger.maxEpoch(10))  // Change to 10 epoch\n\n\n\n\npython\n\n\n# Python need to define in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)\n\n\n\n\nChange the optimization algorithm\n\n\nGradient based optimization algorithms are the most popular algorithms to train the neural\nnetwork model. The most famous one is SGD. SGD has many variants, adagrad, adam, etc.\n\n\nscala\n\n\n// The default is SGD\noptimizer.setOptimMethod(new Adam())  // Change to adam\n\n\n\n\npython\n\n\n# Python need to define the optimization algorithm in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4, optim_method = Adam())\n\n\n\n\nValidate your model in training\n\n\nSometimes, people want to evaluate the model with a seperated dataset. When model\nperforms well on train dataset, but bad on validation dataset, we call the model is overfit or\nweak generalization. People may want to evaluate the model every serveral iterations or \nepochs. BigDL can easily do this by\n\n\nscala\n\n\noptimizer.setValidation(trigger, testData, validationMethod, batchSize)\n\n\n\n\noptimizer.set_validation(batch_size, val_rdd, trigger, validationMethod)\n\n\n\n\nFor validation, you need to provide\n\n\n\n\ntrigger: how often to do validation, maybe each several iterations or epochs\n\n\ntest data: the seperate dataset for test\n\n\nvalidation method: how to evaluate the model, maybe top1 accuracy, etc.\n\n\nbatch size: how many data evaluate in one time\n\n\n\n\nVisualize training process\n\n\nSee \nVisualization with TensorBoard", 
            "title": "Optimizer"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/#optimizer", 
            "text": "You can use Optimizer to distributed train your model with\na spark cluster.", 
            "title": "Optimizer"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/#how-to-use-optimizer", 
            "text": "You need at least provide model, data, loss function and batch size.   model   A neural network model. May be a layer, a sequence of layers or a\ngraph of layers.   data   Your training data. As we train models on Spark, one of\nthe most common distributed data structures is RDD. Of course\nyou can use DataFrame. Please check the BigDL pipeline example.  The element in the RDD is Sample, which is actually a sequence of\nTensors. You need to convert your data record(image, audio, text)\nto Tensors before you feed them into Optimizer. We also provide\nmany utilities to do it.   loss function   In supervised machine learning, loss function compares the output of\nthe model with the ground truth(the labels of the training data). It\noutputs a loss value to measure how good the model is(the lower the\nbetter). It also provides a gradient to indicate how to tune the model.  In BigDL, all loss functions are subclass of Criterion.   batch size   Training is an iterative process. In each iteration, only a batch of data\nis used for training the model. You need to specify the batch size. Please note, \nthe batch size should be divisible by the total cores number.  Here's an example of how to train a Linear classification model  scala  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n// Define the model\nval model = Linear[Float](2, 1)\nmodel.bias.zero()\n\n// Generate 2D dummy data, y = 0.1 * x[1] + 0.3 * x[2]\nval samples = Seq(\n  Sample[Float](Tensor[Float](T(5f, 5f)), Tensor[Float](T(2.0f))),\n  Sample[Float](Tensor[Float](T(-5f, -5f)), Tensor[Float](T(-2.0f))),\n  Sample[Float](Tensor[Float](T(-2f, 5f)), Tensor[Float](T(1.3f))),\n  Sample[Float](Tensor[Float](T(-5f, 2f)), Tensor[Float](T(0.1f))),\n  Sample[Float](Tensor[Float](T(5f, -2f)), Tensor[Float](T(-0.1f))),\n  Sample[Float](Tensor[Float](T(2f, -5f)), Tensor[Float](T(-1.3f)))\n)\nval trainData = sc.parallelize(samples, 1)\n\n// Define the model\nval optimizer = Optimizer[Float](model, trainData, MSECriterion[Float](), 4)\nEngine.init\noptimizer.optimize()\nprintln(model.weight)  The weight of linear is init randomly. But the output should be like  scala  println(model.weight)\n0.09316949      0.2887804\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]  python  from bigdl.nn.layer import Linear\nfrom bigdl.util.common import *\nfrom bigdl.nn.criterion import MSECriterion\nfrom bigdl.optim.optimizer import Optimizer, MaxIteration\nimport numpy as np\n\nmodel = Linear(2, 1)\nsamples = [\n  Sample.from_ndarray(np.array([5, 5]), np.array([2.0])),\n  Sample.from_ndarray(np.array([-5, -5]), np.array([-2.0])),\n  Sample.from_ndarray(np.array([-2, 5]), np.array([1.3])),\n  Sample.from_ndarray(np.array([-5, 2]), np.array([0.1])),\n  Sample.from_ndarray(np.array([5, -2]), np.array([-0.1])),\n  Sample.from_ndarray(np.array([2, -5]), np.array([-1.3]))\n]\ntrain_data = sc.parallelize(samples, 1)\ninit_engine()\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)\noptimizer.optimize()\nmodel.get_weights()[0]  The output should be like  array([[ 0.11578175,  0.28315681]], dtype=float32)  You can see the model is trained.", 
            "title": "How to use Optimizer"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/#define-when-to-end-the-training", 
            "text": "You need define when to end the training. It can be several iterations, or how many round\ndata you want to process, a.k.a epoch.  scala  // The default endWhen in scala is 100 iterations\noptimizer.setEndWhen(Trigger.maxEpoch(10))  // Change to 10 epoch  python  # Python need to define in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)", 
            "title": "Define when to end the training"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/#change-the-optimization-algorithm", 
            "text": "Gradient based optimization algorithms are the most popular algorithms to train the neural\nnetwork model. The most famous one is SGD. SGD has many variants, adagrad, adam, etc.  scala  // The default is SGD\noptimizer.setOptimMethod(new Adam())  // Change to adam  python  # Python need to define the optimization algorithm in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4, optim_method = Adam())", 
            "title": "Change the optimization algorithm"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/#validate-your-model-in-training", 
            "text": "Sometimes, people want to evaluate the model with a seperated dataset. When model\nperforms well on train dataset, but bad on validation dataset, we call the model is overfit or\nweak generalization. People may want to evaluate the model every serveral iterations or \nepochs. BigDL can easily do this by  scala  optimizer.setValidation(trigger, testData, validationMethod, batchSize)  optimizer.set_validation(batch_size, val_rdd, trigger, validationMethod)  For validation, you need to provide   trigger: how often to do validation, maybe each several iterations or epochs  test data: the seperate dataset for test  validation method: how to evaluate the model, maybe top1 accuracy, etc.  batch size: how many data evaluate in one time", 
            "title": "Validate your model in training"
        }, 
        {
            "location": "/APIdocs/Optimizers/DistriOptimizer/#visualize-training-process", 
            "text": "See  Visualization with TensorBoard", 
            "title": "Visualize training process"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/", 
            "text": "Adam\n\n\nScala:\n\n\nval optim = new Adam(learningRate=1e-3, learningRateDecay=0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8)\n\n\n\n\nPython:\n\n\noptim = Adam(learningRate=1e-3, learningRateDecay-0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8, bigdl_type=\nfloat\n)\n\n\n\n\nAn implementation of Adam optimization, first-order gradient-based optimization of stochastic  objective  functions. http://arxiv.org/pdf/1412.6980.pdf\n\n\nlearningRate\n learning rate. Default value is 1e-3. \n\n\nlearningRateDecay\n learning rate decay. Default value is 0.0.\n\n\nbeta1\n first moment coefficient. Default value is 0.9.\n\n\nbeta2\n second moment coefficient. Default value is 0.999.\n\n\nEpsilon\n for numerical stability. Default value is 1e-8.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval optm = new Adam(learningRate=0.002)\ndef rosenBrock(x: Tensor[Float]): (Float, Tensor[Float]) = {\n    // (1) compute f(x)\n    val d = x.size(1)\n\n    // x1 = x(i)\n    val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // x(i + 1) - x(i)^2\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n    // 100 * (x(i + 1) - x(i)^2)^2\n    x1.cmul(x1).mul(100)\n\n    // x0 = x(i)\n    val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // 1-x(i)\n    x0.mul(-1).add(1)\n    x0.cmul(x0)\n    // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n    x1.add(x0)\n\n    val fout = x1.sum()\n\n    // (2) compute f(x)/dx\n    val dxout = Tensor[Float]().resizeAs(x).zero()\n    // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n    x1.copy(x.narrow(1, 1, d - 1))\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n    x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n    x1.add(x0)\n    dxout.narrow(1, 1, d - 1).copy(x1)\n\n    // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n    x0.copy(x.narrow(1, 1, d - 1))\n    x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n    dxout.narrow(1, 2, d - 1).add(x0)\n\n    (fout, dxout)\n  }  \nval x = Tensor(2).fill(0)\n\n print(optm.optimize(rosenBrock, x))\n(0.0019999996\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcD$sp of size 2],[D@302d88d8)\n\n\n\n\nPython example:\n\n\noptim_method = Adam(learningrate=0.002)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)\n\n\n\n\n\nSGD\n\n\nScala:\n\n\nval optimMethod = SGD(learningRate= 1e-3,learningRateDecay=0.0,\n                      weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                      nesterov=false,learningRateSchedule=Default(),\n                      learningRates=null,weightDecays=null)\n\n\n\n\nPython:\n\n\noptim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                   momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                   leaningrate_schedule=None,learningrates=None,\n                   weightdecays=None,bigdl_type=\nfloat\n)\n\n\n\n\nA plain implementation of SGD which provides optimize method. After setting \noptimization method when create Optimize, Optimize will call optimization method at the end of \neach iteration.\n\n\nScala example:\n\n\nval optimMethod = new SGD[Float](learningRate= 1e-3,learningRateDecay=0.0,\n                               weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                               nesterov=false,learningRateSchedule=Default(),\n                               learningRates=null,weightDecays=null)\noptimizer.setOptimMethod(optimMethod)\n\n\n\n\nPython example:\n\n\noptim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                  momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                  leaningrate_schedule=None,learningrates=None,\n                  weightdecays=None,bigdl_type=\nfloat\n)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)\n\n\n\n\nAdadelta\n\n\nAdaDelta\n implementation for \nSGD\n \nIt has been proposed in \nADADELTA: An Adaptive Learning Rate Method\n.\nhttp://arxiv.org/abs/1212.5701.\n\n\nScala:\n\n\nval optimMethod = Adadelta(decayRate = 0.9, Epsilon = 1e-10)\n\n\n\n\nPython:\n\n\noptim_method = AdaDelta(decayrate = 0.9, epsilon = 1e-10)\n\n\n\n\nScala example:\n\n\noptimizer.setOptimMethod(new Adadelta(0.9, 1e-10))\n\n\n\n\n\nPython example:\n\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adadelta(0.9, 0.00001),\n    end_trigger=MaxEpoch(20),\n    batch_size=32)\n\n\n\n\nRMSprop\n\n\nAn implementation of RMSprop (Reference: http://arxiv.org/pdf/1308.0850v5.pdf, Sec 4.2)\n\n learningRate : learning rate\n\n learningRateDecaye : learning rate decay\n\n decayRatee : decayRate, also called rho\n\n Epsilone : for numerical stability\n\n\nAdamax\n\n\nAn implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf\n\n\nArguments:\n\n\n\n\nlearningRate : learning rate\n\n\nbeta1 : first moment coefficient\n\n\nbeta2 : second moment coefficient\n\n\nEpsilon : for numerical stability\n\n\n\n\nReturns:\n\n\nthe new x vector and the function list {fx}, evaluated before the update\n\n\nAdagrad\n\n\nScala:\n\n\nval adagrad = new Adagrad(learningRate = 1e-3,\n                          learningRateDecay = 0.0,\n                          weightDecay = 0.0)\n\n\n\n\n\nAn implementation of Adagrad. See the original paper:\n \nhttp://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor._\nval adagrad = Adagrad(0.01, 0.0, 0.0)\n    def feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n      // (1) compute f(x)\n      val d = x.size(1)\n      // x1 = x(i)\n      val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // x(i + 1) - x(i)^2\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n      // 100 * (x(i + 1) - x(i)^2)^2\n      x1.cmul(x1).mul(100)\n      // x0 = x(i)\n      val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // 1-x(i)\n      x0.mul(-1).add(1)\n      x0.cmul(x0)\n      // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n      x1.add(x0)\n      val fout = x1.sum()\n      // (2) compute f(x)/dx\n      val dxout = Tensor[Float]().resizeAs(x).zero()\n      // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n      x1.copy(x.narrow(1, 1, d - 1))\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n      x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n      x1.add(x0)\n      dxout.narrow(1, 1, d - 1).copy(x1)\n      // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n      x0.copy(x.narrow(1, 1, d - 1))\n      x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n      dxout.narrow(1, 2, d - 1).add(x0)\n      (fout, dxout)\n    }\nval x = Tensor(2).fill(0)\nval config = T(\nlearningRate\n -\n 1e-1)\nfor (i \n- 1 to 10) {\n  adagrad.optimize(feval, x, config, config)\n}\nx after optimize: 0.27779138\n0.07226955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\n\n\n\nScala:\n\n\nval optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)\n\n\n\n\nPython:\n\n\noptim_method = LBFGS(max_iter=20, max_eval=Double.MaxValue, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)\n\n\n\n\nThis implementation of L-BFGS relies on a user-provided line search function\n(state.lineSearch). If this function is not provided, then a simple learningRate\nis used to produce fixed size steps. Fixed size steps are much less costly than line\nsearches, and can be useful for stochastic problems.\n\n\nThe learning rate is used even when a line search is provided.This is also useful for\nlarge-scale stochastic problems, where opfunc is a noisy approximation of f(x). In that\ncase, the learning rate allows a reduction of confidence in the step size.\n\n\nParameters:\n\n\n \nmaxIter\n - Maximum number of iterations allowed. Default: 20\n\n \nmaxEval\n - Maximum number of function evaluations. Default: Double.MaxValue\n\n \ntolFun\n - Termination tolerance on the first-order optimality. Default: 1e-5\n\n \ntolX\n - Termination tol on progress in terms of func/param changes. Default: 1e-9\n\n \nlearningRate\n - the learning rate. Default: 1.0\n\n \nlineSearch\n - A line search function. Default: None\n* \nlineSearchOptions\n - If no line search provided, then a fixed step size is used. Default: None\n\n\nScala example:\n\n\nval optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)\noptimizer.setOptimMethod(optimMethod)\n\n\n\n\nPython example:\n\n\noptim_method = LBFGS(max_iter=20, max_eval=DOUBLEMAX, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Optimization Algorithms"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/#adam", 
            "text": "Scala:  val optim = new Adam(learningRate=1e-3, learningRateDecay=0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8)  Python:  optim = Adam(learningRate=1e-3, learningRateDecay-0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8, bigdl_type= float )  An implementation of Adam optimization, first-order gradient-based optimization of stochastic  objective  functions. http://arxiv.org/pdf/1412.6980.pdf  learningRate  learning rate. Default value is 1e-3.   learningRateDecay  learning rate decay. Default value is 0.0.  beta1  first moment coefficient. Default value is 0.9.  beta2  second moment coefficient. Default value is 0.999.  Epsilon  for numerical stability. Default value is 1e-8.  Scala example:  import com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval optm = new Adam(learningRate=0.002)\ndef rosenBrock(x: Tensor[Float]): (Float, Tensor[Float]) = {\n    // (1) compute f(x)\n    val d = x.size(1)\n\n    // x1 = x(i)\n    val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // x(i + 1) - x(i)^2\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n    // 100 * (x(i + 1) - x(i)^2)^2\n    x1.cmul(x1).mul(100)\n\n    // x0 = x(i)\n    val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // 1-x(i)\n    x0.mul(-1).add(1)\n    x0.cmul(x0)\n    // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n    x1.add(x0)\n\n    val fout = x1.sum()\n\n    // (2) compute f(x)/dx\n    val dxout = Tensor[Float]().resizeAs(x).zero()\n    // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n    x1.copy(x.narrow(1, 1, d - 1))\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n    x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n    x1.add(x0)\n    dxout.narrow(1, 1, d - 1).copy(x1)\n\n    // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n    x0.copy(x.narrow(1, 1, d - 1))\n    x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n    dxout.narrow(1, 2, d - 1).add(x0)\n\n    (fout, dxout)\n  }  \nval x = Tensor(2).fill(0)  print(optm.optimize(rosenBrock, x))\n(0.0019999996\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcD$sp of size 2],[D@302d88d8)  Python example:  optim_method = Adam(learningrate=0.002)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Adam"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/#sgd", 
            "text": "Scala:  val optimMethod = SGD(learningRate= 1e-3,learningRateDecay=0.0,\n                      weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                      nesterov=false,learningRateSchedule=Default(),\n                      learningRates=null,weightDecays=null)  Python:  optim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                   momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                   leaningrate_schedule=None,learningrates=None,\n                   weightdecays=None,bigdl_type= float )  A plain implementation of SGD which provides optimize method. After setting \noptimization method when create Optimize, Optimize will call optimization method at the end of \neach iteration.  Scala example:  val optimMethod = new SGD[Float](learningRate= 1e-3,learningRateDecay=0.0,\n                               weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                               nesterov=false,learningRateSchedule=Default(),\n                               learningRates=null,weightDecays=null)\noptimizer.setOptimMethod(optimMethod)  Python example:  optim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                  momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                  leaningrate_schedule=None,learningrates=None,\n                  weightdecays=None,bigdl_type= float )\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "SGD"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/#adadelta", 
            "text": "AdaDelta  implementation for  SGD  \nIt has been proposed in  ADADELTA: An Adaptive Learning Rate Method .\nhttp://arxiv.org/abs/1212.5701.  Scala:  val optimMethod = Adadelta(decayRate = 0.9, Epsilon = 1e-10)  Python:  optim_method = AdaDelta(decayrate = 0.9, epsilon = 1e-10)  Scala example:  optimizer.setOptimMethod(new Adadelta(0.9, 1e-10))  Python example:  optimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adadelta(0.9, 0.00001),\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Adadelta"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/#rmsprop", 
            "text": "An implementation of RMSprop (Reference: http://arxiv.org/pdf/1308.0850v5.pdf, Sec 4.2)  learningRate : learning rate  learningRateDecaye : learning rate decay  decayRatee : decayRate, also called rho  Epsilone : for numerical stability", 
            "title": "RMSprop"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/#adamax", 
            "text": "An implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf  Arguments:   learningRate : learning rate  beta1 : first moment coefficient  beta2 : second moment coefficient  Epsilon : for numerical stability   Returns:  the new x vector and the function list {fx}, evaluated before the update", 
            "title": "Adamax"
        }, 
        {
            "location": "/APIdocs/Optimizers/Optim_Methods/merged-Optim_Methods/#adagrad", 
            "text": "Scala:  val adagrad = new Adagrad(learningRate = 1e-3,\n                          learningRateDecay = 0.0,\n                          weightDecay = 0.0)  An implementation of Adagrad. See the original paper:\n  http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor._\nval adagrad = Adagrad(0.01, 0.0, 0.0)\n    def feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n      // (1) compute f(x)\n      val d = x.size(1)\n      // x1 = x(i)\n      val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // x(i + 1) - x(i)^2\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n      // 100 * (x(i + 1) - x(i)^2)^2\n      x1.cmul(x1).mul(100)\n      // x0 = x(i)\n      val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // 1-x(i)\n      x0.mul(-1).add(1)\n      x0.cmul(x0)\n      // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n      x1.add(x0)\n      val fout = x1.sum()\n      // (2) compute f(x)/dx\n      val dxout = Tensor[Float]().resizeAs(x).zero()\n      // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n      x1.copy(x.narrow(1, 1, d - 1))\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n      x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n      x1.add(x0)\n      dxout.narrow(1, 1, d - 1).copy(x1)\n      // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n      x0.copy(x.narrow(1, 1, d - 1))\n      x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n      dxout.narrow(1, 2, d - 1).add(x0)\n      (fout, dxout)\n    }\nval x = Tensor(2).fill(0)\nval config = T( learningRate  -  1e-1)\nfor (i  - 1 to 10) {\n  adagrad.optimize(feval, x, config, config)\n}\nx after optimize: 0.27779138\n0.07226955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]  Scala:  val optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)  Python:  optim_method = LBFGS(max_iter=20, max_eval=Double.MaxValue, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)  This implementation of L-BFGS relies on a user-provided line search function\n(state.lineSearch). If this function is not provided, then a simple learningRate\nis used to produce fixed size steps. Fixed size steps are much less costly than line\nsearches, and can be useful for stochastic problems.  The learning rate is used even when a line search is provided.This is also useful for\nlarge-scale stochastic problems, where opfunc is a noisy approximation of f(x). In that\ncase, the learning rate allows a reduction of confidence in the step size.  Parameters:    maxIter  - Maximum number of iterations allowed. Default: 20   maxEval  - Maximum number of function evaluations. Default: Double.MaxValue   tolFun  - Termination tolerance on the first-order optimality. Default: 1e-5   tolX  - Termination tol on progress in terms of func/param changes. Default: 1e-9   learningRate  - the learning rate. Default: 1.0   lineSearch  - A line search function. Default: None\n*  lineSearchOptions  - If no line search provided, then a fixed step size is used. Default: None  Scala example:  val optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)\noptimizer.setOptimMethod(optimMethod)  Python example:  optim_method = LBFGS(max_iter=20, max_eval=DOUBLEMAX, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Adagrad"
        }, 
        {
            "location": "/APIdocs/Optimizers/Triggers/", 
            "text": "Triggers\n\n\nScala:\n\n\nTrigger.severalIteration(iterationNum)\n\n\n\n\nPython:\n\n\nMaxEpoch(endTriggerNum)\n\n\n\n\nor\n\n\nMaxIteration(endTriggerNum)\n\n\n\n\nA trigger specifies a timespot or several timespots during training,\nand a corresponding action will be taken when the timespot(s)\ns reached.\n\n\n\n\neveryEpoch\n\n\n\n\nA trigger that triggers an action when each epoch finishs.\n   Could be used as trigger in setValidation and setCheckpoint\n   in Optimizer, and also in TrainSummary.setSummaryTrigger.\n\n\n\n\n\n\nseveralIteration\n\n\nA trigger that triggers an action every \"n\" iterations.\nCould be used as trigger in setValidation and setCheckpoint\nin Optimizer, and also in TrainSummary.setSummaryTrigger.\n\n\n\n\n\n\nmaxEpoch\n\n\n\n\n\n\nA trigger that triggers an action when training reaches\n   the number of epochs specified by \"max\".\n   Usually used in Optimizer.setEndWhen.\n\n\n\n\n\n\nmaxIteration\n\n\nA trigger that triggers an action when training reaches\nthe number of iterations specified by \"max\".\nUsually used in Optimizer.setEndWhen.", 
            "title": "Triggers"
        }, 
        {
            "location": "/APIdocs/Optimizers/Triggers/#triggers", 
            "text": "Scala:  Trigger.severalIteration(iterationNum)  Python:  MaxEpoch(endTriggerNum)  or  MaxIteration(endTriggerNum)  A trigger specifies a timespot or several timespots during training,\nand a corresponding action will be taken when the timespot(s)\ns reached.   everyEpoch   A trigger that triggers an action when each epoch finishs.\n   Could be used as trigger in setValidation and setCheckpoint\n   in Optimizer, and also in TrainSummary.setSummaryTrigger.    severalIteration  A trigger that triggers an action every \"n\" iterations.\nCould be used as trigger in setValidation and setCheckpoint\nin Optimizer, and also in TrainSummary.setSummaryTrigger.    maxEpoch    A trigger that triggers an action when training reaches\n   the number of epochs specified by \"max\".\n   Usually used in Optimizer.setEndWhen.    maxIteration  A trigger that triggers an action when training reaches\nthe number of iterations specified by \"max\".\nUsually used in Optimizer.setEndWhen.", 
            "title": "Triggers"
        }, 
        {
            "location": "/APIdocs/Optimizers/ResumeTraining/", 
            "text": "Resume Training", 
            "title": "Resume Training"
        }, 
        {
            "location": "/APIdocs/Optimizers/ResumeTraining/#resume-training", 
            "text": "", 
            "title": "Resume Training"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/", 
            "text": "CustomizedInitializer\n\n\nAll customizedInitializer should implement the \nInitializationMethod\n trait\n\n\n/**\n * Initialization method to initialize bias and weight.\n * The init method will be called in Module.reset()\n */\n\ntrait InitializationMethod {\n\n  type Shape = Array[Int]\n\n  /**\n   * Initialize the given variable\n   *\n   * @param variable    the variable to initialize\n   * @param dataFormat  describe the meaning of each dimension of the variable\n   */\n  def init[T](variable: Tensor[T], dataFormat: VariableFormat)\n             (implicit ev: TensorNumeric[T]): Unit\n}\n\n\n\n\nThe \nRandomUniform\n\ncode should give you a good sense of how to implement this trait.\n\n\nPython\n\n\nCustom initialization method in python is not supported right now.\n\n\nZeros\n\n\nScala:\n\n\nval initMethod = Zeros\n\n\n\n\n\nPython:\n\n\ninit_method = Zeros()\n\n\n\n\nInitialization method that set tensor to zeros.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Zeros\nval biasInitMethod = Zeros\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n\n {\n    weight: 0.0 0.0 0.0 \n            0.0 0.0 0.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = Zeros()\nbias_init = Zeros()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createZeros\ncreating: createZeros\ncreating: createLinear\nweight:\n[[ 0.  0.  0.]\n [ 0.  0.  0.]]\nbias: \n[ 0.  0.]\n\n\n\n\nXavier\n\n\nScala:\n\n\nval initMethod = Xavier\n\n\n\n\n\nPython:\n\n\ninit_method = Xavier()\n\n\n\n\nThe Xavier initialization method draws samples from a uniform distribution\nbounded by [-limit, limit) where limit = sqrt(6.0/(fanIn+fanOut)). The rationale\nbehind this formula can be found in the paper\n\nUnderstanding the difficulty of training deep feedforward neural networks\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Xavier\nval biasInitMethod = Xavier\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: -0.78095555 -0.09939616 0.12034761  \n            -0.3019594  0.11734331  0.80369484  \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0727772\n          -0.6703765\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = Xavier()\nbias_init = Xavier()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createXavier\ncreating: createXavier\ncreating: createLinear\nweight:\n[[ 0.00580597 -0.73662472  0.13767919]\n [ 0.16802482 -0.49394709 -0.74967551]]\nbias: \n[-1.12355328  0.0779365 ]\n\n\n\n\nBilinearFiller\n\n\nScala:\n\n\nval initMethod = BilinearFiller\n\n\n\n\n\nPython:\n\n\ninit_method = BilinearFiller()\n\n\n\n\nInitialize the weight with coefficients for bilinear interpolation. A common use case is with the DeconvolutionLayer acting as upsampling. This initialization method can only be used in the weight initialization of SpatialFullConvolution.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = BilinearFiller\nval biasInitMethod - Zeros\nval model = SpatialFullConvolution(2, 3, 2, 2).setName(\nsfconv\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nsfconv\n).get)\n\n\n\n\n{\n    weight: (1,1,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n    bias: 0.0\n          0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradBias: 0.0\n              0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradWeight: (1,1,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = BilinearFiller()\nbias_init = Zeros()\nmodel =  SpatialFullConvolution(2, 3, 2, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createBilinearFiller\ncreating: createZeros\ncreating: createSpatialFullConvolution\nweight:\n[[[[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]\n\n\n  [[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]]]\nbias: \n[ 0.  0.  0.]\n\n\n\n\n\n\nRandomNormal\n\n\nScala:\n\n\nval initMethod = RandomNormal(mean, stdv)\n\n\n\n\n\nPython:\n\n\ninit_method = RandomNormal(mean, stdv)\n\n\n\n\nThis initialization method draws samples from a normal distribution.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomNormal(0, 1)\nval biasInitMethod = RandomNormal(0, 1)\nval linear = Linear(3, 2).setName(\nlinear1\n)\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: -0.5908564  0.32844943  -0.845019   \n            0.21550806  1.2037253   0.6807024   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.5345903\n          -0.76420456\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n  }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nfrom bigdl.nn.layer import *\n\nweight_init = RandomNormal(0, 1)\nbias_init = RandomNormal(0, 1)\nlinear= Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(linear.get_weights()[0])\nprint(\nbias: \n)\nprint(linear.get_weights()[1])\n\n\n\n\ncreating: createRandomNormal\ncreating: createRandomNormal\ncreating: createLinear\nweight:\n[[-0.00784962  0.77845585 -1.16250944]\n [ 0.03195094 -0.15211993  0.6254822 ]]\nbias: \n[-0.37883148 -0.81106091]\n\n\n\n\n\nOnes\n\n\nScala:\n\n\nval initMethod = Ones\n\n\n\n\n\nPython:\n\n\ninit_method = Ones()\n\n\n\n\nInitialization method that set tensor to be ones.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Ones\nval biasInitMethod = Ones\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: 1.0 1.0 1.0 \n            1.0 1.0 1.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0\n          1.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = Ones()\nbias_init = Ones()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createOnes\ncreating: createOnes\ncreating: createLinear\nweight:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\nbias: \n[ 1.  1.]\n\n\n\n\n\nConstInitMethod\n\n\nScala:\n\n\nval initMethod = ConstInitMethod(value: Double)\n\n\n\n\n\nPython:\n\n\ninit_method = ConstInitMethod(value)\n\n\n\n\nInitialization method that set tensor to the specified constant value.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\n\nval weightInitMethod = ConstInitMethod(0.2)\nval biasInitMethod = ConstInitMethod(0.2)\nval linear = Linear(3, 2).setName(\nlinear1\n)\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: 0.2 0.2 0.2\n            0.2 0.2 0.2\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.2\n          0.2\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0\n                0.0 0.0 0.0\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = ConstInitMethod(0.2)\nbias_init = ConstInitMethod(0.2)\nlinear = Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(linear.get_weights()[0])\nprint(\nbias: \n)\nprint(linear.get_weights()[1])\n\n\n\n\ncreating: createConstInitMethod\ncreating: createConstInitMethod\ncreating: createLinear\nweight:\n[[ 0.2  0.2  0.2]\n [ 0.2  0.2  0.2]]\nbias:\n[ 0.2  0.2]\n\n\n\n\n\nRandomUniform\n\n\nScala:\n\n\nval initMethod = RandomUniform(lower, upper)\n\n\n\n\n\nPython:\n\n\ninit_method = RandomUniform(upper=None, lower=None)\n\n\n\n\nThis initialization method draws samples from a uniform distribution. If the lower bound and upper bound of this uniform distribution is not specified, it will be set to [-limit, limit) where limit = 1/sqrt(fanIn).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomUniform\nval biasInitMethod = RandomUniform(0, 1)\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: -0.572536   0.13046022  -0.040449623    \n            -0.547542   0.19093458  0.5632484   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.785292\n          0.63280666\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = RandomUniform()\nbias_init = RandomUniform()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createRandomUniform\ncreating: createRandomUniform\ncreating: createLinear\nweight:\n[[ 0.53153235  0.53016287  0.32831791]\n [-0.45736417 -0.16206641  0.21758588]]\nbias: \n[ 0.32058391  0.26307678]", 
            "title": "Initalizers"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#customizedinitializer", 
            "text": "All customizedInitializer should implement the  InitializationMethod  trait  /**\n * Initialization method to initialize bias and weight.\n * The init method will be called in Module.reset()\n */\n\ntrait InitializationMethod {\n\n  type Shape = Array[Int]\n\n  /**\n   * Initialize the given variable\n   *\n   * @param variable    the variable to initialize\n   * @param dataFormat  describe the meaning of each dimension of the variable\n   */\n  def init[T](variable: Tensor[T], dataFormat: VariableFormat)\n             (implicit ev: TensorNumeric[T]): Unit\n}  The  RandomUniform \ncode should give you a good sense of how to implement this trait.", 
            "title": "CustomizedInitializer"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#python", 
            "text": "Custom initialization method in python is not supported right now.", 
            "title": "Python"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#zeros", 
            "text": "Scala:  val initMethod = Zeros  Python:  init_method = Zeros()  Initialization method that set tensor to zeros.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Zeros\nval biasInitMethod = Zeros\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)  \n {\n    weight: 0.0 0.0 0.0 \n            0.0 0.0 0.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = Zeros()\nbias_init = Zeros()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createZeros\ncreating: createZeros\ncreating: createLinear\nweight:\n[[ 0.  0.  0.]\n [ 0.  0.  0.]]\nbias: \n[ 0.  0.]", 
            "title": "Zeros"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#xavier", 
            "text": "Scala:  val initMethod = Xavier  Python:  init_method = Xavier()  The Xavier initialization method draws samples from a uniform distribution\nbounded by [-limit, limit) where limit = sqrt(6.0/(fanIn+fanOut)). The rationale\nbehind this formula can be found in the paper Understanding the difficulty of training deep feedforward neural networks .  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Xavier\nval biasInitMethod = Xavier\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)   {\n    weight: -0.78095555 -0.09939616 0.12034761  \n            -0.3019594  0.11734331  0.80369484  \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0727772\n          -0.6703765\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = Xavier()\nbias_init = Xavier()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createXavier\ncreating: createXavier\ncreating: createLinear\nweight:\n[[ 0.00580597 -0.73662472  0.13767919]\n [ 0.16802482 -0.49394709 -0.74967551]]\nbias: \n[-1.12355328  0.0779365 ]", 
            "title": "Xavier"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#bilinearfiller", 
            "text": "Scala:  val initMethod = BilinearFiller  Python:  init_method = BilinearFiller()  Initialize the weight with coefficients for bilinear interpolation. A common use case is with the DeconvolutionLayer acting as upsampling. This initialization method can only be used in the weight initialization of SpatialFullConvolution.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = BilinearFiller\nval biasInitMethod - Zeros\nval model = SpatialFullConvolution(2, 3, 2, 2).setName( sfconv )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( sfconv ).get)  {\n    weight: (1,1,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n    bias: 0.0\n          0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradBias: 0.0\n              0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradWeight: (1,1,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = BilinearFiller()\nbias_init = Zeros()\nmodel =  SpatialFullConvolution(2, 3, 2, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createBilinearFiller\ncreating: createZeros\ncreating: createSpatialFullConvolution\nweight:\n[[[[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]\n\n\n  [[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]]]\nbias: \n[ 0.  0.  0.]", 
            "title": "BilinearFiller"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#randomnormal", 
            "text": "Scala:  val initMethod = RandomNormal(mean, stdv)  Python:  init_method = RandomNormal(mean, stdv)  This initialization method draws samples from a normal distribution.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomNormal(0, 1)\nval biasInitMethod = RandomNormal(0, 1)\nval linear = Linear(3, 2).setName( linear1 )\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get( linear1 ).get)   {\n    weight: -0.5908564  0.32844943  -0.845019   \n            0.21550806  1.2037253   0.6807024   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.5345903\n          -0.76420456\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n  }  Python example:  from bigdl.nn.initialization_method import *\nfrom bigdl.nn.layer import *\n\nweight_init = RandomNormal(0, 1)\nbias_init = RandomNormal(0, 1)\nlinear= Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(linear.get_weights()[0])\nprint( bias:  )\nprint(linear.get_weights()[1])  creating: createRandomNormal\ncreating: createRandomNormal\ncreating: createLinear\nweight:\n[[-0.00784962  0.77845585 -1.16250944]\n [ 0.03195094 -0.15211993  0.6254822 ]]\nbias: \n[-0.37883148 -0.81106091]", 
            "title": "RandomNormal"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#ones", 
            "text": "Scala:  val initMethod = Ones  Python:  init_method = Ones()  Initialization method that set tensor to be ones.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Ones\nval biasInitMethod = Ones\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)   {\n    weight: 1.0 1.0 1.0 \n            1.0 1.0 1.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0\n          1.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = Ones()\nbias_init = Ones()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createOnes\ncreating: createOnes\ncreating: createLinear\nweight:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\nbias: \n[ 1.  1.]", 
            "title": "Ones"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#constinitmethod", 
            "text": "Scala:  val initMethod = ConstInitMethod(value: Double)  Python:  init_method = ConstInitMethod(value)  Initialization method that set tensor to the specified constant value.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\n\nval weightInitMethod = ConstInitMethod(0.2)\nval biasInitMethod = ConstInitMethod(0.2)\nval linear = Linear(3, 2).setName( linear1 )\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get( linear1 ).get)   {\n    weight: 0.2 0.2 0.2\n            0.2 0.2 0.2\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.2\n          0.2\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0\n                0.0 0.0 0.0\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = ConstInitMethod(0.2)\nbias_init = ConstInitMethod(0.2)\nlinear = Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(linear.get_weights()[0])\nprint( bias:  )\nprint(linear.get_weights()[1])  creating: createConstInitMethod\ncreating: createConstInitMethod\ncreating: createLinear\nweight:\n[[ 0.2  0.2  0.2]\n [ 0.2  0.2  0.2]]\nbias:\n[ 0.2  0.2]", 
            "title": "ConstInitMethod"
        }, 
        {
            "location": "/APIdocs/Initializers/merged-Initializers/#randomuniform", 
            "text": "Scala:  val initMethod = RandomUniform(lower, upper)  Python:  init_method = RandomUniform(upper=None, lower=None)  This initialization method draws samples from a uniform distribution. If the lower bound and upper bound of this uniform distribution is not specified, it will be set to [-limit, limit) where limit = 1/sqrt(fanIn).  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomUniform\nval biasInitMethod = RandomUniform(0, 1)\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)   {\n    weight: -0.572536   0.13046022  -0.040449623    \n            -0.547542   0.19093458  0.5632484   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.785292\n          0.63280666\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = RandomUniform()\nbias_init = RandomUniform()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createRandomUniform\ncreating: createRandomUniform\ncreating: createLinear\nweight:\n[[ 0.53153235  0.53016287  0.32831791]\n [-0.45736417 -0.16206641  0.21758588]]\nbias: \n[ 0.32058391  0.26307678]", 
            "title": "RandomUniform"
        }, 
        {
            "location": "/APIdocs/Regularizers/merged-Regularizers/", 
            "text": "Regularizers\n\n\nScala:\n\n\nval l1Regularizer = L1Regularizer(l1)\nval l2Regularizer = L2Regularizer(l2)\nval l1l2Regularizer = L1L2Regularizer(l1, l2)\n\n\n\n\nPython:\n\n\nregularizerl1 = L1Regularizer(0.1)\nregularizerl2 = L2Regularizer(0.1)\nregularizerl1l2 = L1L2Regularizer(0.1, 0.1)\n\n\n\n\nL1 and L2 regularizers are used to avoid overfitting.\n\n\nScala example:\n\n\n\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nRNG.setSeed(100)\n\nval input = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L2Regularizer(0.1), bRegularizer = L2Regularizer(0.1))\n\nval output = linear.forward(input)\n\n\n println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x5]\n\n\n\n println(output)\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.34797725     0.25985366      -0.1107063      0.44529563      0.18934922\n-0.36947984     0.3738199       0.033136755     0.68634266      0.31736165\n-0.21293467     -0.16091438     -0.15637109     0.12860553      0.2332296\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\nlinear = Linear(5, 5, wRegularizer = L2Regularizer(0.1), bRegularizer = L2Regularizer(0.1))\noutput = linear.forward(input)\n\n\n input\narray([[ 0.14070892,  0.35661909,  0.00720507,  0.96832764,  0.34936094],\n       [ 0.14347534,  0.74504513,  0.16517557,  0.27037948,  0.28448409],\n       [ 0.28334993,  0.37042555,  0.32039529,  0.66894925,  0.19935906]], dtype=float32)\n\n\n output\narray([[-0.482759  , -0.12087041, -0.76120645,  0.16693172, -0.21038117],\n       [-0.17725618, -0.20931029, -0.53776515,  0.03298397, -0.40130591],\n       [-0.36628127, -0.32192633, -0.64229649,  0.16954683, -0.15714465]], dtype=float32)", 
            "title": "Regularizers"
        }, 
        {
            "location": "/APIdocs/Regularizers/merged-Regularizers/#regularizers", 
            "text": "Scala:  val l1Regularizer = L1Regularizer(l1)\nval l2Regularizer = L2Regularizer(l2)\nval l1l2Regularizer = L1L2Regularizer(l1, l2)  Python:  regularizerl1 = L1Regularizer(0.1)\nregularizerl2 = L2Regularizer(0.1)\nregularizerl1l2 = L1L2Regularizer(0.1, 0.1)  L1 and L2 regularizers are used to avoid overfitting.  Scala example:  \nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nRNG.setSeed(100)\n\nval input = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L2Regularizer(0.1), bRegularizer = L2Regularizer(0.1))\n\nval output = linear.forward(input)  println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x5]  println(output)\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.34797725     0.25985366      -0.1107063      0.44529563      0.18934922\n-0.36947984     0.3738199       0.033136755     0.68634266      0.31736165\n-0.21293467     -0.16091438     -0.15637109     0.12860553      0.2332296\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\nlinear = Linear(5, 5, wRegularizer = L2Regularizer(0.1), bRegularizer = L2Regularizer(0.1))\noutput = linear.forward(input)  input\narray([[ 0.14070892,  0.35661909,  0.00720507,  0.96832764,  0.34936094],\n       [ 0.14347534,  0.74504513,  0.16517557,  0.27037948,  0.28448409],\n       [ 0.28334993,  0.37042555,  0.32039529,  0.66894925,  0.19935906]], dtype=float32)  output\narray([[-0.482759  , -0.12087041, -0.76120645,  0.16693172, -0.21038117],\n       [-0.17725618, -0.20931029, -0.53776515,  0.03298397, -0.40130591],\n       [-0.36628127, -0.32192633, -0.64229649,  0.16954683, -0.15714465]], dtype=float32)", 
            "title": "Regularizers"
        }, 
        {
            "location": "/APIdocs/Metrics/merged-Metrics/", 
            "text": "ValidationMethod\n\n\nValidationMethod is a method to validate the model during model trainning or evaluation.\nThe trait can be extended by user-defined method. Now we have defined Top1Accuracy, Top5Accuracy, Loss.\n\n\nTop1Accuracy\n\n\nCaculate the percentage that output's max probability index equals target.\n\n\nval top1accuracy = new Top1Accuracy()\n\n\nTop5Accuracy\n\n\nCaculate the percentage that target in output's top5 probability indexes.\n\n\nval top5accuracy = new Top5Accuracy()\n\n\nLoss\n\n\nCalculate loss of output and target with criterion. The default criterion is ClassNLLCriterion.\n\n\nval loss = new Loss(criterion)\n\n\nExample code\n\n\nFollowings are examples to evaluate LeNet5 model with Top1Accuracy, Top5Accuracy, Loss validation method.\n\n\nScala example code:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.models.lenet.LeNet5\n\nval conf = Engine.createSparkConf()\nval sc = new SparkContext(conf)\nEngine.init\n\nval data = new Array[Sample[Float]](10)\nvar i = 0\nwhile (i \n data.length) {\n  val input = Tensor[Float](28, 28).fill(0.8f)\n  val label = Tensor[Float](1).fill(1.0f)\n  data(i) = Sample(input, label)\n  i += 1\n}\nval model = LeNet5(classNum = 10)\nval dataSet = sc.parallelize(data, 4)\n\nval result = model.evaluate(dataSet, Array(new Top1Accuracy[Float](), new Top5Accuracy[Float](), new Loss[Float]()))\n\n\n\n\nresult is\n\n\nresult: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array((Accuracy(correct: 0, count: 10, accuracy: 0.0),Top1Accuracy), (Accuracy(correct: 10, count: 10, accuracy: 1.0),Top5Accuracy), ((Loss: 9.21948, count: 4, Average Loss: 2.30487),Loss))\n\n\n\n\nPython example code:\n\n\nfrom pyspark.context import SparkContext\nfrom bigdl.util.common import *\nfrom bigdl.nn.layer import *\nfrom bigdl.models.lenet.lenet5 import *\n\nsc = get_spark_context(conf=create_spark_conf())\ninit_engine()\n\ndata_len = 10\nbatch_size = 8\nFEATURES_DIM = 4\n\ndef gen_rand_sample():\n    features = np.random.uniform(0, 1, (FEATURES_DIM))\n    label = features.sum() + 0.4\n    return Sample.from_ndarray(features, label)\n\ntrainingData = sc.parallelize(range(0, data_len)).map(\n    lambda i: gen_rand_sample())\n\nmodel = Sequential()\nmodel.add(Linear(4, 5))\ntest_results = model.test(trainingData, batch_size, [\nTop1Accuracy\n, \nTop5Accuracy\n, \nLoss\n])\n\n\n\n\nresult is\n\n\n print test_results[0]\nTest result: 0.0, total_num: 10, method: Top1Accuracy\n\n print test_results[1]\nTest result: 0.0, total_num: 10, method: Top5Accuracy\n\n print test_results[2]\nTest result: 0.0870719999075, total_num: 10, method: loss", 
            "title": "Metrics"
        }, 
        {
            "location": "/APIdocs/Metrics/merged-Metrics/#validationmethod", 
            "text": "ValidationMethod is a method to validate the model during model trainning or evaluation.\nThe trait can be extended by user-defined method. Now we have defined Top1Accuracy, Top5Accuracy, Loss.", 
            "title": "ValidationMethod"
        }, 
        {
            "location": "/APIdocs/Metrics/merged-Metrics/#top1accuracy", 
            "text": "Caculate the percentage that output's max probability index equals target.  val top1accuracy = new Top1Accuracy()", 
            "title": "Top1Accuracy"
        }, 
        {
            "location": "/APIdocs/Metrics/merged-Metrics/#top5accuracy", 
            "text": "Caculate the percentage that target in output's top5 probability indexes.  val top5accuracy = new Top5Accuracy()", 
            "title": "Top5Accuracy"
        }, 
        {
            "location": "/APIdocs/Metrics/merged-Metrics/#loss", 
            "text": "Calculate loss of output and target with criterion. The default criterion is ClassNLLCriterion.  val loss = new Loss(criterion)", 
            "title": "Loss"
        }, 
        {
            "location": "/APIdocs/Metrics/merged-Metrics/#example-code", 
            "text": "Followings are examples to evaluate LeNet5 model with Top1Accuracy, Top5Accuracy, Loss validation method.  Scala example code:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.models.lenet.LeNet5\n\nval conf = Engine.createSparkConf()\nval sc = new SparkContext(conf)\nEngine.init\n\nval data = new Array[Sample[Float]](10)\nvar i = 0\nwhile (i   data.length) {\n  val input = Tensor[Float](28, 28).fill(0.8f)\n  val label = Tensor[Float](1).fill(1.0f)\n  data(i) = Sample(input, label)\n  i += 1\n}\nval model = LeNet5(classNum = 10)\nval dataSet = sc.parallelize(data, 4)\n\nval result = model.evaluate(dataSet, Array(new Top1Accuracy[Float](), new Top5Accuracy[Float](), new Loss[Float]()))  result is  result: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array((Accuracy(correct: 0, count: 10, accuracy: 0.0),Top1Accuracy), (Accuracy(correct: 10, count: 10, accuracy: 1.0),Top5Accuracy), ((Loss: 9.21948, count: 4, Average Loss: 2.30487),Loss))  Python example code:  from pyspark.context import SparkContext\nfrom bigdl.util.common import *\nfrom bigdl.nn.layer import *\nfrom bigdl.models.lenet.lenet5 import *\n\nsc = get_spark_context(conf=create_spark_conf())\ninit_engine()\n\ndata_len = 10\nbatch_size = 8\nFEATURES_DIM = 4\n\ndef gen_rand_sample():\n    features = np.random.uniform(0, 1, (FEATURES_DIM))\n    label = features.sum() + 0.4\n    return Sample.from_ndarray(features, label)\n\ntrainingData = sc.parallelize(range(0, data_len)).map(\n    lambda i: gen_rand_sample())\n\nmodel = Sequential()\nmodel.add(Linear(4, 5))\ntest_results = model.test(trainingData, batch_size, [ Top1Accuracy ,  Top5Accuracy ,  Loss ])  result is   print test_results[0]\nTest result: 0.0, total_num: 10, method: Top1Accuracy  print test_results[1]\nTest result: 0.0, total_num: 10, method: Top5Accuracy  print test_results[2]\nTest result: 0.0870719999075, total_num: 10, method: loss", 
            "title": "Example code"
        }, 
        {
            "location": "/APIdocs/Preprocessing/Transformer/", 
            "text": "Transformer\n\n\nTransformer is for pre-processing. In many deep learning workload, input data need to be pre-processed before fed into   model. For example, in CNN, the image file need to be decoded from some compressed format(e.g. jpeg) to float arrays,    normalized and cropped to some fixed shape. You can also find pre-processing in other types of deep learning work        load(e.g. NLP, speech recognition). In BigDL, we provide many pre-process procedures for user. They're implemented as    Transformer.\n\n\nThe transformer interface is\n\n\n\ntrait Transformer[A, B] extends Serializable {\n   def apply(prev: Iterator[A]): Iterator[B]\n }\n\n\n\n\nIt's simple, right? What a transformer do is convert a sequence of objects of Class A to a sequence of objects of Class  B.\n\n\nTransformer is flexible. You can chain them together to do pre-processing. Let's still use the CNN example, say first    we need read image files from given paths, then extract the image binaries to array of float, then normalized the image  content and crop a fixed size from the image at a random position. Here we need 4 transformers, \nPathToImage\n,           \nImageToArray\n, \nNormalizor\n and \nCropper\n. And then chain them together.", 
            "title": "Transformer"
        }, 
        {
            "location": "/APIdocs/Preprocessing/Transformer/#transformer", 
            "text": "Transformer is for pre-processing. In many deep learning workload, input data need to be pre-processed before fed into   model. For example, in CNN, the image file need to be decoded from some compressed format(e.g. jpeg) to float arrays,    normalized and cropped to some fixed shape. You can also find pre-processing in other types of deep learning work        load(e.g. NLP, speech recognition). In BigDL, we provide many pre-process procedures for user. They're implemented as    Transformer.  The transformer interface is  \ntrait Transformer[A, B] extends Serializable {\n   def apply(prev: Iterator[A]): Iterator[B]\n }  It's simple, right? What a transformer do is convert a sequence of objects of Class A to a sequence of objects of Class  B.  Transformer is flexible. You can chain them together to do pre-processing. Let's still use the CNN example, say first    we need read image files from given paths, then extract the image binaries to array of float, then normalized the image  content and crop a fixed size from the image at a random position. Here we need 4 transformers,  PathToImage ,            ImageToArray ,  Normalizor  and  Cropper . And then chain them together.", 
            "title": "Transformer"
        }, 
        {
            "location": "/APIdocs/Model_LoadSave/BigDLModel/", 
            "text": "", 
            "title": "Load/Save a BigDL Model"
        }, 
        {
            "location": "/APIdocs/Model_LoadSave/TensorflowModel/", 
            "text": "", 
            "title": "Load a Tensorflow Model"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/", 
            "text": "Visualization with TensorBoard\n\n\n\n\nGenerating summary info in BigDL\n\n\nTo enable visualization support, you need first properly configure the \nOptimizer\n to generate summary info for training (\nTrainSummary\n) and/or validation (\nValidationSummary\n) before invoking \nOptimizer.optimize()\n, as illustrated below: \n\n\nGenerating summary info in Scala\n\n\nval optimizer = Optimizer(...)\n...\nval logdir = \nmylogdir\n\nval appName = \nmyapp\n\nval trainSummary = TrainSummary(logdir, appName)\nval validationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n...\nval trained_model = optimizer.optimize()\n\n\n\n\nGenerating summary info in Python\n\n\noptimizer = Optimizer(...)\n...\nlog_dir = 'mylogdir'\napp_name = 'myapp'\ntrain_summary = TrainSummary(log_dir=log_dir, app_name=app_name)\nval_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)\noptimizer.set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n...\ntrainedModel = optimizer.optimize()\n\n\n\n\nAfter you start to run your spark job, the train and validation summary will be saved to \nmylogdir/myapp/train\n and \nmylogdir/myapp/validation\n respectively (Note: you may want to use different \nappName\n for different job runs to avoid possible conflicts.) You may then read the summary info as follows:\n\n\nReading summary info in Scala\n\n\nval trainLoss = trainSummary.readScalar(\nLoss\n)\nval validationLoss = validationSummary.readScalar(\nLoss\n)\n...\n\n\n\n\nReading summary info in Python\n\n\nloss = np.array(train_summary.read_scalar('Loss'))\nvalloss = np.array(val_summary.read_scalar('Loss'))\n...\n\n\n\n\nVisualizing training with TensorBoard\n\n\nWith the summary info generated, we can then use \nTensorBoard\n to visualize the behaviors of the BigDL program.  \n\n\nInstalling TensorBoard\n\n\nPrerequisites:\n\n Python verison: 2.7, 3.4, 3.5, or 3.6\n\n Pip version \n= 9.0.1\n\n\nTo install TensorBoard using Python 2, you may run the command:\n\n\npip install tensorboard==1.0.0a4\n\n\n\n\nTo install TensorBoard using Python 3, you may run the command:\n\n\npip3 install tensorboard==1.0.0a4\n\n\n\n\nPlease refer to \nthis page\n for possible issues when installing TensorBoard.\n\n\nLaunching TensorBoard\n\n\nYou can launch TensorBoard using the command below:\n\n\ntensorboard --logdir=/tmp/bigdl_summaries\n\n\n\n\nAfter that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006\n\n\nVisualizations in TensorBoard\n\n\nWithin the TensorBoard dashboard, you will be able to read the visualizations of each run, including the \u201cLoss\u201d and \u201cThroughput\u201d curves under the SCALARS tab (as illustrated below):\n\n\n\nAnd \u201cweights\u201d, \u201cbias\u201d, \u201cgradientWeights\u201d and \u201cgradientBias\u201d under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):", 
            "title": "Visualization"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/#visualization-with-tensorboard", 
            "text": "", 
            "title": "Visualization with TensorBoard"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/#generating-summary-info-in-bigdl", 
            "text": "To enable visualization support, you need first properly configure the  Optimizer  to generate summary info for training ( TrainSummary ) and/or validation ( ValidationSummary ) before invoking  Optimizer.optimize() , as illustrated below:   Generating summary info in Scala  val optimizer = Optimizer(...)\n...\nval logdir =  mylogdir \nval appName =  myapp \nval trainSummary = TrainSummary(logdir, appName)\nval validationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n...\nval trained_model = optimizer.optimize()  Generating summary info in Python  optimizer = Optimizer(...)\n...\nlog_dir = 'mylogdir'\napp_name = 'myapp'\ntrain_summary = TrainSummary(log_dir=log_dir, app_name=app_name)\nval_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)\noptimizer.set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n...\ntrainedModel = optimizer.optimize()  After you start to run your spark job, the train and validation summary will be saved to  mylogdir/myapp/train  and  mylogdir/myapp/validation  respectively (Note: you may want to use different  appName  for different job runs to avoid possible conflicts.) You may then read the summary info as follows:  Reading summary info in Scala  val trainLoss = trainSummary.readScalar( Loss )\nval validationLoss = validationSummary.readScalar( Loss )\n...  Reading summary info in Python  loss = np.array(train_summary.read_scalar('Loss'))\nvalloss = np.array(val_summary.read_scalar('Loss'))\n...", 
            "title": "Generating summary info in BigDL"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/#visualizing-training-with-tensorboard", 
            "text": "With the summary info generated, we can then use  TensorBoard  to visualize the behaviors of the BigDL program.", 
            "title": "Visualizing training with TensorBoard"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/#installing-tensorboard", 
            "text": "Prerequisites:  Python verison: 2.7, 3.4, 3.5, or 3.6  Pip version  = 9.0.1  To install TensorBoard using Python 2, you may run the command:  pip install tensorboard==1.0.0a4  To install TensorBoard using Python 3, you may run the command:  pip3 install tensorboard==1.0.0a4  Please refer to  this page  for possible issues when installing TensorBoard.", 
            "title": "Installing TensorBoard"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/#launching-tensorboard", 
            "text": "You can launch TensorBoard using the command below:  tensorboard --logdir=/tmp/bigdl_summaries  After that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006", 
            "title": "Launching TensorBoard"
        }, 
        {
            "location": "/UserGuide/visualization-with-tensorboard/#visualizations-in-tensorboard", 
            "text": "Within the TensorBoard dashboard, you will be able to read the visualizations of each run, including the \u201cLoss\u201d and \u201cThroughput\u201d curves under the SCALARS tab (as illustrated below):  And \u201cweights\u201d, \u201cbias\u201d, \u201cgradientWeights\u201d and \u201cgradientBias\u201d under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):", 
            "title": "Visualizations in TensorBoard"
        }, 
        {
            "location": "/APIdocs/scaladoc/", 
            "text": "javadoc", 
            "title": "Scala Docs"
        }, 
        {
            "location": "/APIdocs/scaladoc/#javadoc", 
            "text": "", 
            "title": "javadoc"
        }, 
        {
            "location": "/APIdocs/python-api-doc/", 
            "text": "pythonapidoc", 
            "title": "Python API Docs"
        }, 
        {
            "location": "/APIdocs/python-api-doc/#pythonapidoc", 
            "text": "", 
            "title": "pythonapidoc"
        }, 
        {
            "location": "/UserGuide/known-issues/", 
            "text": "Known Issues\n\n\n\n\n\n\n\n\nCurrently, BigDL uses synchronous mini-batch SGD in model training. The mini-batch size is expected to be a multiple of \ntotal cores\n used in the job.\n\n\n\n\n\n\nYou may observe very poor performance when running BigDL for Spark 2.0 with Java 7; it is highly recommended to use Java 8 when building and running BigDL for Spark 2.0.\n\n\n\n\n\n\nOn Spark 2.0, please use default Java serializer instead of Kryo because of \nKryo Issue 341\n. The issue has been fixed in Kryo 4.0. However, Spark 2.0 uses Kryo 3.0.3. Spark 1.5 and 1.6 do not have this problem.\n\n\n\n\n\n\nOn CentOS 6 and 7, please increase the max user processes to a larger value (e.g., 514585); otherwise, you may see errors like \"unable to create new native thread\".\n\n\n\n\n\n\nCurrently, BigDL will load all the training and validation data into memory during training. You may encounter errors if it runs out of memory.\n\n\n\n\n\n\nIf you meet the program stuck after \nSave model...\n on Mesos, check the \nspark.driver.memory\n and increase the value. Eg, VGG on Cifar10 may need 20G+.\n\n\n\n\n\n\nIf you meet \ncan't find executor core number\n on Mesos, you should pass the executor cores through \n--conf spark.executor.cores=xxx", 
            "title": "Known Issues"
        }, 
        {
            "location": "/UserGuide/known-issues/#known-issues", 
            "text": "Currently, BigDL uses synchronous mini-batch SGD in model training. The mini-batch size is expected to be a multiple of  total cores  used in the job.    You may observe very poor performance when running BigDL for Spark 2.0 with Java 7; it is highly recommended to use Java 8 when building and running BigDL for Spark 2.0.    On Spark 2.0, please use default Java serializer instead of Kryo because of  Kryo Issue 341 . The issue has been fixed in Kryo 4.0. However, Spark 2.0 uses Kryo 3.0.3. Spark 1.5 and 1.6 do not have this problem.    On CentOS 6 and 7, please increase the max user processes to a larger value (e.g., 514585); otherwise, you may see errors like \"unable to create new native thread\".    Currently, BigDL will load all the training and validation data into memory during training. You may encounter errors if it runs out of memory.    If you meet the program stuck after  Save model...  on Mesos, check the  spark.driver.memory  and increase the value. Eg, VGG on Cifar10 may need 20G+.    If you meet  can't find executor core number  on Mesos, you should pass the executor cores through  --conf spark.executor.cores=xxx", 
            "title": "Known Issues"
        }
    ]
}