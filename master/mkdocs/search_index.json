{
    "docs": [
        {
            "location": "/", 
            "text": "What is BigDL\n\n\nBigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.\n\n\n\n\n\n\nRich deep learning support.\n Modeled after \nTorch\n, BigDL provides comprehensive support for deep learning, including numeric computing (via \nTensor\n) and high level \nneural networks\n; in addition, users can load pre-trained \nCaffe\n or \nTorch\n models into Spark programs using BigDL.\n\n\n\n\n\n\nExtremely high performance.\n To achieve high performance, BigDL uses \nIntel MKL\n and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source \nCaffe\n, \nTorch\n or \nTensorFlow\n on a single-node Xeon (i.e., comparable with mainstream GPU).\n\n\n\n\n\n\nEfficiently scale-out.\n BigDL can efficiently scale out to perform data analytics at \"Big Data scale\", by leveraging \nApache Spark\n (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark. \n\n\n\n\n\n\n\n\nWhy BigDL?\n\n\nYou may want to write your deep learning programs using BigDL if:\n\n\n\n\n\n\nYou want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).\n\n\n\n\n\n\nYou want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.\n\n\n\n\n\n\nYou want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)\n\n\n\n\n\n\n\n\nGetting Help\n\n\n\n\n\n\nYou can join the \nBigDL Google Group\n (or subscribe to the \nMail List\n) for more questions and discussions on BigDL\n\n\n\n\n\n\nYou can post bug reports and feature requests at the \nIssue Page", 
            "title": "Overview"
        }, 
        {
            "location": "/#what-is-bigdl", 
            "text": "BigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.    Rich deep learning support.  Modeled after  Torch , BigDL provides comprehensive support for deep learning, including numeric computing (via  Tensor ) and high level  neural networks ; in addition, users can load pre-trained  Caffe  or  Torch  models into Spark programs using BigDL.    Extremely high performance.  To achieve high performance, BigDL uses  Intel MKL  and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source  Caffe ,  Torch  or  TensorFlow  on a single-node Xeon (i.e., comparable with mainstream GPU).    Efficiently scale-out.  BigDL can efficiently scale out to perform data analytics at \"Big Data scale\", by leveraging  Apache Spark  (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark.", 
            "title": "What is BigDL"
        }, 
        {
            "location": "/#why-bigdl", 
            "text": "You may want to write your deep learning programs using BigDL if:    You want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).    You want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.    You want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)", 
            "title": "Why BigDL?"
        }, 
        {
            "location": "/#getting-help", 
            "text": "You can join the  BigDL Google Group  (or subscribe to the  Mail List ) for more questions and discussions on BigDL    You can post bug reports and feature requests at the  Issue Page", 
            "title": "Getting Help"
        }, 
        {
            "location": "/release-download/", 
            "text": "These are built BigDL packages including dependency and python files. You can download these packages instead of building them by yourself. This is useful when you want to do something like run some examples or develop python code.\n\n\n\n\nNightly Build\n\n\nHere are the folders for nightly build packages. The packages are built from latest master code. You can download the .zip files with a timestamp suffix in the name. \n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\nWin64\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.2\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.2\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.2\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.1\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\n\n\nRelease 0.2.0\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\nWin64\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.2\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.2\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.2\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.1\n\n\ndownload\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\n\n\nRelease 0.1.1\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\n\n\nRelease 0.1.0\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload", 
            "title": "Download"
        }, 
        {
            "location": "/release-download/#nightly-build", 
            "text": "Here are the folders for nightly build packages. The packages are built from latest master code. You can download the .zip files with a timestamp suffix in the name.       Linux x64  Mac  Win64      Spark 1.5.2  download  download  download    Spark 1.6.2  download  download  download    Spark 2.0.2  download  download  download    Spark 2.1.1  download  download  download", 
            "title": "Nightly Build"
        }, 
        {
            "location": "/release-download/#release-020", 
            "text": "Linux x64  Mac  Win64      Spark 1.5.2  download  download  download    Spark 1.6.2  download  download  download    Spark 2.0.2  download  download  download    Spark 2.1.1  download  download  download", 
            "title": "Release 0.2.0"
        }, 
        {
            "location": "/release-download/#release-011", 
            "text": "Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download", 
            "title": "Release 0.1.1"
        }, 
        {
            "location": "/release-download/#release-010", 
            "text": "Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download", 
            "title": "Release 0.1.0"
        }, 
        {
            "location": "/release-docs/", 
            "text": "Latest Master\n\n\nMaster Docs\n\n\n\n\nRelease 0.2.0\n\n\nBigDL 0.2.0 Docs\n\n\n\n\nRelease 0.1.1\n\n\nBigDL 0.1.1 Docs", 
            "title": "Documentation"
        }, 
        {
            "location": "/release-docs/#latest-master", 
            "text": "Master Docs", 
            "title": "Latest Master"
        }, 
        {
            "location": "/release-docs/#release-020", 
            "text": "BigDL 0.2.0 Docs", 
            "title": "Release 0.2.0"
        }, 
        {
            "location": "/release-docs/#release-011", 
            "text": "BigDL 0.1.1 Docs", 
            "title": "Release 0.1.1"
        }, 
        {
            "location": "/getting-started/", 
            "text": "Before using BigDL\n\n\nBefore using BigDL, you need to install Apache Spark and obtain BigDL libraries. Then in your program, you need to ensure the SparkContext is created successfully and initialize BigDL engine before calling BigDL APIs. Navigate to \nScala User Guide/Install\n or \nPython User Guide/Install\n for details about how to install BigDL, and \nScala User Guide/Run\n or \nPython User Guide/Run\n for how to run programs.  \n\n\n\n\nPrepare your Data\n\n\nYour data need to be transformed into RDD of \nSample\n in order to be fed into BigDL for training, evaluation and prediction (also refer to \nOptimization\n and \nOptimizer API guide\n). \n\n\nTensor\n, \nTable\n are essential data structures that composes the basic dataflow inside the nerual network( e.g. input/output, gradients, weights, etc.). You will need to understand them to get a better idea of layer behaviors. \n\n\n\n\nUse BigDL for Prediction only\n\n\nIf you have an existing model and want to use BigDL only for prediction, you need first load the model, and then do prediction or evaluation. \n\n\nBigDL supports loading models trained and saved in BigDL, or a trained Caffe or Tensorflow model. \n\n\n\n\nTo load a BigDL model, you can use \nModule.load\n interface (Scala) or \nModel.load\n (in Python). Refer to \nModel Save\n for details.  \n\n\nTo load a Tensorflow model, refer to \nTensorflow Support\n for details.\n\n\nTo load a Caffe model, refer to \nCaffe Support\n for details.\n\n\n\n\nRefer to \nModel Predict\n for details about how to use a model for prediction.\n\n\nIf you are using the trained model as a component inside a Spark ML pipeline, refer to\n\nUsing BigDL in Spark ML Pipeline\n page for usage. \n\n\n\n\nTrain a Model from Scratch\n\n\nThe procedure of training a model from scratch usually involves following steps:\n\n\n\n\ndefine your model (by connecting layers/activations into a network)\n\n\ndecide your loss function (which function to optimize)\n\n\noptimization (choose a proper algorithm and hyper parameters, and train)\n\n\nevaluation (evaluate your model) \n\n\n\n\nBefore training models, please make sure BigDL is installed, BigDL engine initialized properly, and your data is in proper format. Refer to \nBefore using BigDL\n and \nPrepare Your Data\n for details.  \n\n\nThe most recommended way to create your first model is to modify from an existing one. BigDL provides plenty of models for you to refer to. See \nScala Models/Examples\n and \nPython Models/Examples and Tutorials\n. \n\n\nTo define a model, you can either use the Sequential API or Functional API. The Functional API is more flexible than Sequential API. Refer to \nSequential API\n and \nFunctional API\n for how to define models in different shapes. Navigate to \nAPI Guide/Layers\n on the side bar to find the documenations of available layers and activation.\n\n\nAfter creating the model, you will have to deside which loss function to use in training. Find the details of losses defined in BigDL in \nLosses\n.  \n\n\nNow you create an \nOptimizer\n and set the loss function, input dataset along with other hyper parameters into the Optimizer. Then call \nOptimizer.optimize\n to train. Refer to \nOptimization\n and \nOptimizer API guide\n for details. \n\n\nModel Evaluation can be performed periodically during a training. Refer to \nValidate your Model in Training\n for details.  For a list of defined metrics, refer to \nMetrics\n.\n\n\nWhen \nOptimizer.optimize\n finishes, it will return a trained model. You can then use the trained model for prediction or evaluation. Refer to \nModel Prediction\n and \nModel Evaluation\n for detailed usage.    \n\n\nIf you prefer to train a model inside a Spark ML pipeline, please refer to  \nUsing BigDL in Spark ML Pipeline\n page for usage.\n\n\n\n\nSave a Model\n\n\nWhen training is finished, you may need to save the final model for later use. \n\n\nBigDL allows you to save your BigDL model on local filesystem, HDFS, or Amazon s3 (refer to \nModel Save\n). \n\n\nYou may also save the model to Tensorflow or Caffe format (refer to \nCaffe Support\n, and \nTensorflow Support\n respectively).  \n\n\n\n\nStop and Resume a Training\n\n\nTraining a deep learning model sometimes takes a very long time. It may be stopped or interrupted and we need the training to resume from where we have left. \n\n\nTo enable this, you have to configure \nOptimizer\n to periodically take snapshots of the model (trained weights, biases, etc.) and optim-method (configurations and states of the optimization) and dump them into files. Refer to \nCheckpointing\n for details. \n\n\nTo resume a training after it stops, refer to \nResume Training\n.\n\n\n\n\nUse Pre-trained Models/Layers\n\n\nPre-train is a useful strategy when training deep learning models. You may use the pre-trained features (e.g. embeddings) in your model, or do a fine-tuning for a different dataset or target.\n\n\nTo use a learnt model as a whole, you can use \nModule.load\n to load the entire model, Then create an \nOptimizer\n with the loaded model set into it. Refer to \nOptmizer API\n and \nModule API\n for details. \n\n\nInstead of using an entire model, you can also use pre-trained weights/biases in certain layers. After a layer is created, use \nsetWeightsBias\n (in Scala) or \nset_weights\n (in Python) on the layer to initialize the weights with pre-trained weights. Then continue to train your model as usual. \n\n\n\n\nMonitor your training\n\n\nBigDL provides a convinient way to monitor/visualize your training progress. It writes the statistics collected during training/validation and they can be visualized in real-time using tensorboard. These statistics can also be retrieved into readable data structures later and visualized in other tools (e.g. Jupyter notebook). For details, refer to \nVisualization\n. \n\n\n\n\nTuning\n\n\nThere're several strategies that may be useful when tuning an optimization. \n\n\n\n\nChange the learning Rate Schedule in SGD. Refer to \nSGD docs\n for details. \n\n\nIf overfit is seen, try use Regularization. Refer to \nRegularizers\n. \n\n\nTry change the initialization methods. Refer to \nInitailizers\n.\n\n\nTry Adam or Adagrad at the first place. If they can't achive a good score, use SGD and find a proper learning rate schedule - it usually takes time, though. RMSProp is recommended for RNN models. Refer to \nOptimization Algorithms\n for a list of supported optimization methods.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#before-using-bigdl", 
            "text": "Before using BigDL, you need to install Apache Spark and obtain BigDL libraries. Then in your program, you need to ensure the SparkContext is created successfully and initialize BigDL engine before calling BigDL APIs. Navigate to  Scala User Guide/Install  or  Python User Guide/Install  for details about how to install BigDL, and  Scala User Guide/Run  or  Python User Guide/Run  for how to run programs.", 
            "title": "Before using BigDL"
        }, 
        {
            "location": "/getting-started/#prepare-your-data", 
            "text": "Your data need to be transformed into RDD of  Sample  in order to be fed into BigDL for training, evaluation and prediction (also refer to  Optimization  and  Optimizer API guide ).   Tensor ,  Table  are essential data structures that composes the basic dataflow inside the nerual network( e.g. input/output, gradients, weights, etc.). You will need to understand them to get a better idea of layer behaviors.", 
            "title": "Prepare your Data"
        }, 
        {
            "location": "/getting-started/#use-bigdl-for-prediction-only", 
            "text": "If you have an existing model and want to use BigDL only for prediction, you need first load the model, and then do prediction or evaluation.   BigDL supports loading models trained and saved in BigDL, or a trained Caffe or Tensorflow model.    To load a BigDL model, you can use  Module.load  interface (Scala) or  Model.load  (in Python). Refer to  Model Save  for details.    To load a Tensorflow model, refer to  Tensorflow Support  for details.  To load a Caffe model, refer to  Caffe Support  for details.   Refer to  Model Predict  for details about how to use a model for prediction.  If you are using the trained model as a component inside a Spark ML pipeline, refer to Using BigDL in Spark ML Pipeline  page for usage.", 
            "title": "Use BigDL for Prediction only"
        }, 
        {
            "location": "/getting-started/#train-a-model-from-scratch", 
            "text": "The procedure of training a model from scratch usually involves following steps:   define your model (by connecting layers/activations into a network)  decide your loss function (which function to optimize)  optimization (choose a proper algorithm and hyper parameters, and train)  evaluation (evaluate your model)    Before training models, please make sure BigDL is installed, BigDL engine initialized properly, and your data is in proper format. Refer to  Before using BigDL  and  Prepare Your Data  for details.    The most recommended way to create your first model is to modify from an existing one. BigDL provides plenty of models for you to refer to. See  Scala Models/Examples  and  Python Models/Examples and Tutorials .   To define a model, you can either use the Sequential API or Functional API. The Functional API is more flexible than Sequential API. Refer to  Sequential API  and  Functional API  for how to define models in different shapes. Navigate to  API Guide/Layers  on the side bar to find the documenations of available layers and activation.  After creating the model, you will have to deside which loss function to use in training. Find the details of losses defined in BigDL in  Losses .    Now you create an  Optimizer  and set the loss function, input dataset along with other hyper parameters into the Optimizer. Then call  Optimizer.optimize  to train. Refer to  Optimization  and  Optimizer API guide  for details.   Model Evaluation can be performed periodically during a training. Refer to  Validate your Model in Training  for details.  For a list of defined metrics, refer to  Metrics .  When  Optimizer.optimize  finishes, it will return a trained model. You can then use the trained model for prediction or evaluation. Refer to  Model Prediction  and  Model Evaluation  for detailed usage.      If you prefer to train a model inside a Spark ML pipeline, please refer to   Using BigDL in Spark ML Pipeline  page for usage.", 
            "title": "Train a Model from Scratch"
        }, 
        {
            "location": "/getting-started/#save-a-model", 
            "text": "When training is finished, you may need to save the final model for later use.   BigDL allows you to save your BigDL model on local filesystem, HDFS, or Amazon s3 (refer to  Model Save ).   You may also save the model to Tensorflow or Caffe format (refer to  Caffe Support , and  Tensorflow Support  respectively).", 
            "title": "Save a Model"
        }, 
        {
            "location": "/getting-started/#stop-and-resume-a-training", 
            "text": "Training a deep learning model sometimes takes a very long time. It may be stopped or interrupted and we need the training to resume from where we have left.   To enable this, you have to configure  Optimizer  to periodically take snapshots of the model (trained weights, biases, etc.) and optim-method (configurations and states of the optimization) and dump them into files. Refer to  Checkpointing  for details.   To resume a training after it stops, refer to  Resume Training .", 
            "title": "Stop and Resume a Training"
        }, 
        {
            "location": "/getting-started/#use-pre-trained-modelslayers", 
            "text": "Pre-train is a useful strategy when training deep learning models. You may use the pre-trained features (e.g. embeddings) in your model, or do a fine-tuning for a different dataset or target.  To use a learnt model as a whole, you can use  Module.load  to load the entire model, Then create an  Optimizer  with the loaded model set into it. Refer to  Optmizer API  and  Module API  for details.   Instead of using an entire model, you can also use pre-trained weights/biases in certain layers. After a layer is created, use  setWeightsBias  (in Scala) or  set_weights  (in Python) on the layer to initialize the weights with pre-trained weights. Then continue to train your model as usual.", 
            "title": "Use Pre-trained Models/Layers"
        }, 
        {
            "location": "/getting-started/#monitor-your-training", 
            "text": "BigDL provides a convinient way to monitor/visualize your training progress. It writes the statistics collected during training/validation and they can be visualized in real-time using tensorboard. These statistics can also be retrieved into readable data structures later and visualized in other tools (e.g. Jupyter notebook). For details, refer to  Visualization .", 
            "title": "Monitor your training"
        }, 
        {
            "location": "/getting-started/#tuning", 
            "text": "There're several strategies that may be useful when tuning an optimization.    Change the learning Rate Schedule in SGD. Refer to  SGD docs  for details.   If overfit is seen, try use Regularization. Refer to  Regularizers .   Try change the initialization methods. Refer to  Initailizers .  Try Adam or Adagrad at the first place. If they can't achive a good score, use SGD and find a proper learning rate schedule - it usually takes time, though. RMSProp is recommended for RNN models. Refer to  Optimization Algorithms  for a list of supported optimization methods.", 
            "title": "Tuning"
        }, 
        {
            "location": "/ScalaUserGuide/install-pre-built/", 
            "text": "Download a pre-built library\n\n\nYou can download the BigDL release and nightly build from the \nRelease Page\n\n\n\n\nLink with a release version\n\n\nCurrently, BigDL releases are hosted on maven central; here's an example to add the BigDL dependency to your own project:\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.bigdl\n/groupId\n\n    \nartifactId\nbigdl\n/artifactId\n\n    \nversion\n${BIGDL_VERSION}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.bigdl\n % \nbigdl\n % \n${BIGDL_VERSION}\n\n\n\n\n\nYou can find the optional \n${BIGDL_VERSION}\n from the \nRelease Page\n.\n\n\nNote\n: the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.bigdl\n/groupId\n\n    \nartifactId\nbigdl-SPARK_2.0\n/artifactId\n\n    \nversion\n${BIGDL_VERSION}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.bigdl\n % \nbigdl-SPARK_2.0\n % \n${BIGDL_VERSION}\n\n\n\n\n\nIf your project runs on MacOS, you should add the dependency below,\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.bigdl.native\n/groupId\n\n    \nartifactId\nmkl-java-mac\n/artifactId\n\n    \nversion\n${BIGDL_VERSION}\n/version\n\n    \nexclusions\n\n        \nexclusion\n\n            \ngroupId\ncom.intel.analytics.bigdl.native\n/groupId\n\n            \nartifactId\nbigdl-native\n/artifactId\n\n        \n/exclusion\n\n    \n/exclusions\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.bigdl.native\n % \nmkl-java-mac\n % \n${BIGDL_VERSION}\n from \nhttp://repo1.maven.org/maven2/com/intel/analytics/bigdl/native/mkl-java-mac/${BIGDL_VERSION}/mkl-java-mac-${BIGDL_VERSION}.jar\n\n\n\n\n\n\n\nLink with a development version\n\n\nCurrently, BigDL development version is hosted on \nSonaType\n. \n\n\nTo link your application with the latest BigDL development version, you should add some dependencies like \nLinking with BigDL releases\n, but set \n${BIGDL_VERSION}\n to \n0.3.0-SNAPSHOT\n, and add below repository to your pom.xml.\n\n\nrepository\n\n    \nid\nsonatype\n/id\n\n    \nname\nsonatype repository\n/name\n\n    \nurl\nhttps://oss.sonatype.org/content/groups/public/\n/url\n\n    \nreleases\n\n        \nenabled\ntrue\n/enabled\n\n    \n/releases\n\n    \nsnapshots\n\n        \nenabled\ntrue\n/enabled\n\n    \n/snapshots\n\n\n/repository\n\n\n\n\n\nSBT developers can use\n\n\nresolvers += \nSonatype OSS Snapshots\n at \nhttps://oss.sonatype.org/content/repositories/snapshots", 
            "title": "Use Pre-built Libs"
        }, 
        {
            "location": "/ScalaUserGuide/install-pre-built/#download-a-pre-built-library", 
            "text": "You can download the BigDL release and nightly build from the  Release Page", 
            "title": "Download a pre-built library"
        }, 
        {
            "location": "/ScalaUserGuide/install-pre-built/#link-with-a-release-version", 
            "text": "Currently, BigDL releases are hosted on maven central; here's an example to add the BigDL dependency to your own project:  dependency \n     groupId com.intel.analytics.bigdl /groupId \n     artifactId bigdl /artifactId \n     version ${BIGDL_VERSION} /version  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.bigdl  %  bigdl  %  ${BIGDL_VERSION}   You can find the optional  ${BIGDL_VERSION}  from the  Release Page .  Note : the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this  dependency \n     groupId com.intel.analytics.bigdl /groupId \n     artifactId bigdl-SPARK_2.0 /artifactId \n     version ${BIGDL_VERSION} /version  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.bigdl  %  bigdl-SPARK_2.0  %  ${BIGDL_VERSION}   If your project runs on MacOS, you should add the dependency below,  dependency \n     groupId com.intel.analytics.bigdl.native /groupId \n     artifactId mkl-java-mac /artifactId \n     version ${BIGDL_VERSION} /version \n     exclusions \n         exclusion \n             groupId com.intel.analytics.bigdl.native /groupId \n             artifactId bigdl-native /artifactId \n         /exclusion \n     /exclusions  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.bigdl.native  %  mkl-java-mac  %  ${BIGDL_VERSION}  from  http://repo1.maven.org/maven2/com/intel/analytics/bigdl/native/mkl-java-mac/${BIGDL_VERSION}/mkl-java-mac-${BIGDL_VERSION}.jar", 
            "title": "Link with a release version"
        }, 
        {
            "location": "/ScalaUserGuide/install-pre-built/#link-with-a-development-version", 
            "text": "Currently, BigDL development version is hosted on  SonaType .   To link your application with the latest BigDL development version, you should add some dependencies like  Linking with BigDL releases , but set  ${BIGDL_VERSION}  to  0.3.0-SNAPSHOT , and add below repository to your pom.xml.  repository \n     id sonatype /id \n     name sonatype repository /name \n     url https://oss.sonatype.org/content/groups/public/ /url \n     releases \n         enabled true /enabled \n     /releases \n     snapshots \n         enabled true /enabled \n     /snapshots  /repository   SBT developers can use  resolvers +=  Sonatype OSS Snapshots  at  https://oss.sonatype.org/content/repositories/snapshots", 
            "title": "Link with a development version"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/", 
            "text": "Download BigDL Source\n\n\nBigDL source code is available at \nGitHub\n\n\n$ git clone https://github.com/intel-analytics/BigDL.git\n\n\n\n\nBy default, \ngit clone\n will download the development version of BigDL, if you want a release version, you can use command \ngit checkout\n to change the version. Available release versions is \nBigDL releases\n.\n\n\nSetup Build Environment\n\n\nThe following instructions are aligned with master code.\n\n\nMaven 3 is needed to build BigDL, you can download it from the \nmaven website\n.\n\n\nAfter installing Maven 3, please set the environment variable MAVEN_OPTS as follows:\n\n\n$ export MAVEN_OPTS=\n-Xmx2g -XX:ReservedCodeCacheSize=512m\n\n\n\n\n\nWhen compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d. \n\n\nBuild with script (Recommended)\n\n\nIt is highly recommended that you build BigDL using the \nmake-dist.sh script\n. And it will handle the MAVEN_OPTS variable.\n\n\nOnce downloaded, you can build BigDL with the following commands:\n\n\n$ bash make-dist.sh\n\n\n\n\nAfter that, you can find a \ndist\n folder, which contains all the needed files to run a BigDL program. The files in \ndist\n include:\n\n\n\n\ndist/bin/bigdl.sh\n: A script used to set up proper environment variables and launch the BigDL program.\n\n\ndist/lib/bigdl-VERSION-jar-with-dependencies.jar\n: This jar package contains all dependencies except Spark classes.\n\n\ndist/lib/bigdl-VERSION-python-api.zip\n: This zip package contains all Python files of BigDL.\n\n\ndist/conf/spark-bigdl.conf\n: This file contains necessary property configurations. \nEngine.createSparkConf\n will populate these properties, so try to use that method in your code. Or you need to pass the file to Spark with the \"--properties-file\" option. \n\n\n\n\nBuild for MacOS\n\n\nThe instructions above will only build for Linux. To build BigDL for macOS, pass \n-P mac\n to the \nmake-dist.sh\n script as follows:\n\n\n$ bash make-dist.sh -P mac\n\n\n\n\nBuild for Windows\n\n\nTo build BigDL for Windows, pass \n-P win64\n to the build command:\n\n\n mvn clean package -DskipTests -P win64\n\n\n\n\nPlease note that we only test it on Windows 10\n\n\n\n\nBuild for Spark 2.0 and above\n\n\n\n\nThe instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 and above (which uses Scala 2.11 by default), pass \n-P spark_2.x\n to the \nmake-dist.sh\n script:\n\n\n$ bash make-dist.sh -P spark_2.x\n\n\n\n\nIt is highly recommended to use \nJava 8\n when running with Spark 2.x; otherwise you may observe very poor performance.\n\n\nBuild for Scala 2.10 or 2.11\n\n\nBy default, \nmake-dist.sh\n uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0.x or 2.1.x. To override the default behaviors, you can pass \n-P scala_2.10\n or \n-P scala_2.11\n to \nmake-dist.sh\n as appropriate.\n\n\n\n\nBuild native libs\n\n\nNote that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:\n\n\n\n\n\n\nDownload and install \nIntel Parallel Studio XE\n in your Linux box.\n\n\n\n\n\n\nPrepare build environment as follows:\n\n\n\n\n\n\n    $ source \ninstall-dir\n/bin/compilervars.sh intel64\n    $ source PATH_TO_MKL/bin/mklvars.sh intel64\n\n\n\n\nwhere the `PATH_TO_MKL` is the installation directory of the MKL.\n\n\n\n\n\nFull build\n\n\n\n\nClone BigDL as follows:\n\n\n   git clone git@github.com:intel-analytics/BigDL.git --recursive \n\n\n\n\nFor already cloned repos, just use:\n\n\n   git submodule update --init --recursive \n\n\n\n\nIf the Intel MKL is not installed to the default path \n/opt/intel\n, please pass your libiomp5.so's directory path to the \nmake-dist.sh\n script:\n\n\n   $ bash make-dist.sh -P full-build -DiompLibDir=\nPATH_TO_LIBIOMP5_DIR\n \n\n\n\n\nOtherwise, only pass \n-P full-build\n to the \nmake-dist.sh\n script:\n\n\n   $ bash make-dist.sh -P full-build\n\n\n\n\n\n\nBuild with Maven\n\n\nTo build BigDL directly using Maven, run the command below:\n\n\n$ mvn clean package -DskipTests\n\n\n\n\nAfter that, you can find that the three jar packages in \nPATH_To_BigDL\n/target/, where \nPATH_To_BigDL\n is the path to the directory of the BigDL. \n\n\nNote that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:\n\n\n\n\n-P mac\n: build for maxOS\n\n\n-P win64\n: build for windows\n\n\n-P spark_2.x\n: build for Spark 2.0 and above (using Scala 2.11). (Again, it is highly recommended to use \nJava 8\n when running with Spark 2.0; otherwise you may observe very poor performance.)\n\n\n-P full-build\n: full build\n\n\n-P scala_2.10\n (or \n-P scala_2.11\n): build using Scala 2.10 (or Scala 2.11) \n\n\n\n\n\n\nSetup IDE\n\n\nWe set the scope of spark related library to \nprovided\n in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.\n\n\nThis will cause a problem in IDE. When you run applications, it will throw \nNoClassDefFoundError\n because the library scope is \nprovided\n.\n\n\nYou can easily change the scopes by the \nall-in-one\n profile.\n\n\n\n\nIn Intellij, go to View -\n Tools Windows -\n Maven Projects. Then in the Maven Projects panel, Profiles -\n click \"all-in-one\".", 
            "title": "Build from Source Code"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#download-bigdl-source", 
            "text": "BigDL source code is available at  GitHub  $ git clone https://github.com/intel-analytics/BigDL.git  By default,  git clone  will download the development version of BigDL, if you want a release version, you can use command  git checkout  to change the version. Available release versions is  BigDL releases .", 
            "title": "Download BigDL Source"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#setup-build-environment", 
            "text": "The following instructions are aligned with master code.  Maven 3 is needed to build BigDL, you can download it from the  maven website .  After installing Maven 3, please set the environment variable MAVEN_OPTS as follows:  $ export MAVEN_OPTS= -Xmx2g -XX:ReservedCodeCacheSize=512m   When compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d.", 
            "title": "Setup Build Environment"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#build-with-script-recommended", 
            "text": "It is highly recommended that you build BigDL using the  make-dist.sh script . And it will handle the MAVEN_OPTS variable.  Once downloaded, you can build BigDL with the following commands:  $ bash make-dist.sh  After that, you can find a  dist  folder, which contains all the needed files to run a BigDL program. The files in  dist  include:   dist/bin/bigdl.sh : A script used to set up proper environment variables and launch the BigDL program.  dist/lib/bigdl-VERSION-jar-with-dependencies.jar : This jar package contains all dependencies except Spark classes.  dist/lib/bigdl-VERSION-python-api.zip : This zip package contains all Python files of BigDL.  dist/conf/spark-bigdl.conf : This file contains necessary property configurations.  Engine.createSparkConf  will populate these properties, so try to use that method in your code. Or you need to pass the file to Spark with the \"--properties-file\" option.", 
            "title": "Build with script (Recommended)"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#build-for-macos", 
            "text": "The instructions above will only build for Linux. To build BigDL for macOS, pass  -P mac  to the  make-dist.sh  script as follows:  $ bash make-dist.sh -P mac", 
            "title": "Build for MacOS"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#build-for-windows", 
            "text": "To build BigDL for Windows, pass  -P win64  to the build command:   mvn clean package -DskipTests -P win64  Please note that we only test it on Windows 10   Build for Spark 2.0 and above   The instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 and above (which uses Scala 2.11 by default), pass  -P spark_2.x  to the  make-dist.sh  script:  $ bash make-dist.sh -P spark_2.x  It is highly recommended to use  Java 8  when running with Spark 2.x; otherwise you may observe very poor performance.", 
            "title": "Build for Windows"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#build-for-scala-210-or-211", 
            "text": "By default,  make-dist.sh  uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0.x or 2.1.x. To override the default behaviors, you can pass  -P scala_2.10  or  -P scala_2.11  to  make-dist.sh  as appropriate.", 
            "title": "Build for Scala 2.10 or 2.11"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#build-native-libs", 
            "text": "Note that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:    Download and install  Intel Parallel Studio XE  in your Linux box.    Prepare build environment as follows:        $ source  install-dir /bin/compilervars.sh intel64\n    $ source PATH_TO_MKL/bin/mklvars.sh intel64  where the `PATH_TO_MKL` is the installation directory of the MKL.   Full build   Clone BigDL as follows:     git clone git@github.com:intel-analytics/BigDL.git --recursive   For already cloned repos, just use:     git submodule update --init --recursive   If the Intel MKL is not installed to the default path  /opt/intel , please pass your libiomp5.so's directory path to the  make-dist.sh  script:     $ bash make-dist.sh -P full-build -DiompLibDir= PATH_TO_LIBIOMP5_DIR    Otherwise, only pass  -P full-build  to the  make-dist.sh  script:     $ bash make-dist.sh -P full-build", 
            "title": "Build native libs"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#build-with-maven", 
            "text": "To build BigDL directly using Maven, run the command below:  $ mvn clean package -DskipTests  After that, you can find that the three jar packages in  PATH_To_BigDL /target/, where  PATH_To_BigDL  is the path to the directory of the BigDL.   Note that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:   -P mac : build for maxOS  -P win64 : build for windows  -P spark_2.x : build for Spark 2.0 and above (using Scala 2.11). (Again, it is highly recommended to use  Java 8  when running with Spark 2.0; otherwise you may observe very poor performance.)  -P full-build : full build  -P scala_2.10  (or  -P scala_2.11 ): build using Scala 2.10 (or Scala 2.11)", 
            "title": "Build with Maven"
        }, 
        {
            "location": "/ScalaUserGuide/install-build-src/#setup-ide", 
            "text": "We set the scope of spark related library to  provided  in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.  This will cause a problem in IDE. When you run applications, it will throw  NoClassDefFoundError  because the library scope is  provided .  You can easily change the scopes by the  all-in-one  profile.   In Intellij, go to View -  Tools Windows -  Maven Projects. Then in the Maven Projects panel, Profiles -  click \"all-in-one\".", 
            "title": "Setup IDE"
        }, 
        {
            "location": "/ScalaUserGuide/run/", 
            "text": "Use Interactive Spark Shell\n\n\nYou can try BigDL easily using the Spark interactive shell. Run below command to start spark shell with BigDL support:\n\n\n$ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \\\n  --jars bigdl-VERSION-jar-with-dependencies.jar\n\n\n\n\nYou will see a welcome message looking like below:\n\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala\n \n\n\n\n\nTo use BigDL, you should first initialize the engine as below. \n\n\nscala\nimport com.intel.analytics.bigdl.utils.Engine\nscala\nEngine.init\n\n\n\n\nOnce the engine is successfully initialted, you'll be able to play with BigDL API's. \nFor instance, to experiment with the \nTensor\n APIs in BigDL, you may try below code:\n\n\nscala\n import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala\n Tensor[Double](2,2).fill(1.0)\nres9: com.intel.analytics.bigdl.tensor.Tensor[Double] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\n\n\nRun as a Spark Program\n\n\nYou can run a BigDL program, e.g., the \nVGG\n training, as a standard Spark program (running in either local mode or cluster mode) as follows:\n\n\n\n\nDownload the CIFAR-10 data from \nhere\n. Remember to choose the binary version.\n\n\n\n\n  # Spark local mode\n  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark standalone mode\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark yarn mode\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n\n\n\nThe parameters used in the above command are:\n\n\n\n\n\n\n-f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.\n\n\n\n\n\n\n-b: The mini-batch size. The mini-batch size is expected to be a multiple of \ntotal cores\n used in the job. In this example, the mini-batch size is suggested to be set to \ntotal cores * 4\n\n\n\n\n\n\nIf you are to run your own program, do remember to create SparkContext and initialize the engine before call other BigDL API's, as shown below. \n\n\n // Scala code example\n val conf = Engine.createSparkConf()\n val sc = new SparkContext(conf)\n Engine.init\n\n\n\n\n\n\nRun as a Local Java/Scala program\n\n\nYou can try BigDL program as a local Java/Scala program. \n\n\nTo run the BigDL model as a local Java/Scala program, you need to set Java property \nbigdl.localMode\n to \ntrue\n. If you want to specify how many cores to be used for training/testing/prediction, you need to set Java property \nbigdl.coreNumber\n to the core number. You can either call \nSystem.setProperty(\"bigdl.localMode\", \"true\")\n and \nSystem.setProperty(\"bigdl.coreNumber\", core_number)\n in the Java/Scala code, or pass -Dbigdl.localMode=true and -Dbigdl.coreNumber=core_number when runing the program.\n\n\nFor example, you may run the \nLenet\n model as a local Scala/Java program as follows:\n\n\n1.First, you can download the MNIST Data from \nhere\n. Unzip all the files and put them in one folder(e.g. mnist).\n\n\n2.Run below command to train lenet as local Java/Scala program:\n\n\njava -cp spark/dl/target/bigdl-VERSION-jar-with-dependencies-and-spark.jar \\\ncom.intel.analytics.bigdl.example.lenetLocal.Train \\\n-f path_to_mnist_folder \\\n-c core_number \\\n-b batch_size \\\n--checkpoint ./model\n\n\n\n\nIn the above commands\n\n\n\n\n-f: where you put your MNIST data\n\n\n-c: The core number on local machine used for this training. The default value is physical cores number. Get it through Runtime.getRuntime().availableProcessors() / 2\n\n\n-b: The mini-batch size. It is expected that the mini-batch size is a multiple of core_number\n\n\n--checkpoint: Where you cache the model/train_state snapshot. You should input a folder and\nmake sure the folder is created when you run this example. The model snapshot will be named as\nmodel.#iteration_number, and train state will be named as state.#iteration_number. Note that if\nthere are some files already exist in the folder, the old file will not be overwrite for the\nsafety of your model files.\n\n\n\n\n3.The above commands will cache the model in specified path(--checkpoint). Run this command will\n   use the trained model to do a validation.\n\n\njava -cp spark/dl/target/bigdl-VERSION-jar-with-dependencies-and-spark.jar \\\ncom.intel.analytics.bigdl.example.lenetLocal.Test \\\n-f path_to_mnist_folder \\\n--model ./model/model.iteration \\\n-c core_number \\\n-b batch_size\n\n\n\n\nIn the above command\n\n\n\n\n-f: where you put your MNIST data\n\n\n--model: the model snapshot file\n\n\n-c: The core number on local machine used for this testing. The default value is physical cores number. Get it through Runtime.getRuntime().availableProcessors() / 2\n\n\n-b: The mini-batch size. It is expected that the mini-batch size is a multiple of core_number   \n\n\n\n\n4.Run below command to predict with trained model:\n\n\njava -cp spark/dl/target/bigdl-VERSION-jar-with-dependencies-and-spark.jar \\\ncom.intel.analytics.bigdl.example.lenetLocal.Predict \\\n-f path_to_mnist_folder \\\n-c core_number \\\n--model ./model/model.iteration\n\n\n\n\nIn the above command\n\n\n\n\n-f: where you put your MNIST data\n\n\n-c: The core number on local machine used for this prediction. The default value is physical cores number. Get it through Runtime.getRuntime().availableProcessors() / 2\n\n\n--model: the model snapshot file\n\n\n\n\nFor Windows User\n\n\nSome BigDL functions depends on Hadoop library, which requires winutils.exe installed on your machine. If you meet \"Could not locate executable null\\bin\\winutils.exe\", see\nthe \nknown issue page\n.", 
            "title": "Run"
        }, 
        {
            "location": "/ScalaUserGuide/run/#use-interactive-spark-shell", 
            "text": "You can try BigDL easily using the Spark interactive shell. Run below command to start spark shell with BigDL support:  $ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \\\n  --jars bigdl-VERSION-jar-with-dependencies.jar  You will see a welcome message looking like below:  Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala    To use BigDL, you should first initialize the engine as below.   scala import com.intel.analytics.bigdl.utils.Engine\nscala Engine.init  Once the engine is successfully initialted, you'll be able to play with BigDL API's. \nFor instance, to experiment with the  Tensor  APIs in BigDL, you may try below code:  scala  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala  Tensor[Double](2,2).fill(1.0)\nres9: com.intel.analytics.bigdl.tensor.Tensor[Double] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]", 
            "title": "Use Interactive Spark Shell"
        }, 
        {
            "location": "/ScalaUserGuide/run/#run-as-a-spark-program", 
            "text": "You can run a BigDL program, e.g., the  VGG  training, as a standard Spark program (running in either local mode or cluster mode) as follows:   Download the CIFAR-10 data from  here . Remember to choose the binary version.     # Spark local mode\n  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark standalone mode\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark yarn mode\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size  The parameters used in the above command are:    -f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.    -b: The mini-batch size. The mini-batch size is expected to be a multiple of  total cores  used in the job. In this example, the mini-batch size is suggested to be set to  total cores * 4    If you are to run your own program, do remember to create SparkContext and initialize the engine before call other BigDL API's, as shown below.    // Scala code example\n val conf = Engine.createSparkConf()\n val sc = new SparkContext(conf)\n Engine.init", 
            "title": "Run as a Spark Program"
        }, 
        {
            "location": "/ScalaUserGuide/run/#run-as-a-local-javascala-program", 
            "text": "You can try BigDL program as a local Java/Scala program.   To run the BigDL model as a local Java/Scala program, you need to set Java property  bigdl.localMode  to  true . If you want to specify how many cores to be used for training/testing/prediction, you need to set Java property  bigdl.coreNumber  to the core number. You can either call  System.setProperty(\"bigdl.localMode\", \"true\")  and  System.setProperty(\"bigdl.coreNumber\", core_number)  in the Java/Scala code, or pass -Dbigdl.localMode=true and -Dbigdl.coreNumber=core_number when runing the program.  For example, you may run the  Lenet  model as a local Scala/Java program as follows:  1.First, you can download the MNIST Data from  here . Unzip all the files and put them in one folder(e.g. mnist).  2.Run below command to train lenet as local Java/Scala program:  java -cp spark/dl/target/bigdl-VERSION-jar-with-dependencies-and-spark.jar \\\ncom.intel.analytics.bigdl.example.lenetLocal.Train \\\n-f path_to_mnist_folder \\\n-c core_number \\\n-b batch_size \\\n--checkpoint ./model  In the above commands   -f: where you put your MNIST data  -c: The core number on local machine used for this training. The default value is physical cores number. Get it through Runtime.getRuntime().availableProcessors() / 2  -b: The mini-batch size. It is expected that the mini-batch size is a multiple of core_number  --checkpoint: Where you cache the model/train_state snapshot. You should input a folder and\nmake sure the folder is created when you run this example. The model snapshot will be named as\nmodel.#iteration_number, and train state will be named as state.#iteration_number. Note that if\nthere are some files already exist in the folder, the old file will not be overwrite for the\nsafety of your model files.   3.The above commands will cache the model in specified path(--checkpoint). Run this command will\n   use the trained model to do a validation.  java -cp spark/dl/target/bigdl-VERSION-jar-with-dependencies-and-spark.jar \\\ncom.intel.analytics.bigdl.example.lenetLocal.Test \\\n-f path_to_mnist_folder \\\n--model ./model/model.iteration \\\n-c core_number \\\n-b batch_size  In the above command   -f: where you put your MNIST data  --model: the model snapshot file  -c: The core number on local machine used for this testing. The default value is physical cores number. Get it through Runtime.getRuntime().availableProcessors() / 2  -b: The mini-batch size. It is expected that the mini-batch size is a multiple of core_number      4.Run below command to predict with trained model:  java -cp spark/dl/target/bigdl-VERSION-jar-with-dependencies-and-spark.jar \\\ncom.intel.analytics.bigdl.example.lenetLocal.Predict \\\n-f path_to_mnist_folder \\\n-c core_number \\\n--model ./model/model.iteration  In the above command   -f: where you put your MNIST data  -c: The core number on local machine used for this prediction. The default value is physical cores number. Get it through Runtime.getRuntime().availableProcessors() / 2  --model: the model snapshot file", 
            "title": "Run as a Local Java/Scala program"
        }, 
        {
            "location": "/ScalaUserGuide/run/#for-windows-user", 
            "text": "Some BigDL functions depends on Hadoop library, which requires winutils.exe installed on your machine. If you meet \"Could not locate executable null\\bin\\winutils.exe\", see\nthe  known issue page .", 
            "title": "For Windows User"
        }, 
        {
            "location": "/ScalaUserGuide/configuration/", 
            "text": "BigDL Configuration\n\n\nBigDL uses Java properties to control its behavior. Here's the list of\nthese properties.\n\n\nHow to set the properties\n\n\nSpark\n\n\nIf you run BigDL on Apache Spark, you can set the properties by passing\nspark-submit options. Here's an example:\n\n\n# Say you want to set property FOO to value BAR\n\nspark-submit ...\n    --conf 'spark.executor.extraJavaOptions=-DFOO=BAR' # Set that property for executor process\n    --conf 'spark.driver.extraJavaOptions=-DFOO=BAR'   # Set that property for driver process\n    ...\n\n\n\n\nLocal Java/Scala program\n\n\nIf you run BigDL in a local Java/Scala program, you can set the properties\nby passing JVM parameters. Here's an example:\n\n\n# Say you want to set property FOO to value BAR\njava -cp xxx.jar -DFOO=BAR your.main.class.name\n\n\n\n\nAvailable Properties\n\n\n\n\n\n\n\n\nCategory\n\n\nProperty\n\n\nDefault value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLogging\n\n\nbigdl.utils.LoggerFilter.disable\n\n\nfalse\n\n\nDisable redirecting logs of Spark and BigDL to  a file.\n\n\n\n\n\n\n\n\nbigdl.utils.LoggerFilter.logFile\n\n\nCurrent_Working_Directory/bigdl.log\n\n\nWhere is the redirecting log.\n\n\n\n\n\n\n\n\nbigdl.utils.LoggerFilter.enableSparkLog\n\n\ntrue\n\n\nEnable redirecting Spark logs to logFile. Set it to false when you don't want to see Spark logs in the redirecting log file.\n\n\n\n\n\n\nMode\n\n\nbigdl.localMode\n\n\nfalse\n\n\nWhether BigDL is running as a local Java/Scala program.\n\n\n\n\n\n\nMulti-threading\n\n\nbigdl.coreNumber\n\n\nhalf of the virtual core number\n\n\nHow many cores BigDL use on your machine. It is only used when bigdl.localMode is set to true. If hyper thread is enabled on your machine, DO NOT set it larger than half of the virtual core number.\n\n\n\n\n\n\n\n\nbigdl.Parameter.syncPoolSize\n\n\n4\n\n\nThread pool size for syncing parameter between executors.\n\n\n\n\n\n\nDistributed Training\n\n\nbigdl.network.nio\n\n\ntrue\n\n\nWhether use NIO as BlockManager backend in Spark 1.5. If it is set to false, user can specify spark.shuffle.blockTransferService to change the BlockManager backend. \nONLY\n used when running on Spark 1.5.\n\n\n\n\n\n\n\n\nbigdl.failure.retryTimes\n\n\n5\n\n\nHow many times to retry when there's failure in distributed Training.\n\n\n\n\n\n\n\n\nbigdl.failure.retryTimeInterval\n\n\n120\n\n\nUnit is second. How long recount the retry times.\n\n\n\n\n\n\n\n\nbigdl.check.singleton\n\n\nfalse\n\n\nWhether to check if multiple partition run on a same executor, which is bad for performance.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ScalaUserGuide/configuration/#bigdl-configuration", 
            "text": "BigDL uses Java properties to control its behavior. Here's the list of\nthese properties.", 
            "title": "BigDL Configuration"
        }, 
        {
            "location": "/ScalaUserGuide/configuration/#how-to-set-the-properties", 
            "text": "", 
            "title": "How to set the properties"
        }, 
        {
            "location": "/ScalaUserGuide/configuration/#spark", 
            "text": "If you run BigDL on Apache Spark, you can set the properties by passing\nspark-submit options. Here's an example:  # Say you want to set property FOO to value BAR\n\nspark-submit ...\n    --conf 'spark.executor.extraJavaOptions=-DFOO=BAR' # Set that property for executor process\n    --conf 'spark.driver.extraJavaOptions=-DFOO=BAR'   # Set that property for driver process\n    ...", 
            "title": "Spark"
        }, 
        {
            "location": "/ScalaUserGuide/configuration/#local-javascala-program", 
            "text": "If you run BigDL in a local Java/Scala program, you can set the properties\nby passing JVM parameters. Here's an example:  # Say you want to set property FOO to value BAR\njava -cp xxx.jar -DFOO=BAR your.main.class.name", 
            "title": "Local Java/Scala program"
        }, 
        {
            "location": "/ScalaUserGuide/configuration/#available-properties", 
            "text": "Category  Property  Default value  Description      Logging  bigdl.utils.LoggerFilter.disable  false  Disable redirecting logs of Spark and BigDL to  a file.     bigdl.utils.LoggerFilter.logFile  Current_Working_Directory/bigdl.log  Where is the redirecting log.     bigdl.utils.LoggerFilter.enableSparkLog  true  Enable redirecting Spark logs to logFile. Set it to false when you don't want to see Spark logs in the redirecting log file.    Mode  bigdl.localMode  false  Whether BigDL is running as a local Java/Scala program.    Multi-threading  bigdl.coreNumber  half of the virtual core number  How many cores BigDL use on your machine. It is only used when bigdl.localMode is set to true. If hyper thread is enabled on your machine, DO NOT set it larger than half of the virtual core number.     bigdl.Parameter.syncPoolSize  4  Thread pool size for syncing parameter between executors.    Distributed Training  bigdl.network.nio  true  Whether use NIO as BlockManager backend in Spark 1.5. If it is set to false, user can specify spark.shuffle.blockTransferService to change the BlockManager backend.  ONLY  used when running on Spark 1.5.     bigdl.failure.retryTimes  5  How many times to retry when there's failure in distributed Training.     bigdl.failure.retryTimeInterval  120  Unit is second. How long recount the retry times.     bigdl.check.singleton  false  Whether to check if multiple partition run on a same executor, which is bad for performance.", 
            "title": "Available Properties"
        }, 
        {
            "location": "/ScalaUserGuide/examples/", 
            "text": "This section is a short introduction of some classic examples/tutorials. They can give you a clear idea of how to build simple deep learning programs using BigDL. Besides these examples, BigDL also provides plenty of models ready for re-use and examples in both Scala and Python - refer to \nResources\n section for details. \n\n\n\n\nTraining LeNet on MNIST - The \"hello world\" for deep learning\n\n\nThis tutorial is an explanation of what is happening in the \nlenet\n example, which trains \nLeNet-5\n on the \nMNIST data\n using BigDL.\n\n\nA BigDL program starts with \nimport com.intel.analytics.bigdl._\n; it then \ncreates the \nSparkContext\n using the \nSparkConf\n returned by the \nEngine\n; after that, it \ninitializes the \nEngine\n.\n\n\n  val conf = Engine.createSparkConf()\n      .setAppName(\nTrain Lenet on MNIST\n)\n      .set(\nspark.task.maxFailures\n, \n1\n)\n  val sc = new SparkContext(conf)\n  Engine.init\n\n\n\n\nEngine.createSparkConf\n will return a \nSparkConf\n populated with some appropriate configuration. And \nEngine.init\n will verify and read some environment information(e.g. executor numbers and executor cores) from the \nSparkContext\n. \n\n\nAfter the initialization, we need to:\n\n\n1.\nCreate the LeNet model\n by calling the \nLeNet5()\n, which creates the LeNet-5 convolutional network model as follows:\n\n\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())\n\n\n\n\n2.Load the data by \ncreating the \nDataSet\n (either a distributed or local one depending on whether it runs on Spark or not), and then \napplying a series of \nTransformer\n (e.g., \nSampleToGreyImg\n, \nGreyImgNormalizer\n and \nGreyImgToBatch\n):\n\n\n    val trainSet = (if (sc.isDefined) {\n        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)\n      } else {\n        DataSet.array(load(trainData, trainLabel))\n      }) -\n SampleToGreyImg(28, 28) -\n GreyImgNormalizer(trainMean, trainStd) -\n GreyImgToBatch(\n        param.batchSize)\n\n\n\n\nAfter that, we \ncreate the \nOptimizer\n (either a distributed or local one depending on whether it runs on Spark or not) by specifying the \nDataSet\n, the model and the \nCriterion\n (which, given input and target, computes gradient per given loss function):\n\n\n  val optimizer = Optimizer(\n    model = model,\n    dataset = trainSet,\n    criterion = ClassNLLCriterion[Float]())\n\n\n\n\nFinally (after optionally specifying the validation data and methods for the \nOptimizer\n), we \ntrain the model by calling \nOptimizer.optimize()\n:\n\n\n  optimizer\n    .setValidation(\n      trigger = Trigger.everyEpoch,\n      dataset = validationSet,\n      vMethods = Array(new Top1Accuracy))\n    .setOptimMethod(new Adagrad(learningRate=0.01, learningRateDecay=0.0002))\n    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))\n    .optimize()\n\n\n\n\n\n\nText Classification - Working with Spark RDD\n\n\nThis tutorial describes the \ntext_classification\n example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by \nthis Keras tutorial\n).\n\n\nAfter importing \ncom.intel.analytics.bigdl._\n and some initialization, the \nexample\n broadcasts the pre-trained world embedding and loads the input data using RDD transformations:\n\n\n  // For large dataset, you might want to get such RDD[(String, Float)] from HDFS\n  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)\n  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)\n  val word2MetaBC = sc.broadcast(word2Meta)\n  val word2VecBC = sc.broadcast(word2Vec)\n  val vectorizedRdd = dataRdd\n      .map {case (text, label) =\n (toTokens(text, word2MetaBC.value), label)}\n      .map {case (tokens, label) =\n (shaping(tokens, sequenceLen), label)}\n      .map {case (tokens, label) =\n (vectorization(\n        tokens, embeddingDim, word2VecBC.value), label)}\n\n\n\n\nThe \nexample\n then converts the processed data (\nvectorizedRdd\n) to an RDD of Sample, and randomly splits the sample RDD (\nsampleRDD\n) into training data (\ntrainingRDD\n) and validation data (\nvalRDD\n):\n\n\n  val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) =\n\n        Sample(\n          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))\n            .transpose(1, 2).contiguous(),\n          labelTensor = Tensor(Array(label), Array(1)))\n      }\n\n  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(\n    Array(trainingSplit, 1 - trainingSplit))\n\n\n\n\nAfter that, the \nexample\n builds the CNN model, creates the \nOptimizer\n, pass the RDD of training data (\ntrainingRDD\n) to the \nOptimizer\n (with specific batch size), and finally trains the model (using \nAdagrad\n as the optimization method, and setting relevant hyper parameters in \nstate\n):\n\n\n  val optimizer = Optimizer(\n    model = buildModel(classNum),\n    sampleRDD = trainingRDD,\n    criterion = new ClassNLLCriterion[Float](),\n    batchSize = param.batchSize\n  )\n  optimizer\n    .setOptimMethod(new Adagrad(learningRate=0.01, learningRateDecay=0.0002))\n    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n    .setEndWhen(Trigger.maxEpoch(2))\n    .optimize()\n\n\n\n\n\n\nImage Classification\n - Working with Spark DataFrame and ML pipeline\n\n\nThis tutorial describes the \nimage_classification\n example, which loads a BigDL (\nInception\n) model or Torch (\nResnet\n) model that is trained on \nImageNet\n data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark \nML pipeline\n.\n\n\nAfter importing \ncom.intel.analytics.bigdl._\n and some initialization, the \nexample\n first \nloads\n the specified model:\n\n\n  def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)\n    (implicit ev: TensorNumeric[T]): Module[T] = {\n    val model = param.modelType match {\n      case TorchModel =\n\n        Module.loadTorch[T](param.modelPath)\n      case BigDlModel =\n\n        Module.load[T](param.modelPath)\n      case _ =\n throw new IllegalArgumentException(s\n${param.modelType}\n)\n    }\n    model\n  }\n\n\n\n\nIt then creates \nDLClassifer\n (a Spark ML pipelines \nTransformer\n) that predicts the input value based on the specified deep learning model:\n\n\n  val model = loadModel(param)\n  val valTrans = new DLClassifierModel(model, Array(3, imageSize, imageSize))\n    .setBatchSize(param.batchSize)\n    .setFeaturesCol(\nfeatures\n)\n    .setPredictionCol(\npredict\n)\n\n\n\n\nAfter that, the \nexample\n  loads the input images into a \nDataFrame\n, and then predicts the class of each each image using the \nDLClassifer\n:\n\n\n  val valRDD = sc.parallelize(imageSet).repartition(partitionNum)\n  val transf = RowToByteRecords() -\n\n      SampleToBGRImg() -\n\n      BGRImgCropper(imageSize, imageSize) -\n\n      BGRImgNormalizer(testMean, testStd) -\n\n      BGRImgToImageVector()\n\n  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)\n\n  valTrans.transform(valDF, paramsTrans)\n      .select(\nimageName\n, \npredict\n)\n      .show(param.showNum)", 
            "title": "Examples"
        }, 
        {
            "location": "/ScalaUserGuide/examples/#training-lenet-on-mnist-the-hello-world-for-deep-learning", 
            "text": "This tutorial is an explanation of what is happening in the  lenet  example, which trains  LeNet-5  on the  MNIST data  using BigDL.  A BigDL program starts with  import com.intel.analytics.bigdl._ ; it then  creates the  SparkContext  using the  SparkConf  returned by the  Engine ; after that, it  initializes the  Engine .    val conf = Engine.createSparkConf()\n      .setAppName( Train Lenet on MNIST )\n      .set( spark.task.maxFailures ,  1 )\n  val sc = new SparkContext(conf)\n  Engine.init  Engine.createSparkConf  will return a  SparkConf  populated with some appropriate configuration. And  Engine.init  will verify and read some environment information(e.g. executor numbers and executor cores) from the  SparkContext .   After the initialization, we need to:  1. Create the LeNet model  by calling the  LeNet5() , which creates the LeNet-5 convolutional network model as follows:      val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())  2.Load the data by  creating the  DataSet  (either a distributed or local one depending on whether it runs on Spark or not), and then  applying a series of  Transformer  (e.g.,  SampleToGreyImg ,  GreyImgNormalizer  and  GreyImgToBatch ):      val trainSet = (if (sc.isDefined) {\n        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)\n      } else {\n        DataSet.array(load(trainData, trainLabel))\n      }) -  SampleToGreyImg(28, 28) -  GreyImgNormalizer(trainMean, trainStd) -  GreyImgToBatch(\n        param.batchSize)  After that, we  create the  Optimizer  (either a distributed or local one depending on whether it runs on Spark or not) by specifying the  DataSet , the model and the  Criterion  (which, given input and target, computes gradient per given loss function):    val optimizer = Optimizer(\n    model = model,\n    dataset = trainSet,\n    criterion = ClassNLLCriterion[Float]())  Finally (after optionally specifying the validation data and methods for the  Optimizer ), we  train the model by calling  Optimizer.optimize() :    optimizer\n    .setValidation(\n      trigger = Trigger.everyEpoch,\n      dataset = validationSet,\n      vMethods = Array(new Top1Accuracy))\n    .setOptimMethod(new Adagrad(learningRate=0.01, learningRateDecay=0.0002))\n    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))\n    .optimize()", 
            "title": "Training LeNet on MNIST - The \"hello world\" for deep learning"
        }, 
        {
            "location": "/ScalaUserGuide/examples/#text-classification-working-with-spark-rdd", 
            "text": "This tutorial describes the  text_classification  example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by  this Keras tutorial ).  After importing  com.intel.analytics.bigdl._  and some initialization, the  example  broadcasts the pre-trained world embedding and loads the input data using RDD transformations:    // For large dataset, you might want to get such RDD[(String, Float)] from HDFS\n  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)\n  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)\n  val word2MetaBC = sc.broadcast(word2Meta)\n  val word2VecBC = sc.broadcast(word2Vec)\n  val vectorizedRdd = dataRdd\n      .map {case (text, label) =  (toTokens(text, word2MetaBC.value), label)}\n      .map {case (tokens, label) =  (shaping(tokens, sequenceLen), label)}\n      .map {case (tokens, label) =  (vectorization(\n        tokens, embeddingDim, word2VecBC.value), label)}  The  example  then converts the processed data ( vectorizedRdd ) to an RDD of Sample, and randomly splits the sample RDD ( sampleRDD ) into training data ( trainingRDD ) and validation data ( valRDD ):    val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) = \n        Sample(\n          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))\n            .transpose(1, 2).contiguous(),\n          labelTensor = Tensor(Array(label), Array(1)))\n      }\n\n  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(\n    Array(trainingSplit, 1 - trainingSplit))  After that, the  example  builds the CNN model, creates the  Optimizer , pass the RDD of training data ( trainingRDD ) to the  Optimizer  (with specific batch size), and finally trains the model (using  Adagrad  as the optimization method, and setting relevant hyper parameters in  state ):    val optimizer = Optimizer(\n    model = buildModel(classNum),\n    sampleRDD = trainingRDD,\n    criterion = new ClassNLLCriterion[Float](),\n    batchSize = param.batchSize\n  )\n  optimizer\n    .setOptimMethod(new Adagrad(learningRate=0.01, learningRateDecay=0.0002))\n    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n    .setEndWhen(Trigger.maxEpoch(2))\n    .optimize()", 
            "title": "Text Classification - Working with Spark RDD"
        }, 
        {
            "location": "/ScalaUserGuide/examples/#image-classification-working-with-spark-dataframe-and-ml-pipeline", 
            "text": "This tutorial describes the  image_classification  example, which loads a BigDL ( Inception ) model or Torch ( Resnet ) model that is trained on  ImageNet  data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark  ML pipeline .  After importing  com.intel.analytics.bigdl._  and some initialization, the  example  first  loads  the specified model:    def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)\n    (implicit ev: TensorNumeric[T]): Module[T] = {\n    val model = param.modelType match {\n      case TorchModel = \n        Module.loadTorch[T](param.modelPath)\n      case BigDlModel = \n        Module.load[T](param.modelPath)\n      case _ =  throw new IllegalArgumentException(s ${param.modelType} )\n    }\n    model\n  }  It then creates  DLClassifer  (a Spark ML pipelines  Transformer ) that predicts the input value based on the specified deep learning model:    val model = loadModel(param)\n  val valTrans = new DLClassifierModel(model, Array(3, imageSize, imageSize))\n    .setBatchSize(param.batchSize)\n    .setFeaturesCol( features )\n    .setPredictionCol( predict )  After that, the  example   loads the input images into a  DataFrame , and then predicts the class of each each image using the  DLClassifer :    val valRDD = sc.parallelize(imageSet).repartition(partitionNum)\n  val transf = RowToByteRecords() - \n      SampleToBGRImg() - \n      BGRImgCropper(imageSize, imageSize) - \n      BGRImgNormalizer(testMean, testStd) - \n      BGRImgToImageVector()\n\n  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)\n\n  valTrans.transform(valDF, paramsTrans)\n      .select( imageName ,  predict )\n      .show(param.showNum)", 
            "title": "Image Classification - Working with Spark DataFrame and ML pipeline"
        }, 
        {
            "location": "/ScalaUserGuide/resources/", 
            "text": "Scala Models\n\n\nBigDL provides loads of popular models ready for use in your application. Some of them are listed blow. See all in \nscala neural network models\n. \n\n\n\n\nLeNet\n: it demonstrates how to use BigDL to train and evaluate the \nLeNet-5\n network on MNIST data.\n\n\nInception\n: it demonstrates how to use BigDL to train and evaluate \nInception v1\n and \nInception v2\n architecture on the ImageNet data.\n\n\nVGG\n: it demonstrates how to use BigDL to train and evaluate a \nVGG-like\n network on CIFAR-10 data.\n\n\nResNet\n: it demonstrates how to use BigDL to train and evaluate the \nResNet\n architecture on CIFAR-10 data.\n\n\nRNN\n: it demonstrates how to use BigDL to build and train a simple recurrent neural network \n(RNN) for language model\n.\n\n\nAuto-encoder\n: it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.\n\n\n\n\n\n\nScala Example\n\n\nBigDL ships plenty of Scala examples to show how to use BigDL to solve real problems. Some are listed blow. See all of them in \nscala deep learning examples\n \n\n\n\n\nText Classification\n:\n    it demonstrates how to use BigDL to build a \ntext classifier\n using a simple convolutional neural network (CNN) model.\n\n\nImage Classification\n:\n    it demonstrates how to load a BigDL or \nTorch\n model trained on ImageNet data (e.g., \nInception\n or \nResNet\n),\n    and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.\n\n\nLoad Model\n:\n    it demonstrates how to use BigDL to load a pre-trained \nTorch\n or \nCaffe\n model into Spark program for prediction.\n\n\nML Pipline\n:\n    it demonstrates how to use BigDL DLClassifier to train a Logistic Regression Model. DLClassifier extends Spark Estimator and can act as a stage in a ML Pipeline.\n\n\nTreeLSTM Sentiment\n:\n    it demonstrates how to use BigDL train a model on \nStandford Treebank dataset\n dataset using binary TreeLSTM and \nGlove\n\n    word embedding vectors.\n\n\nUDF Predictor\n:\n    it demonstrates how to load BigDL model as UDF to perform predictions in Spark SQL/Dataframes.", 
            "title": "More Resources"
        }, 
        {
            "location": "/ScalaUserGuide/resources/#scala-models", 
            "text": "BigDL provides loads of popular models ready for use in your application. Some of them are listed blow. See all in  scala neural network models .    LeNet : it demonstrates how to use BigDL to train and evaluate the  LeNet-5  network on MNIST data.  Inception : it demonstrates how to use BigDL to train and evaluate  Inception v1  and  Inception v2  architecture on the ImageNet data.  VGG : it demonstrates how to use BigDL to train and evaluate a  VGG-like  network on CIFAR-10 data.  ResNet : it demonstrates how to use BigDL to train and evaluate the  ResNet  architecture on CIFAR-10 data.  RNN : it demonstrates how to use BigDL to build and train a simple recurrent neural network  (RNN) for language model .  Auto-encoder : it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.", 
            "title": "Scala Models"
        }, 
        {
            "location": "/ScalaUserGuide/resources/#scala-example", 
            "text": "BigDL ships plenty of Scala examples to show how to use BigDL to solve real problems. Some are listed blow. See all of them in  scala deep learning examples     Text Classification :\n    it demonstrates how to use BigDL to build a  text classifier  using a simple convolutional neural network (CNN) model.  Image Classification :\n    it demonstrates how to load a BigDL or  Torch  model trained on ImageNet data (e.g.,  Inception  or  ResNet ),\n    and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.  Load Model :\n    it demonstrates how to use BigDL to load a pre-trained  Torch  or  Caffe  model into Spark program for prediction.  ML Pipline :\n    it demonstrates how to use BigDL DLClassifier to train a Logistic Regression Model. DLClassifier extends Spark Estimator and can act as a stage in a ML Pipeline.  TreeLSTM Sentiment :\n    it demonstrates how to use BigDL train a model on  Standford Treebank dataset  dataset using binary TreeLSTM and  Glove \n    word embedding vectors.  UDF Predictor :\n    it demonstrates how to load BigDL model as UDF to perform predictions in Spark SQL/Dataframes.", 
            "title": "Scala Example"
        }, 
        {
            "location": "/PythonUserGuide/install-from-pip/", 
            "text": "NOTES\n\n\n\n\nPip install support \nmac\n and \nlinux\n platform but only \nSpark1.6.x\n for now.\n\n\nPip install only support run in \nlocal\n. Might support cluster mode in the future.\n\n\nWe've tested this package with \npython 2.7\n and \npython 3.5\n\n\n\n\nInstall BigDL-0.1.2\n\n\n1.Download Spark1.6.3:  \n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-1.6.3-bin-hadoop2.6.tgz \n\n\n\n\n2.Extract the tar ball and set SPARK_HOME\n\n\ntar -zxvf spark-1.6.3-bin-hadoop2.6.tgz\nexport SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6\n\n\n\n\n3.Install BigDL release via pip (we tested this on pip 9.0.1)\n- NOTE: you might need to \nsudo\n if without permission for the installation\n\n\npip install --upgrade pip\npip install BigDL==0.1.2     # for Python 2.7\npip3 install BigDL==0.1.2  # for Python 3.n\n\n\n\n\nInstall BigDL-0.2.0-snapshot\n\n\n1.Download Spark1.6.3:  \n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-1.6.3-bin-hadoop2.6.tgz\n\n\n\n\n2.Extract the tar ball and set SPARK_HOME\n\n\ntar -zxvf spark-1.6.3-bin-hadoop2.6.tgz\nexport SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6\n\n\n\n\n3.Install BigDL release via pip (we tested this on pip 9.0.1)\n- NOTE: you might need to \nsudo\n if without permission for the installation\n\n\npip install --upgrade pip\npip install BigDL==0.2.0.dev4     # for Python 2.7\npip3 install BigDL==0.2.0.dev4  # for Python 3.n", 
            "title": "From pip"
        }, 
        {
            "location": "/PythonUserGuide/install-from-pip/#notes", 
            "text": "Pip install support  mac  and  linux  platform but only  Spark1.6.x  for now.  Pip install only support run in  local . Might support cluster mode in the future.  We've tested this package with  python 2.7  and  python 3.5", 
            "title": "NOTES"
        }, 
        {
            "location": "/PythonUserGuide/install-from-pip/#install-bigdl-012", 
            "text": "1.Download Spark1.6.3:    wget https://d3kbcqa49mib13.cloudfront.net/spark-1.6.3-bin-hadoop2.6.tgz   2.Extract the tar ball and set SPARK_HOME  tar -zxvf spark-1.6.3-bin-hadoop2.6.tgz\nexport SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6  3.Install BigDL release via pip (we tested this on pip 9.0.1)\n- NOTE: you might need to  sudo  if without permission for the installation  pip install --upgrade pip\npip install BigDL==0.1.2     # for Python 2.7\npip3 install BigDL==0.1.2  # for Python 3.n", 
            "title": "Install BigDL-0.1.2"
        }, 
        {
            "location": "/PythonUserGuide/install-from-pip/#install-bigdl-020-snapshot", 
            "text": "1.Download Spark1.6.3:    wget https://d3kbcqa49mib13.cloudfront.net/spark-1.6.3-bin-hadoop2.6.tgz  2.Extract the tar ball and set SPARK_HOME  tar -zxvf spark-1.6.3-bin-hadoop2.6.tgz\nexport SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6  3.Install BigDL release via pip (we tested this on pip 9.0.1)\n- NOTE: you might need to  sudo  if without permission for the installation  pip install --upgrade pip\npip install BigDL==0.2.0.dev4     # for Python 2.7\npip3 install BigDL==0.2.0.dev4  # for Python 3.n", 
            "title": "Install BigDL-0.2.0-snapshot"
        }, 
        {
            "location": "/PythonUserGuide/install-without-pip/", 
            "text": "Install without pip\n\n\n\n\n\n\nInstall Spark\n\n\n\n\n\n\nYou can download the BigDL release and nightly build from the \nRelease Page\n\n  or build the BigDL package from \nsource\n. \n\n\n\n\n\n\nInstall python dependencies:\n\n\n\n\nBigDL only depend on \nNumpy\n for now.  \n\n\nFor Spark standalone cluster:\n\n\nif you're running in cluster mode, you need to install python dependencies on both client and each worker nodes\n\n\nInstall Numpy: \n   \nsudo apt-get install python-numpy\n (Ubuntu)\n\n\n\n\n\n\nFor Yarn cluster:\n\n\nYou can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the Python dependencies). You  can first package all the required Python dependency into a virtual environment on the localnode (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program on the YARN cluster (using that virtual environment). Please refer to this \nPacking-dependencies\n for more details.", 
            "title": "Without pip"
        }, 
        {
            "location": "/PythonUserGuide/install-without-pip/#install-without-pip", 
            "text": "Install Spark    You can download the BigDL release and nightly build from the  Release Page \n  or build the BigDL package from  source .     Install python dependencies:   BigDL only depend on  Numpy  for now.    For Spark standalone cluster:  if you're running in cluster mode, you need to install python dependencies on both client and each worker nodes  Install Numpy: \n    sudo apt-get install python-numpy  (Ubuntu)    For Yarn cluster:  You can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the Python dependencies). You  can first package all the required Python dependency into a virtual environment on the localnode (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program on the YARN cluster (using that virtual environment). Please refer to this  Packing-dependencies  for more details.", 
            "title": "Install without pip"
        }, 
        {
            "location": "/PythonUserGuide/run-from-pip/", 
            "text": "Precondition\n\n\n\n\nInstall via pip\n\n\n\n\nUse an Interactive Shell\n\n\n\n\nexport SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6 \n\n\ntype \npython\n in commandline to start a REPL\n\n\n\n\nUse Jupyter Notebook\n\n\n\n\nexport SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6 \n\n\nStart jupyter notebook as you normally did, e.g.\n \nbash\n jupyter notebook --notebook-dir=./ --ip=* --no-browser\n\n\n\n\n\n\nExample code to verify if run successfully\n\n\nfrom bigdl.util.common import *\nfrom pyspark import SparkContext\nfrom bigdl.nn.layer import *\nimport bigdl.version\n\n# create sparkcontext with bigdl configuration\nsc = SparkContext.getOrCreate(conf=create_spark_conf()) \ninit_engine() # prepare the bigdl environment \nbigdl.version.__version__ # Get the current BigDL version\nlinear = Linear(2, 3) # Try to create a Linear layer\n\n\n\n\n\nBigDL Configuration\n\n\n\n\nIncrease memory\n\n\nexport SPARK_DRIVER_MEMORY=20g", 
            "title": "After pip install"
        }, 
        {
            "location": "/PythonUserGuide/run-from-pip/#precondition", 
            "text": "Install via pip", 
            "title": "Precondition"
        }, 
        {
            "location": "/PythonUserGuide/run-from-pip/#use-an-interactive-shell", 
            "text": "export SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6   type  python  in commandline to start a REPL", 
            "title": "Use an Interactive Shell"
        }, 
        {
            "location": "/PythonUserGuide/run-from-pip/#use-jupyter-notebook", 
            "text": "export SPARK_HOME=path to spark-1.6.3-bin-hadoop2.6   Start jupyter notebook as you normally did, e.g.\n  bash\n jupyter notebook --notebook-dir=./ --ip=* --no-browser", 
            "title": "Use Jupyter Notebook"
        }, 
        {
            "location": "/PythonUserGuide/run-from-pip/#example-code-to-verify-if-run-successfully", 
            "text": "from bigdl.util.common import *\nfrom pyspark import SparkContext\nfrom bigdl.nn.layer import *\nimport bigdl.version\n\n# create sparkcontext with bigdl configuration\nsc = SparkContext.getOrCreate(conf=create_spark_conf()) \ninit_engine() # prepare the bigdl environment \nbigdl.version.__version__ # Get the current BigDL version\nlinear = Linear(2, 3) # Try to create a Linear layer", 
            "title": "Example code to verify if run successfully"
        }, 
        {
            "location": "/PythonUserGuide/run-from-pip/#bigdl-configuration", 
            "text": "Increase memory  export SPARK_DRIVER_MEMORY=20g", 
            "title": "BigDL Configuration"
        }, 
        {
            "location": "/PythonUserGuide/run-without-pip/", 
            "text": "First of all, you need to obtain the BigDL libs. Refer to \nInstall from pre built\n or \nInstall from source code\n for more details\n\n\nA quick launch for local mode\n\n\ncd $BIGDL_HOME/dist/lib \nBIGDL_VERSION=...\n${SPARK_HOME}/bin/pyspark --master local[4] \\\n--conf spark.driver.extraClassPath=bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar \\\n--py-files bigdl-${BIGDL_VERSION}-python-api.zip \\\n--properties-file ../conf/spark-bigdl.conf \n\n\n\n\nExample code to verify if run successfully\n\n\nRun from spark-submit\n\n\n\n\nA BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. You can try run the BigDL \nlenet Python example\n using \nspark-submit\n as follows:\n\n\nEnsure every path is valid\n \n\n\n\n\n   BigDL_HOME=...\n   SPARK_HOME=...\n   BIGDL_VERSION=...\n   MASTER=...\n   PYTHON_API_ZIP_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-python-api.zip\n   BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar\n   PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\n   ${SPARK_HOME}/bin/spark-submit \\\n       --master ${MASTER} \\\n       --driver-cores 5  \\\n      --driver-memory 10g  \\\n      --total-executor-cores 80  \\\n      --executor-cores 10  \\\n      --executor-memory 20g \\\n       --py-files ${PYTHON_API_ZIP_PATH},${BigDL_HOME}/pyspark/bigdl/models/lenet/lenet5.py  \\\n       --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n       --jars ${BigDL_JAR_PATH} \\\n       --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n       --conf spark.executor.extraClassPath=bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar \\\n       ${BigDL_HOME}/pyspark/bigdl/models/lenet/lenet5.py\n\n\n\n\nRun from pyspark + Jupyter\n\n\n\n\n\n\nWith the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.\n\n\n\n\n\n\nFirst, install all the necessary libraries on the local node where you will run Jupyter, e.g., \n\n\n\n\n\n\nsudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud\n\n\n\n\n\n\nThen, you can launch the Jupyter notebook as follows:\n\n\nEnsure every path is valid\n \n\n\n\n\n   BigDL_HOME=...                                                                                         \n   BIGDL_VERSION=...\n   SPARK_HOME=...\n   MASTER=...\n   PYTHON_API_ZIP_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-python-api.zip\n   BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar\n\n   export PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n   export PYSPARK_DRIVER_PYTHON=jupyter\n   export PYSPARK_DRIVER_PYTHON_OPTS=\nnotebook --notebook-dir=./  --ip=* --no-browser\n\n\n   ${SPARK_HOME}/bin/pyspark \\\n       --master ${MASTER} \\\n       --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n       --driver-cores 5  \\\n      --driver-memory 10g  \\\n      --total-executor-cores 8  \\\n      --executor-cores 1  \\\n      --executor-memory 20g \\\n       --py-files ${PYTHON_API_ZIP_PATH} \\\n       --jars ${BigDL_JAR_PATH} \\\n       --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n       --conf spark.executor.extraClassPath=bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar\n\n\n\n\nAfter successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/\n\n\nExample code to verify if run successfully\n\n\nBigDL Configuration\n\n\nPlease check \nthis page\n\n\nFAQ\n\n\n\n\nImportError\u00a0\u00a0\u00a0from\u00a0bigdl.nn.layer\u00a0import\u00a0*\n\n\nCheck if the path is pointing to python-api.zip: \n--py-files ${PYTHON_API_ZIP_PATH}\n\n\nCheck if the path is pointing to python-api.zip: \nexport PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\n\nPython in worker has different version 2.7 than that in driver 3.4\n\n\nexport PYSPARK_PYTHON=/usr/local/bin/python3.4  # This path should be valid on every worker node.\n\n\n\n\nexport PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3.4 # This path should be valid on every driver node.\n\n\n\n\n\n\nTypeError: 'JavaPackage' object is not callable\n\n\n\n\n\n\nCheck if every path within the launch script is valid expecially the path end with jar\n\n\n\n\n\n\njava.lang.NoSuchMethodError:XXX\n\n\n\n\nCheck if the spark version is match i.e you are using Spark2.x but the underneath BigDL compiled with Spark1.6", 
            "title": "Without pip install"
        }, 
        {
            "location": "/PythonUserGuide/run-without-pip/#a-quick-launch-for-local-mode", 
            "text": "cd $BIGDL_HOME/dist/lib \nBIGDL_VERSION=...\n${SPARK_HOME}/bin/pyspark --master local[4] \\\n--conf spark.driver.extraClassPath=bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar \\\n--py-files bigdl-${BIGDL_VERSION}-python-api.zip \\\n--properties-file ../conf/spark-bigdl.conf   Example code to verify if run successfully", 
            "title": "A quick launch for local mode"
        }, 
        {
            "location": "/PythonUserGuide/run-without-pip/#run-from-spark-submit", 
            "text": "A BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. You can try run the BigDL  lenet Python example  using  spark-submit  as follows:  Ensure every path is valid        BigDL_HOME=...\n   SPARK_HOME=...\n   BIGDL_VERSION=...\n   MASTER=...\n   PYTHON_API_ZIP_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-python-api.zip\n   BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar\n   PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\n   ${SPARK_HOME}/bin/spark-submit \\\n       --master ${MASTER} \\\n       --driver-cores 5  \\\n      --driver-memory 10g  \\\n      --total-executor-cores 80  \\\n      --executor-cores 10  \\\n      --executor-memory 20g \\\n       --py-files ${PYTHON_API_ZIP_PATH},${BigDL_HOME}/pyspark/bigdl/models/lenet/lenet5.py  \\\n       --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n       --jars ${BigDL_JAR_PATH} \\\n       --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n       --conf spark.executor.extraClassPath=bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar \\\n       ${BigDL_HOME}/pyspark/bigdl/models/lenet/lenet5.py", 
            "title": "Run from spark-submit"
        }, 
        {
            "location": "/PythonUserGuide/run-without-pip/#run-from-pyspark-jupyter", 
            "text": "With the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.    First, install all the necessary libraries on the local node where you will run Jupyter, e.g.,     sudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud   Then, you can launch the Jupyter notebook as follows:  Ensure every path is valid        BigDL_HOME=...                                                                                         \n   BIGDL_VERSION=...\n   SPARK_HOME=...\n   MASTER=...\n   PYTHON_API_ZIP_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-python-api.zip\n   BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar\n\n   export PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n   export PYSPARK_DRIVER_PYTHON=jupyter\n   export PYSPARK_DRIVER_PYTHON_OPTS= notebook --notebook-dir=./  --ip=* --no-browser \n\n   ${SPARK_HOME}/bin/pyspark \\\n       --master ${MASTER} \\\n       --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n       --driver-cores 5  \\\n      --driver-memory 10g  \\\n      --total-executor-cores 8  \\\n      --executor-cores 1  \\\n      --executor-memory 20g \\\n       --py-files ${PYTHON_API_ZIP_PATH} \\\n       --jars ${BigDL_JAR_PATH} \\\n       --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n       --conf spark.executor.extraClassPath=bigdl-${BIGDL_VERSION}-jar-with-dependencies.jar  After successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/  Example code to verify if run successfully", 
            "title": "Run from pyspark + Jupyter"
        }, 
        {
            "location": "/PythonUserGuide/run-without-pip/#bigdl-configuration", 
            "text": "Please check  this page", 
            "title": "BigDL Configuration"
        }, 
        {
            "location": "/PythonUserGuide/run-without-pip/#faq", 
            "text": "ImportError\u00a0\u00a0\u00a0from\u00a0bigdl.nn.layer\u00a0import\u00a0*  Check if the path is pointing to python-api.zip:  --py-files ${PYTHON_API_ZIP_PATH}  Check if the path is pointing to python-api.zip:  export PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH  Python in worker has different version 2.7 than that in driver 3.4  export PYSPARK_PYTHON=/usr/local/bin/python3.4  # This path should be valid on every worker node.   export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3.4 # This path should be valid on every driver node.    TypeError: 'JavaPackage' object is not callable    Check if every path within the launch script is valid expecially the path end with jar    java.lang.NoSuchMethodError:XXX   Check if the spark version is match i.e you are using Spark2.x but the underneath BigDL compiled with Spark1.6", 
            "title": "FAQ"
        }, 
        {
            "location": "/PythonUserGuide/python-examples/", 
            "text": "Text Classification using BigDL Python API\n\n\nThis tutorial describes the \ntextclassifier\n example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by \nthis Keras tutorial\n)\n\n\nThe example first creates the \nSparkContext\n using the SparkConf\nreturn by the\ncreate_spark_conf()` method, and then initialize the engine:\n\n\n  sc = SparkContext(appName=\ntext_classifier\n,\n                    conf=create_spark_conf())\n  init_engine()\n\n\n\n\nIt then loads the \n20 Newsgroup dataset\n into RDD, and transforms the input data into an RDD of \nSample\n. (Each \nSample\n in essence contains a tuple of two NumPy ndarray representing the feature and label).\n\n\n  texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])   \n\n\n\n\nAfter that, the example creates the neural network model as follows:\n\n\ndef build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \ncnn\n:\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \nlstm\n:\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \ngru\n:\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model\n\n\n\n\nFinally the example creates the \nOptimizer\n (which accepts both the model and the training Sample RDD) and trains the model by calling \nOptimizer.optimize()\n:\n\n\noptimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=Adagrad())\n...\ntrain_model = optimizer.optimize()", 
            "title": "Examples"
        }, 
        {
            "location": "/PythonUserGuide/python-examples/#text-classification-using-bigdl-python-api", 
            "text": "This tutorial describes the  textclassifier  example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by  this Keras tutorial )  The example first creates the  SparkContext  using the SparkConf return by the create_spark_conf()` method, and then initialize the engine:    sc = SparkContext(appName= text_classifier ,\n                    conf=create_spark_conf())\n  init_engine()  It then loads the  20 Newsgroup dataset  into RDD, and transforms the input data into an RDD of  Sample . (Each  Sample  in essence contains a tuple of two NumPy ndarray representing the feature and label).    texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])     After that, the example creates the neural network model as follows:  def build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() ==  cnn :\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() ==  lstm :\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() ==  gru :\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model  Finally the example creates the  Optimizer  (which accepts both the model and the training Sample RDD) and trains the model by calling  Optimizer.optimize() :  optimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=Adagrad())\n...\ntrain_model = optimizer.optimize()", 
            "title": "Text Classification using BigDL Python API"
        }, 
        {
            "location": "/PythonUserGuide/python-resources/", 
            "text": "Models/Examples\n\n\nBigDL provides plenty of Python models and examples ready for re-use. Some are listed blow. See all in \npython models\n.\n\n\n\n\nLeNet\n: it demonstrates how to use BigDL Python APIs to train and evaluate the \nLeNet-5\n network on MNIST data.\n\n\nText Classifier\n:  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.\n\n\nJupyter Notebook Tutorial\n: it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.\n\n\n\n\n\n\nTutorial Notebooks\n\n\nBigDL Tutorials Notebooks\n - A series of notebooks that step-by- step introduce you how to do data science on Apache Spark and BigDL framework", 
            "title": "More Examples and Tutorials"
        }, 
        {
            "location": "/PythonUserGuide/python-resources/#modelsexamples", 
            "text": "BigDL provides plenty of Python models and examples ready for re-use. Some are listed blow. See all in  python models .   LeNet : it demonstrates how to use BigDL Python APIs to train and evaluate the  LeNet-5  network on MNIST data.  Text Classifier :  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.  Jupyter Notebook Tutorial : it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.", 
            "title": "Models/Examples"
        }, 
        {
            "location": "/PythonUserGuide/python-resources/#tutorial-notebooks", 
            "text": "BigDL Tutorials Notebooks  - A series of notebooks that step-by- step introduce you how to do data science on Apache Spark and BigDL framework", 
            "title": "Tutorial Notebooks"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Sequential/", 
            "text": "BigDL supports two different model definition styles: Sequential API and Functional API.\n\n\nHere we introduce how to define a model in Sequential API.\n\n\n\n\nDefine a simple model\n\n\nSuppose we want to define a model with three layers\n\n\nLinear -\n Sigmoid -\n Softmax\n\n\n\n\nYou can write code like this\n\n\nScala:\n\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())\n\n\n\n\nPython:\n\n\nmodel = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())\n\n\n\n\nIn the above code, we first create a container Sequential. Then add the layers\ninto the container one by one. The order of the layers in the model is same with the insertion\norder. This model definition\nlooks very straightforward.\n\n\nBigDL provides multiple types of contianers allow user to define complex model in sequential\nstyle. We will take a look at it.\n\n\n\n\nDefine a model with branches\n\n\nSuppose we want to define a model like this\n\n\nLinear -\n ReLU --\n Linear -\n ReLU\n               |-\n Linear -\n ReLU\n\n\n\n\nThe model has two outputs from two branches. The inputs of the branches are both the\noutput from the first ReLU.\n\n\nYou can define the model like this\n\n\nScala\n\n\nval branch1 = Sequential().add(Linear(...)).add(ReLU())\nval branch2 = Sequential().add(Linear(...)).add(ReLU())\nval branches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)\n\n\n\n\nPython\n\n\nbranch1 = Sequential().add(Linear(...)).add(ReLU())\nbranch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)\n\n\n\n\nIn the above code, to handle the branch structure, we use another container ConcatTable.\nWhen you add layers into ConcatTable, the new layer won't be placed after the previous one\nbut will become a new branch.\n\n\nThe input of the model is a tensor and the output of the model is two tensors.\n\n\n\n\nDefine a model with merged branch\n\n\nSuppose we want to define a model like this\n\n\nLinear -\n ReLU --\n Linear -\n ReLU ----\n Add\n               |-\n Linear -\n ReLU --|\n\n\n\n\nIn the model, the outputs of the two branches are merged by an add operation.\n\n\nYou can define the model like this\n\n\nScala\n\n\nval branch1 = Sequential().add(Linear(...)).add(ReLU())\nval branch2 = Sequential().add(Linear(...)).add(ReLU())\nval branches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)\nmodel.add(CAddTable())\n\n\n\n\nPython\n\n\nbranch1 = Sequential().add(Linear(...)).add(ReLU())\nbranch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)\nmodel.add(CAddTable())\n\n\n\n\nTo merge the outputs of the branches by an add operation, we use CAddTable. It\ntakes a list of tensors from the previous layer, and merge the tensors by adding them together.\n\n\nBigDL provides many merge layers. Please check Merge layers document page. They all\ntake a list of tensors as input and merge the tensors by some operation.\n\n\n\n\nDefine a model with multiple inputs\n\n\nWe have already seen how to define branches in model and how to merge branches.\nWhat if we have multiple input? Suppose we want to define a model like this\n\n\nLinear -\n ReLU ----\n Add\nLinear -\n ReLU --|\n\n\n\n\nThe above model takes two tensors as input, and merge them together by add operation.\n\n\nYou can define the model like this\n\n\nScala\n\n\nval model = Sequential()\nval branches = ParallelTable()\nval branch1 = Sequential().add(Linear(...)).add(ReLU())\nval branch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches.add(branch1).add(branch2)\nmodel.add(branches).add(CAddTable)\n\n\n\n\nPython\n\n\nmodel = Sequential()\nbranches = ParallelTable()\nbranch1 = Sequential().add(Linear(...)).add(ReLU())\nbranch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches.add(branch1).add(branch2)\nmodel.add(branches).add(CAddTable)\n\n\n\n\nIn the above code, we use ParallelTable to handle the multiple inputs. ParallelTable also\ndefine a multiple branches structure like ConcatTable. The difference is it takes a list\nof tensors as inputs and assign each tensor to the corresponding branch.", 
            "title": "Using Sequential API"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Sequential/#define-a-simple-model", 
            "text": "Suppose we want to define a model with three layers  Linear -  Sigmoid -  Softmax  You can write code like this  Scala:  val model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())  Python:  model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())  In the above code, we first create a container Sequential. Then add the layers\ninto the container one by one. The order of the layers in the model is same with the insertion\norder. This model definition\nlooks very straightforward.  BigDL provides multiple types of contianers allow user to define complex model in sequential\nstyle. We will take a look at it.", 
            "title": "Define a simple model"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Sequential/#define-a-model-with-branches", 
            "text": "Suppose we want to define a model like this  Linear -  ReLU --  Linear -  ReLU\n               |-  Linear -  ReLU  The model has two outputs from two branches. The inputs of the branches are both the\noutput from the first ReLU.  You can define the model like this  Scala  val branch1 = Sequential().add(Linear(...)).add(ReLU())\nval branch2 = Sequential().add(Linear(...)).add(ReLU())\nval branches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)  Python  branch1 = Sequential().add(Linear(...)).add(ReLU())\nbranch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)  In the above code, to handle the branch structure, we use another container ConcatTable.\nWhen you add layers into ConcatTable, the new layer won't be placed after the previous one\nbut will become a new branch.  The input of the model is a tensor and the output of the model is two tensors.", 
            "title": "Define a model with branches"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Sequential/#define-a-model-with-merged-branch", 
            "text": "Suppose we want to define a model like this  Linear -  ReLU --  Linear -  ReLU ----  Add\n               |-  Linear -  ReLU --|  In the model, the outputs of the two branches are merged by an add operation.  You can define the model like this  Scala  val branch1 = Sequential().add(Linear(...)).add(ReLU())\nval branch2 = Sequential().add(Linear(...)).add(ReLU())\nval branches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)\nmodel.add(CAddTable())  Python  branch1 = Sequential().add(Linear(...)).add(ReLU())\nbranch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches = ConcatTable().add(branch1).add(branch2)\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(ReLU())\nmodel.add(branches)\nmodel.add(CAddTable())  To merge the outputs of the branches by an add operation, we use CAddTable. It\ntakes a list of tensors from the previous layer, and merge the tensors by adding them together.  BigDL provides many merge layers. Please check Merge layers document page. They all\ntake a list of tensors as input and merge the tensors by some operation.", 
            "title": "Define a model with merged branch"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Sequential/#define-a-model-with-multiple-inputs", 
            "text": "We have already seen how to define branches in model and how to merge branches.\nWhat if we have multiple input? Suppose we want to define a model like this  Linear -  ReLU ----  Add\nLinear -  ReLU --|  The above model takes two tensors as input, and merge them together by add operation.  You can define the model like this  Scala  val model = Sequential()\nval branches = ParallelTable()\nval branch1 = Sequential().add(Linear(...)).add(ReLU())\nval branch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches.add(branch1).add(branch2)\nmodel.add(branches).add(CAddTable)  Python  model = Sequential()\nbranches = ParallelTable()\nbranch1 = Sequential().add(Linear(...)).add(ReLU())\nbranch2 = Sequential().add(Linear(...)).add(ReLU())\nbranches.add(branch1).add(branch2)\nmodel.add(branches).add(CAddTable)  In the above code, we use ParallelTable to handle the multiple inputs. ParallelTable also\ndefine a multiple branches structure like ConcatTable. The difference is it takes a list\nof tensors as inputs and assign each tensor to the corresponding branch.", 
            "title": "Define a model with multiple inputs"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Functional/", 
            "text": "BigDL supports two different model definition styles: Sequential API and Functional API.\n\n\nIn Functional API, the model is described as a graph. It is more convenient than Sequential API\nwhen define some complex model.\n\n\n\n\nDefine a simple model\n\n\nSuppose we want to define a model with three layers\n\n\nLinear -\n Sigmoid -\n Softmax\n\n\n\n\nYou can write code like this\n\n\nScala:\n\n\nval linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])\n\n\n\n\nPython:\n\n\nlinear = Linear(...)()\nsigmoid = Sigmoid()(linear)\nsoftmax = Softmax()(sigmoid)\nmodel = Model([linear], [softmax])\n\n\n\n\nAn easy way to understand the Funtional API is to think of each layer in the model as a directed\nedge connecting its input and output\n\n\nIn the above code, first we create an input node named as linear by using\nthe Linear layer, then connect it to the sigmoid node with a Sigmoid\nlayer, then connect the sigmoid node to the softmax node with a Softmax layer.\n\n\nAfter defined the graph, we create the model by passing in the input nodes\nand output nodes.\n\n\n\n\nDefine a model with branches\n\n\nSuppose we want to define a model like this\n\n\nLinear -\n ReLU --\n Linear -\n ReLU\n               |-\n Linear -\n ReLU\n\n\n\n\nThe model has two outputs from two branches. The inputs of the branches are both the\noutput from the first ReLU.\n\n\nYou can define the model like this\n\n\nScala:\n\n\nval linear1 = Linear(...).inputs()\nval relu1 = ReLU().inputs(linear1)\nval linear2 = Linear(...).inputs(relu1)\nval relu2 = ReLU().inputs(linear2)\nval linear3 = Linear(...).inputs(relu1)\nval relu3 = ReLU().inputs(linear3)\nval model = Graph(Seq[linear1], Seq[relu2, relu3])\n\n\n\n\nPython:\n\n\nlinear1 = Linear(...)()\nrelu1 = ReLU()(linear1)\nlinear2 = Linear(...)(relu1)\nrelu2 = ReLU()(linear2)\nlinear3 = Linear(...)(relu1)\nrelu3 = ReLU()(linear3)\nmodel = Model(Seq[linear1], Seq[relu2, relu3])\n\n\n\n\nIn the above node, linear2 and linear3 are both from relu1 with separated\nLinear layers, which construct the branch structure. When we create the model,\nthe outputs parameter contains relu2 and relu3 as the model has two outputs.\n\n\n\n\nDefine a model with merged branch\n\n\nSuppose we want to define a model like this\n\n\nLinear -\n ReLU --\n Linear -\n ReLU ----\n Add\n               |-\n Linear -\n ReLU --|\n\n\n\n\nIn the model, the outputs of the two branches are merged by an add operation.\n\n\nYou can define the model like this\n\n\nScala:\n\n\nval linear1 = Linear(...).inputs()\nval relu1 = ReLU().inputs(linear1)\nval linear2 = Linear(...).inputs(relu1)\nval relu2 = ReLU().inputs(linear2)\nval linear3 = Linear(...).inputs(relu1)\nval relu3 = ReLU().inputs(linear3)\nval add = CAddTable().inputs(relu2, relu3)\nval model = Graph(Seq[linear1], Seq[add])\n\n\n\n\nPython:\n\n\nlinear1 = Linear(...)()\nrelu1 = ReLU()(linear1)\nlinear2 = Linear(...)(relu1)\nrelu2 = ReLU()(linear2)\nlinear3 = Linear(...)(relu1)\nrelu3 = ReLU()(linear3)\nadd = CAddTable()(relu2, relu3)\nmodel = Model(Seq[linear1], Seq[add])\n\n\n\n\nIn the above code, to merge the branch, we use the CAddTable, which takes two\ninput nodes, to generate one output node.\n\n\nBigDL provides many merge layers. Please check Merge layers document page. They all\ntake a list of tensors as input and merge the tensors by some operation.\n\n\n\n\nDefine a model with multiple inputs\n\n\nWe have already seen how to define branches in model and how to merge branches.\nWhat if we have multiple input? Suppose we want to define a model like this\n\n\nLinear -\n ReLU ----\n Add\nLinear -\n ReLU --|\n\n\n\n\nYou can define the model like this\n\n\nScala:\n\n\nval linear1 = Linear(...).inputs()\nval relu1 = ReLU().inputs(linear1)\nval linear2 = Linear(...).inputs()\nval relu2 = ReLU().inputs(linear2)\nval add = CAddTable().inputs(relu1, relu2)\nval model = Graph(Seq[linear1, linear2], Seq[add])\n\n\n\n\nPython:\n\n\nlinear1 = Linear(...)()\nrelu1 = ReLU()(linear1)\nlinear2 = Linear(...)()\nrelu2 = ReLU()(linear2)\nadd = CAddTable()(relu1, relu2)\nmodel = Model(Seq[linear1, linear2], Seq[add])\n\n\n\n\nIn the above code, we define two input nodes linear1 and linear2 and put them\ninto the first parameter when create the graph model.", 
            "title": "Using Functional API"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Functional/#define-a-simple-model", 
            "text": "Suppose we want to define a model with three layers  Linear -  Sigmoid -  Softmax  You can write code like this  Scala:  val linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])  Python:  linear = Linear(...)()\nsigmoid = Sigmoid()(linear)\nsoftmax = Softmax()(sigmoid)\nmodel = Model([linear], [softmax])  An easy way to understand the Funtional API is to think of each layer in the model as a directed\nedge connecting its input and output  In the above code, first we create an input node named as linear by using\nthe Linear layer, then connect it to the sigmoid node with a Sigmoid\nlayer, then connect the sigmoid node to the softmax node with a Softmax layer.  After defined the graph, we create the model by passing in the input nodes\nand output nodes.", 
            "title": "Define a simple model"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Functional/#define-a-model-with-branches", 
            "text": "Suppose we want to define a model like this  Linear -  ReLU --  Linear -  ReLU\n               |-  Linear -  ReLU  The model has two outputs from two branches. The inputs of the branches are both the\noutput from the first ReLU.  You can define the model like this  Scala:  val linear1 = Linear(...).inputs()\nval relu1 = ReLU().inputs(linear1)\nval linear2 = Linear(...).inputs(relu1)\nval relu2 = ReLU().inputs(linear2)\nval linear3 = Linear(...).inputs(relu1)\nval relu3 = ReLU().inputs(linear3)\nval model = Graph(Seq[linear1], Seq[relu2, relu3])  Python:  linear1 = Linear(...)()\nrelu1 = ReLU()(linear1)\nlinear2 = Linear(...)(relu1)\nrelu2 = ReLU()(linear2)\nlinear3 = Linear(...)(relu1)\nrelu3 = ReLU()(linear3)\nmodel = Model(Seq[linear1], Seq[relu2, relu3])  In the above node, linear2 and linear3 are both from relu1 with separated\nLinear layers, which construct the branch structure. When we create the model,\nthe outputs parameter contains relu2 and relu3 as the model has two outputs.", 
            "title": "Define a model with branches"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Functional/#define-a-model-with-merged-branch", 
            "text": "Suppose we want to define a model like this  Linear -  ReLU --  Linear -  ReLU ----  Add\n               |-  Linear -  ReLU --|  In the model, the outputs of the two branches are merged by an add operation.  You can define the model like this  Scala:  val linear1 = Linear(...).inputs()\nval relu1 = ReLU().inputs(linear1)\nval linear2 = Linear(...).inputs(relu1)\nval relu2 = ReLU().inputs(linear2)\nval linear3 = Linear(...).inputs(relu1)\nval relu3 = ReLU().inputs(linear3)\nval add = CAddTable().inputs(relu2, relu3)\nval model = Graph(Seq[linear1], Seq[add])  Python:  linear1 = Linear(...)()\nrelu1 = ReLU()(linear1)\nlinear2 = Linear(...)(relu1)\nrelu2 = ReLU()(linear2)\nlinear3 = Linear(...)(relu1)\nrelu3 = ReLU()(linear3)\nadd = CAddTable()(relu2, relu3)\nmodel = Model(Seq[linear1], Seq[add])  In the above code, to merge the branch, we use the CAddTable, which takes two\ninput nodes, to generate one output node.  BigDL provides many merge layers. Please check Merge layers document page. They all\ntake a list of tensors as input and merge the tensors by some operation.", 
            "title": "Define a model with merged branch"
        }, 
        {
            "location": "/ProgrammingGuide/Model/Functional/#define-a-model-with-multiple-inputs", 
            "text": "We have already seen how to define branches in model and how to merge branches.\nWhat if we have multiple input? Suppose we want to define a model like this  Linear -  ReLU ----  Add\nLinear -  ReLU --|  You can define the model like this  Scala:  val linear1 = Linear(...).inputs()\nval relu1 = ReLU().inputs(linear1)\nval linear2 = Linear(...).inputs()\nval relu2 = ReLU().inputs(linear2)\nval add = CAddTable().inputs(relu1, relu2)\nval model = Graph(Seq[linear1, linear2], Seq[add])  Python:  linear1 = Linear(...)()\nrelu1 = ReLU()(linear1)\nlinear2 = Linear(...)()\nrelu2 = ReLU()(linear2)\nadd = CAddTable()(relu1, relu2)\nmodel = Model(Seq[linear1, linear2], Seq[add])  In the above code, we define two input nodes linear1 and linear2 and put them\ninto the first parameter when create the graph model.", 
            "title": "Define a model with multiple inputs"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/", 
            "text": "Use Optimizer for Training\n\n\nYou can use \nOptimizer\n in BigDL to train a model. \n\n\nYou need to first create an \nOptimizer\n, and then call \nOptimizer.optimize\n to start the training. \n\n\nTo create an optimizer, you need at least provide model, data, loss function and batch size.\n\n\n\n\nmodel\n\n\n\n\nA neural network model. May be a layer, a sequence of layers or a\ngraph of layers.\n\n\n\n\ndata\n\n\n\n\nYour training data. As we train models on Spark, one of\nthe most common distributed data structures is RDD. Of course\nyou can use DataFrame. Please check the BigDL pipeline example.\n\n\nThe element in the RDD is \nSample\n, which is actually a sequence of\nTensors. You need to convert your data record(image, audio, text)\nto Tensors before you feed them into Optimizer. We also provide\nmany utilities to do it.\n\n\n\n\nloss function\n\n\n\n\nIn supervised machine learning, loss function compares the output of\nthe model with the ground truth(the labels of the training data). It\noutputs a loss value to measure how good the model is(the lower the\nbetter). It also provides a gradient to indicate how to tune the model.\n\n\nIn BigDL, all loss functions are subclass of Criterion. Refer to \nLosses\n for a list of defined losses.\n\n\n\n\nbatch size\n\n\n\n\nTraining is an iterative process. In each iteration, only a batch of data\nis used for training the model. You need to specify the batch size. Please note, \nthe batch size should be divisible by the total cores number.\n\n\nHere's an example of how to train a Linear classification model\n\n\nscala\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n// Define the model\nval model = Linear[Float](2, 1)\nmodel.bias.zero()\n\n// Generate 2D dummy data, y = 0.1 * x[1] + 0.3 * x[2]\nval samples = Seq(\n  Sample[Float](Tensor[Float](T(5f, 5f)), Tensor[Float](T(2.0f))),\n  Sample[Float](Tensor[Float](T(-5f, -5f)), Tensor[Float](T(-2.0f))),\n  Sample[Float](Tensor[Float](T(-2f, 5f)), Tensor[Float](T(1.3f))),\n  Sample[Float](Tensor[Float](T(-5f, 2f)), Tensor[Float](T(0.1f))),\n  Sample[Float](Tensor[Float](T(5f, -2f)), Tensor[Float](T(-0.1f))),\n  Sample[Float](Tensor[Float](T(2f, -5f)), Tensor[Float](T(-1.3f)))\n)\nval trainData = sc.parallelize(samples, 1)\n\n// Define the model\nval optimizer = Optimizer[Float](model, trainData, MSECriterion[Float](), 4)\nEngine.init\noptimizer.optimize()\nprintln(model.weight)\n\n\n\n\nThe weight of linear is init randomly. But the output should be like\n\n\nscala\n println(model.weight)\n0.09316949      0.2887804\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]\n\n\n\n\npython\n\n\nfrom bigdl.nn.layer import Linear\nfrom bigdl.util.common import *\nfrom bigdl.nn.criterion import MSECriterion\nfrom bigdl.optim.optimizer import Optimizer, MaxIteration\nimport numpy as np\n\nmodel = Linear(2, 1)\nsamples = [\n  Sample.from_ndarray(np.array([5, 5]), np.array([2.0])),\n  Sample.from_ndarray(np.array([-5, -5]), np.array([-2.0])),\n  Sample.from_ndarray(np.array([-2, 5]), np.array([1.3])),\n  Sample.from_ndarray(np.array([-5, 2]), np.array([0.1])),\n  Sample.from_ndarray(np.array([5, -2]), np.array([-0.1])),\n  Sample.from_ndarray(np.array([2, -5]), np.array([-1.3]))\n]\ntrain_data = sc.parallelize(samples, 1)\ninit_engine()\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)\noptimizer.optimize()\nmodel.get_weights()[0]\n\n\n\n\nThe output should be like\n\n\narray([[ 0.11578175,  0.28315681]], dtype=float32)\n\n\n\n\nYou can see the model is trained.\n\n\nDefine when to end the training\n\n\nYou need define when to end the training. It can be several iterations, or how many round\ndata you want to process, a.k.a epoch.\n\n\nscala\n\n\n// The default endWhen in scala is 100 iterations\noptimizer.setEndWhen(Trigger.maxEpoch(10))  // Change to 10 epoch\n\n\n\n\npython\n\n\n# Python need to define in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)\n\n\n\n\nChange the optimization algorithm\n\n\nGradient based optimization algorithms are the most popular algorithms to train the neural\nnetwork model. The most famous one is SGD. SGD has many variants, adagrad, adam, etc.\n\n\nscala\n\n\n// The default is SGD\noptimizer.setOptimMethod(new Adam())  // Change to adam\n\n\n\n\npython\n\n\n# Python need to define the optimization algorithm in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4, optim_method = Adam())\n\n\n\n\nValidate your model in training\n\n\nSometimes, people want to evaluate the model with a seperated dataset. When model\nperforms well on train dataset, but bad on validation dataset, we call the model is overfit or\nweak generalization. People may want to evaluate the model every serveral iterations or \nepochs. BigDL can easily do this by\n\n\nscala\n\n\noptimizer.setValidation(trigger, testData, validationMethod, batchSize)\n\n\n\n\npython\n\n\noptimizer.set_validation(batch_size, val_rdd, trigger, validationMethod)\n\n\n\n\nFor validation, you need to provide\n\n\n\n\ntrigger: how often to do validation, maybe each several iterations or epochs\n\n\ntest data: the seperate dataset for test\n\n\nvalidation method: how to evaluate the model, maybe top1 accuracy, etc.\n\n\nbatch size: how many data evaluate in one time\n\n\n\n\nCheckpointing\n\n\nYou can configure the optimizer to periodically take snapshots of the model (trained weights, biases, etc.) and optim-method (configurations and states of the optimization) and dump them into files. \n\n\nThe model snapshot will be named as \nmodel.#iteration_number\n, and optim method snapshot will be named as \nstate.#iteration_number\n.\n\n\nUsage as below.\n\n\nscala\n\n\noptimizer.setCheckpoint(path, trigger)\n\n\n\n\npython\n\n\noptimizer.set_checkpoint(path, trigger,isOverWrite=True)\n\n\n\n\nParameters you need to specify are:\n\n\n\n\npath - the directory to save the snapshots\n\n\ntrigger - how often to save the check point \n\n\n\n\nIn scala, you can also use \noverWriteCheckpoint()\n to enable overwriting any existing snapshot files with the same name (default is disabled). In Python, you can just set parameter isOverWrite (default is True).\n\n\nscala\n\n\noptimizer.overWriteCheckpoint()`\n\n\n\n\npython\n\n\noptimizer.set_checkpoint(path, trigger,isOverWrite=True)\n\n\n\n\nResume Training\n\n\nAfter training stops, you can resume from any saved point. Choose one of   the model snapshots and the corresponding optim-method snapshot to resume (saved in checkpoint path, details see \nCheckpointing\n).     Use \nModule.load\n (Scala) or \nModel.load\n(Python) to load the model         snapshot into an model object, and \nOptimMethod.load\n (Scala and Python) to load optimization method into an OptimMethod  object. Then create a new \nOptimizer\n with the loaded model and optim       method. Call \nOptimizer.optimize\n, and you will resume from the point       where the snapshot is taken. Refer to \nOptimMethod Load\n and \nModel Load\n for details.\n\n\nYou can also resume training without loading the optim method, if you       intend to change the learning rate schedule or even the optimization        algorithm. Just create an \nOptimizer\n with loaded model and a new instance  of OptimMethod (both Scala and Python).\n\n\nMonitor your training\n\n\nscala\n\n\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n\n\n\n\npython\n\n\nset_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n\n\n\n\nSee details in \nVisualization", 
            "title": "Optimization"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#use-optimizer-for-training", 
            "text": "You can use  Optimizer  in BigDL to train a model.   You need to first create an  Optimizer , and then call  Optimizer.optimize  to start the training.   To create an optimizer, you need at least provide model, data, loss function and batch size.   model   A neural network model. May be a layer, a sequence of layers or a\ngraph of layers.   data   Your training data. As we train models on Spark, one of\nthe most common distributed data structures is RDD. Of course\nyou can use DataFrame. Please check the BigDL pipeline example.  The element in the RDD is  Sample , which is actually a sequence of\nTensors. You need to convert your data record(image, audio, text)\nto Tensors before you feed them into Optimizer. We also provide\nmany utilities to do it.   loss function   In supervised machine learning, loss function compares the output of\nthe model with the ground truth(the labels of the training data). It\noutputs a loss value to measure how good the model is(the lower the\nbetter). It also provides a gradient to indicate how to tune the model.  In BigDL, all loss functions are subclass of Criterion. Refer to  Losses  for a list of defined losses.   batch size   Training is an iterative process. In each iteration, only a batch of data\nis used for training the model. You need to specify the batch size. Please note, \nthe batch size should be divisible by the total cores number.  Here's an example of how to train a Linear classification model  scala  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n// Define the model\nval model = Linear[Float](2, 1)\nmodel.bias.zero()\n\n// Generate 2D dummy data, y = 0.1 * x[1] + 0.3 * x[2]\nval samples = Seq(\n  Sample[Float](Tensor[Float](T(5f, 5f)), Tensor[Float](T(2.0f))),\n  Sample[Float](Tensor[Float](T(-5f, -5f)), Tensor[Float](T(-2.0f))),\n  Sample[Float](Tensor[Float](T(-2f, 5f)), Tensor[Float](T(1.3f))),\n  Sample[Float](Tensor[Float](T(-5f, 2f)), Tensor[Float](T(0.1f))),\n  Sample[Float](Tensor[Float](T(5f, -2f)), Tensor[Float](T(-0.1f))),\n  Sample[Float](Tensor[Float](T(2f, -5f)), Tensor[Float](T(-1.3f)))\n)\nval trainData = sc.parallelize(samples, 1)\n\n// Define the model\nval optimizer = Optimizer[Float](model, trainData, MSECriterion[Float](), 4)\nEngine.init\noptimizer.optimize()\nprintln(model.weight)  The weight of linear is init randomly. But the output should be like  scala  println(model.weight)\n0.09316949      0.2887804\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]  python  from bigdl.nn.layer import Linear\nfrom bigdl.util.common import *\nfrom bigdl.nn.criterion import MSECriterion\nfrom bigdl.optim.optimizer import Optimizer, MaxIteration\nimport numpy as np\n\nmodel = Linear(2, 1)\nsamples = [\n  Sample.from_ndarray(np.array([5, 5]), np.array([2.0])),\n  Sample.from_ndarray(np.array([-5, -5]), np.array([-2.0])),\n  Sample.from_ndarray(np.array([-2, 5]), np.array([1.3])),\n  Sample.from_ndarray(np.array([-5, 2]), np.array([0.1])),\n  Sample.from_ndarray(np.array([5, -2]), np.array([-0.1])),\n  Sample.from_ndarray(np.array([2, -5]), np.array([-1.3]))\n]\ntrain_data = sc.parallelize(samples, 1)\ninit_engine()\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)\noptimizer.optimize()\nmodel.get_weights()[0]  The output should be like  array([[ 0.11578175,  0.28315681]], dtype=float32)  You can see the model is trained.", 
            "title": "Use Optimizer for Training"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#define-when-to-end-the-training", 
            "text": "You need define when to end the training. It can be several iterations, or how many round\ndata you want to process, a.k.a epoch.  scala  // The default endWhen in scala is 100 iterations\noptimizer.setEndWhen(Trigger.maxEpoch(10))  // Change to 10 epoch  python  # Python need to define in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4)", 
            "title": "Define when to end the training"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#change-the-optimization-algorithm", 
            "text": "Gradient based optimization algorithms are the most popular algorithms to train the neural\nnetwork model. The most famous one is SGD. SGD has many variants, adagrad, adam, etc.  scala  // The default is SGD\noptimizer.setOptimMethod(new Adam())  // Change to adam  python  # Python need to define the optimization algorithm in the constructor\noptimizer = Optimizer(model, train_data, MSECriterion(), MaxIteration(100), 4, optim_method = Adam())", 
            "title": "Change the optimization algorithm"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#validate-your-model-in-training", 
            "text": "Sometimes, people want to evaluate the model with a seperated dataset. When model\nperforms well on train dataset, but bad on validation dataset, we call the model is overfit or\nweak generalization. People may want to evaluate the model every serveral iterations or \nepochs. BigDL can easily do this by  scala  optimizer.setValidation(trigger, testData, validationMethod, batchSize)  python  optimizer.set_validation(batch_size, val_rdd, trigger, validationMethod)  For validation, you need to provide   trigger: how often to do validation, maybe each several iterations or epochs  test data: the seperate dataset for test  validation method: how to evaluate the model, maybe top1 accuracy, etc.  batch size: how many data evaluate in one time", 
            "title": "Validate your model in training"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#checkpointing", 
            "text": "You can configure the optimizer to periodically take snapshots of the model (trained weights, biases, etc.) and optim-method (configurations and states of the optimization) and dump them into files.   The model snapshot will be named as  model.#iteration_number , and optim method snapshot will be named as  state.#iteration_number .  Usage as below.  scala  optimizer.setCheckpoint(path, trigger)  python  optimizer.set_checkpoint(path, trigger,isOverWrite=True)  Parameters you need to specify are:   path - the directory to save the snapshots  trigger - how often to save the check point    In scala, you can also use  overWriteCheckpoint()  to enable overwriting any existing snapshot files with the same name (default is disabled). In Python, you can just set parameter isOverWrite (default is True).  scala  optimizer.overWriteCheckpoint()`  python  optimizer.set_checkpoint(path, trigger,isOverWrite=True)", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#resume-training", 
            "text": "After training stops, you can resume from any saved point. Choose one of   the model snapshots and the corresponding optim-method snapshot to resume (saved in checkpoint path, details see  Checkpointing ).     Use  Module.load  (Scala) or  Model.load (Python) to load the model         snapshot into an model object, and  OptimMethod.load  (Scala and Python) to load optimization method into an OptimMethod  object. Then create a new  Optimizer  with the loaded model and optim       method. Call  Optimizer.optimize , and you will resume from the point       where the snapshot is taken. Refer to  OptimMethod Load  and  Model Load  for details.  You can also resume training without loading the optim method, if you       intend to change the learning rate schedule or even the optimization        algorithm. Just create an  Optimizer  with loaded model and a new instance  of OptimMethod (both Scala and Python).", 
            "title": "Resume Training"
        }, 
        {
            "location": "/ProgrammingGuide/optimization/#monitor-your-training", 
            "text": "scala  optimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)  python  set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)  See details in  Visualization", 
            "title": "Monitor your training"
        }, 
        {
            "location": "/ProgrammingGuide/MLPipeline/", 
            "text": "Overview\n\n\nBigDL provides \nDLEstimator\n and \nDLClassifier\n for users with Apache Spark MLlib experience, which\nprovides high level API for training a BigDL Model with the Apache Spark \nEstimator\n/\nTransfomer\n\npattern, thus users can conveniently fit BigDL into a ML pipeline. The fitted model \nDLModel\n and\n\nDLClassiferModel\n contains the trained BigDL model and extends the Spark ML \nModel\n class.\nAlternatively users may also construct a \nDLModel\n with a pre-trained BigDL model to use it in\nSpark ML Pipeline for prediction.\n\n\nCurrently only scala interface are implemented for \nDLEstimator\n and \nDLClassifier\n. Python\nsupport will be added soon.\n\n\n\n\nDLEstimator\n\n\nDLEstimator\n extends \norg.apache.spark.ml.Estimator\n and supports model training from\nApache Spark DataFrame/Dataset. \n\n\nDifferent from many algorithms in Spark MLlib, \nDLEstimator\n supports more data types for the\nlabel column. In many deep learning applications, the label data could be a sequence\nor other data collection. \nDLEstimator\n supports feature and label data in the format\nof \nArray[Double]\n, \nArray[Float]\n, \norg.apache.spark.mllib.linalg.Vector\n (for Apache\nSpark 1.5, 1.6) and \norg.apache.spark.ml.linalg.Vector\n (for Apache Spark 2.0+). Also label\ndata can be of Double type.\n\n\nTo use \nDLEstimator\n for training, user should specify\n\n\n\n\nthe model structure constructed from BigDL layers. You can also use some predefined model\nlike LetNet or ResNet.\n\n\nthe model criterion, which calculates the loss and gradient from model output and label.\n\n\nthe feature data dimensions and label data dimensions (the constructor\nparameters \nfeatureSize\n and \nlabelSize\n respectively). E.g., a sample from\n\nMNist\n may have the \nfeatureSize\n as Array(28, 28) and\n\nlabelSize\n as Array(1). And the feature column contains an array or a \nVector\n of 784 (28 * 28)\nnumbers. Internally the feature and label data are converted to BigDL tensors, to further train\na BigDL model efficiently.\n\n\n\n\nThe return result of \nfit\n function in \nDLEstimator\n is a \nDLModel\n, which contains the\ntrained BigDL models and extends \norg.apache.spark.ml.Transformer\n to be used in prediction.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport org.apache.spark.ml.DLEstimator\nimport org.apache.spark.sql.SQLContext\n\n/**\n *  Multi-label regression with BigDL layers and DLEstimator\n */\nobject DLEstimatorMultiLabelLR {\n\n  def main(args: Array[String]): Unit = {\n    val conf = Engine.createSparkConf()\n      .setAppName(\nDLEstimatorMultiLabelLR\n)\n      .setMaster(\nlocal[1]\n)\n    val sc = new SparkContext(conf)\n    val sqlContext = SQLContext.getOrCreate(sc)\n    Engine.init\n\n    val model = Sequential().add(Linear(2, 2))\n    val criterion = MSECriterion()\n    val estimator = new DLEstimator(model, criterion, Array(2), Array(2))\n      .setBatchSize(4)\n      .setMaxEpoch(10)\n    val data = sc.parallelize(Seq(\n      (Array(2.0, 1.0), Array(1.0, 2.0)),\n      (Array(1.0, 2.0), Array(2.0, 1.0)),\n      (Array(2.0, 1.0), Array(1.0, 2.0)),\n      (Array(1.0, 2.0), Array(2.0, 1.0))))\n    val df = sqlContext.createDataFrame(data).toDF(\nfeatures\n, \nlabel\n)\n    val dlModel = estimator.fit(df)\n    dlModel.transform(df).show(false)\n  }\n}\n\n\n\n\n\nOutput is\n\n\n\n\n\n\n\n\nfeatures\n\n\nlabel\n\n\nprediction\n\n\n\n\n\n\n\n\n\n\n[2.0, 1.0]\n\n\n[1.0, 2.0]\n\n\n[1.0034767389297485, 2.006068706512451]\n\n\n\n\n\n\n[1.0, 2.0]\n\n\n[2.0, 1.0]\n\n\n[2.006953001022339, 1.0039551258087158]\n\n\n\n\n\n\n[2.0, 1.0]\n\n\n[1.0, 2.0]\n\n\n[1.0034767389297485, 2.006068706512451]\n\n\n\n\n\n\n[1.0, 2.0]\n\n\n[2.0, 1.0]\n\n\n[2.006953001022339, 1.0039551258087158]\n\n\n\n\n\n\n\n\n\n\nDLClassifier\n\n\nDLClassifier\n is a specialized \nDLEstimator\n that simplifies the data format for\nclassification tasks. It only supports label column of DoubleType, and the fitted\n\nDLClassifierModel\n will have the prediction column of DoubleType.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.{ClassNLLCriterion, Linear, LogSoftMax, Sequential}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport org.apache.spark.ml.DLClassifier\nimport org.apache.spark.sql.SQLContext\n\n/**\n * Logistic Regression with BigDL layers and DLClassifier\n */\nobject DLClassifierLogisticRegression {\n\n  def main(args: Array[String]): Unit = {\n    val conf = Engine.createSparkConf()\n      .setAppName(\nDLClassifierLogisticRegression\n)\n      .setMaster(\nlocal[1]\n)\n    val sc = new SparkContext(conf)\n    val sqlContext = SQLContext.getOrCreate(sc)\n    Engine.init\n\n    val model = Sequential().add(Linear(2, 2)).add(LogSoftMax())\n    val criterion = ClassNLLCriterion()\n    val estimator = new DLClassifier(model, criterion, Array(2))\n      .setBatchSize(4)\n      .setMaxEpoch(10)\n    val data = sc.parallelize(Seq(\n      (Array(0.0, 1.0), 1.0),\n      (Array(1.0, 0.0), 2.0),\n      (Array(0.0, 1.0), 1.0),\n      (Array(1.0, 0.0), 2.0)))\n    val df = sqlContext.createDataFrame(data).toDF(\nfeatures\n, \nlabel\n)\n    val dlModel = estimator.fit(df)\n    dlModel.transform(df).show(false)\n  }\n}\n\n\n\n\nOutput is\n\n\n\n\n\n\n\n\nfeatures\n\n\nlabel\n\n\nprediction\n\n\n\n\n\n\n\n\n\n\n[0.0, 1.0]\n\n\n1.0\n\n\n1.0\n\n\n\n\n\n\n[1.0, 0.0]\n\n\n2.0\n\n\n2.0\n\n\n\n\n\n\n[0.0, 1.0]\n\n\n1.0\n\n\n1.0\n\n\n\n\n\n\n[1.0, 0.0]\n\n\n2.0\n\n\n2.0\n\n\n\n\n\n\n\n\nMore examples and the full example code can be found from package\ncom.intel.analytics.bigdl.example.MLPipeline", 
            "title": "Use Spark ML"
        }, 
        {
            "location": "/ProgrammingGuide/MLPipeline/#overview", 
            "text": "BigDL provides  DLEstimator  and  DLClassifier  for users with Apache Spark MLlib experience, which\nprovides high level API for training a BigDL Model with the Apache Spark  Estimator / Transfomer \npattern, thus users can conveniently fit BigDL into a ML pipeline. The fitted model  DLModel  and DLClassiferModel  contains the trained BigDL model and extends the Spark ML  Model  class.\nAlternatively users may also construct a  DLModel  with a pre-trained BigDL model to use it in\nSpark ML Pipeline for prediction.  Currently only scala interface are implemented for  DLEstimator  and  DLClassifier . Python\nsupport will be added soon.", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/MLPipeline/#dlestimator", 
            "text": "DLEstimator  extends  org.apache.spark.ml.Estimator  and supports model training from\nApache Spark DataFrame/Dataset.   Different from many algorithms in Spark MLlib,  DLEstimator  supports more data types for the\nlabel column. In many deep learning applications, the label data could be a sequence\nor other data collection.  DLEstimator  supports feature and label data in the format\nof  Array[Double] ,  Array[Float] ,  org.apache.spark.mllib.linalg.Vector  (for Apache\nSpark 1.5, 1.6) and  org.apache.spark.ml.linalg.Vector  (for Apache Spark 2.0+). Also label\ndata can be of Double type.  To use  DLEstimator  for training, user should specify   the model structure constructed from BigDL layers. You can also use some predefined model\nlike LetNet or ResNet.  the model criterion, which calculates the loss and gradient from model output and label.  the feature data dimensions and label data dimensions (the constructor\nparameters  featureSize  and  labelSize  respectively). E.g., a sample from MNist  may have the  featureSize  as Array(28, 28) and labelSize  as Array(1). And the feature column contains an array or a  Vector  of 784 (28 * 28)\nnumbers. Internally the feature and label data are converted to BigDL tensors, to further train\na BigDL model efficiently.   The return result of  fit  function in  DLEstimator  is a  DLModel , which contains the\ntrained BigDL models and extends  org.apache.spark.ml.Transformer  to be used in prediction.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport org.apache.spark.ml.DLEstimator\nimport org.apache.spark.sql.SQLContext\n\n/**\n *  Multi-label regression with BigDL layers and DLEstimator\n */\nobject DLEstimatorMultiLabelLR {\n\n  def main(args: Array[String]): Unit = {\n    val conf = Engine.createSparkConf()\n      .setAppName( DLEstimatorMultiLabelLR )\n      .setMaster( local[1] )\n    val sc = new SparkContext(conf)\n    val sqlContext = SQLContext.getOrCreate(sc)\n    Engine.init\n\n    val model = Sequential().add(Linear(2, 2))\n    val criterion = MSECriterion()\n    val estimator = new DLEstimator(model, criterion, Array(2), Array(2))\n      .setBatchSize(4)\n      .setMaxEpoch(10)\n    val data = sc.parallelize(Seq(\n      (Array(2.0, 1.0), Array(1.0, 2.0)),\n      (Array(1.0, 2.0), Array(2.0, 1.0)),\n      (Array(2.0, 1.0), Array(1.0, 2.0)),\n      (Array(1.0, 2.0), Array(2.0, 1.0))))\n    val df = sqlContext.createDataFrame(data).toDF( features ,  label )\n    val dlModel = estimator.fit(df)\n    dlModel.transform(df).show(false)\n  }\n}  Output is     features  label  prediction      [2.0, 1.0]  [1.0, 2.0]  [1.0034767389297485, 2.006068706512451]    [1.0, 2.0]  [2.0, 1.0]  [2.006953001022339, 1.0039551258087158]    [2.0, 1.0]  [1.0, 2.0]  [1.0034767389297485, 2.006068706512451]    [1.0, 2.0]  [2.0, 1.0]  [2.006953001022339, 1.0039551258087158]", 
            "title": "DLEstimator"
        }, 
        {
            "location": "/ProgrammingGuide/MLPipeline/#dlclassifier", 
            "text": "DLClassifier  is a specialized  DLEstimator  that simplifies the data format for\nclassification tasks. It only supports label column of DoubleType, and the fitted DLClassifierModel  will have the prediction column of DoubleType.  Scala example:  import com.intel.analytics.bigdl.nn.{ClassNLLCriterion, Linear, LogSoftMax, Sequential}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport org.apache.spark.ml.DLClassifier\nimport org.apache.spark.sql.SQLContext\n\n/**\n * Logistic Regression with BigDL layers and DLClassifier\n */\nobject DLClassifierLogisticRegression {\n\n  def main(args: Array[String]): Unit = {\n    val conf = Engine.createSparkConf()\n      .setAppName( DLClassifierLogisticRegression )\n      .setMaster( local[1] )\n    val sc = new SparkContext(conf)\n    val sqlContext = SQLContext.getOrCreate(sc)\n    Engine.init\n\n    val model = Sequential().add(Linear(2, 2)).add(LogSoftMax())\n    val criterion = ClassNLLCriterion()\n    val estimator = new DLClassifier(model, criterion, Array(2))\n      .setBatchSize(4)\n      .setMaxEpoch(10)\n    val data = sc.parallelize(Seq(\n      (Array(0.0, 1.0), 1.0),\n      (Array(1.0, 0.0), 2.0),\n      (Array(0.0, 1.0), 1.0),\n      (Array(1.0, 0.0), 2.0)))\n    val df = sqlContext.createDataFrame(data).toDF( features ,  label )\n    val dlModel = estimator.fit(df)\n    dlModel.transform(df).show(false)\n  }\n}  Output is     features  label  prediction      [0.0, 1.0]  1.0  1.0    [1.0, 0.0]  2.0  2.0    [0.0, 1.0]  1.0  1.0    [1.0, 0.0]  2.0  2.0     More examples and the full example code can be found from package\ncom.intel.analytics.bigdl.example.MLPipeline", 
            "title": "DLClassifier"
        }, 
        {
            "location": "/ProgrammingGuide/visualization/", 
            "text": "Generating summary info in BigDL\n\n\nTo enable visualization support, you need first properly configure the \nOptimizer\n to collect statistics summary in different stages of training (i.e. training (\nTrainSummary\n) and validation (\nValidationSummary\n),respectively). It should be done before the training starts (calling \nOptimizer.optimize()\n). See examples below: \n\n\nExample: Generating summary info in Scala\n\n\nval optimizer = Optimizer(...)\n...\nval logdir = \nmylogdir\n\nval appName = \nmyapp\n\nval trainSummary = TrainSummary(logdir, appName)\nval validationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n...\nval trained_model = optimizer.optimize()\n\n\n\n\nExample: Configure summary generation in Python\n\n\noptimizer = Optimizer(...)\n...\nlog_dir = 'mylogdir'\napp_name = 'myapp'\ntrain_summary = TrainSummary(log_dir=log_dir, app_name=app_name)\nval_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)\noptimizer.set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n...\ntrainedModel = optimizer.optimize()\n\n\n\n\nAfter you start to run your spark job, the train and validation summary will be saved to \nmylogdir/myapp/train\n and \nmylogdir/myapp/validation\n respectively (Note: you may want to use different \nappName\n for different job runs to avoid possible conflicts.)\n\n\n\n\nRetrieving summary info as readable format\n\n\nYou can use provided API \nreadScalar\n(Scala) and \nread_scalar\n(Python) to retrieve the summaries into readable format, and export them to other tools for further analysis or visualization.\n\n\nExample: Reading summary info in Scala\n\n\nval trainLoss = trainSummary.readScalar(\nLoss\n)\nval validationLoss = validationSummary.readScalar(\nLoss\n)\n...\n\n\n\n\nExample: Reading summary info in Python\n\n\nloss = np.array(train_summary.read_scalar('Loss'))\nvalloss = np.array(val_summary.read_scalar('Loss'))\n...\n\n\n\n\n\n\nVisualizing training with TensorBoard\n\n\nWith the summary info generated, we can then use \nTensorBoard\n to visualize the behaviors of the BigDL program.  \n\n\n\n\nInstalling TensorBoard\n\n\n\n\nPrerequisites:\n\n\n\n\nPython verison: 2.7, 3.4, 3.5, or 3.6\n\n\nPip version \n= 9.0.1\n\n\n\n\nTo install TensorBoard using Python 2, you may run the command:\n\n\npip install tensorboard==1.0.0a4\n\n\n\n\nTo install TensorBoard using Python 3, you may run the command:\n\n\npip3 install tensorboard==1.0.0a4\n\n\n\n\nPlease refer to \nthis page\n for possible issues when installing TensorBoard.\n\n\n\n\nLaunching TensorBoard\n\n\n\n\nYou can launch TensorBoard using the command below:\n\n\ntensorboard --logdir=/tmp/bigdl_summaries\n\n\n\n\nAfter that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006\n\n\n\n\nVisualizations in TensorBoard\n\n\n\n\nWithin the TensorBoard dashboard, you will be able to read the visualizations of each run, including the \u201cLoss\u201d and \u201cThroughput\u201d curves under the SCALARS tab (as illustrated below):\n\n\n\nAnd \u201cweights\u201d, \u201cbias\u201d, \u201cgradientWeights\u201d and \u201cgradientBias\u201d under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):\n\n\n\n\n\n\n\nVisualizing training with Jupyter notebook\n\n\nIf you're using Jupyter notebook, you can also draw the training curves using popular plotting tools (e.g. matplotlib) and show the plots inline. \n\n\nFirst, retrieve the summaries as instructed in \nRetrieve Summary\n. The retrived summary is a list of tuples. Each tuple is a recorded event in format (iteration count, recorded value, timestamp). You can convert it to numpy array or dataframe to plot it. See example below:  \n\n\nExample: Plot the train/validation loss in Jupyter\n\n\n#retrieve train and validation summary object and read the loss data into ndarray's. \nloss = np.array(train_summary.read_scalar(\nLoss\n))\nval_loss  = np.array(val_summary.read_scalar(\nLoss\n))\n\n#plot the train and validation curves\n# each event data is a tuple in form of (iteration_count, value, timestamp)\nplt.plot(loss[:,0],loss[:,1],label='train loss')\nplt.plot(val_loss[:,0],val_loss[:,1],label='val loss',color='green')\nplt.scatter(val_loss[:,0],val_loss[:,1],color='green')\nplt.legend();\n\n\n\n\n\n\nLogging\n\n\nBigDL also has a stright-forward logging output on the console along the    training, as shown below. You can see real-time epoch/iteration/loss/       throughput in the log.\n\n\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/               5000][Iteration 1][Wall Clock XXX] Train 512 in   XXXseconds. Throughput    is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/             5000][Iteration 2][Wall Clock XXX] Train 512    in XXXseconds. Throughput   is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/            5000][Iteration 3][Wall Clock XXX] Train 512   in XXXseconds. Throughput    is XXX records/second. Loss is XXX.\n\n\n\n\nThe DistriOptimizer log level is INFO by default. We implement a method     named with \nredirectSparkInfoLogs\n  in \nspark/utils/LoggerFilter.scala\n.    You can import and redirect at first.\n\n\n  import com.intel.analytics.bigdl.utils.LoggerFilter\n  LoggerFilter.redirectSparkInfoLogs()\n\n\n\n\nThis method will redirect all logs of \norg\n, \nakka\n, \nbreeze\n to \nbigdl.    log\n with \nINFO\n level, except \norg.  apache.spark.SparkContext\n. And it    will output all \nERROR\n message in console too.\n\n\nYou can disable the redirection with java property \n-Dbigdl.utils.          LoggerFilter.disable=true\n. By default,   it will do redirect of all        examples and models in our code.\n\n\nYou can set where the \nbigdl.log\n will be generated with \n-Dbigdl.utils.    LoggerFilter.logFile=\npath\n. By    default, it will be generated under     current workspace.", 
            "title": "Visualization"
        }, 
        {
            "location": "/ProgrammingGuide/visualization/#generating-summary-info-in-bigdl", 
            "text": "To enable visualization support, you need first properly configure the  Optimizer  to collect statistics summary in different stages of training (i.e. training ( TrainSummary ) and validation ( ValidationSummary ),respectively). It should be done before the training starts (calling  Optimizer.optimize() ). See examples below:   Example: Generating summary info in Scala  val optimizer = Optimizer(...)\n...\nval logdir =  mylogdir \nval appName =  myapp \nval trainSummary = TrainSummary(logdir, appName)\nval validationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n...\nval trained_model = optimizer.optimize()  Example: Configure summary generation in Python  optimizer = Optimizer(...)\n...\nlog_dir = 'mylogdir'\napp_name = 'myapp'\ntrain_summary = TrainSummary(log_dir=log_dir, app_name=app_name)\nval_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)\noptimizer.set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n...\ntrainedModel = optimizer.optimize()  After you start to run your spark job, the train and validation summary will be saved to  mylogdir/myapp/train  and  mylogdir/myapp/validation  respectively (Note: you may want to use different  appName  for different job runs to avoid possible conflicts.)", 
            "title": "Generating summary info in BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/visualization/#retrieving-summary-info-as-readable-format", 
            "text": "You can use provided API  readScalar (Scala) and  read_scalar (Python) to retrieve the summaries into readable format, and export them to other tools for further analysis or visualization.  Example: Reading summary info in Scala  val trainLoss = trainSummary.readScalar( Loss )\nval validationLoss = validationSummary.readScalar( Loss )\n...  Example: Reading summary info in Python  loss = np.array(train_summary.read_scalar('Loss'))\nvalloss = np.array(val_summary.read_scalar('Loss'))\n...", 
            "title": "Retrieving summary info as readable format"
        }, 
        {
            "location": "/ProgrammingGuide/visualization/#visualizing-training-with-tensorboard", 
            "text": "With the summary info generated, we can then use  TensorBoard  to visualize the behaviors of the BigDL program.     Installing TensorBoard   Prerequisites:   Python verison: 2.7, 3.4, 3.5, or 3.6  Pip version  = 9.0.1   To install TensorBoard using Python 2, you may run the command:  pip install tensorboard==1.0.0a4  To install TensorBoard using Python 3, you may run the command:  pip3 install tensorboard==1.0.0a4  Please refer to  this page  for possible issues when installing TensorBoard.   Launching TensorBoard   You can launch TensorBoard using the command below:  tensorboard --logdir=/tmp/bigdl_summaries  After that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006   Visualizations in TensorBoard   Within the TensorBoard dashboard, you will be able to read the visualizations of each run, including the \u201cLoss\u201d and \u201cThroughput\u201d curves under the SCALARS tab (as illustrated below):  And \u201cweights\u201d, \u201cbias\u201d, \u201cgradientWeights\u201d and \u201cgradientBias\u201d under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):", 
            "title": "Visualizing training with TensorBoard"
        }, 
        {
            "location": "/ProgrammingGuide/visualization/#visualizing-training-with-jupyter-notebook", 
            "text": "If you're using Jupyter notebook, you can also draw the training curves using popular plotting tools (e.g. matplotlib) and show the plots inline.   First, retrieve the summaries as instructed in  Retrieve Summary . The retrived summary is a list of tuples. Each tuple is a recorded event in format (iteration count, recorded value, timestamp). You can convert it to numpy array or dataframe to plot it. See example below:    Example: Plot the train/validation loss in Jupyter  #retrieve train and validation summary object and read the loss data into ndarray's. \nloss = np.array(train_summary.read_scalar( Loss ))\nval_loss  = np.array(val_summary.read_scalar( Loss ))\n\n#plot the train and validation curves\n# each event data is a tuple in form of (iteration_count, value, timestamp)\nplt.plot(loss[:,0],loss[:,1],label='train loss')\nplt.plot(val_loss[:,0],val_loss[:,1],label='val loss',color='green')\nplt.scatter(val_loss[:,0],val_loss[:,1],color='green')\nplt.legend();", 
            "title": "Visualizing training with Jupyter notebook"
        }, 
        {
            "location": "/ProgrammingGuide/visualization/#logging", 
            "text": "BigDL also has a stright-forward logging output on the console along the    training, as shown below. You can see real-time epoch/iteration/loss/       throughput in the log.    2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/               5000][Iteration 1][Wall Clock XXX] Train 512 in   XXXseconds. Throughput    is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/             5000][Iteration 2][Wall Clock XXX] Train 512    in XXXseconds. Throughput   is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/            5000][Iteration 3][Wall Clock XXX] Train 512   in XXXseconds. Throughput    is XXX records/second. Loss is XXX.  The DistriOptimizer log level is INFO by default. We implement a method     named with  redirectSparkInfoLogs   in  spark/utils/LoggerFilter.scala .    You can import and redirect at first.    import com.intel.analytics.bigdl.utils.LoggerFilter\n  LoggerFilter.redirectSparkInfoLogs()  This method will redirect all logs of  org ,  akka ,  breeze  to  bigdl.    log  with  INFO  level, except  org.  apache.spark.SparkContext . And it    will output all  ERROR  message in console too.  You can disable the redirection with java property  -Dbigdl.utils.          LoggerFilter.disable=true . By default,   it will do redirect of all        examples and models in our code.  You can set where the  bigdl.log  will be generated with  -Dbigdl.utils.    LoggerFilter.logFile= path . By    default, it will be generated under     current workspace.", 
            "title": "Logging"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-ec2/", 
            "text": "The Public AMI\n\n\nTo make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.\n\n\n\n\n\n\n\n\nBigDL version\n\n\nAMI version\n\n\nDate\n\n\nAMI ID\n\n\nAMI Name\n\n\nRegion\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nmaster\n\n\n0.2S\n\n\nMar 13, 2017\n\n\nami-37b73957\n\n\nBigDL Client 0.2S\n\n\nUS West (Oregon)\n\n\nActive\n\n\n\n\n\n\nmaster\n\n\n0.2S\n\n\nApr 10, 2017\n\n\nami-8c87099a\n\n\nBigDL Client 0.2S\n\n\nUS East (N. Virginia)\n\n\nActive\n\n\n\n\n\n\n0.1.0\n\n\n0.1.0\n\n\nApr 10, 2017\n\n\nami-9a8818fa\n\n\nBigDL Client 0.1.0\n\n\nUS West (Oregon)\n\n\nActive\n\n\n\n\n\n\n0.1.0\n\n\n0.1.0\n\n\nApr 10, 2017\n\n\nami-6476f872\n\n\nBigDL Client 0.1.0\n\n\nUS East (N. Virginia)\n\n\nActive\n\n\n\n\n\n\n\n\nPlease note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.\n\n\nAfter launching the AMI on EC2, please log on to the instance and run a \"bootstrap.sh\" script to download example scripts.\n\n\n./bootstrap.sh\n\n\n\n\n\n\nBefore you start\n\n\nBefore running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to \nhttps://github.com/amplab/spark-ec2\n for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.\n\n\n\n\nRun BigDL Examples\n\n\nYou can run BigDL examples using the \nrun.example.sh\n script in home directory of your BigDL Client instance (e.g. \n/home/ubuntu/\n) with the following parameters:\n\n\n\n\n\n\nMandatory parameters:\n\n\n\n\n\n\n-m|--model\n which model to train, including\n\n\n\n\n\n\nlenet: train the \nLeNet\n example\n\n\n\n\n\n\nvgg: train the \nVGG\n example\n\n\n\n\n\n\ninception-v1: train the \nInception v1\n example\n\n\n\n\n\n\nperf: test the training speed using the \nInception v1\n model with dummy data\n\n\n\n\n\n\n\n\n\n\n-s|--spark-url\n the master URL for the Spark cluster\n\n\n\n\n\n\n-n|--nodes\n number of Spark slave nodes\n\n\n\n\n\n\n-o|--cores\n number of cores used on each node\n\n\n\n\n\n\n-r|--memory\n memory used on each node, e.g. 200g\n\n\n\n\n\n\n-b|--batch-size\n batch size when training the model; it is expected to be a multiple of \"nodes * cores\"\n\n\n\n\n\n\n-f|--hdfs-data-dir\n HDFS directory for the input images (for the \"inception-v1\" model training only)\n\n\n\n\n\n\n\n\n\n\nOptional parameters:\n\n\n\n\n\n\n-e|--max-epoch\n the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified\n\n\n\n\n\n\n-p|--spark\n by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify \"spark_2.0\" here (it is highly recommended to use \nJava 8\n when running BigDL for Spark 2.0, otherwise you may observe very poor performance)\n\n\n\n\n\n\n-l|--learning-rate\n by default the the example will use an initial learning rate of \"0.01\"; you can specify a different value here\n\n\n\n\n\n\n\n\n\n\nAfter the training, you can check the log files and generated models in the home directory (e.g., \n/home/ubuntu/\n).  \n\n\n\n\nRun the \"inception-v1\" example\n\n\nYou can refer to the \nInception v1\n example to prepare the input \nImageNet\n data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.\n\n\n\n\nDownload and prepare the input image data (a subset of the \nFlickr Style\n data)\n\n\n\n\n  ./download.sh $HDFS-NAMENODE\n\n\n\n\nAfter the download completes, the downloaded images are stored in \nhdfs://HDFS-NAMENODE:9000/seq\n. (If the download fails with error \"Unable to establish SSL connection.\" please check your network connection and retry this later.)\n\n\n\n\nTo run the \"inception-v1\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), run the example command below: \n\n\n\n\n  nohup bash ./run.example.sh --model inception-v1  \\\n         --spark-url spark://SPARK-MASTER:7077    \\\n         --nodes 4 --cores 20 --memory 150g       \\\n         --batch-size 400 --learning-rate 0.0898  \\\n         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \\\n         --spark spark_2.0 --max-epoch 4 \\\n         \n incep.log 2\n1 \n     \n\n\n\n\n\n\nView output of the training in the log file generated by the previous step:\n\n\n\n\n  $ tail -f incep.log\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n\n\n\n\n\n\nRun the \"perf\" example\n\n\nTo run the \"perf\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), you may try the example command below: \n\n\n  nohup bash ./run.example.sh --model perf  \\\n       --spark-url spark://SPARK-MASTER:7077    \\\n       --nodes 4 --cores 20 --memory 150g       \\\n       --spark spark_2.0 --max-epoch 4 \\\n       \n perf.log 2\n1", 
            "title": "Run on Amazon EC2"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-ec2/#the-public-ami", 
            "text": "To make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.     BigDL version  AMI version  Date  AMI ID  AMI Name  Region  Status      master  0.2S  Mar 13, 2017  ami-37b73957  BigDL Client 0.2S  US West (Oregon)  Active    master  0.2S  Apr 10, 2017  ami-8c87099a  BigDL Client 0.2S  US East (N. Virginia)  Active    0.1.0  0.1.0  Apr 10, 2017  ami-9a8818fa  BigDL Client 0.1.0  US West (Oregon)  Active    0.1.0  0.1.0  Apr 10, 2017  ami-6476f872  BigDL Client 0.1.0  US East (N. Virginia)  Active     Please note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.  After launching the AMI on EC2, please log on to the instance and run a \"bootstrap.sh\" script to download example scripts.  ./bootstrap.sh", 
            "title": "The Public AMI"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-ec2/#before-you-start", 
            "text": "Before running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to  https://github.com/amplab/spark-ec2  for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.", 
            "title": "Before you start"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-ec2/#run-bigdl-examples", 
            "text": "You can run BigDL examples using the  run.example.sh  script in home directory of your BigDL Client instance (e.g.  /home/ubuntu/ ) with the following parameters:    Mandatory parameters:    -m|--model  which model to train, including    lenet: train the  LeNet  example    vgg: train the  VGG  example    inception-v1: train the  Inception v1  example    perf: test the training speed using the  Inception v1  model with dummy data      -s|--spark-url  the master URL for the Spark cluster    -n|--nodes  number of Spark slave nodes    -o|--cores  number of cores used on each node    -r|--memory  memory used on each node, e.g. 200g    -b|--batch-size  batch size when training the model; it is expected to be a multiple of \"nodes * cores\"    -f|--hdfs-data-dir  HDFS directory for the input images (for the \"inception-v1\" model training only)      Optional parameters:    -e|--max-epoch  the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified    -p|--spark  by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify \"spark_2.0\" here (it is highly recommended to use  Java 8  when running BigDL for Spark 2.0, otherwise you may observe very poor performance)    -l|--learning-rate  by default the the example will use an initial learning rate of \"0.01\"; you can specify a different value here      After the training, you can check the log files and generated models in the home directory (e.g.,  /home/ubuntu/ ).", 
            "title": "Run BigDL Examples"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-ec2/#run-the-inception-v1-example", 
            "text": "You can refer to the  Inception v1  example to prepare the input  ImageNet  data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.   Download and prepare the input image data (a subset of the  Flickr Style  data)     ./download.sh $HDFS-NAMENODE  After the download completes, the downloaded images are stored in  hdfs://HDFS-NAMENODE:9000/seq . (If the download fails with error \"Unable to establish SSL connection.\" please check your network connection and retry this later.)   To run the \"inception-v1\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), run the example command below:      nohup bash ./run.example.sh --model inception-v1  \\\n         --spark-url spark://SPARK-MASTER:7077    \\\n         --nodes 4 --cores 20 --memory 150g       \\\n         --batch-size 400 --learning-rate 0.0898  \\\n         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \\\n         --spark spark_2.0 --max-epoch 4 \\\n           incep.log 2 1          View output of the training in the log file generated by the previous step:     $ tail -f incep.log\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.", 
            "title": "Run the \"inception-v1\" example"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-ec2/#run-the-perf-example", 
            "text": "To run the \"perf\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), you may try the example command below:     nohup bash ./run.example.sh --model perf  \\\n       --spark-url spark://SPARK-MASTER:7077    \\\n       --nodes 4 --cores 20 --memory 150g       \\\n       --spark spark_2.0 --max-epoch 4 \\\n         perf.log 2 1", 
            "title": "Run the \"perf\" example"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-dataproc/", 
            "text": "The Google Cloud Dataproc Initialization Script\n\n\nTo make it easier to try out BigDL examples on Spark using Google Cloud Dataproc, a public initialization script is provided (the source script is also avaliable in this repo path \nscripts/launch-dataproc.sh\n). The script will automatically retrieve BigDL package (version 0.2.0), run it on Dataproc's Spark Yarn cluster, then configure and setup the Jupyter Notebook and Tensorboard for the interactive usage. Two examples, including LeNet and Text Classifier, will be provided in the Notebook.\n\n\n\n\nBefore You Start\n\n\nBefore using BigDL on Dataproc, you need a valid Google Cloud account and setup your Google Cloud SDK (you may refer to \nhttps://cloud.google.com/sdk/docs/how-to\n for more instructions).\n\n\n\n\nCreate Spark Cluster with BigDL\n\n\nRun the following command to create your cluster\n\n\ngcloud dataproc clusters create bigdl \\\n    --initialization-actions gs://dataproc-initial/bigdl.sh \\\n    --worker-machine-type n1-highmem-4 \\\n    --master-machine-type n1-highmem-2 \\\n    --num-workers 2 \\\n    --zone us-central1-b \\\n    --image-version 1.1\n\n\n\n\nYou can change \nbigdl\n into any other name as the cluster name, and you are also free to upload \nscripts/launch-dataproc.sh\n into your own Google Cloud Storage bucket and use it instead of \ngs://dataproc-initial/bigdl.sh\n in the initialization-actions field.\n\n\nWhen creating a larger cluster with more workers, it is suggested to pass the number of executor into the script via the metadata field as, \n\n\ngcloud dataproc clusters create bigdl \\\n    --initialization-actions gs://dataproc-initial/bigdl.sh \\\n    --metadata \nNUM_EXECUTORS=8\n \\\n    --worker-machine-type n1-highmem-4 \\\n    --master-machine-type n1-highmem-2 \\\n    --num-workers 4 \\\n    --num-preemptible-workers 4 \\\n    --zone us-central1-b \\\n    --image-version 1.1\n\n\n\n\nPlease note that it is highly recommended to run BigDL in the region where the compute instances come with Xeon E5 v3 or v4 processors (you may find the \nGoogle Cloud Regions and Zones\n for more details).\n\n\n\n\nPlay Around with BigDL\n\n\nOnce your dataproc cluster is ready, directly go to the following URL (change \nbigdl\n into your own cluster name if you are using a different one) to play around BigDL in Jupyter Notebook. Note that you need to \ncreate an SSH tunel and SOCKS proxy\n to visit them. \n\n\n\n\n\n\nJupyter Notebook: \nhttp://bigdl-m:8888/\n\n\n\n\n\n\nTensorboard: \nhttp://bigdl-m:6006/\n\n\n\n\n\n\nYARN ResourceManager: \nhttp://bigdl-m:8088/\n\n\n\n\n\n\nInside your Jupyter Notebook, you may find two examples are already there. Start your BigDL journey with them.", 
            "title": "Run on Google Cloud Dataproc"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-dataproc/#the-google-cloud-dataproc-initialization-script", 
            "text": "To make it easier to try out BigDL examples on Spark using Google Cloud Dataproc, a public initialization script is provided (the source script is also avaliable in this repo path  scripts/launch-dataproc.sh ). The script will automatically retrieve BigDL package (version 0.2.0), run it on Dataproc's Spark Yarn cluster, then configure and setup the Jupyter Notebook and Tensorboard for the interactive usage. Two examples, including LeNet and Text Classifier, will be provided in the Notebook.", 
            "title": "The Google Cloud Dataproc Initialization Script"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-dataproc/#before-you-start", 
            "text": "Before using BigDL on Dataproc, you need a valid Google Cloud account and setup your Google Cloud SDK (you may refer to  https://cloud.google.com/sdk/docs/how-to  for more instructions).", 
            "title": "Before You Start"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-dataproc/#create-spark-cluster-with-bigdl", 
            "text": "Run the following command to create your cluster  gcloud dataproc clusters create bigdl \\\n    --initialization-actions gs://dataproc-initial/bigdl.sh \\\n    --worker-machine-type n1-highmem-4 \\\n    --master-machine-type n1-highmem-2 \\\n    --num-workers 2 \\\n    --zone us-central1-b \\\n    --image-version 1.1  You can change  bigdl  into any other name as the cluster name, and you are also free to upload  scripts/launch-dataproc.sh  into your own Google Cloud Storage bucket and use it instead of  gs://dataproc-initial/bigdl.sh  in the initialization-actions field.  When creating a larger cluster with more workers, it is suggested to pass the number of executor into the script via the metadata field as,   gcloud dataproc clusters create bigdl \\\n    --initialization-actions gs://dataproc-initial/bigdl.sh \\\n    --metadata  NUM_EXECUTORS=8  \\\n    --worker-machine-type n1-highmem-4 \\\n    --master-machine-type n1-highmem-2 \\\n    --num-workers 4 \\\n    --num-preemptible-workers 4 \\\n    --zone us-central1-b \\\n    --image-version 1.1  Please note that it is highly recommended to run BigDL in the region where the compute instances come with Xeon E5 v3 or v4 processors (you may find the  Google Cloud Regions and Zones  for more details).", 
            "title": "Create Spark Cluster with BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/run-on-dataproc/#play-around-with-bigdl", 
            "text": "Once your dataproc cluster is ready, directly go to the following URL (change  bigdl  into your own cluster name if you are using a different one) to play around BigDL in Jupyter Notebook. Note that you need to  create an SSH tunel and SOCKS proxy  to visit them.     Jupyter Notebook:  http://bigdl-m:8888/    Tensorboard:  http://bigdl-m:6006/    YARN ResourceManager:  http://bigdl-m:8088/    Inside your Jupyter Notebook, you may find two examples are already there. Start your BigDL journey with them.", 
            "title": "Play Around with BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/", 
            "text": "Loading a Tensorflow model into BigDL\n\n\nIf you have a pre-trained Tensorflow model saved in a \".pb\" file, you can load it\ninto BigDL.\n\n\nFor more information on how to generate\nthe \".pb\" file, you can refer to \nA Tool Developer's Guide to TensorFlow Model Files\n.\nSpecifically, you should generate a model definition file and a set of checkpoints, then use the \nfreeze_graph\n\nscript to freeze the graph definition and weights in checkpoints into a single file.\n\n\nGenerate model definition file and checkpoints in Tensorflow\n\n\nPython\n\n\nimport tensorflow as tf\nxs = tf.placeholder(tf.float32, [None, 1])\nW1 = tf.Variable(tf.zeros([1,10])+0.2)\nb1 = tf.Variable(tf.zeros([10])+0.1)\nWx_plus_b1 = tf.nn.bias_add(tf.matmul(xs,W1), b1)\noutput = tf.nn.tanh(Wx_plus_b1, name=\noutput\n)\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    checkpointpath = saver.save(sess, '/tmp/model/test.chkp')\n    tf.train.write_graph(sess.graph, '/tmp/model', 'test.pbtxt')\n\n\n\n\nFreeze graph definition and checkpoints into a single \".pb\" file\n\n\nShell\n\n\nwget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.0.0/tensorflow/python/tools/freeze_graph.py\npython freeze_graph.py --input_graph /tmp/model/test.pbtxt --input_checkpoint /tmp/model/test.chkp --output_node_names=output --output_graph \n/tmp/model/test.pb\n\n\n\n\n\nLoad Tensorflow model in BigDL\n\n\nScala\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.nn.Module\nimport java.nio.ByteOrder\n\nval path = \n/tmp/model/test.pb\n\nval inputs = Seq(\nPlaceholder\n)\nval outputs = Seq(\noutput\n)\nval model = Module.loadTF(path, Seq(\nPlaceholder\n), Seq(\noutput\n), ByteOrder.LITTLE_ENDIAN)\n\n\n\n\nPython\n\n\nfrom bigdl.nn.layer import *\npath = \n/tmp/model/test.pb\n\ninputs = [\nPlaceholder\n]\noutputs = [\noutput\n]\nmodel = Model.load_tensorflow(path, inputs, outputs, byte_order = \nlittle_endian\n, bigdl_type=\nfloat\n)\n\n\n\n\n\n\nSaving a BigDL functional model to Tensorflow model file\n\n\nYou can also save a \nfunctional model\n to protobuf files so that it can be used in Tensorflow inference.\n\n\nWhen saving the model, placeholders will be added to the tf model as input nodes. So\nyou need to pass in the names and shapes of the placeholders. BigDL model does not have\nsuch information. The order of the placeholder information should be same as the inputs\nof the graph model.\n\n\nScala\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.tf.TensorflowSaver\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n// create a graph model\nval linear = Linear(10, 2).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = SoftMax().inputs(sigmoid)\nval model = Graph(Array(linear), Array(softmax))\n\n// save it to Tensorflow model file\nmodel.saveTF(Seq((\ninput\n, Seq(4, 10))), \n/tmp/model.pb\n)\n\n\n\n\nPython\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\n# create a graph model\nlinear = Linear(10, 2)()\nsigmoid = Sigmoid()(linear)\nsoftmax = SoftMax()(sigmoid)\nmodel = Model([linear], [softmax])\n\n# save it to Tensorflow model file\nmodel.save_tensorflow([(\ninput\n, [4, 10])], \n/tmp/model.pb\n)\n\n\n\n\n\n\nBuild Tensorflow model and run on BigDL\n\n\nYou can construct your BigDL model directly from the input and output nodes of\nTensorflow model. That is to say, you can use Tensorflow to define\na model and use BigDL to run it.\n\n\nPython:\n\n\nimport tensorflow as tf\nimport numpy as np\nfrom bigdl.nn.layer import *\n\ntf.set_random_seed(1234)\ninput = tf.placeholder(tf.float32, [None, 5])\nweight = tf.Variable(tf.random_uniform([5, 10]))\nbias = tf.Variable(tf.random_uniform([10]))\nmiddle = tf.nn.bias_add(tf.matmul(input, weight), bias)\noutput = tf.nn.tanh(middle)\n\n# construct BigDL model and get the result form \nbigdl_model = Model(input, output, model_type=\ntensorflow\n)", 
            "title": "Tensorflow Support"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/#loading-a-tensorflow-model-into-bigdl", 
            "text": "If you have a pre-trained Tensorflow model saved in a \".pb\" file, you can load it\ninto BigDL.  For more information on how to generate\nthe \".pb\" file, you can refer to  A Tool Developer's Guide to TensorFlow Model Files .\nSpecifically, you should generate a model definition file and a set of checkpoints, then use the  freeze_graph \nscript to freeze the graph definition and weights in checkpoints into a single file.", 
            "title": "Loading a Tensorflow model into BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/#generate-model-definition-file-and-checkpoints-in-tensorflow", 
            "text": "Python  import tensorflow as tf\nxs = tf.placeholder(tf.float32, [None, 1])\nW1 = tf.Variable(tf.zeros([1,10])+0.2)\nb1 = tf.Variable(tf.zeros([10])+0.1)\nWx_plus_b1 = tf.nn.bias_add(tf.matmul(xs,W1), b1)\noutput = tf.nn.tanh(Wx_plus_b1, name= output )\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    checkpointpath = saver.save(sess, '/tmp/model/test.chkp')\n    tf.train.write_graph(sess.graph, '/tmp/model', 'test.pbtxt')", 
            "title": "Generate model definition file and checkpoints in Tensorflow"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/#freeze-graph-definition-and-checkpoints-into-a-single-pb-file", 
            "text": "Shell  wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.0.0/tensorflow/python/tools/freeze_graph.py\npython freeze_graph.py --input_graph /tmp/model/test.pbtxt --input_checkpoint /tmp/model/test.chkp --output_node_names=output --output_graph  /tmp/model/test.pb", 
            "title": "Freeze graph definition and checkpoints into a single \".pb\" file"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/#load-tensorflow-model-in-bigdl", 
            "text": "Scala  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.nn.Module\nimport java.nio.ByteOrder\n\nval path =  /tmp/model/test.pb \nval inputs = Seq( Placeholder )\nval outputs = Seq( output )\nval model = Module.loadTF(path, Seq( Placeholder ), Seq( output ), ByteOrder.LITTLE_ENDIAN)  Python  from bigdl.nn.layer import *\npath =  /tmp/model/test.pb \ninputs = [ Placeholder ]\noutputs = [ output ]\nmodel = Model.load_tensorflow(path, inputs, outputs, byte_order =  little_endian , bigdl_type= float )", 
            "title": "Load Tensorflow model in BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/#saving-a-bigdl-functional-model-to-tensorflow-model-file", 
            "text": "You can also save a  functional model  to protobuf files so that it can be used in Tensorflow inference.  When saving the model, placeholders will be added to the tf model as input nodes. So\nyou need to pass in the names and shapes of the placeholders. BigDL model does not have\nsuch information. The order of the placeholder information should be same as the inputs\nof the graph model.  Scala  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.tf.TensorflowSaver\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n// create a graph model\nval linear = Linear(10, 2).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = SoftMax().inputs(sigmoid)\nval model = Graph(Array(linear), Array(softmax))\n\n// save it to Tensorflow model file\nmodel.saveTF(Seq(( input , Seq(4, 10))),  /tmp/model.pb )  Python  from bigdl.nn.layer import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\n# create a graph model\nlinear = Linear(10, 2)()\nsigmoid = Sigmoid()(linear)\nsoftmax = SoftMax()(sigmoid)\nmodel = Model([linear], [softmax])\n\n# save it to Tensorflow model file\nmodel.save_tensorflow([( input , [4, 10])],  /tmp/model.pb )", 
            "title": "Saving a BigDL functional model to Tensorflow model file"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow-support/#build-tensorflow-model-and-run-on-bigdl", 
            "text": "You can construct your BigDL model directly from the input and output nodes of\nTensorflow model. That is to say, you can use Tensorflow to define\na model and use BigDL to run it.  Python:  import tensorflow as tf\nimport numpy as np\nfrom bigdl.nn.layer import *\n\ntf.set_random_seed(1234)\ninput = tf.placeholder(tf.float32, [None, 5])\nweight = tf.Variable(tf.random_uniform([5, 10]))\nbias = tf.Variable(tf.random_uniform([10]))\nmiddle = tf.nn.bias_add(tf.matmul(input, weight), bias)\noutput = tf.nn.tanh(middle)\n\n# construct BigDL model and get the result form \nbigdl_model = Model(input, output, model_type= tensorflow )", 
            "title": "Build Tensorflow model and run on BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/caffe-support/", 
            "text": "If you have a pretrained caffe model(model definition prototxt and model binary file), you can load it as BigDL model.\nYou can also convert a BigDL model to caffe model.\n\n\nLoad Caffe Model\n\n\nAssume you have a \ncaffe.prototxt\n and \ncaffe.model\n,\nyou can load it into BigDL by calling \nModule.loadCaffeModel\n (scala) or \nModel.load_caffe_model\n (python).\n\n\n\n\nScala Example\n\n\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nval model = Module.loadCaffeModel(caffe.prototxt, caffe.model)\n\n\n\n\n\n\nPython Example\n\n\n\n\nmodel = Model.load_caffe_model(caffe.prototxt, caffe.model)\n\n\n\n\nLoad Caffe Model Weights to Predefined BigDL Model\n\n\nIf you have a predefined BigDL model, and want to load caffe model weights into BigDl model\n\n\n\n\nScala Example\n\n\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nval model = Module.loadCaffe(bigdlModel, caffe.prototxt, caffe.model, matchAll = true)\n\n\n\n\n\n\nPython Example\n\n\n\n\nmodel = Model.load_caffe_model(bigdlModel, caffe.prototxt, caffe.model, match_all=True)\n\n\n\n\nNote that if \nmatchAll/match_all = false\n, then only layers with same name will be loaded, the rest will use initialized parameters.\n\n\nSave BigDL Model to Caffe Model\n\n\n\n\nScala Example\n\n\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nbigdlModel.saveCaffe(prototxtPath, modelPath, useV2 = true, overwrite = false)\n\n\n\n\n\n\nPython Example\n\n\n\n\nbigdl_model.save_caffe(prototxt_path, model_path, use_v2 = True, overwrite = False)\n\n\n\n\nIn the above examples, if \nuseV2/use_v2 = true\n, it will convert to caffe V2 layer,\n otherwise, it will convert to caffe V1 layer.\nIf \noverwrite = true\n, it will overwrite the existing files.\n\n\nNote: only graph model can be saved to caffe model.\n\n\nLimitation\n\n\nThis functionality has been tested with some common models like AlexNet, Inception, Resnet which were created with standard Caffe layers, for those models with customized layers such as SSD, it is going to be supported in future work, but you can define your customized conversion method for your own layers.", 
            "title": "Caffe Support"
        }, 
        {
            "location": "/ProgrammingGuide/caffe-support/#load-caffe-model", 
            "text": "Assume you have a  caffe.prototxt  and  caffe.model ,\nyou can load it into BigDL by calling  Module.loadCaffeModel  (scala) or  Model.load_caffe_model  (python).   Scala Example   import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nval model = Module.loadCaffeModel(caffe.prototxt, caffe.model)   Python Example   model = Model.load_caffe_model(caffe.prototxt, caffe.model)", 
            "title": "Load Caffe Model"
        }, 
        {
            "location": "/ProgrammingGuide/caffe-support/#load-caffe-model-weights-to-predefined-bigdl-model", 
            "text": "If you have a predefined BigDL model, and want to load caffe model weights into BigDl model   Scala Example   import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nval model = Module.loadCaffe(bigdlModel, caffe.prototxt, caffe.model, matchAll = true)   Python Example   model = Model.load_caffe_model(bigdlModel, caffe.prototxt, caffe.model, match_all=True)  Note that if  matchAll/match_all = false , then only layers with same name will be loaded, the rest will use initialized parameters.", 
            "title": "Load Caffe Model Weights to Predefined BigDL Model"
        }, 
        {
            "location": "/ProgrammingGuide/caffe-support/#save-bigdl-model-to-caffe-model", 
            "text": "Scala Example   import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nbigdlModel.saveCaffe(prototxtPath, modelPath, useV2 = true, overwrite = false)   Python Example   bigdl_model.save_caffe(prototxt_path, model_path, use_v2 = True, overwrite = False)  In the above examples, if  useV2/use_v2 = true , it will convert to caffe V2 layer,\n otherwise, it will convert to caffe V1 layer.\nIf  overwrite = true , it will overwrite the existing files.  Note: only graph model can be saved to caffe model.", 
            "title": "Save BigDL Model to Caffe Model"
        }, 
        {
            "location": "/ProgrammingGuide/caffe-support/#limitation", 
            "text": "This functionality has been tested with some common models like AlexNet, Inception, Resnet which were created with standard Caffe layers, for those models with customized layers such as SSD, it is going to be supported in future work, but you can define your customized conversion method for your own layers.", 
            "title": "Limitation"
        }, 
        {
            "location": "/APIGuide/Data/", 
            "text": "Tensor\n\n\nModeled after the \nTensor\n class in \nTorch\n, the \nTensor\n \npackage\n (written in Scala and leveraging \nIntel MKL\n) in BigDL provides numeric computing support for the deep learning applications (e.g., the input, output, weight, bias and   gradient of the neural networks).\n\n\nA \nTensor\n is essentially a multi-dimensional array of numeric types (\nFloat\n or \nDouble\n), you can import the numeric implicit objects(\ncom.intel.analytics.bigdl.numeric.NumericFloat\n or \ncom.intel.analytics.bigdl.numeric.NumericDouble\n), to specify the numeric type you want.\n\n\nScala example:\n\n\nYou may check it out in the interactive Scala shell (by typing \nscala -cp bigdl_SPARKVERSION-BIGDLVERSION-SNAPSHOT-jar-with-dependencies.jar\n), for instance:\n\n\n scala\n import com.intel.analytics.bigdl.tensor.Tensor\n import com.intel.analytics.bigdl.tensor.Tensor\n\n scala\n import com.intel.analytics.bigdl.numeric.NumericFloat\n import com.intel.analytics.bigdl.numeric.NumericFloat\n\n scala\n import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.utils.T\n\n scala\n val tensor = Tensor(2, 3)\n tensor: com.intel.analytics.bigdl.tensor.Tensor =\n 0.0     0.0     0.0\n 0.0     0.0     0.0\n [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nTensor can be created with existing data.\n\n\nscala\n val a = Tensor(T(\n     | T(1f, 2f, 3f),\n     | T(4f, 5f, 6f)))\na: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 2.0 3.0\n4.0 5.0 6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nscala\n val b = Tensor(T(\n     | T(6f, 5f, 4f),\n     | T(3f, 2f, 1f)))\nb: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n6.0 5.0 4.0\n3.0 2.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\n\n\n\n+\n \n-\n \n*\n \n/\n can be applied to tensor. When the second parameter is a constant value, \n+\n \n-\n \n*\n \n*\n is element-wise operation. But when the second parameter is a tensor, \n+\n \n-\n \n/\n is element-wise operation to the tensor too, but \n*\n is a matrix multipy on two 2D tensors. \n\n\nscala\n a + 1\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.0 3.0 4.0\n5.0 6.0 7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nscala\n a + b\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n7.0 7.0 7.0\n7.0 7.0 7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\nscala\n a - b\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-5.0    -3.0    -1.0\n1.0 3.0 5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\nscala\n a * b.t\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n28.0    10.0\n73.0    28.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala\n a / b\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.16666667  0.4 0.75\n1.3333334   2.5 6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nFor more API, navigate to \nAPI Guide/Full API docs\n on side bar.\n\n\n\n\nTable\n\n\nModeled after the \nTable\n class in \nTorch\n, the \nTable\n class (defined in package \ncom.intel.analytics.bigdl.utils\n) is widely used in BigDL (e.g., a \nTable\n of \nTensor\n can be used as the input or output of neural networks). In essence, a \nTable\n can be considered as a key-value map, and there is also a syntax sugar to create a \nTable\n using \nT()\n in BigDL.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nprintln(T(Tensor(2,2).fill(1), Tensor(2,2).fill(2)))\n\n\n\n\nOutput is\n\n\n {\n    2: 2.0  2.0 \n       2.0  2.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: 1.0  1.0 \n       1.0  1.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n\n\n\n\n\nSample\n\n\nA \nSample\n represent one record of your data set. One record contains feature and label, feature is one tensor or a few tensors; while label is one tensor or a few tensors, and it may be empty in testing or unsupervised learning. For example, one image and its category in image classification, one word in word2vec and one sentence and its label in RNN language model are all \nSample\n.\n\n\nEvery Sample is actually a set of tensors, and them will be transformed to the input/output of the model. For example, in the case of image classification, a Sample have two tensors. One is 3D tensor representing a image, another is a 1-element tensor representing its category. For the 1-element label, you also can use a \nT\n instead of tensor.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval image = Tensor(3, 32, 32).rand\nval label = 1f\nval sample = Sample(image, label)\n\n\n\n\nPython example:\n\n\nfrom bigdl.util.common import Sample\nimport numpy as np\n\nimage = np.random.rand(3, 32, 32)\nlabel = np.array(1)\nSample.from_ndarray(image, label)\n\n\n\n\n\n\nMiniBatch\n\n\nMiniBatch\n is a data structure to feed input/target to model in \nOptimizer\n. It provide \ngetInput()\n and \ngetTarget()\n function to get the input and target in this \nMiniBatch\n.\n\n\nIn almost all the cases, BigDL's default \nMiniBatch\n class can fit user's requirement. Just create your \nRDD[Sample]\n and pass it to \nOptimizer\n. If \nMiniBatch\n can't meet your requirement, you can implement your own \nMiniBatch\n class by extends \nMiniBatch\n.\n\n\nMiniBatch\n can be created by \nMiniBatch(nInputs: Int, nOutputs: Int)\n, \nnInputs\n means number of inputs, \nnOutputs\n means number of outputs. And you can use \nset(samples: Seq[Sample[T])\n to fill the content in this MiniBatch. If you \nSample\ns are not the same size, you can use \nPaddingParam\n to pad the \nSample\ns to the same size.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.dataset.MiniBatch\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval samples  = Array.tabulate(5)(i =\n Sample(Tensor(1, 3, 3).fill(i), i + 1f))\nval miniBatch = MiniBatch(1, 1).set(samples)\nprintln(miniBatch.getInput())\nprintln(miniBatch.getTarget())\n\n\n\n\nOutput is\n\n\n(1,1,.,.) =\n0.0 0.0 0.0 \n0.0 0.0 0.0 \n0.0 0.0 0.0 \n\n(2,1,.,.) =\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n1.0 1.0 1.0 \n\n(3,1,.,.) =\n2.0 2.0 2.0 \n2.0 2.0 2.0 \n2.0 2.0 2.0 \n\n(4,1,.,.) =\n3.0 3.0 3.0 \n3.0 3.0 3.0 \n3.0 3.0 3.0 \n\n(5,1,.,.) =\n4.0 4.0 4.0 \n4.0 4.0 4.0 \n4.0 4.0 4.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x1x3x3]\n1.0 \n2.0 \n3.0 \n4.0 \n5.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x1]\n\n\n\n\nIf you \nSample\ns are not the same size, you can use \nPaddingParam\n to pad the \nSample\ns to the same size.\n\n\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval sample1 = Sample(Tensor.range(1, 6, 1).resize(2, 3), 1f)\nval sample2 = Sample(Tensor.range(7, 9, 1).resize(1, 3), 2f)\nval sample3 = Sample(Tensor.range(10, 18, 1).resize(3, 3), 3f)\nval samples = Array(sample1, sample2, sample3)\nval featurePadding = PaddingParam(Some(Array(Tensor(T(-1f, -2f, -3f)))), FixedLength(Array(4)))\nval labelPadding = PaddingParam[Float](None, FixedLength(Array(4)))\n\nval miniBatch = MiniBatch(1, 1, Some(featurePadding), Some(labelPadding)).set(samples)\nprintln(miniBatch.getInput())\nprintln(miniBatch.getTarget())\n\n\n\n\nOutput is \n\n\n(1,.,.) =\n1.0 2.0 3.0 \n4.0 5.0 6.0 \n-1.0    -2.0    -3.0    \n-1.0    -2.0    -3.0    \n\n(2,.,.) =\n7.0 8.0 9.0 \n-1.0    -2.0    -3.0    \n-1.0    -2.0    -3.0    \n-1.0    -2.0    -3.0    \n\n(3,.,.) =\n10.0    11.0    12.0    \n13.0    14.0    15.0    \n16.0    17.0    18.0    \n-1.0    -2.0    -3.0    \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4x3]\n\n\n1.0 0.0 0.0 0.0 \n2.0 0.0 0.0 0.0 \n3.0 0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\n\n\n\n\nDataSet\n\n\nDataSet\n is a set of data which is used in the model optimization process. You can use \nDataSet.array()\n and \nDataSet.rdd()\n function to create a \nDataset\n. The \nDataSet\n can be accessed in a random data sample sequence. In the training process, the data sequence is a looped endless sequence. While in the validation process, the data sequence is a limited length sequence. User can use the \ndata()\n method to get the data sequence. \n\n\nNotice: In most case, we recommand using a RDD[Sample] for \nOptimizer\n. Only when you want to write an application with some advanced optimization, using \nDataSet\n directly is recommanded.  \n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.dataset.DataSet\n\nval tensors  = Array.tabulate(5)(i =\n Tensor(1, 3, 3).fill(i))\nval dataset = DataSet.array(tensors) // Local model, just for testing and example.\ndataset.shuffle()\nval iter = dataset.data(false)\nwhile (iter.hasNext) {\n  val d = iter.next()\n  println(d)\n}\n\n\n\n\nOutput may be\n\n\n(1,.,.) =\n4.0 4.0 4.0 \n4.0 4.0 4.0 \n4.0 4.0 4.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n0.0 0.0 0.0 \n0.0 0.0 0.0 \n0.0 0.0 0.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n2.0 2.0 2.0 \n2.0 2.0 2.0 \n2.0 2.0 2.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n1.0 1.0 1.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n3.0 3.0 3.0 \n3.0 3.0 3.0 \n3.0 3.0 3.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]", 
            "title": "Data"
        }, 
        {
            "location": "/APIGuide/Data/#tensor", 
            "text": "Modeled after the  Tensor  class in  Torch , the  Tensor   package  (written in Scala and leveraging  Intel MKL ) in BigDL provides numeric computing support for the deep learning applications (e.g., the input, output, weight, bias and   gradient of the neural networks).  A  Tensor  is essentially a multi-dimensional array of numeric types ( Float  or  Double ), you can import the numeric implicit objects( com.intel.analytics.bigdl.numeric.NumericFloat  or  com.intel.analytics.bigdl.numeric.NumericDouble ), to specify the numeric type you want.  Scala example:  You may check it out in the interactive Scala shell (by typing  scala -cp bigdl_SPARKVERSION-BIGDLVERSION-SNAPSHOT-jar-with-dependencies.jar ), for instance:   scala  import com.intel.analytics.bigdl.tensor.Tensor\n import com.intel.analytics.bigdl.tensor.Tensor\n\n scala  import com.intel.analytics.bigdl.numeric.NumericFloat\n import com.intel.analytics.bigdl.numeric.NumericFloat\n\n scala  import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.utils.T\n\n scala  val tensor = Tensor(2, 3)\n tensor: com.intel.analytics.bigdl.tensor.Tensor =\n 0.0     0.0     0.0\n 0.0     0.0     0.0\n [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Tensor can be created with existing data.  scala  val a = Tensor(T(\n     | T(1f, 2f, 3f),\n     | T(4f, 5f, 6f)))\na: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 2.0 3.0\n4.0 5.0 6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nscala  val b = Tensor(T(\n     | T(6f, 5f, 4f),\n     | T(3f, 2f, 1f)))\nb: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n6.0 5.0 4.0\n3.0 2.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]  +   -   *   /  can be applied to tensor. When the second parameter is a constant value,  +   -   *   *  is element-wise operation. But when the second parameter is a tensor,  +   -   /  is element-wise operation to the tensor too, but  *  is a matrix multipy on two 2D tensors.   scala  a + 1\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.0 3.0 4.0\n5.0 6.0 7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nscala  a + b\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n7.0 7.0 7.0\n7.0 7.0 7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\nscala  a - b\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-5.0    -3.0    -1.0\n1.0 3.0 5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\nscala  a * b.t\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n28.0    10.0\n73.0    28.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala  a / b\nres: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.16666667  0.4 0.75\n1.3333334   2.5 6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  For more API, navigate to  API Guide/Full API docs  on side bar.", 
            "title": "Tensor"
        }, 
        {
            "location": "/APIGuide/Data/#table", 
            "text": "Modeled after the  Table  class in  Torch , the  Table  class (defined in package  com.intel.analytics.bigdl.utils ) is widely used in BigDL (e.g., a  Table  of  Tensor  can be used as the input or output of neural networks). In essence, a  Table  can be considered as a key-value map, and there is also a syntax sugar to create a  Table  using  T()  in BigDL.  Scala example:  import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nprintln(T(Tensor(2,2).fill(1), Tensor(2,2).fill(2)))  Output is   {\n    2: 2.0  2.0 \n       2.0  2.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: 1.0  1.0 \n       1.0  1.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }", 
            "title": "Table"
        }, 
        {
            "location": "/APIGuide/Data/#sample", 
            "text": "A  Sample  represent one record of your data set. One record contains feature and label, feature is one tensor or a few tensors; while label is one tensor or a few tensors, and it may be empty in testing or unsupervised learning. For example, one image and its category in image classification, one word in word2vec and one sentence and its label in RNN language model are all  Sample .  Every Sample is actually a set of tensors, and them will be transformed to the input/output of the model. For example, in the case of image classification, a Sample have two tensors. One is 3D tensor representing a image, another is a 1-element tensor representing its category. For the 1-element label, you also can use a  T  instead of tensor.  Scala example:  import com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval image = Tensor(3, 32, 32).rand\nval label = 1f\nval sample = Sample(image, label)  Python example:  from bigdl.util.common import Sample\nimport numpy as np\n\nimage = np.random.rand(3, 32, 32)\nlabel = np.array(1)\nSample.from_ndarray(image, label)", 
            "title": "Sample"
        }, 
        {
            "location": "/APIGuide/Data/#minibatch", 
            "text": "MiniBatch  is a data structure to feed input/target to model in  Optimizer . It provide  getInput()  and  getTarget()  function to get the input and target in this  MiniBatch .  In almost all the cases, BigDL's default  MiniBatch  class can fit user's requirement. Just create your  RDD[Sample]  and pass it to  Optimizer . If  MiniBatch  can't meet your requirement, you can implement your own  MiniBatch  class by extends  MiniBatch .  MiniBatch  can be created by  MiniBatch(nInputs: Int, nOutputs: Int) ,  nInputs  means number of inputs,  nOutputs  means number of outputs. And you can use  set(samples: Seq[Sample[T])  to fill the content in this MiniBatch. If you  Sample s are not the same size, you can use  PaddingParam  to pad the  Sample s to the same size.  Scala example:  import com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.dataset.MiniBatch\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval samples  = Array.tabulate(5)(i =  Sample(Tensor(1, 3, 3).fill(i), i + 1f))\nval miniBatch = MiniBatch(1, 1).set(samples)\nprintln(miniBatch.getInput())\nprintln(miniBatch.getTarget())  Output is  (1,1,.,.) =\n0.0 0.0 0.0 \n0.0 0.0 0.0 \n0.0 0.0 0.0 \n\n(2,1,.,.) =\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n1.0 1.0 1.0 \n\n(3,1,.,.) =\n2.0 2.0 2.0 \n2.0 2.0 2.0 \n2.0 2.0 2.0 \n\n(4,1,.,.) =\n3.0 3.0 3.0 \n3.0 3.0 3.0 \n3.0 3.0 3.0 \n\n(5,1,.,.) =\n4.0 4.0 4.0 \n4.0 4.0 4.0 \n4.0 4.0 4.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x1x3x3]\n1.0 \n2.0 \n3.0 \n4.0 \n5.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x1]  If you  Sample s are not the same size, you can use  PaddingParam  to pad the  Sample s to the same size.  import com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval sample1 = Sample(Tensor.range(1, 6, 1).resize(2, 3), 1f)\nval sample2 = Sample(Tensor.range(7, 9, 1).resize(1, 3), 2f)\nval sample3 = Sample(Tensor.range(10, 18, 1).resize(3, 3), 3f)\nval samples = Array(sample1, sample2, sample3)\nval featurePadding = PaddingParam(Some(Array(Tensor(T(-1f, -2f, -3f)))), FixedLength(Array(4)))\nval labelPadding = PaddingParam[Float](None, FixedLength(Array(4)))\n\nval miniBatch = MiniBatch(1, 1, Some(featurePadding), Some(labelPadding)).set(samples)\nprintln(miniBatch.getInput())\nprintln(miniBatch.getTarget())  Output is   (1,.,.) =\n1.0 2.0 3.0 \n4.0 5.0 6.0 \n-1.0    -2.0    -3.0    \n-1.0    -2.0    -3.0    \n\n(2,.,.) =\n7.0 8.0 9.0 \n-1.0    -2.0    -3.0    \n-1.0    -2.0    -3.0    \n-1.0    -2.0    -3.0    \n\n(3,.,.) =\n10.0    11.0    12.0    \n13.0    14.0    15.0    \n16.0    17.0    18.0    \n-1.0    -2.0    -3.0    \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4x3]\n\n\n1.0 0.0 0.0 0.0 \n2.0 0.0 0.0 0.0 \n3.0 0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]", 
            "title": "MiniBatch"
        }, 
        {
            "location": "/APIGuide/Data/#dataset", 
            "text": "DataSet  is a set of data which is used in the model optimization process. You can use  DataSet.array()  and  DataSet.rdd()  function to create a  Dataset . The  DataSet  can be accessed in a random data sample sequence. In the training process, the data sequence is a looped endless sequence. While in the validation process, the data sequence is a limited length sequence. User can use the  data()  method to get the data sequence.   Notice: In most case, we recommand using a RDD[Sample] for  Optimizer . Only when you want to write an application with some advanced optimization, using  DataSet  directly is recommanded.    Scala example:  import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.dataset.DataSet\n\nval tensors  = Array.tabulate(5)(i =  Tensor(1, 3, 3).fill(i))\nval dataset = DataSet.array(tensors) // Local model, just for testing and example.\ndataset.shuffle()\nval iter = dataset.data(false)\nwhile (iter.hasNext) {\n  val d = iter.next()\n  println(d)\n}  Output may be  (1,.,.) =\n4.0 4.0 4.0 \n4.0 4.0 4.0 \n4.0 4.0 4.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n0.0 0.0 0.0 \n0.0 0.0 0.0 \n0.0 0.0 0.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n2.0 2.0 2.0 \n2.0 2.0 2.0 \n2.0 2.0 2.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n1.0 1.0 1.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n(1,.,.) =\n3.0 3.0 3.0 \n3.0 3.0 3.0 \n3.0 3.0 3.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]", 
            "title": "DataSet"
        }, 
        {
            "location": "/APIGuide/Module/", 
            "text": "Model Save\n\n\nBigDL supports saving models to local file system, HDFS and AWS S3. After a model is created, you can use \nsave\n on created model to save it. Below example shows how to save a model. \n\n\nScala example\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval model = Sequential().add(Linear(10, 5)).add(Sigmoid()).add(SoftMax())\n//...train\n\nmodel.save(\n/tmp/model.bigdl\n, true) //save to local fs\nmodel.save(\nhdfs://...\n) //save to hdfs\nmodel.save(\ns3://...\n) //save to s3\n\n\n\n\n\nPython example\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.util.common import *\nfrom bigdl.optim.optimizer import *\n\nmodel = Sequential().add(Linear(10, 5)).add(Sigmoid()).add(SoftMax())\n//...train\nmodel.save(\n/tmp/model.bigdl\n, True) //save to local fs\nmodel.save(\nhdfs://...\n) //save to hdfs\nmodel.save(\ns3://...\n) //save to s3\n\n\n\n\nIn \nmodel.save\n, the first parameter is the path where we want to save our model, the second paramter is to specify if we need to overwrite the file if it already exists, it's set to false by default\n\n\nModel Load\n\n\nUse \nModule.load\n(in Scala) or \nModel.load\n (in Python) to load an existing model.  \nModule\n (Scala) or \nModel\n(Python) is a utilily class provided in BigDL. We just need to specify the model path where we previously saved the model to load it to memory for resume training or prediction purpose.\n\n\nScala example\n\n\nval model = Module.load(\n/tmp/model.bigdl\n) //load from local fs\nval model = Module.load(\nhdfs://...\n) //load from hdfs\nval model = Module.load(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nmodel = Model.load(\n/tmp/model.bigdl\n) //load from local fs\nmodel = Model.load(\nhdfs://...\n) //load from hdfs\nmodel = Model.load(\ns3://...\n) //load from s3\n\n\n\n\nModel Evaluation\n\n\nScala\n\n\nmodel.evaluate(dataset,vMethods,batchSize = None)\n\n\n\n\nPython\n\n\nmodel.test(val_rdd, batch_size, val_methods)\n\n\n\n\nUse \nevaluate\n on the model for evaluation. The parameter \ndataset\n (Scala) or \nval_rdd\n (Python) in is the validation dataset, and \nvMethods\n (Scala) or \nval_methods\n(Python) is an array of ValidationMethods. Refer to \nMetrics\n for the list of defined ValidationMethods. \n\n\nScala example\n\n\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.optim.Top1Accuracy\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n//create some dummy dataset for evaluation\nval feature = Tensor(10).rand()\nval label = Tensor(1).randn()\n\nval testSample = Sample(feature, label)\n//sc is is the SparkContxt instance\nval testSet = sc.parallelize(Seq(testSample))\n\n//train a new model or load an existing model\n//val model=...\nval evaluateResult = model.evaluate(testSet, Array(new Top1Accuracy))\n\n\n\n\nPython example\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.util.common import *\nfrom bigdl.optim.optimizer import *\nimport numpy as np\n\nsamples=[Sample.from_ndarray(np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]), np.array([2.0]))]\ntestSet = sc.parallelize(samples)\n\n//train a model or load an existing model...\n//model = ...\nevaluateResult = model.test(testSet, 1, [Top1Accuracy])\n\n\n\n\nModel Prediction\n\n\nScala\n\n\nmodel.predict(dataset)\nmodel.predictClass(dataset)\n\n\n\n\nPython\n\n\nmodel.predict(data_rdd)\nmodel.predict_class(data_rdd)\n\n\n\n\nUse \npredict\n or \npredictClass\n or \npredict_class\n on model for Prediction. \npredict\n returns return the probability distribution of each class, and \npredictClass\n/\npredict_class\n returns the predict label. They both accepts the test dataset as parameter. \n\n\nScala example\n\n\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.optim.Top1Accuracy\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n//create some dummy dataset for prediction as example\nval feature = Tensor(10).rand()\nval label = Tensor(1).randn()\nval predictSample = Sample(feature, label)\nval predictSet = sc.parallelize(Seq(predictSample))\n\n//train a new model or load an existing model\n//val model=... \nval preductResult = model.predict(predictSet)\n\n\n\n\nPython example\n\n\n from bigdl.nn.layer import *\n from bigdl.util.common import *\n from bigdl.optim.optimizer import *\n import numpy as np\n\n samples=[Sample.from_ndarray(np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]), np.  array([2.0]))]\n\n predictSet = sc.parallelize(samples)\n\n //train a model or load an existing model...\n //model = ...\n preductResult = model.predict(predictSet)\n\n\n\n\nModule Freeze\n\n\nTo \"freeze\" a module means to exclude some layers of model from training.\n\n\nlayer.freeze()\nlayer.unFreeze()\nmodel.freeze(Array(\nlayer1\n, \nlayer2\n))\nmodel.unFreeze()\nmodel.stopGradient(Array(\nlayer1\n))\n\n\n\n\n\n\nA single layer can be \"freezed\" by calling \nfreeze()\n. If a layer is freezed,\nits parameters(weight/bias, if exists) are not changed in training process\n\n\nA single layer can be \"unFreezed\" by calling \nunFreeze()\n.\n\n\nUser can set freeze of list of layers in model by calling \nfreeze\n\n\nUser can unfreeze all layers by calling \nunFreeze\n\n\nstop the input gradient of layers that match the given names. Their input gradient are not computed.\nAnd they will not contributed to the input gradient computation of layers that depend on them.\n\n\n\n\nPython\n\n\nlayer.freeze()\nlayer.unfreeze()\nmodel.freeze([\nlayer1\n, \nlayer2\n])\nmodel.unfreeze()\nmodel.stop_gradient([\nlayer1\n])\n\n\n\n\nScala\n\nOriginal model without \"freeze\" or \"stop gradient\"\n\n\nval reshape = Reshape(Array(4)).inputs()\nval fc1 = Linear(4, 2).setName(\nfc1\n).inputs()\nval fc2 = Linear(4, 2).setName(\nfc2\n).inputs(reshape)\nval cadd_1 = CAddTable().setName(\ncadd\n).inputs(fc1, fc2)\nval output1_1 = ReLU().inputs(cadd_1)\nval output2_1 = Threshold(10.0).inputs(cadd_1)\n\nval model = Graph(Array(reshape, fc1), Array(output1_1, output2_1))\n\nval input = T(Tensor(T(0.1f, 0.2f, -0.3f, -0.4f)),\n  Tensor(T(0.5f, 0.4f, -0.2f, -0.1f)))\nval gradOutput = T(Tensor(T(1.0f, 2.0f)), Tensor(T(3.0f, 4.0f)))\n\nfc1.element.getParameters()._1.apply1(_ =\n 1.0f)\nfc2.element.getParameters()._1.apply1(_ =\n 2.0f)\nmodel.zeroGradParameters()\nprintln(\noutput1: \\n\n, model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln(\nfc2 weight \\n\n, fc2.element.parameters()._1(0))\n\n\n\n\n(output1:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc2 weight\n,1.9    1.8 2.3 2.4\n1.8 1.6 2.6 2.8\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])\n\n\n\n\n\"Freeze\" \nfc2\n, the parameters of \nfc2\n is not changed.\n\n\nfc1.element.getParameters()._1.apply1(_ =\n 1.0f)\nfc2.element.getParameters()._1.apply1(_ =\n 2.0f)\nmodel.zeroGradParameters()\nmodel.freeze(Array(\nfc2\n))\nprintln(\noutput2: \\n\n, model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln(\nfc2 weight \\n\n, fc2.element.parameters()._1(0))\n\n\n\n\n(output2:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc2 weight\n,2.0    2.0 2.0 2.0\n2.0 2.0 2.0 2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])\n\n\n\n\n\"unFreeze\" \nfc2\n, the parameters of \nfc2\n will be updated.\n\n\nfc1.element.getParameters()._1.apply1(_ =\n 1.0f)\nfc2.element.getParameters()._1.apply1(_ =\n 2.0f)\nmodel.zeroGradParameters()\nmodel.unFreeze()\nprintln(\noutput3: \\n\n, model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln(\nfc2 weight \\n\n, fc2.element.parameters()._1(0))\n\n\n\n\n(output3:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc2 weight\n,1.9    1.8 2.3 2.4\n1.8 1.6 2.6 2.8\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])\n\n\n\n\n\"stop gradient\" at \ncadd\n, the parameters of \nfc1\n and \nfc2\n are not changed.\n\n\nfc1.element.getParameters()._1.apply1(_ =\n 1.0f)\nfc2.element.getParameters()._1.apply1(_ =\n 2.0f)\nmodel.stopGradient(Array(\ncadd\n))\nmodel.zeroGradParameters()\nprintln(\noutput4: \\n\n, model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln(\nfc1 weight \\n\n, fc1.element.parameters()._1(0))\nprintln(\nfc2 weight \\n\n, fc2.element.parameters()._1(0))\n\n\n\n\n(output4:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc1 weight\n,1.0    1.0 1.0 1.0\n1.0 1.0 1.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])\n(fc2 weight\n,2.0    2.0 2.0 2.0\n2.0 2.0 2.0 2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])\n\n\n\n\nPython\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nreshape = Reshape([4])()\nfc1 = Linear(4, 2).set_name(\nfc1\n)()\nfc2 = Linear(4, 2).set_name(\nfc2\n)(reshape)\ncadd = CAddTable().set_name(\ncadd\n)([fc1, fc2])\noutput1 = ReLU()(cadd)\noutput2 = Threshold(10.0)(cadd)\nmodel = Model([reshape, fc1], [output1, output2])\n\ninput = [\n    np.array([0.1, 0.2, -0.3, -0.4]),\n    np.array([0.5, 0.4, -0.2, -0.1])]\ngradOutput = [\n    np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n\nfc1.element().set_weights([np.array([[1,1,1,1],[1,1,1,1]]), np.array([1,1])])\nfc2.element().set_weights([np.array([[2,2,2,2],[2,2,2,2]]), np.array([2,2])])\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint \noutput1: \n, output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint \nfc2 weight \\n\n, fc2.element().parameters()['fc2']['weight']\n\n\n\n\n output1\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]\n\n\n fc2 weight\n[[ 1.89999998  1.79999995  2.29999995  2.4000001 ]\n [ 1.79999995  1.60000002  2.5999999   2.79999995]]\n\n\n\n\nfc1.element().set_weights([np.array([[1,1,1,1],[1,1,1,1]]), np.array([1,1])])\nfc2.element().set_weights([np.array([[2,2,2,2],[2,2,2,2]]), np.array([2,2])])\nm3 = model.freeze([\nfc2\n])\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint \noutput2 \n, output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint \nfc2 weight \\n\n, fc2.element().parameters()['fc2']['weight']\n\n\n\n\n output2\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]\n\n\n fc2 weight\n[[ 2.  2.  2.  2.]\n [ 2.  2.  2.  2.]]\n\n\n\n\nfc1.element().set_weights([np.array([[1,1,1,1],[1,1,1,1]]), np.array([1,1])])\nfc2.element().set_weights([np.array([[2,2,2,2],[2,2,2,2]]), np.array([2,2])])\nm3 = model.unfreeze()\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint \noutput3 \n, output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint \nfc2 weight \\n\n, fc2.element().parameters()['fc2']['weight']\n\n\n\n\n output3\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]\n\n\n fc2 weight\n[[ 1.89999998  1.79999995  2.29999995  2.4000001 ]\n [ 1.79999995  1.60000002  2.5999999   2.79999995]]\n\n\n\n\nm3 = model.stop_gradient([\ncadd\n])\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint \noutput4 \n, output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint \nfc1 weight \\n\n, fc1.element().parameters()['fc1']['weight']\nprint \nfc2 weight \\n\n, fc2.element().parameters()['fc2']['weight']\n\n\n\n\n output4\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]\n\n\n fc1 weight\n[[ 1.  1.  1.  1.]\n [ 1.  1.  1.  1.]]\n\n\n fc2 weight\n[[ 2.  2.  2.  2.]\n [ 2.  2.  2.  2.]]", 
            "title": "Model"
        }, 
        {
            "location": "/APIGuide/Module/#model-save", 
            "text": "BigDL supports saving models to local file system, HDFS and AWS S3. After a model is created, you can use  save  on created model to save it. Below example shows how to save a model.   Scala example  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval model = Sequential().add(Linear(10, 5)).add(Sigmoid()).add(SoftMax())\n//...train\n\nmodel.save( /tmp/model.bigdl , true) //save to local fs\nmodel.save( hdfs://... ) //save to hdfs\nmodel.save( s3://... ) //save to s3  Python example  from bigdl.nn.layer import *\nfrom bigdl.util.common import *\nfrom bigdl.optim.optimizer import *\n\nmodel = Sequential().add(Linear(10, 5)).add(Sigmoid()).add(SoftMax())\n//...train\nmodel.save( /tmp/model.bigdl , True) //save to local fs\nmodel.save( hdfs://... ) //save to hdfs\nmodel.save( s3://... ) //save to s3  In  model.save , the first parameter is the path where we want to save our model, the second paramter is to specify if we need to overwrite the file if it already exists, it's set to false by default", 
            "title": "Model Save"
        }, 
        {
            "location": "/APIGuide/Module/#model-load", 
            "text": "Use  Module.load (in Scala) or  Model.load  (in Python) to load an existing model.   Module  (Scala) or  Model (Python) is a utilily class provided in BigDL. We just need to specify the model path where we previously saved the model to load it to memory for resume training or prediction purpose.  Scala example  val model = Module.load( /tmp/model.bigdl ) //load from local fs\nval model = Module.load( hdfs://... ) //load from hdfs\nval model = Module.load( s3://... ) //load from s3  Python example  model = Model.load( /tmp/model.bigdl ) //load from local fs\nmodel = Model.load( hdfs://... ) //load from hdfs\nmodel = Model.load( s3://... ) //load from s3", 
            "title": "Model Load"
        }, 
        {
            "location": "/APIGuide/Module/#model-evaluation", 
            "text": "Scala  model.evaluate(dataset,vMethods,batchSize = None)  Python  model.test(val_rdd, batch_size, val_methods)  Use  evaluate  on the model for evaluation. The parameter  dataset  (Scala) or  val_rdd  (Python) in is the validation dataset, and  vMethods  (Scala) or  val_methods (Python) is an array of ValidationMethods. Refer to  Metrics  for the list of defined ValidationMethods.   Scala example  import com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.optim.Top1Accuracy\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n//create some dummy dataset for evaluation\nval feature = Tensor(10).rand()\nval label = Tensor(1).randn()\n\nval testSample = Sample(feature, label)\n//sc is is the SparkContxt instance\nval testSet = sc.parallelize(Seq(testSample))\n\n//train a new model or load an existing model\n//val model=...\nval evaluateResult = model.evaluate(testSet, Array(new Top1Accuracy))  Python example  from bigdl.nn.layer import *\nfrom bigdl.util.common import *\nfrom bigdl.optim.optimizer import *\nimport numpy as np\n\nsamples=[Sample.from_ndarray(np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]), np.array([2.0]))]\ntestSet = sc.parallelize(samples)\n\n//train a model or load an existing model...\n//model = ...\nevaluateResult = model.test(testSet, 1, [Top1Accuracy])", 
            "title": "Model Evaluation"
        }, 
        {
            "location": "/APIGuide/Module/#model-prediction", 
            "text": "Scala  model.predict(dataset)\nmodel.predictClass(dataset)  Python  model.predict(data_rdd)\nmodel.predict_class(data_rdd)  Use  predict  or  predictClass  or  predict_class  on model for Prediction.  predict  returns return the probability distribution of each class, and  predictClass / predict_class  returns the predict label. They both accepts the test dataset as parameter.   Scala example  import com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.optim.Top1Accuracy\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n//create some dummy dataset for prediction as example\nval feature = Tensor(10).rand()\nval label = Tensor(1).randn()\nval predictSample = Sample(feature, label)\nval predictSet = sc.parallelize(Seq(predictSample))\n\n//train a new model or load an existing model\n//val model=... \nval preductResult = model.predict(predictSet)  Python example   from bigdl.nn.layer import *\n from bigdl.util.common import *\n from bigdl.optim.optimizer import *\n import numpy as np\n\n samples=[Sample.from_ndarray(np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]), np.  array([2.0]))]\n\n predictSet = sc.parallelize(samples)\n\n //train a model or load an existing model...\n //model = ...\n preductResult = model.predict(predictSet)", 
            "title": "Model Prediction"
        }, 
        {
            "location": "/APIGuide/Module/#module-freeze", 
            "text": "To \"freeze\" a module means to exclude some layers of model from training.  layer.freeze()\nlayer.unFreeze()\nmodel.freeze(Array( layer1 ,  layer2 ))\nmodel.unFreeze()\nmodel.stopGradient(Array( layer1 ))   A single layer can be \"freezed\" by calling  freeze() . If a layer is freezed,\nits parameters(weight/bias, if exists) are not changed in training process  A single layer can be \"unFreezed\" by calling  unFreeze() .  User can set freeze of list of layers in model by calling  freeze  User can unfreeze all layers by calling  unFreeze  stop the input gradient of layers that match the given names. Their input gradient are not computed.\nAnd they will not contributed to the input gradient computation of layers that depend on them.   Python  layer.freeze()\nlayer.unfreeze()\nmodel.freeze([ layer1 ,  layer2 ])\nmodel.unfreeze()\nmodel.stop_gradient([ layer1 ])  Scala \nOriginal model without \"freeze\" or \"stop gradient\"  val reshape = Reshape(Array(4)).inputs()\nval fc1 = Linear(4, 2).setName( fc1 ).inputs()\nval fc2 = Linear(4, 2).setName( fc2 ).inputs(reshape)\nval cadd_1 = CAddTable().setName( cadd ).inputs(fc1, fc2)\nval output1_1 = ReLU().inputs(cadd_1)\nval output2_1 = Threshold(10.0).inputs(cadd_1)\n\nval model = Graph(Array(reshape, fc1), Array(output1_1, output2_1))\n\nval input = T(Tensor(T(0.1f, 0.2f, -0.3f, -0.4f)),\n  Tensor(T(0.5f, 0.4f, -0.2f, -0.1f)))\nval gradOutput = T(Tensor(T(1.0f, 2.0f)), Tensor(T(3.0f, 4.0f)))\n\nfc1.element.getParameters()._1.apply1(_ =  1.0f)\nfc2.element.getParameters()._1.apply1(_ =  2.0f)\nmodel.zeroGradParameters()\nprintln( output1: \\n , model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln( fc2 weight \\n , fc2.element.parameters()._1(0))  (output1:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc2 weight\n,1.9    1.8 2.3 2.4\n1.8 1.6 2.6 2.8\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])  \"Freeze\"  fc2 , the parameters of  fc2  is not changed.  fc1.element.getParameters()._1.apply1(_ =  1.0f)\nfc2.element.getParameters()._1.apply1(_ =  2.0f)\nmodel.zeroGradParameters()\nmodel.freeze(Array( fc2 ))\nprintln( output2: \\n , model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln( fc2 weight \\n , fc2.element.parameters()._1(0))  (output2:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc2 weight\n,2.0    2.0 2.0 2.0\n2.0 2.0 2.0 2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])  \"unFreeze\"  fc2 , the parameters of  fc2  will be updated.  fc1.element.getParameters()._1.apply1(_ =  1.0f)\nfc2.element.getParameters()._1.apply1(_ =  2.0f)\nmodel.zeroGradParameters()\nmodel.unFreeze()\nprintln( output3: \\n , model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln( fc2 weight \\n , fc2.element.parameters()._1(0))  (output3:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc2 weight\n,1.9    1.8 2.3 2.4\n1.8 1.6 2.6 2.8\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])  \"stop gradient\" at  cadd , the parameters of  fc1  and  fc2  are not changed.  fc1.element.getParameters()._1.apply1(_ =  1.0f)\nfc2.element.getParameters()._1.apply1(_ =  2.0f)\nmodel.stopGradient(Array( cadd ))\nmodel.zeroGradParameters()\nprintln( output4: \\n , model.forward(input))\nmodel.backward(input, gradOutput)\nmodel.updateParameters(1)\nprintln( fc1 weight \\n , fc1.element.parameters()._1(0))\nprintln( fc2 weight \\n , fc2.element.parameters()._1(0))  (output4:\n, {\n    2: 0.0\n       0.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    1: 2.8\n       2.8\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n })\n(fc1 weight\n,1.0    1.0 1.0 1.0\n1.0 1.0 1.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])\n(fc2 weight\n,2.0    2.0 2.0 2.0\n2.0 2.0 2.0 2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4])  Python  from bigdl.nn.layer import *\nimport numpy as np\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nreshape = Reshape([4])()\nfc1 = Linear(4, 2).set_name( fc1 )()\nfc2 = Linear(4, 2).set_name( fc2 )(reshape)\ncadd = CAddTable().set_name( cadd )([fc1, fc2])\noutput1 = ReLU()(cadd)\noutput2 = Threshold(10.0)(cadd)\nmodel = Model([reshape, fc1], [output1, output2])\n\ninput = [\n    np.array([0.1, 0.2, -0.3, -0.4]),\n    np.array([0.5, 0.4, -0.2, -0.1])]\ngradOutput = [\n    np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n\nfc1.element().set_weights([np.array([[1,1,1,1],[1,1,1,1]]), np.array([1,1])])\nfc2.element().set_weights([np.array([[2,2,2,2],[2,2,2,2]]), np.array([2,2])])\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint  output1:  , output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint  fc2 weight \\n , fc2.element().parameters()['fc2']['weight']   output1\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]  fc2 weight\n[[ 1.89999998  1.79999995  2.29999995  2.4000001 ]\n [ 1.79999995  1.60000002  2.5999999   2.79999995]]  fc1.element().set_weights([np.array([[1,1,1,1],[1,1,1,1]]), np.array([1,1])])\nfc2.element().set_weights([np.array([[2,2,2,2],[2,2,2,2]]), np.array([2,2])])\nm3 = model.freeze([ fc2 ])\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint  output2  , output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint  fc2 weight \\n , fc2.element().parameters()['fc2']['weight']   output2\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]  fc2 weight\n[[ 2.  2.  2.  2.]\n [ 2.  2.  2.  2.]]  fc1.element().set_weights([np.array([[1,1,1,1],[1,1,1,1]]), np.array([1,1])])\nfc2.element().set_weights([np.array([[2,2,2,2],[2,2,2,2]]), np.array([2,2])])\nm3 = model.unfreeze()\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint  output3  , output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint  fc2 weight \\n , fc2.element().parameters()['fc2']['weight']   output3\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]  fc2 weight\n[[ 1.89999998  1.79999995  2.29999995  2.4000001 ]\n [ 1.79999995  1.60000002  2.5999999   2.79999995]]  m3 = model.stop_gradient([ cadd ])\nmodel.zero_grad_parameters()\noutput = model.forward(input)\nprint  output4  , output\ngradInput = model.backward(input, gradOutput)\nmodel.update_parameters(1.0)\nprint  fc1 weight \\n , fc1.element().parameters()['fc1']['weight']\nprint  fc2 weight \\n , fc2.element().parameters()['fc2']['weight']   output4\n[array([ 2.79999995,  2.79999995], dtype=float32), array([ 0.,  0.], dtype=float32)]  fc1 weight\n[[ 1.  1.  1.  1.]\n [ 1.  1.  1.  1.]]  fc2 weight\n[[ 2.  2.  2.  2.]\n [ 2.  2.  2.  2.]]", 
            "title": "Module Freeze"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/", 
            "text": "Sequential\n\n\nScala:\n\n\nval module = Sequential()\n\n\n\n\nPython:\n\n\nseq = Sequential()\n\n\n\n\nSequential provides a means to plug layers together\nin a feed-forward fully connected manner.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.nn.{Sequential, Linear}\n\nval module = Sequential()\nmodule.add(Linear(10, 25))\nmodule.add(Linear(25, 10))\n\nval input = Tensor(10).range(1, 10, 1)\nval gradOutput = Tensor(10).range(1, 10, 1)\n\nval output = module.forward(input).toTensor\nval gradInput = module.backward(input, gradOutput).toTensor\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\nGives the output,\n\n\n-2.3750305\n2.4512818\n1.6998017\n-0.47432393\n4.3048754\n-0.044168986\n-1.1643536\n0.60341483\n2.0216258\n2.1190155\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]\n\n\n\n\nGives the gradInput,\n\n\n2.593382\n-1.4137214\n-1.8271983\n1.229643\n0.51384985\n1.509845\n2.9537349\n1.088281\n0.2618509\n1.4840821\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nseq = Sequential()\nseq.add(Linear(10, 25))\nseq.add(Linear(25, 10))\n\ninput = np.arange(1, 11, 1).astype(\nfloat32\n)\ninput = input.reshape(1, 10)\n\noutput = seq.forward(input)\nprint output\n\ngradOutput = np.arange(1, 11, 1).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(1, 10)\n\ngradInput = seq.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nGives the output,\n\n\n[array([[ 1.08462083, -2.03257799, -0.5400058 ,  0.27452484,  1.85562158,\n         1.64338267,  2.45694995,  1.70170391, -2.12998056, -1.28924525]], dtype=float32)]\n\n\n\n\nGives the gradInput,\n\n\n\n[array([[ 1.72007763,  1.64403224,  2.52977395, -1.00021958,  0.1134415 ,\n         2.06711197,  2.29631734, -3.39587498,  1.01093054, -0.54482007]], dtype=float32)]\n\n\n\n\nGraph\n\n\nScala:\n\n\nval graph = Graph(Array(Node), Array(Node))\n\n\n\n\nPython:\n\n\nmodel = Model([Node], [Node])\n\n\n\n\nA graph container. Each node can have multiple inputs. The output of the node should be a tensor.\n The output tensor can be connected to multiple nodes. So the module in each node can have a\n tensor or table input, and should have a tensor output.\n\n\nThe graph container can have multiple inputs and multiple outputs. If there's one input, the\n input data fed to the graph module should be a tensor. If there're multiple inputs, the input\n data fed to the graph module should be a table, which is actually an sequence of tensor. The\n order of the input tensors should be same with the order of the input nodes. This is also\n applied to the gradient from the module in the back propagation.\n\n\nAll of the input modules must accept a tensor input. If your input module accept multiple\n tensors as input, you should add some \nInput layer\n before\n it as input nodes and connect the output of the Input modules to that module.\n\n\nIf there's one output, the module output is a tensor. If there're multiple outputs, the module\n output is a table, which is actually an sequence of tensor. The order of the output tensors is\n same with the order of the output modules. This is also applied to the gradient passed to the\n module in the back propagation.\n\n\nAll inputs should be able to connect to outputs through some paths in the graph. It is\n allowed that some successors of the inputs node are not connect to outputs. If so, these nodes\n will be excluded in the computation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\n\nval input1 = Input()\nval input2 = Input()\nval cadd = CAddTable().inputs(input1, input2)\nval graph = Graph(Array(input1, input2), cadd)\n\nval output = graph.forward(T(Tensor(T(0.1f, 0.2f, -0.3f, -0.4f)),\n    Tensor(T(0.5f, 0.4f, -0.2f, -0.1f))))\nval gradInput = graph.backward(T(Tensor(T(0.1f, 0.2f, -0.3f, -0.4f)),\n    Tensor(T(0.5f, 0.4f, -0.2f, -0.1f))),\n    Tensor(T(0.1f, 0.2f, 0.3f, 0.4f)))\n\n\n println(output)\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.6\n0.6\n-0.5\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n {\n        2: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n        1: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n }\n\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\n\ninput1 = Input()\ninput2 = Input()\ncadd = CAddTable()([input1, input2])\nmodel = Model([input1, input2], [cadd])\noutput = model.forward([\n    np.array([0.1, 0.2, -0.3, -0.4]),\n    np.array([0.5, 0.4, -0.2, -0.1])])\n\n\n output\narray([ 0.60000002,  0.60000002, -0.5       , -0.5       ], dtype=float32)\n\ngradInput = model.backward([\n        np.array([0.1, 0.2, -0.3, -0.4]),\n        np.array([0.5, 0.4, -0.2, -0.1])\n    ],\n    np.array([0.1, 0.2, 0.3, 0.4])\n)\n\n\n gradInput\n[array([ 0.1       ,  0.2       ,  0.30000001,  0.40000001], dtype=float32),\n    array([ 0.1       ,  0.2       ,  0.30000001,  0.40000001], dtype=float32)]\n\n\n\n\n\n\nConcat\n\n\nScala:\n\n\nval module = Concat(dimension)\n\n\n\n\nPython:\n\n\nmodule = Concat(dimension)\n\n\n\n\nConcat is a container who concatenates the output of it's submodules along the\nprovided \ndimension\n: all submodules take the same inputs, and their output is\nconcatenated.\n\n\n                 +----Concat----+\n            +----\n  submodule1  -----+\n            |    |              |    |\n input -----+----\n  submodule2  -----+----\n output\n            |    |              |    |\n            +----\n  submodule3  -----+\n                 +--------------+\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\n\nprintln(mlp.forward(Tensor(2, 3).rand()))\n\n\n\n\nGives the output,\n\n\n-0.17087375 0.12954286  0.15685591  -0.027277306    0.38549712  -0.20375136\n-0.9473443  0.030516684 0.23380546  0.625985    -0.031360716    0.40449825\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\nprint(mlp.forward(np.array([[1, 2, 3], [1, 2, 3]])))\n\n\n\n\nGives the output,\n\n\n[array([\n[-0.71994132,  2.17439198, -1.46522939,  0.64588934,  2.61534023, -2.39528942],\n[-0.89125222,  5.49583197, -2.8865242 ,  1.44914722,  5.26639175, -6.26586771]]\n      dtype=float32)]\n\n\n\n\n\nParallelTable\n\n\nScala:\n\n\nval module = ParallelTable()\n\n\n\n\nPython:\n\n\nmodule = ParallelTable()\n\n\n\n\nIt is a container module that applies the i-th member module to the i-th\n input, and outputs an output in the form of Table\n\n\n+----------+         +-----------+\n| {input1, +---------\n {member1, |\n|          |         |           |\n|  input2, +---------\n  member2, |\n|          |         |           |\n|  input3} +---------\n  member3} |\n+----------+         +-----------+\n\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = ParallelTable()\nval log = Log()\nval exp = Exp()\nmodule.add(log)\nmodule.add(exp)\nval input1 = Tensor(3, 3).rand(0, 1)\nval input2 = Tensor(3).rand(0, 1)\nval input = T(1 -\n input1, 2 -\n input2)\n\n print(module.forward(input))\n {\n        2: 2.6996834\n           2.0741253\n           1.0625387\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: -1.073425    -0.6672964      -1.8160943\n           -0.54094607  -1.3029919      -1.7064717\n           -0.66175103  -0.08288143     -1.1840979\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nmodule = ParallelTable()\nlog = Log()\nexp = Exp()\nmodule.add(log)\nmodule.add(exp)\ninput1 = np.random.rand(3,3)\ninput2 = np.random.rand(3)\n\nmodule.forward([input1, input2])\n[array([[-1.27472472, -2.18216252, -0.60752904],\n        [-2.76213861, -1.77966928, -0.13652121],\n        [-1.47725129, -0.03578046, -1.37736678]], dtype=float32),\n array([ 1.10634041,  1.46384597,  1.96525407], dtype=float32)]\n\n\n\n\nConcatTable\n\n\nScala:\n\n\nval module = ConcatTable()\n\n\n\n\nPython:\n\n\nmodule = ConcatTable()\n\n\n\n\nConcateTable is a container module like Concate. Applies an input\nto each member module, input can be a tensor or a table.\n\n\nConcateTable usually works with CAddTable and CMulTable to\n implement element wise add/multiply on outputs of two modules.\n\n\n                   +-----------+\n             +----\n {member1, |\n+-------+    |    |           |\n| input +----+----\n  member2, |\n+-------+    |    |           |\n   or        +----\n  member3} |\n {input}          +-----------+\n\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = ConcatTable()\nmlp.add(Linear(3, 2))\nmlp.add(Linear(3, 4))\n\n\n print(mlp.forward(Tensor(2, 3).rand()))\n\n{\n    2: -0.37111914  0.8542446   -0.362602   -0.75522065 \n       -0.28713673  0.6021913   -0.16525984 -0.44689763 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n    1: -0.79941726  0.8303885   \n       -0.8342782   0.89961016  \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nmlp = ConcatTable()\nmlp.add(Linear(3, 2))   \nmlp.add(Linear(3, 4))\n\n mlp.forward(np.array([[1, 2, 3], [1, 2, 3]]))\nout: [array([[ 1.16408789, -0.1508013 ],\n             [ 1.16408789, -0.1508013 ]], dtype=float32),\n      array([[-0.24672163, -0.56958938, -0.51390374,  0.64546645],\n             [-0.24672163, -0.56958938, -0.51390374,  0.64546645]], dtype=float32)]\n\n\n\n\n\nBottle\n\n\nScala:\n\n\nval model = Bottle(module, nInputDim, nOutputDim)\n\n\n\n\nPython:\n\n\nmodel = Bottle(module, nInputDim, nOutputDim)\n\n\n\n\nBottle allows varying dimensionality input to be forwarded through any module that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Bottle(Linear(3, 2), 2, 2)\nval input = Tensor(2, 3, 3).rand()\n\nscala\n print(input)\n(1,.,.) =\n0.7843752   0.17286697  0.20767091  \n0.8594811   0.9100018   0.8448141   \n0.7683892   0.36661968  0.76637685  \n\n(2,.,.) =\n0.7163263   0.083962396 0.81222403  \n0.7947034   0.09976136  0.114404656 \n0.14890474  0.43289232  0.1489096   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3x3] \n\nval output = model.forward(input)\n\nscala\n print(output)\n(1,.,.) =\n-0.31146684 0.40719786  \n-0.51778656 0.58715886  \n-0.51676923 0.4027511   \n\n(2,.,.) =\n-0.5498678  0.29658738  \n-0.280177   0.39901164  \n-0.2387946  0.24809375  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2]\n\n\n\n\nPython example:\n\n\nmodel = Bottle(Linear(3, 2), 2, 2)\n\ninput = np.random.randn(2, 3, 3)\noutput = model.forward(input)\n\n\n print(input)\n[[[ 0.42370589 -1.7938942   0.56666373]\n  [-1.78501381  0.55676471 -0.50150367]\n  [-1.59262182  0.82079469  1.1873599 ]]\n\n [[ 0.95799792 -0.71447244  1.05344083]\n  [-0.07838376 -0.88780484 -1.80491177]\n  [ 0.99996222  1.39876002 -0.16326094]]]\n\n print(output)\n[[[ 0.26298434  0.74947536]\n  [-1.24375117 -0.33148435]\n  [-1.35218966  0.17042145]]\n\n [[ 0.08041853  0.91245329]\n  [-0.08317742 -0.13909879]\n  [-0.52287608  0.3667658 ]]]\n\n\n\n\nMapTable\n\n\nScala:\n\n\nval mod = MapTable(module=null)\n\n\n\n\nPython:\n\n\nmod = MapTable(module=None)\n\n\n\n\nThis class is a container for a single module which will be applied\nto all input elements. The member module is cloned as necessary to\nprocess all input elements.\n\n\nmodule\n a member module.  \n\n\n+----------+         +-----------+\n| {input1, +---------\n {member,  |\n|          |         |           |\n|  input2, +---------\n  clone,   |\n|          |         |           |\n|  input3} +---------\n  clone}   |\n+----------+         +-----------+\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \n\nval map = MapTable()\nmap.add(Linear(10, 3))\nval input = T(\n      Tensor(10).randn(),\n      Tensor(10).randn())\n\n print(map.forward(input))\n{\n    2: 0.2444828\n       -1.1700082\n       0.15887381\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    1: 0.06696482\n       0.18692614\n       -1.432079\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nmap = MapTable()\nmap.add(Linear(10, 3))\ninput = [np.random.rand(10), np.random.rand(10)]\n\nmap.forward(input)\n[array([ 0.69586945, -0.70547599, -0.05802459], dtype=float32),\n array([ 0.47995114, -0.67459631, -0.52500772], dtype=float32)]\n\n\n\n\nContainer\n\n\nContainer is a subclass of abstract class AbstractModule, which\ndeclares methods defined in all containers. A container usually\ncontains some other modules in the \nmodules\n variable. It overrides\nmany module methods such that calls are propogated to the contained\nmodules.", 
            "title": "Containers"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#sequential", 
            "text": "Scala:  val module = Sequential()  Python:  seq = Sequential()  Sequential provides a means to plug layers together\nin a feed-forward fully connected manner.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.nn.{Sequential, Linear}\n\nval module = Sequential()\nmodule.add(Linear(10, 25))\nmodule.add(Linear(25, 10))\n\nval input = Tensor(10).range(1, 10, 1)\nval gradOutput = Tensor(10).range(1, 10, 1)\n\nval output = module.forward(input).toTensor\nval gradInput = module.backward(input, gradOutput).toTensor\n\nprintln(output)\nprintln(gradInput)  Gives the output,  -2.3750305\n2.4512818\n1.6998017\n-0.47432393\n4.3048754\n-0.044168986\n-1.1643536\n0.60341483\n2.0216258\n2.1190155\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]  Gives the gradInput,  2.593382\n-1.4137214\n-1.8271983\n1.229643\n0.51384985\n1.509845\n2.9537349\n1.088281\n0.2618509\n1.4840821\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nseq = Sequential()\nseq.add(Linear(10, 25))\nseq.add(Linear(25, 10))\n\ninput = np.arange(1, 11, 1).astype( float32 )\ninput = input.reshape(1, 10)\n\noutput = seq.forward(input)\nprint output\n\ngradOutput = np.arange(1, 11, 1).astype( float32 )\ngradOutput = gradOutput.reshape(1, 10)\n\ngradInput = seq.backward(input, gradOutput)\nprint gradInput  Gives the output,  [array([[ 1.08462083, -2.03257799, -0.5400058 ,  0.27452484,  1.85562158,\n         1.64338267,  2.45694995,  1.70170391, -2.12998056, -1.28924525]], dtype=float32)]  Gives the gradInput,  \n[array([[ 1.72007763,  1.64403224,  2.52977395, -1.00021958,  0.1134415 ,\n         2.06711197,  2.29631734, -3.39587498,  1.01093054, -0.54482007]], dtype=float32)]", 
            "title": "Sequential"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#graph", 
            "text": "Scala:  val graph = Graph(Array(Node), Array(Node))  Python:  model = Model([Node], [Node])  A graph container. Each node can have multiple inputs. The output of the node should be a tensor.\n The output tensor can be connected to multiple nodes. So the module in each node can have a\n tensor or table input, and should have a tensor output.  The graph container can have multiple inputs and multiple outputs. If there's one input, the\n input data fed to the graph module should be a tensor. If there're multiple inputs, the input\n data fed to the graph module should be a table, which is actually an sequence of tensor. The\n order of the input tensors should be same with the order of the input nodes. This is also\n applied to the gradient from the module in the back propagation.  All of the input modules must accept a tensor input. If your input module accept multiple\n tensors as input, you should add some  Input layer  before\n it as input nodes and connect the output of the Input modules to that module.  If there's one output, the module output is a tensor. If there're multiple outputs, the module\n output is a table, which is actually an sequence of tensor. The order of the output tensors is\n same with the order of the output modules. This is also applied to the gradient passed to the\n module in the back propagation.  All inputs should be able to connect to outputs through some paths in the graph. It is\n allowed that some successors of the inputs node are not connect to outputs. If so, these nodes\n will be excluded in the computation.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\n\nval input1 = Input()\nval input2 = Input()\nval cadd = CAddTable().inputs(input1, input2)\nval graph = Graph(Array(input1, input2), cadd)\n\nval output = graph.forward(T(Tensor(T(0.1f, 0.2f, -0.3f, -0.4f)),\n    Tensor(T(0.5f, 0.4f, -0.2f, -0.1f))))\nval gradInput = graph.backward(T(Tensor(T(0.1f, 0.2f, -0.3f, -0.4f)),\n    Tensor(T(0.5f, 0.4f, -0.2f, -0.1f))),\n    Tensor(T(0.1f, 0.2f, 0.3f, 0.4f)))  println(output)\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.6\n0.6\n-0.5\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  println(gradInput)\ngradInput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n {\n        2: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n        1: 0.1\n           0.2\n           0.3\n           0.4\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\n\ninput1 = Input()\ninput2 = Input()\ncadd = CAddTable()([input1, input2])\nmodel = Model([input1, input2], [cadd])\noutput = model.forward([\n    np.array([0.1, 0.2, -0.3, -0.4]),\n    np.array([0.5, 0.4, -0.2, -0.1])])  output\narray([ 0.60000002,  0.60000002, -0.5       , -0.5       ], dtype=float32)\n\ngradInput = model.backward([\n        np.array([0.1, 0.2, -0.3, -0.4]),\n        np.array([0.5, 0.4, -0.2, -0.1])\n    ],\n    np.array([0.1, 0.2, 0.3, 0.4])\n)  gradInput\n[array([ 0.1       ,  0.2       ,  0.30000001,  0.40000001], dtype=float32),\n    array([ 0.1       ,  0.2       ,  0.30000001,  0.40000001], dtype=float32)]", 
            "title": "Graph"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#concat", 
            "text": "Scala:  val module = Concat(dimension)  Python:  module = Concat(dimension)  Concat is a container who concatenates the output of it's submodules along the\nprovided  dimension : all submodules take the same inputs, and their output is\nconcatenated.                   +----Concat----+\n            +----   submodule1  -----+\n            |    |              |    |\n input -----+----   submodule2  -----+----  output\n            |    |              |    |\n            +----   submodule3  -----+\n                 +--------------+  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\n\nprintln(mlp.forward(Tensor(2, 3).rand()))  Gives the output,  -0.17087375 0.12954286  0.15685591  -0.027277306    0.38549712  -0.20375136\n-0.9473443  0.030516684 0.23380546  0.625985    -0.031360716    0.40449825\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Concat(2)\nmlp.add(Linear(3,2))\nmlp.add(Linear(3,4))\nprint(mlp.forward(np.array([[1, 2, 3], [1, 2, 3]])))  Gives the output,  [array([\n[-0.71994132,  2.17439198, -1.46522939,  0.64588934,  2.61534023, -2.39528942],\n[-0.89125222,  5.49583197, -2.8865242 ,  1.44914722,  5.26639175, -6.26586771]]\n      dtype=float32)]", 
            "title": "Concat"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#paralleltable", 
            "text": "Scala:  val module = ParallelTable()  Python:  module = ParallelTable()  It is a container module that applies the i-th member module to the i-th\n input, and outputs an output in the form of Table  +----------+         +-----------+\n| {input1, +---------  {member1, |\n|          |         |           |\n|  input2, +---------   member2, |\n|          |         |           |\n|  input3} +---------   member3} |\n+----------+         +-----------+  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = ParallelTable()\nval log = Log()\nval exp = Exp()\nmodule.add(log)\nmodule.add(exp)\nval input1 = Tensor(3, 3).rand(0, 1)\nval input2 = Tensor(3).rand(0, 1)\nval input = T(1 -  input1, 2 -  input2)  print(module.forward(input))\n {\n        2: 2.6996834\n           2.0741253\n           1.0625387\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: -1.073425    -0.6672964      -1.8160943\n           -0.54094607  -1.3029919      -1.7064717\n           -0.66175103  -0.08288143     -1.1840979\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }  Python example:  from bigdl.nn.layer import *\n\nmodule = ParallelTable()\nlog = Log()\nexp = Exp()\nmodule.add(log)\nmodule.add(exp)\ninput1 = np.random.rand(3,3)\ninput2 = np.random.rand(3) module.forward([input1, input2])\n[array([[-1.27472472, -2.18216252, -0.60752904],\n        [-2.76213861, -1.77966928, -0.13652121],\n        [-1.47725129, -0.03578046, -1.37736678]], dtype=float32),\n array([ 1.10634041,  1.46384597,  1.96525407], dtype=float32)]", 
            "title": "ParallelTable"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#concattable", 
            "text": "Scala:  val module = ConcatTable()  Python:  module = ConcatTable()  ConcateTable is a container module like Concate. Applies an input\nto each member module, input can be a tensor or a table.  ConcateTable usually works with CAddTable and CMulTable to\n implement element wise add/multiply on outputs of two modules.                     +-----------+\n             +----  {member1, |\n+-------+    |    |           |\n| input +----+----   member2, |\n+-------+    |    |           |\n   or        +----   member3} |\n {input}          +-----------+  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = ConcatTable()\nmlp.add(Linear(3, 2))\nmlp.add(Linear(3, 4))  print(mlp.forward(Tensor(2, 3).rand()))\n\n{\n    2: -0.37111914  0.8542446   -0.362602   -0.75522065 \n       -0.28713673  0.6021913   -0.16525984 -0.44689763 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n    1: -0.79941726  0.8303885   \n       -0.8342782   0.89961016  \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n }  Python example:  from bigdl.nn.layer import *\n\nmlp = ConcatTable()\nmlp.add(Linear(3, 2))   \nmlp.add(Linear(3, 4))  mlp.forward(np.array([[1, 2, 3], [1, 2, 3]]))\nout: [array([[ 1.16408789, -0.1508013 ],\n             [ 1.16408789, -0.1508013 ]], dtype=float32),\n      array([[-0.24672163, -0.56958938, -0.51390374,  0.64546645],\n             [-0.24672163, -0.56958938, -0.51390374,  0.64546645]], dtype=float32)]", 
            "title": "ConcatTable"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#bottle", 
            "text": "Scala:  val model = Bottle(module, nInputDim, nOutputDim)  Python:  model = Bottle(module, nInputDim, nOutputDim)  Bottle allows varying dimensionality input to be forwarded through any module that accepts input of nInputDim dimensions, and generates output of nOutputDim dimensions.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Bottle(Linear(3, 2), 2, 2)\nval input = Tensor(2, 3, 3).rand()\n\nscala  print(input)\n(1,.,.) =\n0.7843752   0.17286697  0.20767091  \n0.8594811   0.9100018   0.8448141   \n0.7683892   0.36661968  0.76637685  \n\n(2,.,.) =\n0.7163263   0.083962396 0.81222403  \n0.7947034   0.09976136  0.114404656 \n0.14890474  0.43289232  0.1489096   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3x3] \n\nval output = model.forward(input)\n\nscala  print(output)\n(1,.,.) =\n-0.31146684 0.40719786  \n-0.51778656 0.58715886  \n-0.51676923 0.4027511   \n\n(2,.,.) =\n-0.5498678  0.29658738  \n-0.280177   0.39901164  \n-0.2387946  0.24809375  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2]  Python example:  model = Bottle(Linear(3, 2), 2, 2)\n\ninput = np.random.randn(2, 3, 3)\noutput = model.forward(input)  print(input)\n[[[ 0.42370589 -1.7938942   0.56666373]\n  [-1.78501381  0.55676471 -0.50150367]\n  [-1.59262182  0.82079469  1.1873599 ]]\n\n [[ 0.95799792 -0.71447244  1.05344083]\n  [-0.07838376 -0.88780484 -1.80491177]\n  [ 0.99996222  1.39876002 -0.16326094]]]  print(output)\n[[[ 0.26298434  0.74947536]\n  [-1.24375117 -0.33148435]\n  [-1.35218966  0.17042145]]\n\n [[ 0.08041853  0.91245329]\n  [-0.08317742 -0.13909879]\n  [-0.52287608  0.3667658 ]]]", 
            "title": "Bottle"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#maptable", 
            "text": "Scala:  val mod = MapTable(module=null)  Python:  mod = MapTable(module=None)  This class is a container for a single module which will be applied\nto all input elements. The member module is cloned as necessary to\nprocess all input elements.  module  a member module.    +----------+         +-----------+\n| {input1, +---------  {member,  |\n|          |         |           |\n|  input2, +---------   clone,   |\n|          |         |           |\n|  input3} +---------   clone}   |\n+----------+         +-----------+  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T \n\nval map = MapTable()\nmap.add(Linear(10, 3))\nval input = T(\n      Tensor(10).randn(),\n      Tensor(10).randn())  print(map.forward(input))\n{\n    2: 0.2444828\n       -1.1700082\n       0.15887381\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    1: 0.06696482\n       0.18692614\n       -1.432079\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }  Python example:  from bigdl.nn.layer import *\n\nmap = MapTable()\nmap.add(Linear(10, 3))\ninput = [np.random.rand(10), np.random.rand(10)] map.forward(input)\n[array([ 0.69586945, -0.70547599, -0.05802459], dtype=float32),\n array([ 0.47995114, -0.67459631, -0.52500772], dtype=float32)]", 
            "title": "MapTable"
        }, 
        {
            "location": "/APIGuide/Layers/Containers/#container", 
            "text": "Container is a subclass of abstract class AbstractModule, which\ndeclares methods defined in all containers. A container usually\ncontains some other modules in the  modules  variable. It overrides\nmany module methods such that calls are propogated to the contained\nmodules.", 
            "title": "Container"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/", 
            "text": "Linear\n\n\nScala:\n\n\nval module = Linear(\n  inputSize,\n  outputSize,\n  withBias = true,\n  wRegularizer = null,\n  bRegularizer = null,\n  initWeight = null,\n  initBias = null,\n  initGradWeight = null,\n  initGradBias = null)\n\n\n\n\nPython:\n\n\nmodule = Linear(\n  input_size,\n  output_size,\n  init_method=\ndefault\n,\n  with_bias=True,\n  wRegularizer=None,\n  bRegularizer=None,\n  init_weight=None,\n  init_bias=None,\n  init_grad_weight=None,\n  init_grad_bias=None)\n\n\n\n\nThe \nLinear\n module applies a linear transformation to the input data,\ni.e. \ny = Wx + b\n. The \ninput\n given in \nforward(input)\n must be either\na vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must\nhave the size of \ninputSize\n. If it is a matrix, then each row is assumed to be\nan input sample of given batch (the number of rows means the batch size and\nthe number of columns should be equal to the \ninputSize\n).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Linear(3, 5)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.79338956\n-2.3417668\n-2.7557678\n-0.07507719\n-1.009765\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Linear(3, 5)\n\nprint(module.forward(np.arange(1, 4, 1)))\n\n\n\n\nGives the output,\n\n\n[array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]\n\n\n\n\n\n\nReverse\n\n\nScala:\n\n\nval m = Reverse(dim = 1, isInplace = false)\n\n\n\n\nPython:\n\n\nm = Reverse(dimension=1)\n\n\n\n\nReverse the input w.r.t given dimension.\n The input can be a Tensor or Table. \nDimension\n is one-based index.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(0, 1)\nval input = Tensor(2, 3)\ninput.apply1(x =\n randomn().toFloat)\nprintln(\ninput:\n)\nprintln(input)\nval layer = new Reverse(1)\nprintln(\noutput:\n)\nprintln(layer.forward(input))\n\n\n\n\ninput:\n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\noutput:\n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\n\nPython example:\n\n\ninput = np.random.random((2,3))\nlayer = Reverse(1)\nprint(\ninput:\n)\nprint(input)\nprint(\noutput:\n)\nprint(layer.forward(input))\n\n\n\n\ncreating: createReverse\ninput:\n[[ 0.89089717  0.07629756  0.30863782]\n [ 0.16066851  0.06421963  0.96719367]]\noutput:\n[[ 0.16066851  0.06421963  0.96719366]\n [ 0.89089715  0.07629756  0.30863783]]\n\n\n\n\n\n\n\n\nReshape\n\n\nScala:\n\n\nval reshape = Reshape(size, batchMode)\n\n\n\n\nPython:\n\n\nreshape = Reshape(size, batch_mode)\n\n\n\n\nThe \nforward(input)\n reshape the input tensor into \nsize(0) * size(1) * ...\n tensor,\ntaking the elements row-wise.\n\n\nparameters:\n\n \nsize\n the size after reshape\n\n \nbatchMode\n It is a optional argument. If it is set to \nSome(true)\n,\n                  the first dimension of input is considered as batch dimension,\n                  and thus keep this dimension size fixed. This is necessary\n                  when dealing with batch sizes of one. When set to \nSome(false)\n,\n                  it forces the entire input (including the first dimension) to be reshaped\n                  to the input size. Default is \nNone\n, which means the module considers\n                  inputs with more elements than the product of provided sizes (\nsize(0) *\n                  size(1) * ..\n) to be batches, otherwise in no batch mode.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval reshape = Reshape(Array(3, 2))\nval input = Tensor(2, 2, 3).rand()\nval output = reshape.forward(input)\n-\n print(output.size().toList)      \nList(2, 3, 2)\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nreshape =  Reshape([3, 2])\ninput = np.random.rand(2, 2, 3)\noutput = reshape.forward(input)\n-\n print output[0].shape\n(2, 3, 2)\n\n\n\n\n\n\nIndex\n\n\nScala:\n\n\nval model = Index(dimension)\n\n\n\n\nPython:\n\n\nmodel = Index(dimension)\n\n\n\n\nApplies the Tensor index operation along the given dimension.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).rand()\nval input2 = Tensor(4)\ninput2(Array(1)) = 1.0f\ninput2(Array(2)) = 2.0f\ninput2(Array(3)) = 2.0f\ninput2(Array(4)) = 3.0f\n\nval input = T(input1, input2)\nval model = Index(1)\nval output = model.forward(input)\n\nscala\n print(input)\n {\n    2: 1.0\n       2.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4]\n    1: 0.124325536\n       0.8768922\n       0.6378146\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\nscala\n print(output)\n0.124325536\n0.8768922\n0.8768922\n0.6378146\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.randn(3)\ninput2 = np.array([1, 2, 2, 3])\ninput = [input1, input2]\n\nmodel = Index(1)\noutput = model.forward(input)\n\n\n print(input)\n[array([-0.45804847, -0.20176707,  0.50963248]), array([1, 2, 2, 3])]\n\n\n print(output)\n[-0.45804846 -0.20176707 -0.20176707  0.50963247]\n\n\n\n\n\n\nIdentity\n\n\nScala:\n\n\nval identity = Identity()\n\n\n\n\nPython:\n\n\nidentity = Identity()\n\n\n\n\nIdentity just return input as the output which is useful in same parallel container to get an origin input\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval identity = Identity()\n\nval input = Tensor(3, 3).rand()\n\n print(input)\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n print(identity.forward(input))\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nidentity = Identity()\n\n  identity.forward(np.array([[1, 2, 3], [4, 5, 6]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]], dtype=float32)]\n\n\n\n\n\n\n\nNarrow\n\n\nScala:\n\n\nval layer = Narrow(dimension, offset, length = 1)\n\n\n\n\nPython:\n\n\nlayer = Narrow(dimension, offset, length=1)\n\n\n\n\nNarrow is an application of narrow operation in a module.\nThe module further supports a negative length in order to handle inputs with an unknown size.\n\n\nParameters:\n\n \ndimension\n narrow along this dimension\n\n \noffset\n the start index on the given dimension\n* \nlength\n length to narrow, default value is 1\n\n\nScala Example\n\n\nimport com.intel.analytics.bigdl.nn.Narrow\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Narrow(2, 2)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\n2.0\n3.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]\n\nval grad = layer.backward(input, gradOutput)\n0.0 3.0 0.0\n0.0 4.0 0.0\n0.0 5.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython Example\n\n\nlayer = Narrow(2, 2)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[ 2.]\n [ 3.]\n [ 4.]]\n\nprint grad\n[[ 0.  3.  0.]\n [ 0.  4.  0.]\n [ 0.  5.  0.]]\n\n\n\n\n\n\nUnsqueeze\n\n\nScala:\n\n\nval layer = Unsqueeze(dim)\n\n\n\n\nPython:\n\n\nlayer = Unsqueeze(dim)\n\n\n\n\nInsert singleton dim (i.e., dimension 1) at position pos. For an input with \ndim = input.dim()\n,\nthere are \ndim + 1\n possible positions to insert the singleton dimension. The dim starts from 1.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval layer = Unsqueeze(2)\nval input = Tensor(2, 2, 2).rand\nval gradOutput = Tensor(2, 1, 2, 2).rand\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)\n\n\n println(input.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n println(gradOutput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]\n\n\n println(output.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]\n\n\n println(gradInput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nlayer = Unsqueeze(2)\ninput = np.random.uniform(0, 1, (2, 2, 2)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (2, 1, 2, 2)).astype(\nfloat32\n)\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)\n\n\n output\n[array([[[[ 0.97488612,  0.43463323],\n          [ 0.39069486,  0.0949123 ]]],\n\n\n        [[[ 0.19310953,  0.73574477],\n          [ 0.95347691,  0.37380624]]]], dtype=float32)]\n\n gradInput\n[array([[[ 0.9995622 ,  0.69787127],\n         [ 0.65975296,  0.87002522]],\n\n        [[ 0.76349133,  0.96734989],\n         [ 0.88068211,  0.07284366]]], dtype=float32)]\n\n\n\n\n\n\nSqueeze\n\n\nScala:\n\n\nval module = Squeeze(dims=null, numInputDims=Int.MinValue)\n\n\n\n\nPython:\n\n\nmodule = Squeeze(dims, numInputDims=-2147483648)\n\n\n\n\nDelete all singleton dimensions or a specific singleton dimension.\n\n\n\n\ndims\n Optional. If this dimension is singleton dimension, it will be deleted.\n           The first index starts from 1. Default: delete all dimensions.\n\n\nnum_input_dims\n Optional. If in a batch model, set to the inputDims.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = Squeeze(2)\n\n print(layer.forward(Tensor(2, 1, 3).rand()))\n0.43709445  0.42752415  0.43069172  \n0.67029667  0.95641375  0.28823504  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = Squeeze(2)\n\nlayer.forward(np.array([[[1, 2, 3]], [[1, 2, 3]]]))\nout: array([[ 1.,  2.,  3.],\n            [ 1.,  2.,  3.]], dtype=float32)\n\n\n\n\n\n\n\nSelect\n\n\nScala:\n\n\nval layer = Select(dim, index)\n\n\n\n\nPython:\n\n\nlayer = Select(dim, index)\n\n\n\n\nA Simple layer selecting an index of the input tensor in the given dimension.\nPlease note that the index and dimension start from 1. In collaborative filtering, it can used together with LookupTable to create embeddings for users or items.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = Select(1, 2)\nlayer.forward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\n\nlayer.backward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), Tensor(T(0.1f, 0.2f, 0.3f)))\n\n\n\n\nGives the output,\n\n\n4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n0.0     0.0     0.0\n0.1     0.2     0.3\n0.0     0.0     0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import Select\nimport numpy as np\n\nlayer = Select(1, 2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), np.array([0.1, 0.2, 0.3]))\n\n\n\n\nGives the output,\n\n\narray([ 4.,  5.,  6.], dtype=float32)\n\narray([[ 0.        ,  0.        ,  0.        ],\n       [ 0.1       ,  0.2       ,  0.30000001],\n       [ 0.        ,  0.        ,  0.        ]], dtype=float32)\n\n\n\n\n\n\nMaskedSelect\n\n\nScala:\n\n\nval module = MaskedSelect()\n\n\n\n\nPython:\n\n\nmodule = MaskedSelect()\n\n\n\n\nPerforms a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument\n with the input on the forward and backward passes.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport scala.util.Random\n\n\nval layer = MaskedSelect()\nval input1 = Tensor(2, 2).apply1(e =\n Random.nextFloat())\nval mask = Tensor(2, 2)\nmask(Array(1, 1)) = 1\nmask(Array(1, 2)) = 0\nmask(Array(2, 1)) = 0\nmask(Array(2, 2)) = 1\nval input = T()\ninput(1.0) = input1\ninput(2.0) = mask\n\n print(layer.forward(input))\n0.2577119\n0.5061479\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = MaskedSelect()\ninput1 = np.random.rand(2,2)\nmask = np.array([[1,0], [0, 1]])\n\nlayer.forward([input1, mask])\narray([ 0.1525335 ,  0.05474588], dtype=float32)\n\n\n\n\n\n\nTranspose\n\n\nScala:\n\n\nval module = Transpose(permutations)\n\n\n\n\nPython:\n\n\nmodule = Transpose(permutations)\n\n\n\n\nConcat is a layer who transpose input along specified dimensions.\npermutations are dimension pairs that need to swap.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).rand()\nval layer = Transpose(Array((1, 2)))\nval output = layer.forward(input)\n\n\n input\n0.6653826   0.25350887  0.33434764  \n0.9618287   0.5484164   0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\n\n output\n0.6653826   0.9618287   \n0.25350887  0.5484164   \n0.33434764  0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nlayer = Transpose([(1,2)])\ninput = np.array([[0.6653826, 0.25350887, 0.33434764], [0.9618287, 0.5484164, 0.64844745]])\noutput = layer.forward(input)\n\n\n output\n[array([[ 0.66538262,  0.96182871],\n       [ 0.25350887,  0.54841638],\n       [ 0.33434764,  0.64844745]], dtype=float32)]\n\n\n\n\n\n\n\nInferReshape\n\n\nScala:\n\n\nval layer = InferReshape(size, batchMode = false)\n\n\n\n\nPython:\n\n\nlayer = InferReshape(size, batch_mode=False)\n\n\n\n\nReshape the input tensor with automatic size inference support.\nPositive numbers in the \nsize\n argument are used to reshape the input to the\ncorresponding dimension size.\n\n\nThere are also two special values allowed in \nsize\n:\n\n\n\n\n0\n means keep the corresponding dimension size of the input unchanged.\n      i.e., if the 1st dimension size of the input is 2,\n      the 1st dimension size of output will be set as 2 as well.\n\n\n-1\n means infer this dimension size from other dimensions.\n      This dimension size is calculated by keeping the amount of output elements\n      consistent with the input.\n      Only one \n-1\n is allowable in \nsize\n.\n\n\n\n\nFor example,\n\n\n   Input tensor with size: (4, 5, 6, 7)\n   -\n InferReshape(Array(4, 0, 3, -1))\n   Output tensor with size: (4, 5, 3, 14)\n\n\n\n\nThe 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,\nand inferred the last dim as 14.\n\n\nParameters:\n\n \nsize\n the target tensor size\n\n \nbatchMode\n whether in batch mode\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.InferReshape\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval layer = InferReshape(Array(0, 3, -1))\nval input = Tensor(1, 2, 3).rand()\nval gradOutput = Tensor(1, 3, 2).rand()\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n0.8170822   0.40073588\n0.49389255  0.3782435\n0.42660004  0.5917206\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x2]\n\nprintln(grad)\n(1,.,.) =\n0.8294597   0.57101834  0.90910035\n0.32783163  0.30494633  0.7339092\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3]\n\n\n\n\nPython example:\n\n\nlayer = InferReshape([0, 3, -1])\ninput = np.random.rand(1, 2, 3)\n\ngradOutput = np.random.rand(1, 3, 2)\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 0.68635464  0.21277553]\n  [ 0.13390459  0.65662414]\n  [ 0.1021723   0.92319047]]]\n\nprint grad\n[[[ 0.84927064  0.55205333  0.25077972]\n  [ 0.76105869  0.30828172  0.1237276 ]]]\n\n\n\n\n\n\nReplicate\n\n\nScala:\n\n\nval module = Replicate(\n  nFeatures,\n  dim = 1,\n  nDim = Int.MaxValue)\n\n\n\n\nPython:\n\n\nmodule = Replicate(\n  n_features,\n  dim=1,\n  n_dim=INTMAX)\n\n\n\n\nReplicate repeats input \nnFeatures\n times along its \ndim\n dimension\n\n\nNotice: No memory copy, it set the stride along the \ndim\n-th dimension to zero.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Replicate(4, 1, 2)\n\nprintln(module.forward(Tensor.range(1, 6, 1).resize(1, 2, 3)))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,2,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,3,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,4,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Replicate(4, 1, 2)\n\nprint(module.forward(np.arange(1, 7, 1).reshape(1, 2, 3)))\n\n\n\n\nGives the output, \n\n\n[array([[[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]]]], dtype=float32)]\n\n\n\n\nView\n\n\nScala:\n\n\nval view = View(2, 8)\n\n\n\n\nor\n\n\nval view = View(Array(2, 8))\n\n\n\n\nPython:\n\n\nview = View([2, 8])\n\n\n\n\nThis module creates a new view of the input tensor using the sizes passed to the constructor.\nThe method setNumInputDims() allows to specify the expected number of dimensions of the inputs\nof the modules. This makes it possible to use minibatch inputs\nwhen using a size -1 for one of the dimensions.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.View\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval view = View(2, 8)\n\nval input = Tensor(4, 4).randn()\nval gradOutput = Tensor(2, 8).randn()\n\nval output = view.forward(input)\nval gradInput = view.backward(input, gradOutput)\n\n\n\n\nGives the output,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.43037438     1.2982363       -1.4723133      -0.2602826      0.7178128       -1.8763185      0.88629466      0.8346704\n0.20963766      -0.9349786      1.0376515       1.3153045       1.5450214       1.084113        -0.29929757     -0.18356979\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]\n\n\n\n\nGives the gradInput,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7360089       0.9133299       0.40443268      -0.94965595\n0.80520976      -0.09671917     -0.5498001      -0.098691925\n-2.3119886      -0.8455147      0.75891125      1.2985301\n0.5023749       1.4983269       0.42038065      -1.7002305\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nview = View([2, 8])\n\ninput = np.random.uniform(0, 1, [4, 4]).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, [2, 8]).astype(\nfloat32\n)\n\noutput = view.forward(input)\ngradInput = view.backward(input, gradOutput)\n\nprint output\nprint gradInput\n\n\n\n\n\n\nContiguous\n\n\nBe used to make input, gradOutput both contiguous\n\n\nScala:\n\n\nval contiguous = Contiguous()\n\n\n\n\nPython:\n\n\ncontiguous = Contiguous()\n\n\n\n\nUsed to make input, gradOutput both contiguous\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Contiguous\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(5).range(1, 5, 1)\nval contiguous = new Contiguous()\nval output = contiguous.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(5).range(2, 6, 1)\nval gradInput = contiguous.backward(input, gradOutput)\nprintln(gradOutput)\n\n\n\n\nGives the output,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n2.0\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\n\n\n\nGives the gradInput,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.0\n3.0\n4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ncontiguous = Contiguous()\n\ninput = np.arange(1, 6, 1).astype(\nfloat32\n)\ninput = input.reshape(1, 5)\n\noutput = contiguous.forward(input)\nprint output\n\ngradOutput = np.arange(2, 7, 1).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(1, 5)\n\ngradInput = contiguous.backward(input, gradOutput)\nprint gradInput\n\n\n\n\n\nGives the output,\n\n\n[array([[ 1.,  2.,  3.,  4.,  5.]], dtype=float32)]\n\n\n\n\nGives the gradInput,\n\n\n[array([[ 2.,  3.,  4.,  5.,  6.]], dtype=float32)]", 
            "title": "Simple Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#linear", 
            "text": "Scala:  val module = Linear(\n  inputSize,\n  outputSize,\n  withBias = true,\n  wRegularizer = null,\n  bRegularizer = null,\n  initWeight = null,\n  initBias = null,\n  initGradWeight = null,\n  initGradBias = null)  Python:  module = Linear(\n  input_size,\n  output_size,\n  init_method= default ,\n  with_bias=True,\n  wRegularizer=None,\n  bRegularizer=None,\n  init_weight=None,\n  init_bias=None,\n  init_grad_weight=None,\n  init_grad_bias=None)  The  Linear  module applies a linear transformation to the input data,\ni.e.  y = Wx + b . The  input  given in  forward(input)  must be either\na vector (1D tensor) or matrix (2D tensor). If the input is a vector, it must\nhave the size of  inputSize . If it is a matrix, then each row is assumed to be\nan input sample of given batch (the number of rows means the batch size and\nthe number of columns should be equal to the  inputSize ).  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Linear(3, 5)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))  Gives the output,  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.79338956\n-2.3417668\n-2.7557678\n-0.07507719\n-1.009765\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Linear(3, 5)\n\nprint(module.forward(np.arange(1, 4, 1)))  Gives the output,  [array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]", 
            "title": "Linear"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#reverse", 
            "text": "Scala:  val m = Reverse(dim = 1, isInplace = false)  Python:  m = Reverse(dimension=1)  Reverse the input w.r.t given dimension.\n The input can be a Tensor or Table.  Dimension  is one-based index.  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(0, 1)\nval input = Tensor(2, 3)\ninput.apply1(x =  randomn().toFloat)\nprintln( input: )\nprintln(input)\nval layer = new Reverse(1)\nprintln( output: )\nprintln(layer.forward(input))  input:\n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\noutput:\n0.4003877849318087  0.5567442716564983  0.14120339532382786 \n0.17271264898590744 0.019822501810267568    0.18107921979390085 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  input = np.random.random((2,3))\nlayer = Reverse(1)\nprint( input: )\nprint(input)\nprint( output: )\nprint(layer.forward(input))  creating: createReverse\ninput:\n[[ 0.89089717  0.07629756  0.30863782]\n [ 0.16066851  0.06421963  0.96719367]]\noutput:\n[[ 0.16066851  0.06421963  0.96719366]\n [ 0.89089715  0.07629756  0.30863783]]", 
            "title": "Reverse"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#reshape", 
            "text": "Scala:  val reshape = Reshape(size, batchMode)  Python:  reshape = Reshape(size, batch_mode)  The  forward(input)  reshape the input tensor into  size(0) * size(1) * ...  tensor,\ntaking the elements row-wise.  parameters:   size  the size after reshape   batchMode  It is a optional argument. If it is set to  Some(true) ,\n                  the first dimension of input is considered as batch dimension,\n                  and thus keep this dimension size fixed. This is necessary\n                  when dealing with batch sizes of one. When set to  Some(false) ,\n                  it forces the entire input (including the first dimension) to be reshaped\n                  to the input size. Default is  None , which means the module considers\n                  inputs with more elements than the product of provided sizes ( size(0) *\n                  size(1) * .. ) to be batches, otherwise in no batch mode.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval reshape = Reshape(Array(3, 2))\nval input = Tensor(2, 2, 3).rand()\nval output = reshape.forward(input)\n-  print(output.size().toList)      \nList(2, 3, 2)  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nreshape =  Reshape([3, 2])\ninput = np.random.rand(2, 2, 3)\noutput = reshape.forward(input)\n-  print output[0].shape\n(2, 3, 2)", 
            "title": "Reshape"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#index", 
            "text": "Scala:  val model = Index(dimension)  Python:  model = Index(dimension)  Applies the Tensor index operation along the given dimension.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).rand()\nval input2 = Tensor(4)\ninput2(Array(1)) = 1.0f\ninput2(Array(2)) = 2.0f\ninput2(Array(3)) = 2.0f\ninput2(Array(4)) = 3.0f\n\nval input = T(input1, input2)\nval model = Index(1)\nval output = model.forward(input)\n\nscala  print(input)\n {\n    2: 1.0\n       2.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4]\n    1: 0.124325536\n       0.8768922\n       0.6378146\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\nscala  print(output)\n0.124325536\n0.8768922\n0.8768922\n0.6378146\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.randn(3)\ninput2 = np.array([1, 2, 2, 3])\ninput = [input1, input2]\n\nmodel = Index(1)\noutput = model.forward(input)  print(input)\n[array([-0.45804847, -0.20176707,  0.50963248]), array([1, 2, 2, 3])]  print(output)\n[-0.45804846 -0.20176707 -0.20176707  0.50963247]", 
            "title": "Index"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#identity", 
            "text": "Scala:  val identity = Identity()  Python:  identity = Identity()  Identity just return input as the output which is useful in same parallel container to get an origin input  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval identity = Identity()\n\nval input = Tensor(3, 3).rand()  print(input)\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(identity.forward(input))\n0.043098174 0.1035049   0.7522675   \n0.9999951   0.794151    0.18344955  \n0.9419861   0.02398399  0.6228095   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  Python example:  from bigdl.nn.layer import *\nidentity = Identity()   identity.forward(np.array([[1, 2, 3], [4, 5, 6]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]], dtype=float32)]", 
            "title": "Identity"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#narrow", 
            "text": "Scala:  val layer = Narrow(dimension, offset, length = 1)  Python:  layer = Narrow(dimension, offset, length=1)  Narrow is an application of narrow operation in a module.\nThe module further supports a negative length in order to handle inputs with an unknown size.  Parameters:   dimension  narrow along this dimension   offset  the start index on the given dimension\n*  length  length to narrow, default value is 1  Scala Example  import com.intel.analytics.bigdl.nn.Narrow\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Narrow(2, 2)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\n2.0\n3.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]\n\nval grad = layer.backward(input, gradOutput)\n0.0 3.0 0.0\n0.0 4.0 0.0\n0.0 5.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python Example  layer = Narrow(2, 2)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[ 2.]\n [ 3.]\n [ 4.]]\n\nprint grad\n[[ 0.  3.  0.]\n [ 0.  4.  0.]\n [ 0.  5.  0.]]", 
            "title": "Narrow"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#unsqueeze", 
            "text": "Scala:  val layer = Unsqueeze(dim)  Python:  layer = Unsqueeze(dim)  Insert singleton dim (i.e., dimension 1) at position pos. For an input with  dim = input.dim() ,\nthere are  dim + 1  possible positions to insert the singleton dimension. The dim starts from 1.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval layer = Unsqueeze(2)\nval input = Tensor(2, 2, 2).rand\nval gradOutput = Tensor(2, 1, 2, 2).rand\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)  println(input.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  println(gradOutput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]  println(output.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x2x2]  println(gradInput.size)\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nlayer = Unsqueeze(2)\ninput = np.random.uniform(0, 1, (2, 2, 2)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (2, 1, 2, 2)).astype( float32 )\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)  output\n[array([[[[ 0.97488612,  0.43463323],\n          [ 0.39069486,  0.0949123 ]]],\n\n\n        [[[ 0.19310953,  0.73574477],\n          [ 0.95347691,  0.37380624]]]], dtype=float32)]  gradInput\n[array([[[ 0.9995622 ,  0.69787127],\n         [ 0.65975296,  0.87002522]],\n\n        [[ 0.76349133,  0.96734989],\n         [ 0.88068211,  0.07284366]]], dtype=float32)]", 
            "title": "Unsqueeze"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#squeeze", 
            "text": "Scala:  val module = Squeeze(dims=null, numInputDims=Int.MinValue)  Python:  module = Squeeze(dims, numInputDims=-2147483648)  Delete all singleton dimensions or a specific singleton dimension.   dims  Optional. If this dimension is singleton dimension, it will be deleted.\n           The first index starts from 1. Default: delete all dimensions.  num_input_dims  Optional. If in a batch model, set to the inputDims.   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = Squeeze(2)  print(layer.forward(Tensor(2, 1, 3).rand()))\n0.43709445  0.42752415  0.43069172  \n0.67029667  0.95641375  0.28823504  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\nlayer = Squeeze(2) layer.forward(np.array([[[1, 2, 3]], [[1, 2, 3]]]))\nout: array([[ 1.,  2.,  3.],\n            [ 1.,  2.,  3.]], dtype=float32)", 
            "title": "Squeeze"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#select", 
            "text": "Scala:  val layer = Select(dim, index)  Python:  layer = Select(dim, index)  A Simple layer selecting an index of the input tensor in the given dimension.\nPlease note that the index and dimension start from 1. In collaborative filtering, it can used together with LookupTable to create embeddings for users or items.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = Select(1, 2)\nlayer.forward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\n\nlayer.backward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), Tensor(T(0.1f, 0.2f, 0.3f)))  Gives the output,  4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n0.0     0.0     0.0\n0.1     0.2     0.3\n0.0     0.0     0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import Select\nimport numpy as np\n\nlayer = Select(1, 2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), np.array([0.1, 0.2, 0.3]))  Gives the output,  array([ 4.,  5.,  6.], dtype=float32)\n\narray([[ 0.        ,  0.        ,  0.        ],\n       [ 0.1       ,  0.2       ,  0.30000001],\n       [ 0.        ,  0.        ,  0.        ]], dtype=float32)", 
            "title": "Select"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#maskedselect", 
            "text": "Scala:  val module = MaskedSelect()  Python:  module = MaskedSelect()  Performs a torch.MaskedSelect on a Tensor. The mask is supplied as a tabular argument\n with the input on the forward and backward passes.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport scala.util.Random\n\n\nval layer = MaskedSelect()\nval input1 = Tensor(2, 2).apply1(e =  Random.nextFloat())\nval mask = Tensor(2, 2)\nmask(Array(1, 1)) = 1\nmask(Array(1, 2)) = 0\nmask(Array(2, 1)) = 0\nmask(Array(2, 2)) = 1\nval input = T()\ninput(1.0) = input1\ninput(2.0) = mask  print(layer.forward(input))\n0.2577119\n0.5061479\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]  Python example:  from bigdl.nn.layer import *\n\nlayer = MaskedSelect()\ninput1 = np.random.rand(2,2)\nmask = np.array([[1,0], [0, 1]]) layer.forward([input1, mask])\narray([ 0.1525335 ,  0.05474588], dtype=float32)", 
            "title": "MaskedSelect"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#transpose", 
            "text": "Scala:  val module = Transpose(permutations)  Python:  module = Transpose(permutations)  Concat is a layer who transpose input along specified dimensions.\npermutations are dimension pairs that need to swap.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).rand()\nval layer = Transpose(Array((1, 2)))\nval output = layer.forward(input)  input\n0.6653826   0.25350887  0.33434764  \n0.9618287   0.5484164   0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]  output\n0.6653826   0.9618287   \n0.25350887  0.5484164   \n0.33434764  0.64844745  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nlayer = Transpose([(1,2)])\ninput = np.array([[0.6653826, 0.25350887, 0.33434764], [0.9618287, 0.5484164, 0.64844745]])\noutput = layer.forward(input)  output\n[array([[ 0.66538262,  0.96182871],\n       [ 0.25350887,  0.54841638],\n       [ 0.33434764,  0.64844745]], dtype=float32)]", 
            "title": "Transpose"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#inferreshape", 
            "text": "Scala:  val layer = InferReshape(size, batchMode = false)  Python:  layer = InferReshape(size, batch_mode=False)  Reshape the input tensor with automatic size inference support.\nPositive numbers in the  size  argument are used to reshape the input to the\ncorresponding dimension size.  There are also two special values allowed in  size :   0  means keep the corresponding dimension size of the input unchanged.\n      i.e., if the 1st dimension size of the input is 2,\n      the 1st dimension size of output will be set as 2 as well.  -1  means infer this dimension size from other dimensions.\n      This dimension size is calculated by keeping the amount of output elements\n      consistent with the input.\n      Only one  -1  is allowable in  size .   For example,     Input tensor with size: (4, 5, 6, 7)\n   -  InferReshape(Array(4, 0, 3, -1))\n   Output tensor with size: (4, 5, 3, 14)  The 1st and 3rd dim are set to given sizes, keep the 2nd dim unchanged,\nand inferred the last dim as 14.  Parameters:   size  the target tensor size   batchMode  whether in batch mode  Scala example:  import com.intel.analytics.bigdl.nn.InferReshape\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval layer = InferReshape(Array(0, 3, -1))\nval input = Tensor(1, 2, 3).rand()\nval gradOutput = Tensor(1, 3, 2).rand()\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n0.8170822   0.40073588\n0.49389255  0.3782435\n0.42660004  0.5917206\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x2]\n\nprintln(grad)\n(1,.,.) =\n0.8294597   0.57101834  0.90910035\n0.32783163  0.30494633  0.7339092\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3]  Python example:  layer = InferReshape([0, 3, -1])\ninput = np.random.rand(1, 2, 3)\n\ngradOutput = np.random.rand(1, 3, 2)\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 0.68635464  0.21277553]\n  [ 0.13390459  0.65662414]\n  [ 0.1021723   0.92319047]]]\n\nprint grad\n[[[ 0.84927064  0.55205333  0.25077972]\n  [ 0.76105869  0.30828172  0.1237276 ]]]", 
            "title": "InferReshape"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#replicate", 
            "text": "Scala:  val module = Replicate(\n  nFeatures,\n  dim = 1,\n  nDim = Int.MaxValue)  Python:  module = Replicate(\n  n_features,\n  dim=1,\n  n_dim=INTMAX)  Replicate repeats input  nFeatures  times along its  dim  dimension  Notice: No memory copy, it set the stride along the  dim -th dimension to zero.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Replicate(4, 1, 2)\n\nprintln(module.forward(Tensor.range(1, 6, 1).resize(1, 2, 3)))  Gives the output,  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,2,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,3,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n(1,4,.,.) =\n1.0 2.0 3.0\n4.0 5.0 6.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x2x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Replicate(4, 1, 2)\n\nprint(module.forward(np.arange(1, 7, 1).reshape(1, 2, 3)))  Gives the output,   [array([[[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]]]], dtype=float32)]", 
            "title": "Replicate"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#view", 
            "text": "Scala:  val view = View(2, 8)  or  val view = View(Array(2, 8))  Python:  view = View([2, 8])  This module creates a new view of the input tensor using the sizes passed to the constructor.\nThe method setNumInputDims() allows to specify the expected number of dimensions of the inputs\nof the modules. This makes it possible to use minibatch inputs\nwhen using a size -1 for one of the dimensions.  Scala example:  import com.intel.analytics.bigdl.nn.View\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval view = View(2, 8)\n\nval input = Tensor(4, 4).randn()\nval gradOutput = Tensor(2, 8).randn()\n\nval output = view.forward(input)\nval gradInput = view.backward(input, gradOutput)  Gives the output,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.43037438     1.2982363       -1.4723133      -0.2602826      0.7178128       -1.8763185      0.88629466      0.8346704\n0.20963766      -0.9349786      1.0376515       1.3153045       1.5450214       1.084113        -0.29929757     -0.18356979\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]  Gives the gradInput,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7360089       0.9133299       0.40443268      -0.94965595\n0.80520976      -0.09671917     -0.5498001      -0.098691925\n-2.3119886      -0.8455147      0.75891125      1.2985301\n0.5023749       1.4983269       0.42038065      -1.7002305  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nview = View([2, 8])\n\ninput = np.random.uniform(0, 1, [4, 4]).astype( float32 )\ngradOutput = np.random.uniform(0, 1, [2, 8]).astype( float32 )\n\noutput = view.forward(input)\ngradInput = view.backward(input, gradOutput)\n\nprint output\nprint gradInput", 
            "title": "View"
        }, 
        {
            "location": "/APIGuide/Layers/Simple-Layers/#contiguous", 
            "text": "Be used to make input, gradOutput both contiguous  Scala:  val contiguous = Contiguous()  Python:  contiguous = Contiguous()  Used to make input, gradOutput both contiguous  Scala example:  import com.intel.analytics.bigdl.nn.Contiguous\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(5).range(1, 5, 1)\nval contiguous = new Contiguous()\nval output = contiguous.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(5).range(2, 6, 1)\nval gradInput = contiguous.backward(input, gradOutput)\nprintln(gradOutput)  Gives the output,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n2.0\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]  Gives the gradInput,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.0\n3.0\n4.0\n5.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ncontiguous = Contiguous()\n\ninput = np.arange(1, 6, 1).astype( float32 )\ninput = input.reshape(1, 5)\n\noutput = contiguous.forward(input)\nprint output\n\ngradOutput = np.arange(2, 7, 1).astype( float32 )\ngradOutput = gradOutput.reshape(1, 5)\n\ngradInput = contiguous.backward(input, gradOutput)\nprint gradInput  Gives the output,  [array([[ 1.,  2.,  3.,  4.,  5.]], dtype=float32)]  Gives the gradInput,  [array([[ 2.,  3.,  4.,  5.,  6.]], dtype=float32)]", 
            "title": "Contiguous"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/", 
            "text": "SpatialConvolution\n\n\nScala:\n\n\nval m = SpatialConvolution(nInputPlane,nOutputPlane,kernelW,kernelH,strideW=1,strideH=1,padW=0,padH=0,nGroup=1,propagateBack=true,wRegularizer=null,bRegularizer=null,initWeight=null, initBias=null, initGradWeight=null, initGradBias=null, withBias=true, dataFormat=DataFormat.NCHW)\n\n\n\n\nPython:\n\n\nm = SpatialConvolution(n_input_plane,n_output_plane,kernel_w,kernel_h,stride_w=1,stride_h=1,pad_w=0,pad_h=0,n_group=1,propagate_back=True,wRegularizer=None,bRegularizer=None,init_weight=None,init_bias=None,init_grad_weight=None,init_grad_bias=None, with_bias=True, data_format=\nNCHW\n)\n\n\n\n\nSpatialConvolution is a module that applies a 2D convolution over an input image.\n\n\nThe input tensor in \nforward(input)\n is expected to be\neither a 4D tensor (\nbatch x nInputPlane x height x width\n) or a 3D tensor (\nnInputPlane x height x width\n). The convolution is performed on the last two dimensions.\n\n\nAs for padding, when padW and padH are both -1, we use a padding algorithm similar to the \"SAME\" padding of tensorflow. That is\n\n\n outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)\n outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)\n\n padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)\n padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)\n\n padTop = padAlongHeight / 2\n padLeft = padAlongWidth / 2\n\n\n\n\nDetailed paramter explaination for the constructor.\n\n\n\n\nnInputPlane\n The number of expected input planes in the image given into forward()\n\n\nnOutputPlane\n The number of output planes the convolution layer will produce.\n\n\nkernelW\n The kernel width of the convolution\n\n\nkernelH\n The kernel height of the convolution\n\n\nstrideW\n The step of the convolution in the width dimension.\n\n\nstrideH\n The step of the convolution in the height dimension\n\n\npadW\n  padding to be added to width to the input.\n\n\npadH\n padding to be added to height to the input.\n\n\nnGroup\n Kernel group number\n\n\npropagateBack\n whether to propagate gradient back\n\n\nwRegularizer\n regularizer on weight. an instance of [[Regularizer]] (e.g. L1 or L2)\n\n\nbRegularizer\n regularizer on bias. an instance of [[Regularizer]] (e.g. L1 or L2).\n\n\ninitWeight\n weight initializer\n\n\ninitBias\n  bias initializer\n\n\ninitGradWeight\n weight gradient initializer\n\n\ninitGradBias\n bias gradient initializer\n\n\nwith_bias\n the optional initial value for if need bias\n\n\ndata_format\n a string value (or DataFormat Object in Scala) of \"NHWC\" or \"NCHW\" to specify the input data format of this layer. In \"NHWC\" format\n                        data is stored in the order of [batch_size, height, width, channels], in \"NCHW\" format data is stored\n                        in the order of [batch_size, channels, height, width].\n\n\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialConvolution(2,1,2,2,1,1,0,0)\nm.setInitMethod(weightInitMethod = BilinearFiller, biasInitMethod = Zeros)\nval params = m.getParameters()\n\nscala\n print(params)\n(1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9],0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9])\n\nscala\n\nval input = Tensor(1,2,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,1,2,2).fill(0.2f)\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n(1,1,.,.) =\n-0.37011376     0.13565119      -0.73574775\n-0.19486316     -0.4430604      -0.62543416\n0.7017611       -0.6441595      -1.2953792\n\n(1,2,.,.) =\n-0.9903588      0.5669722       0.2630131\n0.03392942      -0.6984676      -0.12389368\n0.78704715      0.5411976       -1.3877676\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3]\n\nscala\n print(output)\n(1,1,.,.) =\n-1.3604726      0.70262337\n-0.16093373     -1.141528\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\nscala\n print(gradOut)\n(1,1,.,.) =\n0.2     0.2\n0.2     0.2\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x2x2]\n\nscala\n print(gradIn)\n(1,1,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n(1,2,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(1,3,3,3)\nprint \ninput is :\n,input\n\nm = SpatialConvolution(3,1,2,2,1,1,0,0)\nout = m.forward(input)\nprint \noutput m is :\n,out\n\ngrad_out = np.random.rand(1,1,2,2)\nprint \ngrad out of m is :\n,grad_out\ngrad_in = m.backward(input,grad_out)\nprint \ngrad input of m is :\n,grad_in\n\n\n\n\nGives the output,\n\n\ninput is : [[[[ 0.75276617  0.44212513  0.90275949]\n   [ 0.78205279  0.77864714  0.83647254]\n   [ 0.76220944  0.22106036  0.68762202]]\n\n  [[ 0.37346971  0.31532213  0.33276243]\n   [ 0.69872884  0.07262236  0.66372462]\n   [ 0.47803013  0.80194459  0.53313873]]\n\n  [[ 0.56196833  0.20599878  0.47575818]\n   [ 0.35454298  0.96910557  0.36234704]\n   [ 0.64017738  0.95762579  0.50073035]]]]\ncreating: createSpatialConvolution\noutput m is : [[[[-1.08398974 -0.67615652]\n   [-0.77027249 -0.82885492]]]]\ngrad out of m is : [[[[ 0.38295452  0.77048361]\n   [ 0.11671955  0.76357513]]]]\ngrad input of m is : [[[[-0.02344826 -0.06515953 -0.03618064]\n   [-0.06770924 -0.22586647 -0.14004168]\n   [-0.01845866 -0.13653883 -0.10325129]]\n\n  [[-0.09294108 -0.14361492  0.08727306]\n   [-0.09885897 -0.21209857  0.29151234]\n   [-0.02149716 -0.10957514  0.20318349]]\n\n  [[-0.05926216 -0.04542646  0.14849319]\n   [-0.09506465 -0.34244278 -0.03763583]\n   [-0.02346931 -0.1815301  -0.18314059]]]]\n\n\n\n\nVolumetricConvolution\n\n\nScala:\n\n\nval module = VolumetricConvolution(nInputPlane, nOutputPlane, kT, kW, kH,\n  dT=1, dW=1, dH=1, padT=0, padW=0, padH=0, withBias=true, wRegularizer=null, bRegularizer=null)\n\n\n\n\nPython:\n\n\nmodule = VolumetricConvolution(n_input_plane, n_output_plane, k_t, k_w, k_h,\n  d_t=1, d_w=1, d_h=1, pad_t=0, pad_w=0, pad_h=0, with_bias=true, wRegularizer=null, bRegularizer=null)\n\n\n\n\nApplies a 3D convolution over an input image composed of several input planes. The input tensor\nin forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).\n\n\n\n\nnInputPlane\n The number of expected input planes in the image given into forward()\n\n\nnOutputPlane\n The number of output planes the convolution layer will produce.\n\n\nkT\n The kernel size of the convolution in time\n\n\nkW\n The kernel width of the convolution\n\n\nkH\n The kernel height of the convolution\n\n\ndT\n The step of the convolution in the time dimension. Default is 1\n\n\ndW\n The step of the convolution in the width dimension. Default is 1\n\n\ndH\n The step of the convolution in the height dimension. Default is 1\n\n\npadT\n Additional zeros added to the input plane data on both sides of time axis.\n         Default is 0. \n(kT-1)/2\n is often used here.\n\n\npadW\n The additional zeros added per width to the input planes.\n\n\npadH\n The additional zeros added per height to the input planes.\n\n\nwithBias\n whether with bias.\n\n\nwRegularizer\n instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nbRegularizer\n instance of [[Regularizer]]\n                   applied to the bias.\n\n\n\n\nScala example:\n\n\nval layer = VolumetricConvolution(2, 3, 2, 2, 2, dT=1, dW=1, dH=1,\n  padT=0, padW=0, padH=0, withBias=true)\nval input = Tensor(2, 2, 2, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.54846555      0.5549177\n0.43748873      0.6596535\n\n(1,2,.,.) =\n0.87915933      0.5955469\n0.67464 0.40921077\n\n(2,1,.,.) =\n0.24127467      0.49356017\n0.6707502       0.5421975\n\n(2,2,.,.) =\n0.007834963     0.08188637\n0.51387626      0.7376101\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\nlayer.forward(input)\nres16: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.6680023\n\n(2,1,.,.) =\n0.41926455\n\n(3,1,.,.) =\n-0.029196609\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x1x1]\n\n\n\n\nPython example:\n\n\nlayer = VolumetricConvolution(2, 3, 2, 2, 2, d_t=1, d_w=1, d_h=1,\n          pad_t=0, pad_w=0, pad_h=0, with_bias=True, init_method=\ndefault\n,\n          bigdl_type=\nfloat\n)\ninput = np.random.rand(2,2,2,2)\n array([[[[ 0.47639062,  0.76800312],\n         [ 0.28834351,  0.21883535]],\n\n        [[ 0.86097919,  0.89812597],\n         [ 0.43632181,  0.58004824]]],\n\n\n       [[[ 0.65784027,  0.34700039],\n         [ 0.64511955,  0.1660241 ]],\n\n        [[ 0.36060054,  0.71265665],\n         [ 0.51755249,  0.6508298 ]]]])\n\nlayer.forward(input)\narray([[[[ 0.54268712]]],\n\n\n       [[[ 0.17670505]]],\n\n\n       [[[ 0.40953237]]]], dtype=float32)\n\n\n\n\n\nSpatialDilatedConvolution\n\n\nScala:\n\n\nval layer = SpatialDilatedConvolution(\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)\n\n\n\n\nPython:\n\n\nlayer = SpatialDilatedConvolution(\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)\n\n\n\n\nApply a 2D dilated convolution over an input image.\n\n\nThe input tensor is expected to be a 3D or 4D(with batch) tensor.\n\n\nFor a normal SpatialConvolution, the kernel will multiply with input\nimage element-by-element contiguous. In dilated convolution, it\u2019s possible\nto have filters that have spaces between each cell. For example, filter w and\nimage x, when dilatiionW and dilationH both = 1, this is normal 2D convolution\n\n\nw(0, 0) * x(0, 0), w(0, 1) * x(0, 1)\nw(1, 0) * x(1, 0), w(1, 1) * x(1, 1)\n\n\n\n\nwhen dilationW and dilationH both = 2\n\n\nw(0, 0) * x(0, 0), w(0, 1) * x(0, 2)\nw(1, 0) * x(2, 0), w(1, 1) * x(2, 2)\n\n\n\n\nwhen dilationW and dilationH both = 3\n\n\nw(0, 0) * x(0, 0), w(0, 1) * x(0, 3)\nw(1, 0) * x(3, 0), w(1, 1) * x(3, 3)\n\n\n\n\nIf input is a 3D tensor nInputPlane x height x width,\n * \nowidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1\n\n * \noheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1\n\n\nReference Paper:\n\n\n\n\nYu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].\narXiv preprint arXiv:1511.07122, 2015.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SpatialDilatedConvolution(1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\nval input = Tensor(T(T(\n  T(1.0f, 2.0f, 3.0f, 4.0f),\n  T(5.0f, 6.0f, 7.0f, 8.0f),\n  T(9.0f, 1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f, 7.0f)\n)))\nval filter = Tensor(T(T(T(\n  T(1.0f, 1.0f),\n  T(1.0f, 1.0f)\n))))\nlayer.weight.copy(filter)\nlayer.bias.zero()\nlayer.forward(input)\nlayer.backward(input, Tensor(T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n))))\n\n\n\n\nGives the output,\n\n\n(1,.,.) =\n15.0    10.0\n22.0    26.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n(1,.,.) =\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import SpatialDilatedConvolution\nimport numpy as np\n\nlayer = SpatialDilatedConvolution(1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\ninput = np.array([[\n  [1.0, 2.0, 3.0, 4.0],\n  [5.0, 6.0, 7.0, 8.0],\n  [9.0, 1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0, 7.0]\n]])\nfilter = np.array([[[\n  [1.0, 1.0],\n  [1.0, 1.0]\n]]])\nbias = np.array([0.0])\nlayer.set_weights([filter, bias])\nlayer.forward(input)\nlayer.backward(input, np.array([[[0.1, 0.2], [0.3, 0.4]]]))\n\n\n\n\nGives the output,\n\n\narray([[[ 15.,  10.],\n        [ 22.,  26.]]], dtype=float32)\n\narray([[[ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001],\n        [ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001]]], dtype=float32)\n\n\n\n\n\n\n\nSpatialShareConvolution\n\n\nScala:\n\n\nval layer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)\n\n\n\n\nPython:\n\n\nlayer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n\n\n\n\nApplies a 2D convolution over an input image composed of several input planes.\n The input tensor in forward(input) is expected to be\n a 3D tensor (nInputPlane x height x width).\n\n\nThis layer has been optimized to save memory. If using this layer to construct multiple convolution\n layers, please add sharing script for the fInput and fGradInput. Please refer to the ResNet example.\n\n\nScala example:\n\n\n\n    import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n    import com.intel.analytics.bigdl.nn._\n    import com.intel.analytics.bigdl.tensor._\n\n    val nInputPlane = 1\n    val nOutputPlane = 1\n    val kW = 2\n    val kH = 2\n    val dW = 1\n    val dH = 1\n    val padW = 0\n    val padH = 0\n    val layer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)\n\n    val inputData = Array(\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1\n    )\n\n    val kernelData = Array(\n      2.0, 3,\n      4, 5\n    )\n\n    val biasData = Array(0.0)\n\n    layer.weight.copy(Tensor(Storage(kernelData), 1,\n      Array(nOutputPlane, nInputPlane, kH, kW)))\n    layer.bias.copy(Tensor(Storage(biasData), 1, Array(nOutputPlane)))\n\n    val input = Tensor(Storage(inputData), 1, Array(3, 1, 3, 4))\n    val output = layer.updateOutput(input)\n\n    \n output\nres2: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(2,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(3,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n\n\n\nPython example:\n\n\nnInputPlane = 1\nnOutputPlane = 1\nkW = 2\nkH = 2\ndW = 1\ndH = 1\npadW = 0\npadH = 0\nlayer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n\ninput = np.array([\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1]\n    ).astype(\nfloat32\n).reshape(3, 1, 3, 4)\nlayer.forward(input)\n\n\n print (output)\narray([[[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]]], dtype=float32)\n\n\n\n\n\n\nSpatialFullConvolution\n\n\nScala:\n\n\nval m  = SpatialFullConvolution(nInputPlane, nOutputPlane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0,nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)\n\n\n\n\nPython:\n\n\nm = SpatialFullConvolution(n_input_plane,n_output_plane,kw,kh,dw=1,dh=1,pad_w=0,pad_h=0,adj_w=0,adj_h=0,n_group=1,no_bias=False,init_method='default',wRegularizer=None,bRegularizer=None)\n\n\n\n\nSpatialFullConvolution is a module that applies a 2D full convolution over an input image. \n\n\nThe input tensor in \nforward(input)\n is expected to be\neither a 4D tensor (\nbatch x nInputPlane x height x width\n) or a 3D tensor (\nnInputPlane x height x width\n). The convolution is performed on the last two dimensions. \nadjW\n and \nadjH\n are used to adjust the size of the output image. The size of output tensor of \nforward\n will be :\n\n\n  output width  = (width  - 1) * dW - 2*padW + kW + adjW\n  output height = (height - 1) * dH - 2*padH + kH + adjH\n\n\n\n\nNote, scala API also accepts a table input with two tensors: \nT(convInput, sizeTensor)\n where \nconvInput\n is the standard input tensor, and the size of \nsizeTensor\n is used to set the size of the output (will ignore the \nadjW\n and \nadjH\n values used to construct the module). Use \nSpatialFullConvolution[Table, T](...)\n instead of \nSpatialFullConvolution[Tensor,T](...)\n) for table input.\n\n\nThis module can also be used without a bias by setting parameter \nnoBias = true\n while constructing the module.\n\n\nOther frameworks may call this operation \"In-network Upsampling\", \"Fractionally-strided convolution\", \"Backwards Convolution,\" \"Deconvolution\", or \"Upconvolution.\"\n\n\nReference: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3431-3440.\n\n\nDetailed explaination of arguments in constructor. \n\n\n\n\nnInputPlane\n The number of expected input planes in the image given into forward()\n\n\nnOutputPlane\n The number of output planes the convolution layer will produce.\n\n\nkW\n The kernel width of the convolution.\n\n\nkH\n The kernel height of the convolution.\n\n\ndW\n The step of the convolution in the width dimension. Default is 1.\n\n\ndH\n The step of the convolution in the height dimension. Default is 1.\n\n\npadW\n The additional zeros added per width to the input planes. Default is 0.\n\n\npadH\n The additional zeros added per height to the input planes. Default is 0.\n\n\nadjW\n Extra width to add to the output image. Default is 0.\n\n\nadjH\n Extra height to add to the output image. Default is 0.\n\n\nnGroup\n Kernel group number.\n\n\nnoBias\n If bias is needed.\n\n\nwRegularizer\n instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nbRegularizer\n instance of [[Regularizer]]\n                   applied to the bias.\n\n\n\n\nScala example:\n\n\nTensor Input example: \n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input = Tensor(1,1,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n(1,1,.,.) =\n0.18219171      1.3252861       -1.3991559\n0.82611334      1.0313315       0.6075537\n-0.7336061      0.3156875       -0.70616096\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x3x3]\n\nscala\n print(output)\n(1,1,.,.) =\n-0.49278542     -0.5823938      -0.8304068      -0.077556044\n-0.5028842      -0.7281958      -1.1927067      -0.34262076\n-0.41680115     -0.41400516     -0.7599415      -0.42024887\n-0.5286566      -0.30015367     -0.5997892      -0.32439864\n\n(1,2,.,.) =\n-0.13131973     -0.5770084      1.1069719       -0.6003375\n-0.40302444     -0.07293816     -0.2654545      0.39749345\n0.37311426      -0.49090374     0.3088816       -0.41700447\n-0.12861171     0.09394867      -0.17229918     0.05556257\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x4]\n\nscala\n print(gradOut)\n(1,1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(1,2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x4x4]\n\nscala\n print(gradIn)\n(1,1,.,.) =\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x3x3]\n\n\n\n\n\n\nTable input Example\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.{T, Table}\n\nval m = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input1 = Tensor(1, 3, 3).randn()\nval input2 = Tensor(3, 3).fill(2.0f)\nval input = T(input1, input2)\nval output = m.forward(input)\nval gradOut = Tensor(2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n {\n        2: 2.0  2.0     2.0\n           2.0  2.0     2.0\n           2.0  2.0     2.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: (1,.,.) =\n           1.276177     0.62761325      0.2715257\n           -0.030832397 0.5046206       0.6835176\n           -0.5832693   0.17266633      0.7461992\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n }\n\nscala\n print(output)\n(1,.,.) =\n-0.18339296     0.04208675      -0.17708774     -0.30901802\n-0.1484881      0.23592418      0.115615785     -0.11288056\n-0.47266048     -0.41772115     0.07501307      0.041751802\n-0.4851033      -0.5427048      -0.18293871     -0.12682784\n\n(2,.,.) =\n0.6391188       0.845774        0.41208875      0.13754106\n-0.45785713     0.31221163      0.6006259       0.36563575\n-0.24076991     -0.31931365     0.31651747      0.4836449\n0.24247466      -0.16731171     -0.20887817     0.19513035\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x4]\n\nscala\n print(gradOut)\n(1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x4x4]\n\nscala\n print(gradIn)\n {\n        2: 0.0  0.0     0.0\n           0.0  0.0     0.0\n           0.0  0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: (1,.,.) =\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nm = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, False)\n\nprint \n--------- tensor input---------\n\ntensor_input = np.random.rand(1,3,3)\nprint \ninput is :\n,tensor_input\nout = m.forward(tensor_input)\nprint \noutput m is :\n,out\n\nprint \n----------- table input --------\n\nadj_input=np.empty([3,3])\nadj_input.fill(2.0)\ntable_input = [tensor_input,adj_input]\nprint \ninput is :\n,table_input\nout = m.forward(table_input)\nprint \noutput m is :\n,out\n\n\n\n\nGives the output,\n\n\ncreating: createSpatialFullConvolution\n--------- tensor input---------\ninput is : [[[  9.03998497e-01   4.43054896e-01   6.19571211e-01]\n  [  4.24573060e-01   3.29886286e-04   5.48427154e-02]\n  [  8.99004782e-01   3.25514441e-01   6.85294650e-01]]]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]\n----------- table input --------\ninput is : [array([[[  9.03998497e-01,   4.43054896e-01,   6.19571211e-01],\n        [  4.24573060e-01,   3.29886286e-04,   5.48427154e-02],\n        [  8.99004782e-01,   3.25514441e-01,   6.85294650e-01]]]), array([[ 2.,  2.,  2.],\n       [ 2.,  2.,  2.],\n       [ 2.,  2.,  2.]])]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]\n\n\n\n\nSpatialConvolutionMap\n\n\nScala:\n\n\nval layer = SpatialConvolutionMap(\n  connTable,\n  kW,\n  kH,\n  dW = 1,\n  dH = 1,\n  padW = 0,\n  padH = 0,\n  wRegularizer = null,\n  bRegularizer = null)\n\n\n\n\nPython:\n\n\nlayer = SpatialConvolutionMap(\n  conn_table,\n  kw,\n  kh,\n  dw=1,\n  dh=1,\n  pad_w=0,\n  pad_h=0,\n  wRegularizer=None,\n  bRegularizer=None)\n\n\n\n\nThis class is a generalization of SpatialConvolution.\nIt uses a generic connection table between input and output features.\nThe SpatialConvolution is equivalent to using a full connection table.\n\nA Connection Table is the mapping of input/output feature map, stored in a 2D Tensor. The first column is the input feature maps. The second column is output feature maps.\n\n\nFull Connection table:\n\n\nval conn = SpatialConvolutionMap.full(nin: Int, nout: In)\n\n\n\n\nOne to One connection table:\n\n\nval conn = SpatialConvolutionMap.oneToOne(nfeat: Int)\n\n\n\n\nRandom Connection table:\n\n\nval conn = SpatialConvolutionMap.random(nin: Int, nout: Int, nto: Int)\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval conn = SpatialConvolutionMap.oneToOne(3)\n\n\n\n\nconn\n is\n\n\nconn: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 1.0\n2.0 2.0\n3.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\n\n\n\nval module = SpatialConvolutionMap(SpatialConvolutionMap.oneToOne(3), 2, 2)\n\npritnln(module.forward(Tensor.range(1, 48, 1).resize(3, 4, 4)))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n4.5230045   5.8323975   7.1417904\n9.760576    11.069969   12.379362\n14.998148   16.30754    17.616934\n\n(2,.,.) =\n-5.6122046  -5.9227824  -6.233361\n-6.8545156  -7.165093   -7.4756703\n-8.096827   -8.407404   -8.71798\n\n(3,.,.) =\n13.534529   13.908197   14.281864\n15.029203   15.402873   15.77654\n16.523876   16.897545   17.271214\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = SpatialConvolutionMap(np.array([(1, 1), (2, 2), (3, 3)]), 2, 2)\n\nprint(module.forward(np.arange(1, 49, 1).reshape(3, 4, 4)))\n\n\n\n\nGives the output,\n\n\n[array([[[-1.24280548, -1.70889318, -2.17498088],\n        [-3.10715604, -3.57324386, -4.03933144],\n        [-4.97150755, -5.43759441, -5.90368223]],\n\n       [[-5.22062826, -5.54696751, -5.87330723],\n        [-6.52598572, -6.85232496, -7.17866373],\n        [-7.8313427 , -8.15768337, -8.48402214]],\n\n       [[ 0.5065825 ,  0.55170798,  0.59683061],\n        [ 0.68707776,  0.73219943,  0.77732348],\n        [ 0.86757064,  0.91269422,  0.95781779]]], dtype=float32)]\n\n\n\n\nTemporalConvolution\n\n\nScala:\n\n\nval module = TemporalConvolution(\n  inputFrameSize, outputFrameSize, kernelW, strideW = 1, propagateBack = true,\n  wRegularizer = null, bRegularizer = null, initWeight = null, initBias = null,\n  initGradWeight = null, initGradBias = null\n  )\n\n\n\n\nPython:\n\n\nmodule = TemporalConvolution(\n  input_frame_size, output_frame_size, kernel_w, stride_w = 1, propagate_back = True,\n  w_regularizer = None, b_regularizer = None, init_weight = None, init_bias = None,\n  init_grad_weight = None, init_grad_bias = None\n  )\n\n\n\n\nApplies a 1D convolution over an input sequence composed of nInputFrame frames.\nThe input tensor in \nforward(input)\n is expected to be a 2D tensor\n(\nnInputFrame\n x \ninputFrameSize\n) or a 3D tensor\n(\nnBatchFrame\n x \nnInputFrame\n x \ninputFrameSize\n).\n\n\n\n\ninputFrameSize\n The input frame size expected in sequences given into \nforward()\n.\n\n\noutputFrameSize\n The output frame size the convolution layer will produce.\n\n\nkernelW\n The kernel width of the convolution\n\n\nstrideW\n The step of the convolution in the width dimension.\n\n\npropagateBack\n Whether propagate gradient back, default is true.\n\n\nwRegularizer\n instance of \nRegularizer\n\n                     (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nbRegularizer\n instance of \nRegularizer\n\n                     applied to the bias.\n\n\ninitWeight\n Initial weight\n\n\ninitBias\n Initial bias\n\n\ninitGradWeight\n Initial gradient weight\n\n\ninitGradBias\n Initial gradient bias\n\n\nT\n The numeric type in the criterion, usually which are \nFloat\n or \nDouble\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nval seed = 100\nRNG.setSeed(seed)\nval inputFrameSize = 5\nval outputFrameSize = 3\nval kW = 5\nval dW = 2\nval layer = TemporalConvolution(inputFrameSize, outputFrameSize, kW, dW)\n\nRandom.setSeed(seed)\nval input = Tensor(10, 5).apply1(e =\n Random.nextFloat())\nval gradOutput = Tensor(3, 3).apply1(e =\n Random.nextFloat())\n\nval output = layer.updateOutput(input)\n\n println(output)\n2017-07-21 06:18:00 INFO  ThreadPool$:79 - Set mkl threads to 1 on thread 1\n-0.34987333 -0.0063185245   -0.45821175 \n-0.20838472 0.15102878  -0.5656665  \n-0.13935827 -0.099345684    -0.76407385 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nval gradInput = layer.updateGradInput(input, gradOutput)\n\n println(gradInput)\n0.018415622 -0.10201519 -0.15641063 -0.08271551 -0.060939234    \n0.13609992  0.14934899  0.06083451  -0.13943195 -0.11092151 \n-0.14552939 -0.024670592    -0.29887137 -0.14555064 -0.05840567 \n0.09920926  0.2705848   0.016875947 -0.27233958 -0.069991685    \n-0.0024300043   -0.15160085 -0.20593905 -0.2894306  -0.057458147    \n0.06390554  0.07710219  0.105445914 -0.26714328 -0.18871497 \n0.13901645  -0.10651534 0.006758575 -0.08754986 -0.13747974 \n-0.026543075    -0.044046614    0.13146847  -0.01198944 -0.030542556    \n0.18396454  -0.055985756    -0.03506116 -0.02156017 -0.09211717 \n0.0 0.0 0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import TemporalConvolution\nimport numpy as np\ninputFrameSize = 5\noutputFrameSize = 3\nkW = 5\ndW = 2\nlayer = TemporalConvolution(inputFrameSize, outputFrameSize, kW, dW)\n\ninput = np.random.rand(10, 5)\ngradOutput = np.random.rand(3, 3)\n\noutput = layer.forward(input)\n\n print(output)\n[[ 0.43262666  0.52964264 -0.09026626]\n [ 0.46828389  0.3391096   0.04789509]\n [ 0.37985104  0.13899082 -0.05767119]]\n\ngradInput = layer.backward(input, gradOutput)\n\n print(gradInput)\n[[-0.08801709  0.03619258  0.06944641 -0.01570761  0.00682773]\n [-0.02754797  0.07474414 -0.08249797  0.04756897  0.0096445 ]\n [-0.14383194  0.05168077  0.27049363  0.10419817  0.05263135]\n [ 0.12452157 -0.02296585  0.14436334  0.02482709 -0.12260982]\n [ 0.04890725 -0.19043611  0.2909058  -0.10708418  0.07759682]\n [ 0.05745121  0.10499261  0.02989995  0.13047372  0.09119483]\n [-0.09693538 -0.12962547  0.22133902 -0.09149387  0.29208034]\n [ 0.2622599  -0.12875232  0.21714815  0.11484481 -0.00040091]\n [ 0.07558989  0.00072951  0.12860702 -0.27085134  0.10740379]\n [ 0.          0.          0.          0.          0.        ]]\n\n\n\n\n\n\n\nVolumetricFullConvolution\n\n\nScala:\n\n\nval m  = VolumetricFullConvolution(\n  nInputPlane, nOutputPlane,\n  kT, kW, kH,\n  dT, dW = 1, dH = 1,\n  padT = 0, padW = 0, padH = 0,\n  adjT = 0, adjW = 0, adjH = 0,\n  nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)\n\n\n\n\nPython:\n\n\nm = VolumetricFullConvolution(\n    n_input_plane, n_output_plane,\n    kt, kw, kh, \n    dt=1, dw=1,dh=1,\n    pad_t=0, pad_w=0, pad_h=0, \n    adj_t=0, adj_w=0,adj_h=0,\n    n_group=1,no_bias=False,init_method='default',wRegularizer=None,bRegularizer=None)\n\n\n\n\nVolumetricFullConvolution\n Apply a 3D full convolution over an 3D input image, a sequence of images, or a video etc.\nThe input tensor is expected to be a 4D or 5D(with batch) tensor. Note that instead\nof setting adjT, adjW and adjH, \nVolumetricConvolution\n also accepts a table input\nwith two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,\nand the size of sizeTensor is used to set the size of the output (will ignore the adjT, adjW and\nadjH values used to construct the module). This module can be used without a bias by setting\nparameter noBias = true while constructing the module.\n\n\nIf input is a 4D tensor nInputPlane x depth x height x width,\n\n\nodepth  = (depth  - 1) * dT - 2*padT + kT + adjT\nowidth  = (width  - 1) * dW - 2*padW + kW + adjW\noheight = (height - 1) * dH - 2*padH + kH + adjH\n\n\n\n\nOther frameworks call this operation \"In-network Upsampling\", \"Fractionally-strided convolution\",\n\"Backwards Convolution,\" \"Deconvolution\", or \"Upconvolution.\"\n\n\nReference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic\nsegmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n2015: 3431-3440.\n\n\n\n\nnInputPlane The number of expected input planes in the image given into forward()\n\n\nnOutputPlane The number of output planes the convolution layer will produce.\n\n\nkT The kernel depth of the convolution.\n\n\nkW The kernel width of the convolution.\n\n\nkH The kernel height of the convolution.\n\n\ndT The step of the convolution in the depth dimension. Default is 1.\n\n\ndW The step of the convolution in the width dimension. Default is 1.\n\n\ndH The step of the convolution in the height dimension. Default is 1.\n\n\npadT The additional zeros added per depth to the input planes. Default is 0.\n\n\npadW The additional zeros added per width to the input planes. Default is 0.\n\n\npadH The additional zeros added per height to the input planes. Default is 0.\n\n\nadjT Extra depth to add to the output image. Default is 0.\n\n\nadjW Extra width to add to the output image. Default is 0.\n\n\nadjH Extra height to add to the output image. Default is 0.\n\n\nnGroup Kernel group number.\n\n\nnoBias If bias is needed.\n\n\nwRegularizer: instance of \nRegularizer\n\n\n(eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nbRegularizer: instance of \nRegularizer\n\n                   applied to the bias.\n\n\n\n\nScala example:\n\n\nTensor Input example: \n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval m = VolumetricFullConvolution(2, 1, 2, 2, 2)\n\nval input = Tensor(1, 2, 2, 3, 3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1, 1, 3, 4, 4).fill(0.2f)\nval gradIn = m.backward(input, gradOut)\n\nscala\n println(input)\n(1,1,1,.,.) =\n0.3903321   -0.90453357 1.735308    \n-1.2824814  -0.27802613 -0.3977802  \n-0.08534186 0.6385388   -0.86845094 \n\n(1,1,2,.,.) =\n-0.24652982 0.69465446  0.1713606   \n0.07106233  -0.88137305 1.0625362   \n-0.553569   1.1822331   -2.2488093  \n\n(1,2,1,.,.) =\n0.552869    0.4108489   1.7802315   \n0.018191056 0.72422534  -0.6423254  \n-0.4077748  0.024120487 -0.42820823 \n\n(1,2,2,.,.) =\n-1.3711191  -0.37988988 -2.1587164  \n-0.85155743 -1.5785019  -0.77727056 \n0.42253423  0.79593533  0.15303874  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x2x3x3]\n\nscala\n println(output)\n(1,1,1,.,.) =\n-0.29154167 -0.027156994    -0.6949123  -0.22638178 \n0.091479614 -0.106284864    -0.23198327 -0.5334093  \n0.092822656 -0.13807209 -0.07207352 -0.023272723    \n-0.19217497 -0.18892932 -0.089907974    -0.059967346    \n\n(1,1,2,.,.) =\n0.08078699  -0.0242998  0.27271587  0.48551774  \n-0.30726838 0.5497404   -0.7220843  0.48132813  \n0.007951438 -0.39301366 0.56711966  -0.39552623 \n-0.016941413    -0.5530351  0.21254264  -0.22647215 \n\n(1,1,3,.,.) =\n-0.38189644 -0.5241636  -0.49781954 -0.59505236 \n-0.23887709 -0.99911994 -0.773817   -0.63575095 \n-0.1193203  0.016682416 -0.41216886 -0.5211964  \n-0.06341652 -0.32541442 0.43984014  -0.16862796 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x3x4x4]\n\nscala\n println(gradOut)\n(1,1,1,.,.) =\n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n\n(1,1,2,.,.) =\n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n\n(1,1,3,.,.) =\n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x3x4x4]\nscala\n println(gradIn)\n(1,1,1,.,.) =\n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n\n(1,1,2,.,.) =\n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n\n(1,2,1,.,.) =\n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n\n(1,2,2,.,.) =\n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x3x3]\n\n\n\n\n\nTable input Example\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval m = VolumetricFullConvolution(1, 2, 2, 2, 2)\n\nval input1 = Tensor(1, 3, 3, 3).randn()\nval input2 = Tensor(3, 3, 3).fill(2.0f)\nval input = T(input1, input2)\nval output = m.forward(input)\nval gradOut = Tensor(2, 4, 4, 4).fill(0.1f)\nval gradIn = m.backward(input, gradOut)\n\nscala\n println(input)\n{\n  2: (1,.,.) =\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n\n  (2,.,.) =\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n\n  (3,.,.) =\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3x3]\n  1: (1,1,.,.) =\n  0.23809154    1.2167819   0.3664989\n  0.8797001 1.5262067   0.15420714\n  0.38004395    -0.24190372 -1.1151218\n\n  (1,2,.,.) =\n  -1.895742 1.8554556   0.62502027\n  -0.6004498    0.056441266 -0.66499823\n  0.7039313 -0.08569297 -0.08191566\n\n  (1,3,.,.) =\n  -1.9555066    -0.20133287 -0.22135374\n  0.8918014 -1.2684877  0.14211883\n  2.5802526 1.1118578   -1.3165624\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3x3]\n}\n\nscala\n println(output)\n(1,1,.,.) =\n-0.2578445  -0.48271507 -0.28246504 -0.20139077\n-0.43916196 -0.72301924 -0.2915339  -0.20471849\n-0.41347015 -0.36456454 0.021684423 -0.20852578\n-0.255981   -0.17165771 -0.04553239 -0.19543594\n\n(1,2,.,.) =\n0.18660262  -0.8204256  -0.08807768 -0.1023551\n0.026309028 -0.49442527 0.3699256   -0.12729678\n-0.34651133 0.08542377  0.24221262  -0.47949657\n-0.29622912 -0.15598825 -0.23278731 -0.32802662\n\n(1,3,.,.) =\n0.6303606   -1.0451282  0.21740273  -0.03673452\n-0.039471984    -0.2264648  0.15774214  -0.30815765\n-1.0726243  -0.13914594 0.08537227  -0.30611742\n-0.55404246 -0.29725668 -0.037192106    -0.20331946\n\n(1,4,.,.) =\n0.19113302  -0.68506914 -0.21211714 -0.26207167\n-0.40826926 0.068062216 -0.5962198  -0.18985644\n-0.7111124  0.3466564   0.2185097   -0.5388211\n-0.16902745 0.10249108  -0.09487718 -0.35127735\n\n(2,1,.,.) =\n-0.2744591  -0.21165672 -0.17422867 -0.25680506\n-0.24608877 -0.1242196  -0.02206999 -0.23146236\n-0.27057967 -0.17076656 -0.18083718 -0.35417527\n-0.28634468 -0.24118122 -0.30961025 -0.41247135\n\n(2,2,.,.) =\n-0.41682464 -0.5772195  -0.159199   -0.2294753\n-0.41187716 -0.41886678 0.4104582   -0.1382559\n-0.08818802 0.459113    0.48080307  -0.3373265\n-0.18515268 -0.14088067 -0.67644227 -0.67253566\n\n(2,3,.,.) =\n-0.009801388    -0.83997947 -0.39409852 -0.29002026\n-0.6333371  -0.66267097 0.52607954  -0.10082486\n-0.46748784 -0.08717018 -0.54928875 -0.59819674\n-0.103552   0.22147804  -0.20562811 -0.46321797\n\n(2,4,.,.) =\n0.090245515 -0.28537494 -0.24673338 -0.289634\n-0.98199505 -0.7408645  -0.4654177  -0.35744694\n-0.5410351  -0.48618284 -0.40212065 -0.26319134\n0.4081596   0.8880725   -0.26220837 -0.73146355\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x4x4]\n\nscala\n println(gradOut)\n(1,1,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(1,2,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(1,3,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(1,4,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,1,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,2,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,3,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,4,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x4x4x4]\n\nscala\n println(gradIn)\n{\n  2: (1,.,.) =\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n\n  (2,.,.) =\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n\n  (3,.,.) =\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3x3]\n  1: (1,1,.,.) =\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n\n  (1,2,.,.) =\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n\n  (1,3,.,.) =\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nm = VolumetricFullConvolution(2, 1, 2, 2, 2)\n\nprint \n--------- tensor input---------\n\ntensor_input = np.random.rand(1, 2, 2, 3, 3)\nprint \ninput is :\n,tensor_input\nout = m.forward(tensor_input)\nprint \noutput m is :\n,out\n\nprint \n----------- table input --------\n\nadj_input=np.empty([3, 3, 3])\nadj_input.fill(2.0)\ntable_input = [tensor_input,adj_input]\nprint \ninput is :\n,table_input\nout = m.forward(table_input)\nprint \noutput m is :\n,out\n\n\n\n\ncreating: createVolumetricFullConvolution\n--------- tensor input---------\ninput is : [[[[[ 0.41632522  0.62726142  0.11133406]\n    [ 0.61013369  0.76320391  0.27937597]\n    [ 0.3596402   0.85087329  0.18706284]]\n\n   [[ 0.19224562  0.79333622  0.02064112]\n    [ 0.34019388  0.36193739  0.0189533 ]\n    [ 0.01245767  0.59638721  0.97882726]]]\n\n\n  [[[ 0.03641869  0.92804035  0.08934243]\n    [ 0.96598196  0.54331079  0.9157464 ]\n    [ 0.31659511  0.48128023  0.13775686]]\n\n   [[ 0.44624135  0.02830871  0.95668413]\n    [ 0.32971474  0.46466264  0.58239329]\n    [ 0.94129846  0.27284845  0.59931096]]]]]\noutput m is : [[[[[ 0.24059629  0.11875484 -0.07601731  0.18490529]\n    [ 0.17978033 -0.05925606 -0.06877603 -0.00254188]\n    [ 0.33574528  0.10908454 -0.01606898  0.22380096]\n    [ 0.24050319  0.17277193  0.10569186  0.20417407]]\n\n   [[ 0.26733595  0.26336247 -0.16927747  0.04417276]\n    [ 0.39058518 -0.08025722 -0.11981271  0.08441451]\n    [ 0.21994853 -0.1127445  -0.01282334 -0.25795668]\n    [ 0.34960991  0.17045188  0.0885388   0.08292522]]\n\n   [[ 0.29700345  0.22094724  0.27189076  0.07538646]\n    [ 0.27829763  0.01766421  0.32052374 -0.09809484]\n    [ 0.28885722  0.08438809  0.24915564 -0.08578731]\n    [ 0.25339472 -0.09679155  0.09070791  0.21198538]]]]]\n----------- table input --------\ninput is : [array([[[[[ 0.41632522,  0.62726142,  0.11133406],\n          [ 0.61013369,  0.76320391,  0.27937597],\n          [ 0.3596402 ,  0.85087329,  0.18706284]],\n\n         [[ 0.19224562,  0.79333622,  0.02064112],\n          [ 0.34019388,  0.36193739,  0.0189533 ],\n          [ 0.01245767,  0.59638721,  0.97882726]]],\n\n\n        [[[ 0.03641869,  0.92804035,  0.08934243],\n          [ 0.96598196,  0.54331079,  0.9157464 ],\n          [ 0.31659511,  0.48128023,  0.13775686]],\n\n         [[ 0.44624135,  0.02830871,  0.95668413],\n          [ 0.32971474,  0.46466264,  0.58239329],\n          [ 0.94129846,  0.27284845,  0.59931096]]]]]), array([[[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]],\n\n       [[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]],\n\n       [[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]]])]\noutput m is : [[[[[ 0.24059629  0.11875484 -0.07601731  0.18490529]\n    [ 0.17978033 -0.05925606 -0.06877603 -0.00254188]\n    [ 0.33574528  0.10908454 -0.01606898  0.22380096]\n    [ 0.24050319  0.17277193  0.10569186  0.20417407]]\n\n   [[ 0.26733595  0.26336247 -0.16927747  0.04417276]\n    [ 0.39058518 -0.08025722 -0.11981271  0.08441451]\n    [ 0.21994853 -0.1127445  -0.01282334 -0.25795668]\n    [ 0.34960991  0.17045188  0.0885388   0.08292522]]\n\n   [[ 0.29700345  0.22094724  0.27189076  0.07538646]\n    [ 0.27829763  0.01766421  0.32052374 -0.09809484]\n    [ 0.28885722  0.08438809  0.24915564 -0.08578731]\n    [ 0.25339472 -0.09679155  0.09070791  0.21198538]]]]]", 
            "title": "Convolution Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#spatialconvolution", 
            "text": "Scala:  val m = SpatialConvolution(nInputPlane,nOutputPlane,kernelW,kernelH,strideW=1,strideH=1,padW=0,padH=0,nGroup=1,propagateBack=true,wRegularizer=null,bRegularizer=null,initWeight=null, initBias=null, initGradWeight=null, initGradBias=null, withBias=true, dataFormat=DataFormat.NCHW)  Python:  m = SpatialConvolution(n_input_plane,n_output_plane,kernel_w,kernel_h,stride_w=1,stride_h=1,pad_w=0,pad_h=0,n_group=1,propagate_back=True,wRegularizer=None,bRegularizer=None,init_weight=None,init_bias=None,init_grad_weight=None,init_grad_bias=None, with_bias=True, data_format= NCHW )  SpatialConvolution is a module that applies a 2D convolution over an input image.  The input tensor in  forward(input)  is expected to be\neither a 4D tensor ( batch x nInputPlane x height x width ) or a 3D tensor ( nInputPlane x height x width ). The convolution is performed on the last two dimensions.  As for padding, when padW and padH are both -1, we use a padding algorithm similar to the \"SAME\" padding of tensorflow. That is   outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)\n outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)\n\n padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)\n padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)\n\n padTop = padAlongHeight / 2\n padLeft = padAlongWidth / 2  Detailed paramter explaination for the constructor.   nInputPlane  The number of expected input planes in the image given into forward()  nOutputPlane  The number of output planes the convolution layer will produce.  kernelW  The kernel width of the convolution  kernelH  The kernel height of the convolution  strideW  The step of the convolution in the width dimension.  strideH  The step of the convolution in the height dimension  padW   padding to be added to width to the input.  padH  padding to be added to height to the input.  nGroup  Kernel group number  propagateBack  whether to propagate gradient back  wRegularizer  regularizer on weight. an instance of [[Regularizer]] (e.g. L1 or L2)  bRegularizer  regularizer on bias. an instance of [[Regularizer]] (e.g. L1 or L2).  initWeight  weight initializer  initBias   bias initializer  initGradWeight  weight gradient initializer  initGradBias  bias gradient initializer  with_bias  the optional initial value for if need bias  data_format  a string value (or DataFormat Object in Scala) of \"NHWC\" or \"NCHW\" to specify the input data format of this layer. In \"NHWC\" format\n                        data is stored in the order of [batch_size, height, width, channels], in \"NCHW\" format data is stored\n                        in the order of [batch_size, channels, height, width].   Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialConvolution(2,1,2,2,1,1,0,0)\nm.setInitMethod(weightInitMethod = BilinearFiller, biasInitMethod = Zeros)\nval params = m.getParameters()\n\nscala  print(params)\n(1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9],0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9])\n\nscala \nval input = Tensor(1,2,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,1,2,2).fill(0.2f)\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n(1,1,.,.) =\n-0.37011376     0.13565119      -0.73574775\n-0.19486316     -0.4430604      -0.62543416\n0.7017611       -0.6441595      -1.2953792\n\n(1,2,.,.) =\n-0.9903588      0.5669722       0.2630131\n0.03392942      -0.6984676      -0.12389368\n0.78704715      0.5411976       -1.3877676\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3]\n\nscala  print(output)\n(1,1,.,.) =\n-1.3604726      0.70262337\n-0.16093373     -1.141528\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\nscala  print(gradOut)\n(1,1,.,.) =\n0.2     0.2\n0.2     0.2\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x2x2]\n\nscala  print(gradIn)\n(1,1,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n(1,2,.,.) =\n0.2     0.2     0.0\n0.2     0.2     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(1,3,3,3)\nprint  input is : ,input\n\nm = SpatialConvolution(3,1,2,2,1,1,0,0)\nout = m.forward(input)\nprint  output m is : ,out\n\ngrad_out = np.random.rand(1,1,2,2)\nprint  grad out of m is : ,grad_out\ngrad_in = m.backward(input,grad_out)\nprint  grad input of m is : ,grad_in  Gives the output,  input is : [[[[ 0.75276617  0.44212513  0.90275949]\n   [ 0.78205279  0.77864714  0.83647254]\n   [ 0.76220944  0.22106036  0.68762202]]\n\n  [[ 0.37346971  0.31532213  0.33276243]\n   [ 0.69872884  0.07262236  0.66372462]\n   [ 0.47803013  0.80194459  0.53313873]]\n\n  [[ 0.56196833  0.20599878  0.47575818]\n   [ 0.35454298  0.96910557  0.36234704]\n   [ 0.64017738  0.95762579  0.50073035]]]]\ncreating: createSpatialConvolution\noutput m is : [[[[-1.08398974 -0.67615652]\n   [-0.77027249 -0.82885492]]]]\ngrad out of m is : [[[[ 0.38295452  0.77048361]\n   [ 0.11671955  0.76357513]]]]\ngrad input of m is : [[[[-0.02344826 -0.06515953 -0.03618064]\n   [-0.06770924 -0.22586647 -0.14004168]\n   [-0.01845866 -0.13653883 -0.10325129]]\n\n  [[-0.09294108 -0.14361492  0.08727306]\n   [-0.09885897 -0.21209857  0.29151234]\n   [-0.02149716 -0.10957514  0.20318349]]\n\n  [[-0.05926216 -0.04542646  0.14849319]\n   [-0.09506465 -0.34244278 -0.03763583]\n   [-0.02346931 -0.1815301  -0.18314059]]]]", 
            "title": "SpatialConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#volumetricconvolution", 
            "text": "Scala:  val module = VolumetricConvolution(nInputPlane, nOutputPlane, kT, kW, kH,\n  dT=1, dW=1, dH=1, padT=0, padW=0, padH=0, withBias=true, wRegularizer=null, bRegularizer=null)  Python:  module = VolumetricConvolution(n_input_plane, n_output_plane, k_t, k_w, k_h,\n  d_t=1, d_w=1, d_h=1, pad_t=0, pad_w=0, pad_h=0, with_bias=true, wRegularizer=null, bRegularizer=null)  Applies a 3D convolution over an input image composed of several input planes. The input tensor\nin forward(input) is expected to be a 4D tensor (nInputPlane x time x height x width).   nInputPlane  The number of expected input planes in the image given into forward()  nOutputPlane  The number of output planes the convolution layer will produce.  kT  The kernel size of the convolution in time  kW  The kernel width of the convolution  kH  The kernel height of the convolution  dT  The step of the convolution in the time dimension. Default is 1  dW  The step of the convolution in the width dimension. Default is 1  dH  The step of the convolution in the height dimension. Default is 1  padT  Additional zeros added to the input plane data on both sides of time axis.\n         Default is 0.  (kT-1)/2  is often used here.  padW  The additional zeros added per width to the input planes.  padH  The additional zeros added per height to the input planes.  withBias  whether with bias.  wRegularizer  instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.  bRegularizer  instance of [[Regularizer]]\n                   applied to the bias.   Scala example:  val layer = VolumetricConvolution(2, 3, 2, 2, 2, dT=1, dW=1, dH=1,\n  padT=0, padW=0, padH=0, withBias=true)\nval input = Tensor(2, 2, 2, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.54846555      0.5549177\n0.43748873      0.6596535\n\n(1,2,.,.) =\n0.87915933      0.5955469\n0.67464 0.40921077\n\n(2,1,.,.) =\n0.24127467      0.49356017\n0.6707502       0.5421975\n\n(2,2,.,.) =\n0.007834963     0.08188637\n0.51387626      0.7376101\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\nlayer.forward(input)\nres16: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.6680023\n\n(2,1,.,.) =\n0.41926455\n\n(3,1,.,.) =\n-0.029196609\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x1x1]  Python example:  layer = VolumetricConvolution(2, 3, 2, 2, 2, d_t=1, d_w=1, d_h=1,\n          pad_t=0, pad_w=0, pad_h=0, with_bias=True, init_method= default ,\n          bigdl_type= float )\ninput = np.random.rand(2,2,2,2)\n array([[[[ 0.47639062,  0.76800312],\n         [ 0.28834351,  0.21883535]],\n\n        [[ 0.86097919,  0.89812597],\n         [ 0.43632181,  0.58004824]]],\n\n\n       [[[ 0.65784027,  0.34700039],\n         [ 0.64511955,  0.1660241 ]],\n\n        [[ 0.36060054,  0.71265665],\n         [ 0.51755249,  0.6508298 ]]]])\n\nlayer.forward(input)\narray([[[[ 0.54268712]]],\n\n\n       [[[ 0.17670505]]],\n\n\n       [[[ 0.40953237]]]], dtype=float32)", 
            "title": "VolumetricConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#spatialdilatedconvolution", 
            "text": "Scala:  val layer = SpatialDilatedConvolution(\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)  Python:  layer = SpatialDilatedConvolution(\n  inputPlanes,\n  outputPlanes,\n  kernelW,\n  kernelH,\n  strideW,\n  strideH,\n  paddingW,\n  paddingH,\n  dilationW,\n  dilationH\n)  Apply a 2D dilated convolution over an input image.  The input tensor is expected to be a 3D or 4D(with batch) tensor.  For a normal SpatialConvolution, the kernel will multiply with input\nimage element-by-element contiguous. In dilated convolution, it\u2019s possible\nto have filters that have spaces between each cell. For example, filter w and\nimage x, when dilatiionW and dilationH both = 1, this is normal 2D convolution  w(0, 0) * x(0, 0), w(0, 1) * x(0, 1)\nw(1, 0) * x(1, 0), w(1, 1) * x(1, 1)  when dilationW and dilationH both = 2  w(0, 0) * x(0, 0), w(0, 1) * x(0, 2)\nw(1, 0) * x(2, 0), w(1, 1) * x(2, 2)  when dilationW and dilationH both = 3  w(0, 0) * x(0, 0), w(0, 1) * x(0, 3)\nw(1, 0) * x(3, 0), w(1, 1) * x(3, 3)  If input is a 3D tensor nInputPlane x height x width,\n *  owidth  = floor(width + 2 * padW - dilationW * (kW-1) - 1) / dW + 1 \n *  oheight = floor(height + 2 * padH - dilationH * (kH-1) - 1) / dH + 1  Reference Paper:   Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J].\narXiv preprint arXiv:1511.07122, 2015.   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SpatialDilatedConvolution(1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\nval input = Tensor(T(T(\n  T(1.0f, 2.0f, 3.0f, 4.0f),\n  T(5.0f, 6.0f, 7.0f, 8.0f),\n  T(9.0f, 1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f, 7.0f)\n)))\nval filter = Tensor(T(T(T(\n  T(1.0f, 1.0f),\n  T(1.0f, 1.0f)\n))))\nlayer.weight.copy(filter)\nlayer.bias.zero()\nlayer.forward(input)\nlayer.backward(input, Tensor(T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n))))  Gives the output,  (1,.,.) =\n15.0    10.0\n22.0    26.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n(1,.,.) =\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n0.1     0.2     0.1     0.2\n0.3     0.4     0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x4x4]  Python example:  from bigdl.nn.layer import SpatialDilatedConvolution\nimport numpy as np\n\nlayer = SpatialDilatedConvolution(1, 1, 2, 2, 1, 1, 0, 0, 2, 2)\ninput = np.array([[\n  [1.0, 2.0, 3.0, 4.0],\n  [5.0, 6.0, 7.0, 8.0],\n  [9.0, 1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0, 7.0]\n]])\nfilter = np.array([[[\n  [1.0, 1.0],\n  [1.0, 1.0]\n]]])\nbias = np.array([0.0])\nlayer.set_weights([filter, bias])\nlayer.forward(input)\nlayer.backward(input, np.array([[[0.1, 0.2], [0.3, 0.4]]]))  Gives the output,  array([[[ 15.,  10.],\n        [ 22.,  26.]]], dtype=float32)\n\narray([[[ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001],\n        [ 0.1       ,  0.2       ,  0.1       ,  0.2       ],\n        [ 0.30000001,  0.40000001,  0.30000001,  0.40000001]]], dtype=float32)", 
            "title": "SpatialDilatedConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#spatialshareconvolution", 
            "text": "Scala:  val layer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)  Python:  layer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)  Applies a 2D convolution over an input image composed of several input planes.\n The input tensor in forward(input) is expected to be\n a 3D tensor (nInputPlane x height x width).  This layer has been optimized to save memory. If using this layer to construct multiple convolution\n layers, please add sharing script for the fInput and fGradInput. Please refer to the ResNet example.  Scala example:  \n    import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n    import com.intel.analytics.bigdl.nn._\n    import com.intel.analytics.bigdl.tensor._\n\n    val nInputPlane = 1\n    val nOutputPlane = 1\n    val kW = 2\n    val kH = 2\n    val dW = 1\n    val dH = 1\n    val padW = 0\n    val padH = 0\n    val layer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH,\n      padW, padH)\n\n    val inputData = Array(\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1\n    )\n\n    val kernelData = Array(\n      2.0, 3,\n      4, 5\n    )\n\n    val biasData = Array(0.0)\n\n    layer.weight.copy(Tensor(Storage(kernelData), 1,\n      Array(nOutputPlane, nInputPlane, kH, kW)))\n    layer.bias.copy(Tensor(Storage(biasData), 1, Array(nOutputPlane)))\n\n    val input = Tensor(Storage(inputData), 1, Array(3, 1, 3, 4))\n    val output = layer.updateOutput(input)\n\n      output\nres2: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(2,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0\n\n(3,1,.,.) =\n49.0    63.0    38.0\n91.0    105.0   56.0  Python example:  nInputPlane = 1\nnOutputPlane = 1\nkW = 2\nkH = 2\ndW = 1\ndH = 1\npadW = 0\npadH = 0\nlayer = SpatialShareConvolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH)\n\ninput = np.array([\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1,\n      1.0, 2, 3, 1,\n      4, 5, 6, 1,\n      7, 8, 9, 1]\n    ).astype( float32 ).reshape(3, 1, 3, 4)\nlayer.forward(input)  print (output)\narray([[[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]],\n\n\n       [[[-3.55372381, -4.0352459 , -2.65861344],\n         [-4.99829054, -5.4798131 , -3.29477644]]]], dtype=float32)", 
            "title": "SpatialShareConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#spatialfullconvolution", 
            "text": "Scala:  val m  = SpatialFullConvolution(nInputPlane, nOutputPlane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0,nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)  Python:  m = SpatialFullConvolution(n_input_plane,n_output_plane,kw,kh,dw=1,dh=1,pad_w=0,pad_h=0,adj_w=0,adj_h=0,n_group=1,no_bias=False,init_method='default',wRegularizer=None,bRegularizer=None)  SpatialFullConvolution is a module that applies a 2D full convolution over an input image.   The input tensor in  forward(input)  is expected to be\neither a 4D tensor ( batch x nInputPlane x height x width ) or a 3D tensor ( nInputPlane x height x width ). The convolution is performed on the last two dimensions.  adjW  and  adjH  are used to adjust the size of the output image. The size of output tensor of  forward  will be :    output width  = (width  - 1) * dW - 2*padW + kW + adjW\n  output height = (height - 1) * dH - 2*padH + kH + adjH  Note, scala API also accepts a table input with two tensors:  T(convInput, sizeTensor)  where  convInput  is the standard input tensor, and the size of  sizeTensor  is used to set the size of the output (will ignore the  adjW  and  adjH  values used to construct the module). Use  SpatialFullConvolution[Table, T](...)  instead of  SpatialFullConvolution[Tensor,T](...) ) for table input.  This module can also be used without a bias by setting parameter  noBias = true  while constructing the module.  Other frameworks may call this operation \"In-network Upsampling\", \"Fractionally-strided convolution\", \"Backwards Convolution,\" \"Deconvolution\", or \"Upconvolution.\"  Reference: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3431-3440.  Detailed explaination of arguments in constructor.    nInputPlane  The number of expected input planes in the image given into forward()  nOutputPlane  The number of output planes the convolution layer will produce.  kW  The kernel width of the convolution.  kH  The kernel height of the convolution.  dW  The step of the convolution in the width dimension. Default is 1.  dH  The step of the convolution in the height dimension. Default is 1.  padW  The additional zeros added per width to the input planes. Default is 0.  padH  The additional zeros added per height to the input planes. Default is 0.  adjW  Extra width to add to the output image. Default is 0.  adjH  Extra height to add to the output image. Default is 0.  nGroup  Kernel group number.  noBias  If bias is needed.  wRegularizer  instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.  bRegularizer  instance of [[Regularizer]]\n                   applied to the bias.   Scala example:  Tensor Input example:   \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval m = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input = Tensor(1,1,3,3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1,2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n(1,1,.,.) =\n0.18219171      1.3252861       -1.3991559\n0.82611334      1.0313315       0.6075537\n-0.7336061      0.3156875       -0.70616096\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x3x3]\n\nscala  print(output)\n(1,1,.,.) =\n-0.49278542     -0.5823938      -0.8304068      -0.077556044\n-0.5028842      -0.7281958      -1.1927067      -0.34262076\n-0.41680115     -0.41400516     -0.7599415      -0.42024887\n-0.5286566      -0.30015367     -0.5997892      -0.32439864\n\n(1,2,.,.) =\n-0.13131973     -0.5770084      1.1069719       -0.6003375\n-0.40302444     -0.07293816     -0.2654545      0.39749345\n0.37311426      -0.49090374     0.3088816       -0.41700447\n-0.12861171     0.09394867      -0.17229918     0.05556257\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x4]\n\nscala  print(gradOut)\n(1,1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(1,2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x4x4]\n\nscala  print(gradIn)\n(1,1,.,.) =\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n-0.05955213     -0.05955213     -0.05955213\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x3x3]  Table input Example  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.{T, Table}\n\nval m = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, false)\n\nval input1 = Tensor(1, 3, 3).randn()\nval input2 = Tensor(3, 3).fill(2.0f)\nval input = T(input1, input2)\nval output = m.forward(input)\nval gradOut = Tensor(2,4,4).fill(0.1f)\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n {\n        2: 2.0  2.0     2.0\n           2.0  2.0     2.0\n           2.0  2.0     2.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: (1,.,.) =\n           1.276177     0.62761325      0.2715257\n           -0.030832397 0.5046206       0.6835176\n           -0.5832693   0.17266633      0.7461992\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n }\n\nscala  print(output)\n(1,.,.) =\n-0.18339296     0.04208675      -0.17708774     -0.30901802\n-0.1484881      0.23592418      0.115615785     -0.11288056\n-0.47266048     -0.41772115     0.07501307      0.041751802\n-0.4851033      -0.5427048      -0.18293871     -0.12682784\n\n(2,.,.) =\n0.6391188       0.845774        0.41208875      0.13754106\n-0.45785713     0.31221163      0.6006259       0.36563575\n-0.24076991     -0.31931365     0.31651747      0.4836449\n0.24247466      -0.16731171     -0.20887817     0.19513035\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x4]\n\nscala  print(gradOut)\n(1,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n(2,.,.) =\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n0.1     0.1     0.1     0.1\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x4x4]\n\nscala  print(gradIn)\n {\n        2: 0.0  0.0     0.0\n           0.0  0.0     0.0\n           0.0  0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: (1,.,.) =\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n           0.16678208   0.16678208      0.16678208\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nm = SpatialFullConvolution(1, 2, 2, 2, 1, 1,0, 0, 0, 0, 1, False)\n\nprint  --------- tensor input--------- \ntensor_input = np.random.rand(1,3,3)\nprint  input is : ,tensor_input\nout = m.forward(tensor_input)\nprint  output m is : ,out\n\nprint  ----------- table input -------- \nadj_input=np.empty([3,3])\nadj_input.fill(2.0)\ntable_input = [tensor_input,adj_input]\nprint  input is : ,table_input\nout = m.forward(table_input)\nprint  output m is : ,out  Gives the output,  creating: createSpatialFullConvolution\n--------- tensor input---------\ninput is : [[[  9.03998497e-01   4.43054896e-01   6.19571211e-01]\n  [  4.24573060e-01   3.29886286e-04   5.48427154e-02]\n  [  8.99004782e-01   3.25514441e-01   6.85294650e-01]]]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]\n----------- table input --------\ninput is : [array([[[  9.03998497e-01,   4.43054896e-01,   6.19571211e-01],\n        [  4.24573060e-01,   3.29886286e-04,   5.48427154e-02],\n        [  8.99004782e-01,   3.25514441e-01,   6.85294650e-01]]]), array([[ 2.,  2.,  2.],\n       [ 2.,  2.,  2.],\n       [ 2.,  2.,  2.]])]\noutput m is : [[[-0.04712385  0.21949144  0.0843184   0.14336972]\n  [-0.28748769  0.39192575  0.00372696  0.27235305]\n  [-0.16292028  0.41943201  0.03476509  0.18813471]\n  [-0.28051955  0.29929382 -0.0689255   0.28749463]]\n\n [[-0.21336153 -0.35994443 -0.29239666 -0.38612381]\n  [-0.33000433 -0.41727966 -0.36827195 -0.34524575]\n  [-0.2410759  -0.38439807 -0.27613443 -0.39401439]\n  [-0.38188276 -0.36746511 -0.37627563 -0.34141305]]]", 
            "title": "SpatialFullConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#spatialconvolutionmap", 
            "text": "Scala:  val layer = SpatialConvolutionMap(\n  connTable,\n  kW,\n  kH,\n  dW = 1,\n  dH = 1,\n  padW = 0,\n  padH = 0,\n  wRegularizer = null,\n  bRegularizer = null)  Python:  layer = SpatialConvolutionMap(\n  conn_table,\n  kw,\n  kh,\n  dw=1,\n  dh=1,\n  pad_w=0,\n  pad_h=0,\n  wRegularizer=None,\n  bRegularizer=None)  This class is a generalization of SpatialConvolution.\nIt uses a generic connection table between input and output features.\nThe SpatialConvolution is equivalent to using a full connection table. \nA Connection Table is the mapping of input/output feature map, stored in a 2D Tensor. The first column is the input feature maps. The second column is output feature maps.  Full Connection table:  val conn = SpatialConvolutionMap.full(nin: Int, nout: In)  One to One connection table:  val conn = SpatialConvolutionMap.oneToOne(nfeat: Int)  Random Connection table:  val conn = SpatialConvolutionMap.random(nin: Int, nout: Int, nto: Int)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval conn = SpatialConvolutionMap.oneToOne(3)  conn  is  conn: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 1.0\n2.0 2.0\n3.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]  val module = SpatialConvolutionMap(SpatialConvolutionMap.oneToOne(3), 2, 2)\n\npritnln(module.forward(Tensor.range(1, 48, 1).resize(3, 4, 4)))  Gives the output,  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n4.5230045   5.8323975   7.1417904\n9.760576    11.069969   12.379362\n14.998148   16.30754    17.616934\n\n(2,.,.) =\n-5.6122046  -5.9227824  -6.233361\n-6.8545156  -7.165093   -7.4756703\n-8.096827   -8.407404   -8.71798\n\n(3,.,.) =\n13.534529   13.908197   14.281864\n15.029203   15.402873   15.77654\n16.523876   16.897545   17.271214\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = SpatialConvolutionMap(np.array([(1, 1), (2, 2), (3, 3)]), 2, 2)\n\nprint(module.forward(np.arange(1, 49, 1).reshape(3, 4, 4)))  Gives the output,  [array([[[-1.24280548, -1.70889318, -2.17498088],\n        [-3.10715604, -3.57324386, -4.03933144],\n        [-4.97150755, -5.43759441, -5.90368223]],\n\n       [[-5.22062826, -5.54696751, -5.87330723],\n        [-6.52598572, -6.85232496, -7.17866373],\n        [-7.8313427 , -8.15768337, -8.48402214]],\n\n       [[ 0.5065825 ,  0.55170798,  0.59683061],\n        [ 0.68707776,  0.73219943,  0.77732348],\n        [ 0.86757064,  0.91269422,  0.95781779]]], dtype=float32)]", 
            "title": "SpatialConvolutionMap"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#temporalconvolution", 
            "text": "Scala:  val module = TemporalConvolution(\n  inputFrameSize, outputFrameSize, kernelW, strideW = 1, propagateBack = true,\n  wRegularizer = null, bRegularizer = null, initWeight = null, initBias = null,\n  initGradWeight = null, initGradBias = null\n  )  Python:  module = TemporalConvolution(\n  input_frame_size, output_frame_size, kernel_w, stride_w = 1, propagate_back = True,\n  w_regularizer = None, b_regularizer = None, init_weight = None, init_bias = None,\n  init_grad_weight = None, init_grad_bias = None\n  )  Applies a 1D convolution over an input sequence composed of nInputFrame frames.\nThe input tensor in  forward(input)  is expected to be a 2D tensor\n( nInputFrame  x  inputFrameSize ) or a 3D tensor\n( nBatchFrame  x  nInputFrame  x  inputFrameSize ).   inputFrameSize  The input frame size expected in sequences given into  forward() .  outputFrameSize  The output frame size the convolution layer will produce.  kernelW  The kernel width of the convolution  strideW  The step of the convolution in the width dimension.  propagateBack  Whether propagate gradient back, default is true.  wRegularizer  instance of  Regularizer \n                     (eg. L1 or L2 regularization), applied to the input weights matrices.  bRegularizer  instance of  Regularizer \n                     applied to the bias.  initWeight  Initial weight  initBias  Initial bias  initGradWeight  Initial gradient weight  initGradBias  Initial gradient bias  T  The numeric type in the criterion, usually which are  Float  or  Double   Scala example:  import com.intel.analytics.bigdl.numeric.NumericFloat\nval seed = 100\nRNG.setSeed(seed)\nval inputFrameSize = 5\nval outputFrameSize = 3\nval kW = 5\nval dW = 2\nval layer = TemporalConvolution(inputFrameSize, outputFrameSize, kW, dW)\n\nRandom.setSeed(seed)\nval input = Tensor(10, 5).apply1(e =  Random.nextFloat())\nval gradOutput = Tensor(3, 3).apply1(e =  Random.nextFloat())\n\nval output = layer.updateOutput(input)  println(output)\n2017-07-21 06:18:00 INFO  ThreadPool$:79 - Set mkl threads to 1 on thread 1\n-0.34987333 -0.0063185245   -0.45821175 \n-0.20838472 0.15102878  -0.5656665  \n-0.13935827 -0.099345684    -0.76407385 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nval gradInput = layer.updateGradInput(input, gradOutput)  println(gradInput)\n0.018415622 -0.10201519 -0.15641063 -0.08271551 -0.060939234    \n0.13609992  0.14934899  0.06083451  -0.13943195 -0.11092151 \n-0.14552939 -0.024670592    -0.29887137 -0.14555064 -0.05840567 \n0.09920926  0.2705848   0.016875947 -0.27233958 -0.069991685    \n-0.0024300043   -0.15160085 -0.20593905 -0.2894306  -0.057458147    \n0.06390554  0.07710219  0.105445914 -0.26714328 -0.18871497 \n0.13901645  -0.10651534 0.006758575 -0.08754986 -0.13747974 \n-0.026543075    -0.044046614    0.13146847  -0.01198944 -0.030542556    \n0.18396454  -0.055985756    -0.03506116 -0.02156017 -0.09211717 \n0.0 0.0 0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 10x5]  Python example:  from bigdl.nn.layer import TemporalConvolution\nimport numpy as np\ninputFrameSize = 5\noutputFrameSize = 3\nkW = 5\ndW = 2\nlayer = TemporalConvolution(inputFrameSize, outputFrameSize, kW, dW)\n\ninput = np.random.rand(10, 5)\ngradOutput = np.random.rand(3, 3)\n\noutput = layer.forward(input)  print(output)\n[[ 0.43262666  0.52964264 -0.09026626]\n [ 0.46828389  0.3391096   0.04789509]\n [ 0.37985104  0.13899082 -0.05767119]]\n\ngradInput = layer.backward(input, gradOutput)  print(gradInput)\n[[-0.08801709  0.03619258  0.06944641 -0.01570761  0.00682773]\n [-0.02754797  0.07474414 -0.08249797  0.04756897  0.0096445 ]\n [-0.14383194  0.05168077  0.27049363  0.10419817  0.05263135]\n [ 0.12452157 -0.02296585  0.14436334  0.02482709 -0.12260982]\n [ 0.04890725 -0.19043611  0.2909058  -0.10708418  0.07759682]\n [ 0.05745121  0.10499261  0.02989995  0.13047372  0.09119483]\n [-0.09693538 -0.12962547  0.22133902 -0.09149387  0.29208034]\n [ 0.2622599  -0.12875232  0.21714815  0.11484481 -0.00040091]\n [ 0.07558989  0.00072951  0.12860702 -0.27085134  0.10740379]\n [ 0.          0.          0.          0.          0.        ]]", 
            "title": "TemporalConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Convolution-Layers/#volumetricfullconvolution", 
            "text": "Scala:  val m  = VolumetricFullConvolution(\n  nInputPlane, nOutputPlane,\n  kT, kW, kH,\n  dT, dW = 1, dH = 1,\n  padT = 0, padW = 0, padH = 0,\n  adjT = 0, adjW = 0, adjH = 0,\n  nGroup=1, noBias=false,wRegularizer=null,bRegularizer=null)  Python:  m = VolumetricFullConvolution(\n    n_input_plane, n_output_plane,\n    kt, kw, kh, \n    dt=1, dw=1,dh=1,\n    pad_t=0, pad_w=0, pad_h=0, \n    adj_t=0, adj_w=0,adj_h=0,\n    n_group=1,no_bias=False,init_method='default',wRegularizer=None,bRegularizer=None)  VolumetricFullConvolution  Apply a 3D full convolution over an 3D input image, a sequence of images, or a video etc.\nThe input tensor is expected to be a 4D or 5D(with batch) tensor. Note that instead\nof setting adjT, adjW and adjH,  VolumetricConvolution  also accepts a table input\nwith two tensors: T(convInput, sizeTensor) where convInput is the standard input tensor,\nand the size of sizeTensor is used to set the size of the output (will ignore the adjT, adjW and\nadjH values used to construct the module). This module can be used without a bias by setting\nparameter noBias = true while constructing the module.  If input is a 4D tensor nInputPlane x depth x height x width,  odepth  = (depth  - 1) * dT - 2*padT + kT + adjT\nowidth  = (width  - 1) * dW - 2*padW + kW + adjW\noheight = (height - 1) * dH - 2*padH + kH + adjH  Other frameworks call this operation \"In-network Upsampling\", \"Fractionally-strided convolution\",\n\"Backwards Convolution,\" \"Deconvolution\", or \"Upconvolution.\"  Reference Paper: Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic\nsegmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n2015: 3431-3440.   nInputPlane The number of expected input planes in the image given into forward()  nOutputPlane The number of output planes the convolution layer will produce.  kT The kernel depth of the convolution.  kW The kernel width of the convolution.  kH The kernel height of the convolution.  dT The step of the convolution in the depth dimension. Default is 1.  dW The step of the convolution in the width dimension. Default is 1.  dH The step of the convolution in the height dimension. Default is 1.  padT The additional zeros added per depth to the input planes. Default is 0.  padW The additional zeros added per width to the input planes. Default is 0.  padH The additional zeros added per height to the input planes. Default is 0.  adjT Extra depth to add to the output image. Default is 0.  adjW Extra width to add to the output image. Default is 0.  adjH Extra height to add to the output image. Default is 0.  nGroup Kernel group number.  noBias If bias is needed.  wRegularizer: instance of  Regularizer  (eg. L1 or L2 regularization), applied to the input weights matrices.  bRegularizer: instance of  Regularizer \n                   applied to the bias.   Scala example:  Tensor Input example:   scala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval m = VolumetricFullConvolution(2, 1, 2, 2, 2)\n\nval input = Tensor(1, 2, 2, 3, 3).randn()\nval output = m.forward(input)\nval gradOut = Tensor(1, 1, 3, 4, 4).fill(0.2f)\nval gradIn = m.backward(input, gradOut)\n\nscala  println(input)\n(1,1,1,.,.) =\n0.3903321   -0.90453357 1.735308    \n-1.2824814  -0.27802613 -0.3977802  \n-0.08534186 0.6385388   -0.86845094 \n\n(1,1,2,.,.) =\n-0.24652982 0.69465446  0.1713606   \n0.07106233  -0.88137305 1.0625362   \n-0.553569   1.1822331   -2.2488093  \n\n(1,2,1,.,.) =\n0.552869    0.4108489   1.7802315   \n0.018191056 0.72422534  -0.6423254  \n-0.4077748  0.024120487 -0.42820823 \n\n(1,2,2,.,.) =\n-1.3711191  -0.37988988 -2.1587164  \n-0.85155743 -1.5785019  -0.77727056 \n0.42253423  0.79593533  0.15303874  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x2x3x3]\n\nscala  println(output)\n(1,1,1,.,.) =\n-0.29154167 -0.027156994    -0.6949123  -0.22638178 \n0.091479614 -0.106284864    -0.23198327 -0.5334093  \n0.092822656 -0.13807209 -0.07207352 -0.023272723    \n-0.19217497 -0.18892932 -0.089907974    -0.059967346    \n\n(1,1,2,.,.) =\n0.08078699  -0.0242998  0.27271587  0.48551774  \n-0.30726838 0.5497404   -0.7220843  0.48132813  \n0.007951438 -0.39301366 0.56711966  -0.39552623 \n-0.016941413    -0.5530351  0.21254264  -0.22647215 \n\n(1,1,3,.,.) =\n-0.38189644 -0.5241636  -0.49781954 -0.59505236 \n-0.23887709 -0.99911994 -0.773817   -0.63575095 \n-0.1193203  0.016682416 -0.41216886 -0.5211964  \n-0.06341652 -0.32541442 0.43984014  -0.16862796 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x3x4x4]\n\nscala  println(gradOut)\n(1,1,1,.,.) =\n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n\n(1,1,2,.,.) =\n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n\n(1,1,3,.,.) =\n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n0.2 0.2 0.2 0.2 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x3x4x4]\nscala  println(gradIn)\n(1,1,1,.,.) =\n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n\n(1,1,2,.,.) =\n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n-0.089189366    -0.089189366    -0.089189366    \n\n(1,2,1,.,.) =\n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n\n(1,2,2,.,.) =\n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n0.06755526  0.06755526  0.06755526  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x3x3]  Table input Example  scala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval m = VolumetricFullConvolution(1, 2, 2, 2, 2)\n\nval input1 = Tensor(1, 3, 3, 3).randn()\nval input2 = Tensor(3, 3, 3).fill(2.0f)\nval input = T(input1, input2)\nval output = m.forward(input)\nval gradOut = Tensor(2, 4, 4, 4).fill(0.1f)\nval gradIn = m.backward(input, gradOut)\n\nscala  println(input)\n{\n  2: (1,.,.) =\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n\n  (2,.,.) =\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n\n  (3,.,.) =\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n  2.0   2.0 2.0\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3x3]\n  1: (1,1,.,.) =\n  0.23809154    1.2167819   0.3664989\n  0.8797001 1.5262067   0.15420714\n  0.38004395    -0.24190372 -1.1151218\n\n  (1,2,.,.) =\n  -1.895742 1.8554556   0.62502027\n  -0.6004498    0.056441266 -0.66499823\n  0.7039313 -0.08569297 -0.08191566\n\n  (1,3,.,.) =\n  -1.9555066    -0.20133287 -0.22135374\n  0.8918014 -1.2684877  0.14211883\n  2.5802526 1.1118578   -1.3165624\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3x3]\n}\n\nscala  println(output)\n(1,1,.,.) =\n-0.2578445  -0.48271507 -0.28246504 -0.20139077\n-0.43916196 -0.72301924 -0.2915339  -0.20471849\n-0.41347015 -0.36456454 0.021684423 -0.20852578\n-0.255981   -0.17165771 -0.04553239 -0.19543594\n\n(1,2,.,.) =\n0.18660262  -0.8204256  -0.08807768 -0.1023551\n0.026309028 -0.49442527 0.3699256   -0.12729678\n-0.34651133 0.08542377  0.24221262  -0.47949657\n-0.29622912 -0.15598825 -0.23278731 -0.32802662\n\n(1,3,.,.) =\n0.6303606   -1.0451282  0.21740273  -0.03673452\n-0.039471984    -0.2264648  0.15774214  -0.30815765\n-1.0726243  -0.13914594 0.08537227  -0.30611742\n-0.55404246 -0.29725668 -0.037192106    -0.20331946\n\n(1,4,.,.) =\n0.19113302  -0.68506914 -0.21211714 -0.26207167\n-0.40826926 0.068062216 -0.5962198  -0.18985644\n-0.7111124  0.3466564   0.2185097   -0.5388211\n-0.16902745 0.10249108  -0.09487718 -0.35127735\n\n(2,1,.,.) =\n-0.2744591  -0.21165672 -0.17422867 -0.25680506\n-0.24608877 -0.1242196  -0.02206999 -0.23146236\n-0.27057967 -0.17076656 -0.18083718 -0.35417527\n-0.28634468 -0.24118122 -0.30961025 -0.41247135\n\n(2,2,.,.) =\n-0.41682464 -0.5772195  -0.159199   -0.2294753\n-0.41187716 -0.41886678 0.4104582   -0.1382559\n-0.08818802 0.459113    0.48080307  -0.3373265\n-0.18515268 -0.14088067 -0.67644227 -0.67253566\n\n(2,3,.,.) =\n-0.009801388    -0.83997947 -0.39409852 -0.29002026\n-0.6333371  -0.66267097 0.52607954  -0.10082486\n-0.46748784 -0.08717018 -0.54928875 -0.59819674\n-0.103552   0.22147804  -0.20562811 -0.46321797\n\n(2,4,.,.) =\n0.090245515 -0.28537494 -0.24673338 -0.289634\n-0.98199505 -0.7408645  -0.4654177  -0.35744694\n-0.5410351  -0.48618284 -0.40212065 -0.26319134\n0.4081596   0.8880725   -0.26220837 -0.73146355\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x4x4]\n\nscala  println(gradOut)\n(1,1,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(1,2,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(1,3,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(1,4,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,1,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,2,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,3,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n(2,4,.,.) =\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n0.1 0.1 0.1 0.1\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x4x4x4]\n\nscala  println(gradIn)\n{\n  2: (1,.,.) =\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n\n  (2,.,.) =\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n\n  (3,.,.) =\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n  0.0   0.0 0.0\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3x3]\n  1: (1,1,.,.) =\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n\n  (1,2,.,.) =\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n\n  (1,3,.,.) =\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n  0.048819613   0.048819613 0.048819613\n\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nm = VolumetricFullConvolution(2, 1, 2, 2, 2)\n\nprint  --------- tensor input--------- \ntensor_input = np.random.rand(1, 2, 2, 3, 3)\nprint  input is : ,tensor_input\nout = m.forward(tensor_input)\nprint  output m is : ,out\n\nprint  ----------- table input -------- \nadj_input=np.empty([3, 3, 3])\nadj_input.fill(2.0)\ntable_input = [tensor_input,adj_input]\nprint  input is : ,table_input\nout = m.forward(table_input)\nprint  output m is : ,out  creating: createVolumetricFullConvolution\n--------- tensor input---------\ninput is : [[[[[ 0.41632522  0.62726142  0.11133406]\n    [ 0.61013369  0.76320391  0.27937597]\n    [ 0.3596402   0.85087329  0.18706284]]\n\n   [[ 0.19224562  0.79333622  0.02064112]\n    [ 0.34019388  0.36193739  0.0189533 ]\n    [ 0.01245767  0.59638721  0.97882726]]]\n\n\n  [[[ 0.03641869  0.92804035  0.08934243]\n    [ 0.96598196  0.54331079  0.9157464 ]\n    [ 0.31659511  0.48128023  0.13775686]]\n\n   [[ 0.44624135  0.02830871  0.95668413]\n    [ 0.32971474  0.46466264  0.58239329]\n    [ 0.94129846  0.27284845  0.59931096]]]]]\noutput m is : [[[[[ 0.24059629  0.11875484 -0.07601731  0.18490529]\n    [ 0.17978033 -0.05925606 -0.06877603 -0.00254188]\n    [ 0.33574528  0.10908454 -0.01606898  0.22380096]\n    [ 0.24050319  0.17277193  0.10569186  0.20417407]]\n\n   [[ 0.26733595  0.26336247 -0.16927747  0.04417276]\n    [ 0.39058518 -0.08025722 -0.11981271  0.08441451]\n    [ 0.21994853 -0.1127445  -0.01282334 -0.25795668]\n    [ 0.34960991  0.17045188  0.0885388   0.08292522]]\n\n   [[ 0.29700345  0.22094724  0.27189076  0.07538646]\n    [ 0.27829763  0.01766421  0.32052374 -0.09809484]\n    [ 0.28885722  0.08438809  0.24915564 -0.08578731]\n    [ 0.25339472 -0.09679155  0.09070791  0.21198538]]]]]\n----------- table input --------\ninput is : [array([[[[[ 0.41632522,  0.62726142,  0.11133406],\n          [ 0.61013369,  0.76320391,  0.27937597],\n          [ 0.3596402 ,  0.85087329,  0.18706284]],\n\n         [[ 0.19224562,  0.79333622,  0.02064112],\n          [ 0.34019388,  0.36193739,  0.0189533 ],\n          [ 0.01245767,  0.59638721,  0.97882726]]],\n\n\n        [[[ 0.03641869,  0.92804035,  0.08934243],\n          [ 0.96598196,  0.54331079,  0.9157464 ],\n          [ 0.31659511,  0.48128023,  0.13775686]],\n\n         [[ 0.44624135,  0.02830871,  0.95668413],\n          [ 0.32971474,  0.46466264,  0.58239329],\n          [ 0.94129846,  0.27284845,  0.59931096]]]]]), array([[[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]],\n\n       [[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]],\n\n       [[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]]])]\noutput m is : [[[[[ 0.24059629  0.11875484 -0.07601731  0.18490529]\n    [ 0.17978033 -0.05925606 -0.06877603 -0.00254188]\n    [ 0.33574528  0.10908454 -0.01606898  0.22380096]\n    [ 0.24050319  0.17277193  0.10569186  0.20417407]]\n\n   [[ 0.26733595  0.26336247 -0.16927747  0.04417276]\n    [ 0.39058518 -0.08025722 -0.11981271  0.08441451]\n    [ 0.21994853 -0.1127445  -0.01282334 -0.25795668]\n    [ 0.34960991  0.17045188  0.0885388   0.08292522]]\n\n   [[ 0.29700345  0.22094724  0.27189076  0.07538646]\n    [ 0.27829763  0.01766421  0.32052374 -0.09809484]\n    [ 0.28885722  0.08438809  0.24915564 -0.08578731]\n    [ 0.25339472 -0.09679155  0.09070791  0.21198538]]]]]", 
            "title": "VolumetricFullConvolution"
        }, 
        {
            "location": "/APIGuide/Layers/Pooling-Layers/", 
            "text": "SpatialMaxPooling\n\n\nScala:\n\n\nval mp = SpatialMaxPooling(2, 2, dW=2, dH=2, padW=0, padH=0, format=DataFormat.NCHW)\n\n\n\n\nPython:\n\n\nmp = SpatialMaxPooling(2, 2, dw=2, dh=2, pad_w=0, pad_h=0, to_ceil=false, format=\nNCHW\n)\n\n\n\n\nApplies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.\nThe number of output features is equal to the number of input planes.\nIf the input image is a 3D tensor nInputPlane x height x width,\nthe output image size will be nOutputPlane x oheight x owidth where\n\n\n\n\nowidth  = op((width  + 2*padW - kW) / dW + 1)\n\n\noheight = op((height + 2*padH - kH) / dH + 1)\n\n\n\n\nop is a rounding operator. By default, it is floor.\nIt can be changed by calling ceil() or floor() methods.\n\n\nAs for padding, when padW and padH are both -1, we use a padding algorithm similar to the \"SAME\" padding of tensorflow. That is\n\n\n outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)\n outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)\n\n padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)\n padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)\n\n padTop = padAlongHeight / 2\n padLeft = padAlongWidth / 2\n\n\n\n\nThe format parameter is a string value (or DataFormat Object in Scala) of \"NHWC\" or \"NCHW\" to specify the input data format of this layer. In \"NHWC\" format\ndata is stored in the order of [batch_size, height, width, channels], in \"NCHW\" format data is stored\nin the order of [batch_size, channels, height, width].\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.SpatialMaxPooling\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mp = SpatialMaxPooling(2, 2, 2, 2)\nval input = Tensor(1, 3, 3)\n\ninput(Array(1, 1, 1)) = 0.5336726f\ninput(Array(1, 1, 2)) = 0.7963769f\ninput(Array(1, 1, 3)) = 0.5674766f\ninput(Array(1, 2, 1)) = 0.1803996f\ninput(Array(1, 2, 2)) = 0.2460861f\ninput(Array(1, 2, 3)) = 0.2295625f\ninput(Array(1, 3, 1)) = 0.3073633f\ninput(Array(1, 3, 2)) = 0.5973460f\ninput(Array(1, 3, 3)) = 0.4298954f\n\nval gradOutput = Tensor(1, 1, 1)\ngradOutput(Array(1, 1, 1)) = 0.023921491578221f\n\nval output = mp.forward(input)\nval gradInput = mp.backward(input, gradOutput)\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\nThe output is,\n\n\n(1,.,.) =\n0.7963769\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1]\n\n\n\n\nThe gradInput is,\n\n\n(1,.,.) =\n0.0     0.023921492     0.0\n0.0     0.0     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmp = SpatialMaxPooling(2, 2, 2, 2)\n\n\ninput = np.array([0.5336726, 0.7963769, 0.5674766, 0.1803996, 0.2460861, 0.2295625, 0.3073633, 0.5973460, 0.4298954]).astype(\nfloat32\n)\ninput = input.reshape(1, 3, 3)\n\noutput = mp.forward(input)\nprint output\n\ngradOutput = np.array([0.023921491578221]).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(1, 1, 1)\n\ngradInput = mp.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nThe output is,\n\n\n[array([[[ 0.79637688]]], dtype=float32)]\n\n\n\n\nThe gradInput is,\n\n\n[array([[[ 0.        ,  0.02392149,  0.        ],\n        [ 0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ]]], dtype=float32)]\n\n\n\n\nSpatialAveragePooling\n\n\nScala:\n\n\nval m = SpatialAveragePooling(kW, kH, dW=1, dH=1, padW=0, padH=0, globalPooling=false, ceilMode=false, countIncludePad=true, divide=true, format=DataFormat.NCHW)\n\n\n\n\nPython:\n\n\nm = SpatialAveragePooling(kw, kh, dw=1, dh=1, pad_w=0, pad_h=0, global_pooling=False, ceil_mode=False, count_include_pad=True, divide=True, format=\nNCHW\n)\n\n\n\n\nSpatialAveragePooling is a module that applies 2D average-pooling operation in \nkW\nx\nkH\n regions by step size \ndW\nx\ndH\n.\n\n\nThe number of output features is equal to the number of input planes.\n\n\nAs for padding, when padW and padH are both -1, we use a padding algorithm similar to the \"SAME\" padding of tensorflow. That is\n\n\n outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)\n outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)\n\n padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)\n padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)\n\n padTop = padAlongHeight / 2\n padLeft = padAlongWidth / 2\n\n\n\n\nThe format parameter is a string value (or DataFormat Object in Scala) of \"NHWC\" or \"NCHW\" to specify the input data format of this layer. In \"NHWC\" format\ndata is stored in the order of [batch_size, height, width, channels], in \"NCHW\" format data is stored\nin the order of [batch_size, channels, height, width].\n\n\nScala example:\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(1, 3, 3).randn()\nval m = SpatialAveragePooling(3, 2, 2, 1)\nval output = m.forward(input)\nval gradOut = Tensor(1, 2, 1).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n(1,.,.) =\n0.9916249       1.0299556       0.5608558\n-0.1664829      1.5031902       0.48598626\n0.37362042      -0.0966136      -1.4257964\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n\nscala\n print(output)\n(1,.,.) =\n0.7341883\n0.1123173\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1]\n\nscala\n print(gradOut)\n(1,.,.) =\n-0.42837557\n-1.5104272\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x1]\n\nscala\n print(gradIn)\n(1,.,.) =\n-0.071395926    -0.071395926    -0.071395926\n-0.3231338      -0.3231338      -0.3231338\n-0.25173786     -0.25173786     -0.25173786\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.randn(1,3,3)\nprint \ninput is :\n,input\n\nm = SpatialAveragePooling(3,2,2,1)\nout = m.forward(input)\nprint \noutput of m is :\n,out\n\ngrad_out = np.random.rand(1,3,1)\ngrad_in = m.backward(input,grad_out)\nprint \ngrad input of m is :\n,grad_in\n\n\n\n\nproduces output:\n\n\ninput is : [[[ 1.50602425 -0.92869054 -1.9393117 ]\n  [ 0.31447547  0.63450578 -0.92485516]\n  [-2.07858315 -0.05688643  0.73648798]]]\ncreating: createSpatialAveragePooling\noutput of m is : [array([[[-0.22297533],\n        [-0.22914261]]], dtype=float32)]\ngrad input of m is : [array([[[ 0.06282618,  0.06282618,  0.06282618],\n        [ 0.09333335,  0.09333335,  0.09333335],\n        [ 0.03050717,  0.03050717,  0.03050717]]], dtype=float32)]\n\n\n\n\n\nVolumetricMaxPooling\n\n\nScala:\n\n\nval layer = VolumetricMaxPooling(\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)\n\n\n\n\nPython:\n\n\nlayer = VolumetricMaxPooling(\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)\n\n\n\n\nApplies 3D max-pooling operation in kT x kW x kH regions by step size dT x dW x dH.\nThe number of output features is equal to the number of input planes / dT.\nThe input can optionally be padded with zeros. Padding should be smaller than\nhalf of kernel size. That is, padT \n kT/2, padW \n kW/2 and padH \n kH/2\n\n\nThe input layout should be [batch, plane, time, height, width] or [plane, time, height, width]\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = VolumetricMaxPooling(\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\nval input = Tensor(T(T(\n  T(\n    T(1.0f, 2.0f, 3.0f),\n    T(4.0f, 5.0f, 6.0f),\n    T(7.0f, 8.0f, 9.0f)\n  ),\n  T(\n    T(4.0f, 5.0f, 6.0f),\n    T(1.0f, 3.0f, 9.0f),\n    T(2.0f, 3.0f, 7.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor(T(T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n)))))\n\n\n\n\nIts output should be\n\n\n(1,1,.,.) =\n5.0     9.0\n8.0     9.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\n(1,1,.,.) =\n0.0     0.0     0.0\n0.0     0.1     0.0\n0.0     0.3     0.4\n\n(1,2,.,.) =\n0.0     0.0     0.0\n0.0     0.0     0.2\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import VolumetricMaxPooling\nimport numpy as np\n\nlayer = VolumetricMaxPooling(\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\ninput = np.array([[\n  [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n  ],\n  [\n    [4.0, 5.0, 6.0],\n    [1.0, 3.0, 9.0],\n    [2.0, 3.0, 7.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[[\n  [0.1, 0.2],\n  [0.3, 0.4]\n]]]))\n\n\n\n\nIts output should be\n\n\narray([[[[ 5.,  9.],\n         [ 8.,  9.]]]], dtype=float32)\n\narray([[[[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.1       ,  0.        ],\n         [ 0.        ,  0.30000001,  0.40000001]],\n\n        [[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.        ,  0.2       ],\n         [ 0.        ,  0.        ,  0.        ]]]], dtype=float32)\n\n\n\n\nRoiPooling\n\n\nScala:\n\n\nval m =  RoiPooling(pooled_w, pooled_h, spatial_scale)\n\n\n\n\nPython:\n\n\nm = RoiPooling(pooled_w, pooled_h, spatial_scale)\n\n\n\n\nRoiPooling is a module that performs Region of Interest pooling. \n\n\nIt uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of pooledH \u00d7 pooledW (e.g., 7 \u00d7 7).\n\n\nAn RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its top-left corner (x1, y1) and its bottom-right corner (x2, y2).\n\n\nRoI max pooling works by dividing the h \u00d7 w RoI window into an pooledH \u00d7 pooledW grid of sub-windows of approximate size h/H \u00d7 w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel\n\n\nforward\n accepts a table containing 2 tensors as input, the first tensor is the input image, the second tensor is the ROI regions. The dimension of the second tensor should be (*,5) (5 are  \nbatch_num, x1, y1, x2, y2\n).  \n\n\nScala example:\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input_data = Tensor(2,2,6,8).randn()\nval rois = Array(0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3)\nval input_rois = Tensor(Storage(rois.map(x =\n x.toFloat))).resize(4, 5)\nval input = T(input_data,input_rois)\nval m = RoiPooling(3, 2, 1)\nval output = m.forward(input)\n\nscala\n print(input)\n {\n        2: 0.0  0.0     0.0     7.0     5.0\n           1.0  6.0     2.0     7.0     5.0\n           1.0  3.0     1.0     6.0     4.0\n           0.0  3.0     3.0     3.0     3.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4x5]\n        1: (1,1,.,.) =\n           0.48066297   1.0994664       0.32474303      2.3391871       -0.79605865     0.836963950.36107457      1.2622415\n           0.657079     0.12720469      0.39894578      -0.41185552     -0.53111094     -0.36016005       -0.9726423      -2.5785272\n           0.3091435    -0.03613516     0.2375721       -1.1920663      -0.6757661      1.10612681.5409279        -0.17411499\n           0.23274016   -0.7149633      0.5473105       -0.40570387     -1.7966263      0.2071798-1.1530842       -0.010083453\n           -1.5769979   0.17043112      -0.28578365     -0.90779626     0.61457515      -0.1553582-0.3912479      -0.15326484\n           -0.24283029  1.3215472       1.3795123       -0.36933053     0.7077386       -0.56398267       0.6159163       0.5802894\n\n           (1,2,.,.) =\n           -1.1817129   -0.20470592     -1.3201113      0.36523122      -0.18260211     1.30210171.214403 1.1019816\n           0.7186407    0.78731173      1.5452348       0.0396181       0.5927014       1.17697431.0501136        -0.58295316\n           -0.96753055  0.6427254       -1.1396345      0.8701054       -0.22860864     -1.18719451.3372624       0.8616691\n           0.796831     -0.16609778     0.2950535       0.4595303       0.192339        0.6086106-0.76351887      -0.65964174\n           -0.12746814  -0.036058053    0.8858275       0.9677718       -1.1074747      -1.36859390.8783633       -0.11723315\n           -0.6947403   -0.23226547     -1.8510057      -1.3695518      -0.22317407     -0.36249024       -0.24097045     1.5691053\n\n           (2,1,.,.) =\n           0.84056973   1.144949        -1.0660682      0.4416162       -0.94440234     -0.24461010.91145027      -0.88650835\n           -0.81542057  0.14578317      -0.6531974      0.60776395      -0.32058007     -1.80771481.7660322       1.0680646\n           1.1328241    0.43677545      -0.9402618      -1.3002211      0.26012567      1.69481340.37849447       0.39286092\n           1.9443163    0.5415504       1.0793099       1.3312546       0.48346 1.2019655       0.3718734 0.21091922\n           0.5499047    1.6418253       0.8064177       0.37626198      0.8736181       -0.40816033       -0.5806787      1.286581\n           -0.5904657   -0.21188398     -0.040509004    1.2989452       1.6827602       1.3229258-0.68433124      0.87974\n\n           (2,2,.,.) =\n           -0.09759476  -0.32767114     0.16223079      2.3114302       -0.48496276     1.19290720.8572289        0.43429425\n           -1.0245247   0.19002944      1.5659521       -1.3689835      -1.4437296      -0.38216656       0.6333655       -0.57124794\n           -0.31111157  1.5184602       -1.3835855      -0.9295573      2.244521        -1.11849820.5451996       -0.4441631\n           -1.534093    -0.5599659      1.1980947       -1.0140935      1.3288999       0.19487387-0.1261734      -1.2222558\n           -0.070535585 0.9047848       -0.6719811      -1.6532638      -0.5290511      -0.18300447       0.69385433      0.018756092\n           0.24767837   0.620484        -0.5346291      1.0685066       -0.36903372     -0.26955062       1.1042496       0.5944603\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x6x8]\n }\n\nscala\n print(output)\n(1,1,.,.) =\n1.0994664       2.3391871       1.5409279\n1.3795123       1.3795123       0.6159163\n\n(1,2,.,.) =\n1.5452348       1.5452348       1.3372624\n0.8858275       0.9677718       1.5691053\n\n(2,1,.,.) =\n0.37849447      0.39286092      0.39286092\n-0.5806787      1.286581        1.286581\n\n(2,2,.,.) =\n0.5451996       0.5451996       -0.4441631\n1.1042496       1.1042496       0.5944603\n\n(3,1,.,.) =\n0.60776395      1.6948134       1.7660322\n1.3312546       1.2019655       1.2019655\n\n(3,2,.,.) =\n2.244521        2.244521        0.6333655\n1.3288999       1.3288999       0.69385433\n\n(4,1,.,.) =\n-0.40570387     -0.40570387     -0.40570387\n-0.40570387     -0.40570387     -0.40570387\n\n(4,2,.,.) =\n0.4595303       0.4595303       0.4595303\n0.4595303       0.4595303       0.4595303\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x2x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput_data = np.random.randn(2,2,6,8)\ninput_rois = np.array([0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3],dtype='float64').reshape(4,5)\nprint \ninput is :\n,[input_data, input_rois]\n\nm = RoiPooling(3,2,1.0)\nout = m.forward([input_data,input_rois])\nprint \noutput of m is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [array([[[[ 0.08500103,  0.33421796,  0.29084699,  1.60344635, -0.24289341,\n          -0.4793888 ,  0.09452426,  0.16842477],\n         [-1.18575497, -0.53337542,  0.11661001,  0.9647904 , -0.25187936,\n           0.36516823, -0.16647209, -0.08095158],\n         [ 1.1982232 , -0.33549174,  0.11721347, -0.29319686, -0.01290122,\n           0.12344296,  0.30074829, -2.34951463],\n         [-0.60470899, -0.84657051,  0.1269276 , -0.06152321, -1.68838416,\n          -0.69808296, -2.06112892, -1.44790449],\n         [ 1.03944288,  0.13871728,  0.91478479,  0.47517105,  1.24638374,\n           0.98666841,  0.49403488,  1.26101127],\n         [-1.03949343, -0.39291108,  1.39107512,  1.73779253,  0.91656129,\n           0.103381  ,  0.956243  ,  0.44743548]],\n\n        [[ 0.79028054,  0.64244228, -0.37997334, -0.09130215, -2.3903429 ,\n           0.71919208, -0.14079786,  0.98304272],\n         [ 1.14678457,  1.58825227,  0.17137367, -0.62121819, -0.36103113,\n          -0.04452576, -0.0886136 , -1.32884721],\n         [ 0.06728957, -0.29701304, -0.52754207, -1.5785875 ,  1.47354834,\n          -0.28545156,  0.49874194,  0.10277613],\n         [-0.10117571, -1.34902427, -1.40789327,  0.09853599,  0.60420022,\n           0.54869115, -0.49067696,  0.26696793],\n         [ 1.11780279, -0.77929016,  1.13772094,  0.14374057,  0.33199688,\n          -0.54057374, -0.45718861,  1.1577623 ],\n         [-1.4005645 ,  1.15870496,  0.39292003,  0.88379515,  0.06440974,\n           0.65013063,  0.03759244,  0.18730126]]],\n\n\n       [[[-2.28272906,  0.06056305,  0.73632597,  0.10063274, -1.27497525,\n          -0.95597581, -0.22745785,  0.40146498],\n         [-1.37783475,  1.66000653, -1.80071745, -0.11805539, -0.27160583,\n           0.30691418,  2.62243232, -1.95274516],\n         [ 1.61364148,  1.91470546, -1.51984424,  2.13598224, -0.23156685,\n          -0.74203698,  0.65316888,  0.08018846],\n         [-1.8912854 , -0.50106158,  0.94937966, -0.10930541,  0.82136627,\n          -1.33209063,  1.43371302, -1.36370916],\n         [-0.52737928, -0.0681305 , -0.63472587,  0.41979229, -0.57093624,\n          -0.15968764, -1.005951  , -2.06873572],\n         [-2.34089346,  1.02593977,  0.90183415,  0.09504819,  0.53185448,\n           1.11305345,  1.290016  , -1.76216646]],\n\n        [[-0.10885459, -0.57089742, -0.55340708, -1.94445884,  1.30130049,\n           0.6333372 , -1.03100083,  0.0111167 ],\n         [ 0.59678149, -0.67601521, -1.25288718, -0.10922251,  3.06808996,\n          -1.46701513, -0.42140765,  1.12485412],\n         [ 1.21301567, -1.43304957, -0.56047239,  0.20716087,  1.40737646,\n          -0.08386437, -0.21916043,  0.85692906],\n         [ 1.59992399, -1.37044315, -0.71884386,  2.61830979, -0.74305496,\n          -0.32021174,  1.43275058, -0.3891857 ],\n         [-0.41355145,  0.22589689,  0.33154415,  0.86146815, -1.66326091,\n           0.37581697, -3.2250516 , -0.48807863],\n         [-2.52968957,  0.95801598, -1.20118154,  0.01141421, -0.11871498,\n           0.04555184,  1.3950473 ,  0.37887998]]]]), array([[ 0.,  0.,  0.,  7.,  5.],\n       [ 1.,  6.,  2.,  7.,  5.],\n       [ 1.,  3.,  1.,  6.,  4.],\n       [ 0.,  3.,  3.,  3.,  3.]])]\ncreating: createRoiPooling\noutput of m is : [[[[ 1.19822323  1.60344636  0.36516821]\n   [ 1.39107513  1.73779249  1.26101124]]\n\n  [[ 1.58825231  1.47354829  0.98304272]\n   [ 1.158705    1.13772094  1.15776229]]]\n\n\n [[[ 1.43371308  1.43371308  0.08018846]\n   [ 1.29001606  1.29001606 -1.7621665 ]]\n\n  [[ 1.43275058  1.43275058  0.85692906]\n   [ 1.39504731  1.39504731  0.37887999]]]\n\n\n [[[ 2.13598228  0.30691418  2.62243223]\n   [ 0.82136625  0.82136625  1.43371308]]\n\n  [[ 3.06808996  3.06808996 -0.08386437]\n   [ 2.61830974  0.37581697  1.43275058]]]\n\n\n [[[-0.06152321 -0.06152321 -0.06152321]\n   [-0.06152321 -0.06152321 -0.06152321]]\n\n  [[ 0.09853599  0.09853599  0.09853599]\n   [ 0.09853599  0.09853599  0.09853599]]]]", 
            "title": "Pooling Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Pooling-Layers/#spatialmaxpooling", 
            "text": "Scala:  val mp = SpatialMaxPooling(2, 2, dW=2, dH=2, padW=0, padH=0, format=DataFormat.NCHW)  Python:  mp = SpatialMaxPooling(2, 2, dw=2, dh=2, pad_w=0, pad_h=0, to_ceil=false, format= NCHW )  Applies 2D max-pooling operation in kWxkH regions by step size dWxdH steps.\nThe number of output features is equal to the number of input planes.\nIf the input image is a 3D tensor nInputPlane x height x width,\nthe output image size will be nOutputPlane x oheight x owidth where   owidth  = op((width  + 2*padW - kW) / dW + 1)  oheight = op((height + 2*padH - kH) / dH + 1)   op is a rounding operator. By default, it is floor.\nIt can be changed by calling ceil() or floor() methods.  As for padding, when padW and padH are both -1, we use a padding algorithm similar to the \"SAME\" padding of tensorflow. That is   outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)\n outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)\n\n padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)\n padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)\n\n padTop = padAlongHeight / 2\n padLeft = padAlongWidth / 2  The format parameter is a string value (or DataFormat Object in Scala) of \"NHWC\" or \"NCHW\" to specify the input data format of this layer. In \"NHWC\" format\ndata is stored in the order of [batch_size, height, width, channels], in \"NCHW\" format data is stored\nin the order of [batch_size, channels, height, width].  Scala example:  import com.intel.analytics.bigdl.nn.SpatialMaxPooling\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mp = SpatialMaxPooling(2, 2, 2, 2)\nval input = Tensor(1, 3, 3)\n\ninput(Array(1, 1, 1)) = 0.5336726f\ninput(Array(1, 1, 2)) = 0.7963769f\ninput(Array(1, 1, 3)) = 0.5674766f\ninput(Array(1, 2, 1)) = 0.1803996f\ninput(Array(1, 2, 2)) = 0.2460861f\ninput(Array(1, 2, 3)) = 0.2295625f\ninput(Array(1, 3, 1)) = 0.3073633f\ninput(Array(1, 3, 2)) = 0.5973460f\ninput(Array(1, 3, 3)) = 0.4298954f\n\nval gradOutput = Tensor(1, 1, 1)\ngradOutput(Array(1, 1, 1)) = 0.023921491578221f\n\nval output = mp.forward(input)\nval gradInput = mp.backward(input, gradOutput)\n\nprintln(output)\nprintln(gradInput)  The output is,  (1,.,.) =\n0.7963769\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1]  The gradInput is,  (1,.,.) =\n0.0     0.023921492     0.0\n0.0     0.0     0.0\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmp = SpatialMaxPooling(2, 2, 2, 2)\n\n\ninput = np.array([0.5336726, 0.7963769, 0.5674766, 0.1803996, 0.2460861, 0.2295625, 0.3073633, 0.5973460, 0.4298954]).astype( float32 )\ninput = input.reshape(1, 3, 3)\n\noutput = mp.forward(input)\nprint output\n\ngradOutput = np.array([0.023921491578221]).astype( float32 )\ngradOutput = gradOutput.reshape(1, 1, 1)\n\ngradInput = mp.backward(input, gradOutput)\nprint gradInput  The output is,  [array([[[ 0.79637688]]], dtype=float32)]  The gradInput is,  [array([[[ 0.        ,  0.02392149,  0.        ],\n        [ 0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ]]], dtype=float32)]", 
            "title": "SpatialMaxPooling"
        }, 
        {
            "location": "/APIGuide/Layers/Pooling-Layers/#spatialaveragepooling", 
            "text": "Scala:  val m = SpatialAveragePooling(kW, kH, dW=1, dH=1, padW=0, padH=0, globalPooling=false, ceilMode=false, countIncludePad=true, divide=true, format=DataFormat.NCHW)  Python:  m = SpatialAveragePooling(kw, kh, dw=1, dh=1, pad_w=0, pad_h=0, global_pooling=False, ceil_mode=False, count_include_pad=True, divide=True, format= NCHW )  SpatialAveragePooling is a module that applies 2D average-pooling operation in  kW x kH  regions by step size  dW x dH .  The number of output features is equal to the number of input planes.  As for padding, when padW and padH are both -1, we use a padding algorithm similar to the \"SAME\" padding of tensorflow. That is   outHeight = Math.ceil(inHeight.toFloat/strideH.toFloat)\n outWidth = Math.ceil(inWidth.toFloat/strideW.toFloat)\n\n padAlongHeight = Math.max(0, (outHeight - 1) * strideH + kernelH - inHeight)\n padAlongWidth = Math.max(0, (outWidth - 1) * strideW + kernelW - inWidth)\n\n padTop = padAlongHeight / 2\n padLeft = padAlongWidth / 2  The format parameter is a string value (or DataFormat Object in Scala) of \"NHWC\" or \"NCHW\" to specify the input data format of this layer. In \"NHWC\" format\ndata is stored in the order of [batch_size, height, width, channels], in \"NCHW\" format data is stored\nin the order of [batch_size, channels, height, width].  Scala example:  scala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(1, 3, 3).randn()\nval m = SpatialAveragePooling(3, 2, 2, 1)\nval output = m.forward(input)\nval gradOut = Tensor(1, 2, 1).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n(1,.,.) =\n0.9916249       1.0299556       0.5608558\n-0.1664829      1.5031902       0.48598626\n0.37362042      -0.0966136      -1.4257964\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x3x3]\n\nscala  print(output)\n(1,.,.) =\n0.7341883\n0.1123173\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1]\n\nscala  print(gradOut)\n(1,.,.) =\n-0.42837557\n-1.5104272\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x1]\n\nscala  print(gradIn)\n(1,.,.) =\n-0.071395926    -0.071395926    -0.071395926\n-0.3231338      -0.3231338      -0.3231338\n-0.25173786     -0.25173786     -0.25173786\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.randn(1,3,3)\nprint  input is : ,input\n\nm = SpatialAveragePooling(3,2,2,1)\nout = m.forward(input)\nprint  output of m is : ,out\n\ngrad_out = np.random.rand(1,3,1)\ngrad_in = m.backward(input,grad_out)\nprint  grad input of m is : ,grad_in  produces output:  input is : [[[ 1.50602425 -0.92869054 -1.9393117 ]\n  [ 0.31447547  0.63450578 -0.92485516]\n  [-2.07858315 -0.05688643  0.73648798]]]\ncreating: createSpatialAveragePooling\noutput of m is : [array([[[-0.22297533],\n        [-0.22914261]]], dtype=float32)]\ngrad input of m is : [array([[[ 0.06282618,  0.06282618,  0.06282618],\n        [ 0.09333335,  0.09333335,  0.09333335],\n        [ 0.03050717,  0.03050717,  0.03050717]]], dtype=float32)]", 
            "title": "SpatialAveragePooling"
        }, 
        {
            "location": "/APIGuide/Layers/Pooling-Layers/#volumetricmaxpooling", 
            "text": "Scala:  val layer = VolumetricMaxPooling(\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)  Python:  layer = VolumetricMaxPooling(\n  kernelT, kernelW, kernelH,\n  strideT, strideW, strideH,\n  paddingT, paddingW, paddingH\n)  Applies 3D max-pooling operation in kT x kW x kH regions by step size dT x dW x dH.\nThe number of output features is equal to the number of input planes / dT.\nThe input can optionally be padded with zeros. Padding should be smaller than\nhalf of kernel size. That is, padT   kT/2, padW   kW/2 and padH   kH/2  The input layout should be [batch, plane, time, height, width] or [plane, time, height, width]  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = VolumetricMaxPooling(\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\nval input = Tensor(T(T(\n  T(\n    T(1.0f, 2.0f, 3.0f),\n    T(4.0f, 5.0f, 6.0f),\n    T(7.0f, 8.0f, 9.0f)\n  ),\n  T(\n    T(4.0f, 5.0f, 6.0f),\n    T(1.0f, 3.0f, 9.0f),\n    T(2.0f, 3.0f, 7.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor(T(T(T(\n  T(0.1f, 0.2f),\n  T(0.3f, 0.4f)\n)))))  Its output should be  (1,1,.,.) =\n5.0     9.0\n8.0     9.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2]\n\n(1,1,.,.) =\n0.0     0.0     0.0\n0.0     0.1     0.0\n0.0     0.3     0.4\n\n(1,2,.,.) =\n0.0     0.0     0.0\n0.0     0.0     0.2\n0.0     0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x3]  Python example:  from bigdl.nn.layer import VolumetricMaxPooling\nimport numpy as np\n\nlayer = VolumetricMaxPooling(\n  2, 2, 2,\n  1, 1, 1,\n  0, 0, 0\n)\n\ninput = np.array([[\n  [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n  ],\n  [\n    [4.0, 5.0, 6.0],\n    [1.0, 3.0, 9.0],\n    [2.0, 3.0, 7.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[[\n  [0.1, 0.2],\n  [0.3, 0.4]\n]]]))  Its output should be  array([[[[ 5.,  9.],\n         [ 8.,  9.]]]], dtype=float32)\n\narray([[[[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.1       ,  0.        ],\n         [ 0.        ,  0.30000001,  0.40000001]],\n\n        [[ 0.        ,  0.        ,  0.        ],\n         [ 0.        ,  0.        ,  0.2       ],\n         [ 0.        ,  0.        ,  0.        ]]]], dtype=float32)", 
            "title": "VolumetricMaxPooling"
        }, 
        {
            "location": "/APIGuide/Layers/Pooling-Layers/#roipooling", 
            "text": "Scala:  val m =  RoiPooling(pooled_w, pooled_h, spatial_scale)  Python:  m = RoiPooling(pooled_w, pooled_h, spatial_scale)  RoiPooling is a module that performs Region of Interest pooling.   It uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of pooledH \u00d7 pooledW (e.g., 7 \u00d7 7).  An RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (x1, y1, x2, y2) that specifies its top-left corner (x1, y1) and its bottom-right corner (x2, y2).  RoI max pooling works by dividing the h \u00d7 w RoI window into an pooledH \u00d7 pooledW grid of sub-windows of approximate size h/H \u00d7 w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel  forward  accepts a table containing 2 tensors as input, the first tensor is the input image, the second tensor is the ROI regions. The dimension of the second tensor should be (*,5) (5 are   batch_num, x1, y1, x2, y2 ).    Scala example:  scala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input_data = Tensor(2,2,6,8).randn()\nval rois = Array(0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3)\nval input_rois = Tensor(Storage(rois.map(x =  x.toFloat))).resize(4, 5)\nval input = T(input_data,input_rois)\nval m = RoiPooling(3, 2, 1)\nval output = m.forward(input)\n\nscala  print(input)\n {\n        2: 0.0  0.0     0.0     7.0     5.0\n           1.0  6.0     2.0     7.0     5.0\n           1.0  3.0     1.0     6.0     4.0\n           0.0  3.0     3.0     3.0     3.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 4x5]\n        1: (1,1,.,.) =\n           0.48066297   1.0994664       0.32474303      2.3391871       -0.79605865     0.836963950.36107457      1.2622415\n           0.657079     0.12720469      0.39894578      -0.41185552     -0.53111094     -0.36016005       -0.9726423      -2.5785272\n           0.3091435    -0.03613516     0.2375721       -1.1920663      -0.6757661      1.10612681.5409279        -0.17411499\n           0.23274016   -0.7149633      0.5473105       -0.40570387     -1.7966263      0.2071798-1.1530842       -0.010083453\n           -1.5769979   0.17043112      -0.28578365     -0.90779626     0.61457515      -0.1553582-0.3912479      -0.15326484\n           -0.24283029  1.3215472       1.3795123       -0.36933053     0.7077386       -0.56398267       0.6159163       0.5802894\n\n           (1,2,.,.) =\n           -1.1817129   -0.20470592     -1.3201113      0.36523122      -0.18260211     1.30210171.214403 1.1019816\n           0.7186407    0.78731173      1.5452348       0.0396181       0.5927014       1.17697431.0501136        -0.58295316\n           -0.96753055  0.6427254       -1.1396345      0.8701054       -0.22860864     -1.18719451.3372624       0.8616691\n           0.796831     -0.16609778     0.2950535       0.4595303       0.192339        0.6086106-0.76351887      -0.65964174\n           -0.12746814  -0.036058053    0.8858275       0.9677718       -1.1074747      -1.36859390.8783633       -0.11723315\n           -0.6947403   -0.23226547     -1.8510057      -1.3695518      -0.22317407     -0.36249024       -0.24097045     1.5691053\n\n           (2,1,.,.) =\n           0.84056973   1.144949        -1.0660682      0.4416162       -0.94440234     -0.24461010.91145027      -0.88650835\n           -0.81542057  0.14578317      -0.6531974      0.60776395      -0.32058007     -1.80771481.7660322       1.0680646\n           1.1328241    0.43677545      -0.9402618      -1.3002211      0.26012567      1.69481340.37849447       0.39286092\n           1.9443163    0.5415504       1.0793099       1.3312546       0.48346 1.2019655       0.3718734 0.21091922\n           0.5499047    1.6418253       0.8064177       0.37626198      0.8736181       -0.40816033       -0.5806787      1.286581\n           -0.5904657   -0.21188398     -0.040509004    1.2989452       1.6827602       1.3229258-0.68433124      0.87974\n\n           (2,2,.,.) =\n           -0.09759476  -0.32767114     0.16223079      2.3114302       -0.48496276     1.19290720.8572289        0.43429425\n           -1.0245247   0.19002944      1.5659521       -1.3689835      -1.4437296      -0.38216656       0.6333655       -0.57124794\n           -0.31111157  1.5184602       -1.3835855      -0.9295573      2.244521        -1.11849820.5451996       -0.4441631\n           -1.534093    -0.5599659      1.1980947       -1.0140935      1.3288999       0.19487387-0.1261734      -1.2222558\n           -0.070535585 0.9047848       -0.6719811      -1.6532638      -0.5290511      -0.18300447       0.69385433      0.018756092\n           0.24767837   0.620484        -0.5346291      1.0685066       -0.36903372     -0.26955062       1.1042496       0.5944603\n\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x6x8]\n }\n\nscala  print(output)\n(1,1,.,.) =\n1.0994664       2.3391871       1.5409279\n1.3795123       1.3795123       0.6159163\n\n(1,2,.,.) =\n1.5452348       1.5452348       1.3372624\n0.8858275       0.9677718       1.5691053\n\n(2,1,.,.) =\n0.37849447      0.39286092      0.39286092\n-0.5806787      1.286581        1.286581\n\n(2,2,.,.) =\n0.5451996       0.5451996       -0.4441631\n1.1042496       1.1042496       0.5944603\n\n(3,1,.,.) =\n0.60776395      1.6948134       1.7660322\n1.3312546       1.2019655       1.2019655\n\n(3,2,.,.) =\n2.244521        2.244521        0.6333655\n1.3288999       1.3288999       0.69385433\n\n(4,1,.,.) =\n-0.40570387     -0.40570387     -0.40570387\n-0.40570387     -0.40570387     -0.40570387\n\n(4,2,.,.) =\n0.4595303       0.4595303       0.4595303\n0.4595303       0.4595303       0.4595303\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x2x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput_data = np.random.randn(2,2,6,8)\ninput_rois = np.array([0, 0, 0, 7, 5, 1, 6, 2, 7, 5, 1, 3, 1, 6, 4, 0, 3, 3, 3, 3],dtype='float64').reshape(4,5)\nprint  input is : ,[input_data, input_rois]\n\nm = RoiPooling(3,2,1.0)\nout = m.forward([input_data,input_rois])\nprint  output of m is : ,out  produces output:  input is : [array([[[[ 0.08500103,  0.33421796,  0.29084699,  1.60344635, -0.24289341,\n          -0.4793888 ,  0.09452426,  0.16842477],\n         [-1.18575497, -0.53337542,  0.11661001,  0.9647904 , -0.25187936,\n           0.36516823, -0.16647209, -0.08095158],\n         [ 1.1982232 , -0.33549174,  0.11721347, -0.29319686, -0.01290122,\n           0.12344296,  0.30074829, -2.34951463],\n         [-0.60470899, -0.84657051,  0.1269276 , -0.06152321, -1.68838416,\n          -0.69808296, -2.06112892, -1.44790449],\n         [ 1.03944288,  0.13871728,  0.91478479,  0.47517105,  1.24638374,\n           0.98666841,  0.49403488,  1.26101127],\n         [-1.03949343, -0.39291108,  1.39107512,  1.73779253,  0.91656129,\n           0.103381  ,  0.956243  ,  0.44743548]],\n\n        [[ 0.79028054,  0.64244228, -0.37997334, -0.09130215, -2.3903429 ,\n           0.71919208, -0.14079786,  0.98304272],\n         [ 1.14678457,  1.58825227,  0.17137367, -0.62121819, -0.36103113,\n          -0.04452576, -0.0886136 , -1.32884721],\n         [ 0.06728957, -0.29701304, -0.52754207, -1.5785875 ,  1.47354834,\n          -0.28545156,  0.49874194,  0.10277613],\n         [-0.10117571, -1.34902427, -1.40789327,  0.09853599,  0.60420022,\n           0.54869115, -0.49067696,  0.26696793],\n         [ 1.11780279, -0.77929016,  1.13772094,  0.14374057,  0.33199688,\n          -0.54057374, -0.45718861,  1.1577623 ],\n         [-1.4005645 ,  1.15870496,  0.39292003,  0.88379515,  0.06440974,\n           0.65013063,  0.03759244,  0.18730126]]],\n\n\n       [[[-2.28272906,  0.06056305,  0.73632597,  0.10063274, -1.27497525,\n          -0.95597581, -0.22745785,  0.40146498],\n         [-1.37783475,  1.66000653, -1.80071745, -0.11805539, -0.27160583,\n           0.30691418,  2.62243232, -1.95274516],\n         [ 1.61364148,  1.91470546, -1.51984424,  2.13598224, -0.23156685,\n          -0.74203698,  0.65316888,  0.08018846],\n         [-1.8912854 , -0.50106158,  0.94937966, -0.10930541,  0.82136627,\n          -1.33209063,  1.43371302, -1.36370916],\n         [-0.52737928, -0.0681305 , -0.63472587,  0.41979229, -0.57093624,\n          -0.15968764, -1.005951  , -2.06873572],\n         [-2.34089346,  1.02593977,  0.90183415,  0.09504819,  0.53185448,\n           1.11305345,  1.290016  , -1.76216646]],\n\n        [[-0.10885459, -0.57089742, -0.55340708, -1.94445884,  1.30130049,\n           0.6333372 , -1.03100083,  0.0111167 ],\n         [ 0.59678149, -0.67601521, -1.25288718, -0.10922251,  3.06808996,\n          -1.46701513, -0.42140765,  1.12485412],\n         [ 1.21301567, -1.43304957, -0.56047239,  0.20716087,  1.40737646,\n          -0.08386437, -0.21916043,  0.85692906],\n         [ 1.59992399, -1.37044315, -0.71884386,  2.61830979, -0.74305496,\n          -0.32021174,  1.43275058, -0.3891857 ],\n         [-0.41355145,  0.22589689,  0.33154415,  0.86146815, -1.66326091,\n           0.37581697, -3.2250516 , -0.48807863],\n         [-2.52968957,  0.95801598, -1.20118154,  0.01141421, -0.11871498,\n           0.04555184,  1.3950473 ,  0.37887998]]]]), array([[ 0.,  0.,  0.,  7.,  5.],\n       [ 1.,  6.,  2.,  7.,  5.],\n       [ 1.,  3.,  1.,  6.,  4.],\n       [ 0.,  3.,  3.,  3.,  3.]])]\ncreating: createRoiPooling\noutput of m is : [[[[ 1.19822323  1.60344636  0.36516821]\n   [ 1.39107513  1.73779249  1.26101124]]\n\n  [[ 1.58825231  1.47354829  0.98304272]\n   [ 1.158705    1.13772094  1.15776229]]]\n\n\n [[[ 1.43371308  1.43371308  0.08018846]\n   [ 1.29001606  1.29001606 -1.7621665 ]]\n\n  [[ 1.43275058  1.43275058  0.85692906]\n   [ 1.39504731  1.39504731  0.37887999]]]\n\n\n [[[ 2.13598228  0.30691418  2.62243223]\n   [ 0.82136625  0.82136625  1.43371308]]\n\n  [[ 3.06808996  3.06808996 -0.08386437]\n   [ 2.61830974  0.37581697  1.43275058]]]\n\n\n [[[-0.06152321 -0.06152321 -0.06152321]\n   [-0.06152321 -0.06152321 -0.06152321]]\n\n  [[ 0.09853599  0.09853599  0.09853599]\n   [ 0.09853599  0.09853599  0.09853599]]]]", 
            "title": "RoiPooling"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/", 
            "text": "SoftSign\n\n\nScala:\n\n\nval softSign = SoftSign()\n\n\n\n\nPython:\n\n\nsoftSign = SoftSign()\n\n\n\n\nSoftSign applies SoftSign function to the input tensor\n\n\nSoftSign function: \nf_i(x) = x_i / (1+|x_i|)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval softSign = SoftSign()\nval input = Tensor(3, 3).rand()\n\n\n print(input)\n0.6733504   0.7566517   0.43793806  \n0.09683273  0.05829774  0.4567967   \n0.20021072  0.11158377  0.31668025\n\n\n print(softSign.forward(input))\n0.40239656  0.4307352   0.30455974  \n0.08828395  0.05508633  0.31356242  \n0.16681297  0.10038269  0.24051417  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nsoftSign=SoftSign()\n\n softSign.forward(np.array([[1, 2, 4],[-1, -2, -4]]))\n[array([[ 0.5       ,  0.66666669,  0.80000001],\n       [-0.5       , -0.66666669, -0.80000001]], dtype=float32)]\n\n\n\n\n\n\n\nReLU6\n\n\nScala:\n\n\nval module = ReLU6(inplace = false)\n\n\n\n\nPython:\n\n\nmodule = ReLU6(inplace=False)\n\n\n\n\nSame as ReLU except that the rectifying function f(x) saturates at x = 6 \nReLU6 is defined as:\n\nf(x) = min(max(0, x), 6)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = ReLU6()\n\nprintln(module.forward(Tensor.range(-2, 8, 1)))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n6.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 11]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = ReLU6()\n\nprint(module.forward(np.arange(-2, 9, 1)))\n\n\n\n\nGives the output,\n\n\n[array([ 0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  6.,  6.], dtype=float32)]\n\n\n\n\n\n\nTanhShrink\n\n\nScala:\n\n\nval tanhShrink = TanhShrink()\n\n\n\n\nPython:\n\n\ntanhShrink = TanhShrink()\n\n\n\n\nTanhShrink applies element-wise Tanh and Shrink function to the input\n\n\nTanhShrink function : \nf(x) = scala.math.tanh(x) - 1\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval tanhShrink = TanhShrink()\nval input = Tensor(3, 3).rand()\n\n\n print(input)\n0.7056571   0.25239098  0.75746965  \n0.89736927  0.31193605  0.23842576  \n0.69492024  0.7512544   0.8386124   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n print(tanhShrink.forward(input))\n0.09771085  0.0052260756    0.11788553  \n0.18235475  0.009738684 0.004417494 \n0.09378672  0.1153577   0.153539    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\ntanhShrink = TanhShrink()\n\n\n  tanhShrink.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 0.23840582,  1.03597236,  2.00494528],\n       [ 3.00067067,  4.0000906 ,  5.0000124 ],\n       [ 6.00000191,  7.        ,  8.        ]], dtype=float32)]\n\n\n\n\n\n\n\nSoftMax\n\n\nScala:\n\n\nval layer = SoftMax()\n\n\n\n\nPython:\n\n\nlayer = SoftMax()\n\n\n\n\nApplies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.\nSoftmax is defined as:\nf_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)\n\nwhere \nshift = max_i(x_i)\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SoftMax()\nval input = Tensor(3)\ninput.apply1(_ =\n 1.0f * 10)\nval gradOutput = Tensor(T(\n1.0f,\n0.0f,\n0.0f\n))\nval output = layer.forward(input)\nval gradient = layer.backward(input, gradOutput)\n-\n print(output)\n0.33333334\n0.33333334\n0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n-\n print(gradient)\n0.22222221\n-0.11111112\n-0.11111112\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nlayer = SoftMax()\ninput = np.ones(3)*10\ngrad_output = np.array([1.0, 0.0, 0.0])\noutput = layer.forward(input)\ngradient = layer.backward(input, grad_output)\n-\n print output\n[ 0.33333334  0.33333334  0.33333334]\n-\n print gradient\n[ 0.22222221 -0.11111112 -0.11111112]\n\n\n\n\n\n\nPReLU\n\n\nScala:\n\n\nval module = PReLU(nOutputPlane = 0)\n\n\n\n\nPython:\n\n\nmodule = PReLU(nOutputPlane=0)\n\n\n\n\nApplies parametric ReLU, which parameter varies the slope of the negative part.\n\n\nPReLU: f(x) = max(0, x) + a * min(0, x)\n\n\n\n\nnOutputPlane's default value is 0, that means using PReLU in shared version and has\nonly one parameters. nOutputPlane is the input map number(Default is 0).\n\n\nNotice: Please don't use weight decay on this.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = PReLU(2)\nval input = Tensor(2, 2, 3).randn()\nval output = module.forward(input)\n\n\n input\n(1,.,.) =\n-0.17810068 -0.69607687 0.25582042\n-1.2140307  -1.5410945  1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.16203058 -0.5643519  0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x3]\n\n\n output\n(1,.,.) =\n-0.04452517 -0.17401922 0.25582042\n-0.3035077  -0.38527364 1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.040507644    -0.14108798 0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = PReLU(2)\ninput = np.random.randn(2, 2, 3)\noutput = module.forward(input)\n\n\n input\n[[[ 2.50596953 -0.06593339 -1.90273409]\n  [ 0.2464341   0.45941315 -0.41977094]]\n\n [[-0.8584367   2.19389229  0.93136755]\n  [-0.39209027  0.16507514 -0.35850447]]]\n\n\n output\n[array([[[ 2.50596952, -0.01648335, -0.47568351],\n         [ 0.24643411,  0.45941314, -0.10494273]],\n\n        [[-0.21460918,  2.19389224,  0.93136758],\n         [-0.09802257,  0.16507514, -0.08962612]]], dtype=float32)]\n\n\n\n\n\n\nReLU\n\n\nScala:\n\n\nval relu = ReLU(ip = false)\n\n\n\n\nPython:\n\n\nrelu = ReLU(ip)\n\n\n\n\nReLU applies the element-wise rectified linear unit (ReLU) function to the input\n\n\nip\n illustrate if the ReLU fuction is done on the origin input\n\n\nReLU function : f(x) = max(0, x)\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval relu = ReLU(false)\n\nval input = Tensor(3, 3).rand()\n\n print(input)\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n print(relu.forward(input))\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nrelu = ReLU(False)\n\n relu.forward(np.array([[-1, -2, -3], [0, 0, 0], [1, 2, 3]]))\n[array([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 1.,  2.,  3.]], dtype=float32)]\n\n\n\n\n\n\n\nSoftMin\n\n\nScala:\n\n\nval sm = SoftMin()\n\n\n\n\nPython:\n\n\nsm = SoftMin()\n\n\n\n\nApplies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.\nSoftmin is defined as: \nf_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)\n\nwhere \nshift = max_i(-x_i)\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.SoftMin\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval sm = SoftMin()\nval input = Tensor(3, 3).range(1, 3 * 3)\n\nval output = sm.forward(input)\n\nval gradOutput = Tensor(3, 3).range(1, 3 * 3).apply1(x =\n (x / 10.0).toFloat)\nval gradInput = sm.backward(input, gradOutput)\n\n\n\n\n\nGives the output,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nGives the gradInput,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.02825874      -0.014077038    -0.014181711\n0.028258756     -0.01407703     -0.01418171\n0.028258756     -0.014077038    -0.014181707\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsm = SoftMin()\n\ninput = np.arange(1, 10, 1).astype(\nfloat32\n)\ninput = input.reshape(3, 3)\n\noutput = sm.forward(input)\nprint output\n\ngradOutput = np.arange(1, 10, 1).astype(\nfloat32\n)\ngradOutput = np.vectorize(lambda t: t / 10)(gradOutput)\ngradOutput = gradOutput.reshape(3, 3)\n\ngradInput = sm.backward(input, gradOutput)\nprint gradInput\n\n\n\n\n\n\n\nELU\n\n\nScala:\n\n\nval m = ELU(alpha = 1.0, inplace = false)\n\n\n\n\nPython:\n\n\nm = ELU(alpha=1.0, inplace=False)\n\n\n\n\nApplies exponential linear unit (\nELU\n), which parameter a varies the convergence value of the exponential function below zero:\n\n\nELU\n is defined as:\n\n\nf(x) = max(0, x) + min(0, alpha * (exp(x) - 1))\n\n\n\n\nThe output dimension is always equal to input dimension.\n\n\nFor reference see \nFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval xs = Tensor(4).randn()\nprintln(xs)\nprintln(ELU(4).forward(xs))\n\n\n\n\n1.0217569\n-0.17189966\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n1.0217569\n-0.63174534\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\nxs = np.linspace(-3, 3, num=200)\ngo = np.ones(200)\n\ndef f(a):\n    return ELU(a).forward(xs)[0]\ndef df(a):\n    m = ELU(a)\n    m.forward(xs)\n    return m.backward(xs, go)[0]\n\nplt.plot(xs, f(0.1), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, f(1.0), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, df(0.1), '-', label='dw ELU, alpha = 0.1')\nplt.plot(xs, df(1.0), '-', label='dw ELU, alpha = 0.1')\n\nplt.legend(loc='best', shadow=True, fancybox=True)\nplt.show()\n\n\n\n\n\n\n\nSoftShrink\n\n\nScala:\n\n\nval layer = SoftShrink(lambda = 0.5)\n\n\n\n\nPython:\n\n\nlayer = SoftShrink(the_lambda=0.5)\n\n\n\n\nApply the soft shrinkage function element-wise to the input Tensor\n\n\nSoftShrinkage operator:\n\n\n       \u23a7 x - lambda, if x \n  lambda\nf(x) = \u23a8 x + lambda, if x \n -lambda\n       \u23a9 0, otherwise\n\n\n\n\nParameters:\n* \nlambda\n a factor, default is 0.5\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.SoftShrink\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = SoftShrink()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.5    1.5 2.5\n-1.5    2.5 3.5\n-2.5    3.5 4.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n3.0 4.0 5.0\n2.0 3.0 4.0\n1.0 2.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nScala example:\n\n\nactivation = SoftShrink()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n\nprint grad\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]\n\n\n\n\n\n\nSigmoid\n\n\nScala:\n\n\nval module = Sigmoid()\n\n\n\n\nPython:\n\n\nmodule = Sigmoid()\n\n\n\n\nApplies the Sigmoid function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.\n\n\nSigmoid is defined as: \nf(x) = 1 / (1 + exp(-x))\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = new Sigmoid()\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =\n {i += 1; i})\n\n print(layer.forward(input))\n0.7310586   0.880797    0.95257413  \n0.98201376  0.9933072   0.9975274   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = Sigmoid()\ninput = np.array([[1, 2, 3], [4, 5, 6]])\n\nlayer.forward(input)\narray([[ 0.7310586 ,  0.88079703,  0.95257413],\n       [ 0.98201376,  0.99330717,  0.99752742]], dtype=float32)\n\n\n\n\n\n\nTanh\n\n\nScala:\n\n\nval activation = Tanh()\n\n\n\n\nPython:\n\n\nactivation = Tanh()\n\n\n\n\nApplies the Tanh function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.\nTanh is defined as\n\n\nf(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Tanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = Tanh()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n0.7615942   0.9640276   0.9950548\n0.9640276   0.9950548   0.9993293\n0.9950548   0.9993293   0.9999092\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n1.259923    0.28260326  0.049329996\n0.14130163  0.029597998 0.0053634644\n0.009865999 0.0026817322    5.4466724E-4\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nactivation = Tanh()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[ 0.76159418  0.96402758  0.99505478]\n [ 0.96402758  0.99505478  0.99932933]\n [ 0.99505478  0.99932933  0.99990922]]\n\nprint grad\n[[  1.25992298e+00   2.82603264e-01   4.93299961e-02]\n [  1.41301632e-01   2.95979977e-02   5.36346436e-03]\n [  9.86599922e-03   2.68173218e-03   9.07778740e-04]]\n\n\n\n\n\n\nSoftPlus\n\n\nScala:\n\n\nval model = SoftPlus(beta = 1.0)\n\n\n\n\nPython:\n\n\nmodel = SoftPlus(beta = 1.0)\n\n\n\n\nApply the SoftPlus function to an n-dimensional input tensor.\nSoftPlus function: \n\n\nf_i(x) = 1/beta * log(1 + exp(beta * x_i))\n\n\n\n\n\n\nparam beta Controls sharpness of transfer function\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = SoftPlus()\nval input = Tensor(2, 3, 4).rand()\nval output = model.forward(input)\n\nscala\n println(input)\n(1,.,.) =\n0.9812126   0.7044107   0.0657767   0.9173636   \n0.20853543  0.76482195  0.60774535  0.47837523  \n0.62954164  0.56440496  0.28893307  0.40742245  \n\n(2,.,.) =\n0.18701692  0.7700966   0.98496467  0.8958407   \n0.037015386 0.34626052  0.36459026  0.8460807   \n0.051016055 0.6742781   0.14469075  0.07565566  \n\nscala\n println(output)\n(1,.,.) =\n1.2995617   1.1061354   0.7265762   1.2535294   \n0.80284095  1.1469617   1.0424956   0.9606715   \n1.0566612   1.0146512   0.8480129   0.91746557  \n\n(2,.,.) =\n0.7910212   1.1505641   1.3022922   1.2381986   \n0.71182615  0.88119024  0.8919668   1.203121    \n0.7189805   1.0860726   0.7681072   0.7316903   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodel = SoftPlus()\ninput = np.random.randn(2, 3, 4)\noutput = model.forward(input)\n\n\n print(input)\n[[[ 0.82634972 -0.09853824  0.97570235  1.84464617]\n  [ 0.38466503  0.08963732  1.29438774  1.25204527]\n  [-0.01910449 -0.19560752 -0.81769143 -1.06365733]]\n\n [[-0.56284365 -0.28473239 -0.58206869 -1.97350909]\n  [-0.28303919 -0.59735361  0.73282102  0.0176838 ]\n  [ 0.63439133  1.84904987 -1.24073643  2.13275833]]]\n\n print(output)\n[[[ 1.18935537  0.6450913   1.2955569   1.99141073]\n  [ 0.90386271  0.73896986  1.53660071  1.50351918]\n  [ 0.68364054  0.60011864  0.36564925  0.29653603]]\n\n [[ 0.45081255  0.56088102  0.44387865  0.1301229 ]\n  [ 0.56160825  0.43842646  1.12523568  0.70202816]\n  [ 1.0598278   1.99521446  0.2539995   2.24475574]]]\n\n\n\n\n\n\nL1Penalty\n\n\nScala:\n\n\nval l1Penalty = L1Penalty(l1weight, sizeAverage = false, provideOutput = true)\n\n\n\n\nPython:\n\n\nl1Penalty = L1Penalty( l1weight, size_average=False, provide_output=True)\n\n\n\n\nL1Penalty adds an L1 penalty to an input \nFor forward, the output is the same as input and a L1 loss of the latent state will be calculated each time\nFor backward, gradInput = gradOutput + gradLoss\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval l1Penalty = L1Penalty(1, true, true)\nval input = Tensor(3, 3).rand()\n\n\n print(input)\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\n\n print(l1Penalty.forward(input))\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]   \n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nl1Penalty = L1Penalty(1, True, True)\n\n\n l1Penalty.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.],\n       [ 7.,  8.,  9.]], dtype=float32)]\n\n\n\n\n\n\n\nHardShrink\n\n\nScala:\n\n\nval m = HardShrink(lambda = 0.5)\n\n\n\n\nPython:\n\n\nm = HardShrink(the_lambda=0.5)\n\n\n\n\nApplies the hard shrinkage function element-wise to the input Tensor. lambda is set to 0.5 by default.\n\n\nHardShrinkage operator is defined as:\n\n\n       \u23a7 x, if x \n  lambda\nf(x) = \u23a8 x, if x \n -lambda\n       \u23a9 0, otherwise\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.utils._\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(-10, 10)\nval input = Tensor(3, 4)\ninput.apply1(x =\n randomn().toFloat)\n\nval layer = new HardShrink(8)\nprintln(\ninput:\n)\nprintln(input)\nprintln(\noutput:\n)\nprintln(layer.forward(input))\n\n\n\n\ninput:\n8.53746839798987    -2.25314284209162   2.838596091605723   0.7181660132482648  \n0.8278933027759194  8.986027473583817   -3.6885232804343104 -2.4018199276179075 \n-9.51015486381948   2.6402589259669185  5.438693333417177   -6.577442386187613  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\noutput:\n8.53746839798987    0.0 0.0 0.0 \n0.0 8.986027473583817   0.0 0.0 \n-9.51015486381948   0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\ninput = np.linspace(-5, 5, num=10)\nlayer = HardShrink(the_lambda=3.0)\nprint(\ninput:\n)\nprint(input)\nprint(\noutput: \n)\nprint(layer.forward(input))\n\n\n\n\ncreating: createHardShrink\ninput:\n[-5.         -3.88888889 -2.77777778 -1.66666667 -0.55555556  0.55555556\n  1.66666667  2.77777778  3.88888889  5.        ]\noutput: \n[-5.         -3.88888884  0.          0.          0.          0.          0.\n  0.          3.88888884  5.        ]\n\n\n\n\n\n\n\nRReLU\n\n\nScala:\n\n\nval layer = RReLU(lower, upper, inPlace)\n\n\n\n\nPython:\n\n\nlayer = RReLU(lower, upper, inPlace)\n\n\n\n\nApplies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension. Informally the RReLU is also known as 'insanity' layer.\n\n\nRReLU is defined as: \nf(x) = max(0,x) + a * min(0, x) where a ~ U(l, u)\n.\n\n\nIn training mode negative inputs are multiplied by a factor drawn from a uniform random\ndistribution U(l, u). In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean\nfactor \na = (l + u) / 2\n.\n\n\nBy default, \nl = 1/8\n and \nu = 1/3\n. If \nl == u\n a RReLU effectively becomes a LeakyReLU.\n\n\nRegardless of operating in in-place mode a RReLU will internally allocate an input-sized noise tensor to store random factors for negative inputs.\n\n\nThe backward() operation assumes that forward() has been called before.\n\n\nFor reference see \nEmpirical Evaluation of Rectified Activations in Convolutional Network\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = RReLU()\nlayer.forward(Tensor(T(1.0f, 2.0f, -1.0f, -2.0f)))\nlayer.backward(Tensor(T(1.0f, 2.0f, -1.0f, -2.0f)),Tensor(T(0.1f, 0.2f, -0.1f, -0.2f)))\n\n\n\n\nThere's random factor. Gives the output,\n\n\n1.0\n2.0\n-0.24342789\n-0.43175703\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n0.1\n0.2\n-0.024342788\n-0.043175705\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import RReLU\nimport numpy as np\n\nlayer = RReLU()\nlayer.forward(np.array([1.0, 2.0, -1.0, -2.0]))\nlayer.backward(np.array([1.0, 2.0, -1.0, -2.0]),\n  np.array([0.1, 0.2, -0.1, -0.2]))\n\n\n\n\nThere's random factor. Gives the ouput like\n\n\narray([ 1.,  2., -0.15329693, -0.40423378], dtype=float32)\n\narray([ 0.1, 0.2, -0.01532969, -0.04042338], dtype=float32)\n\n\n\n\n\n\nHardTanh\n\n\nScala:\n\n\nval activation = HardTanh(\n    minValue = -1,\n    maxValue = 1,\n    inplace = false)\n\n\n\n\nPython:\n\n\nactivation = HardTanh(\n    min_value=-1.0,\n    max_value=1.0,\n    inplace=False)\n\n\n\n\nApplies non-linear function HardTanh to each element of input, HardTanh is defined:\n\n\n           \u23a7  maxValue, if x \n maxValue\n    f(x) = \u23a8  minValue, if x \n minValue\n           \u23a9  x, otherwise\n\n\n\n\nParameters:\n\n \nminValue\n minValue in f(x), default is -1.\n\n \nmaxValue\n maxValue in f(x), default is 1.\n* \ninplace\n  weather inplace update output from input. default is false.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.HardTanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = HardTanh()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.0 0.0 0.0\n0.0 0.0 0.0\n0.0 0.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nactivation = HardTanh()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-1.  1.  1.]\n [-1.  1.  1.]\n [-1.  1.  1.]]\n\nprint grad\n[[ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]\n\n\n\n\n\n\nLeakyReLU\n\n\nScala:\n\n\nlayer = LeakyReLU(negval=0.01,inplace=false)\n\n\n\n\nPython:\n\n\nlayer = LeakyReLU(negval=0.01,inplace=False,bigdl_type=\nfloat\n)\n\n\n\n\nIt is a transfer module that applies LeakyReLU, which parameter\nnegval sets the slope of the negative part:\n LeakyReLU is defined as:\n  \nf(x) = max(0, x) + negval * min(0, x)\n\n\n\n\nnegval\n sets the slope of the negative partl, default is 0.01\n\n\ninplace\n if it is true, doing the operation in-place without\n                using extra state memory, default is false\n\n\n\n\nScala example:\n\n\nval layer = LeakyReLU(negval=0.01,inplace=false)\nval input = Tensor(3, 2).rand(-1, 1)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.6923256      -0.14086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nlayer.forward(input)\nres7: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.006923256    -0.0014086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n\n\n\n\nPython example:\n\n\nlayer = LeakyReLU(negval=0.01,inplace=False,bigdl_type=\nfloat\n)\ninput = np.random.rand(3, 2)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056004,  0.35643192],\n       [ 0.25075111,  0.18904582]])\n\nlayer.forward(input)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056001,  0.35643193],\n       [ 0.25075111,  0.18904583]], dtype=float32)\n\n\n\n\n\n\nLogSigmoid\n\n\nScala:\n\n\nval activation = LogSigmoid()\n\n\n\n\nPython:\n\n\nactivation = LogSigmoid()\n\n\n\n\nThis class is a activation layer corresponding to the non-linear function sigmoid function:\n\n\nf(x) = Log(1 / (1 + e ^ (-x)))\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.LogSigmoid\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = LogSigmoid()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.3132617  -0.12692802 -0.04858735\n-0.12692802 -0.04858735 -0.01814993\n-0.04858735 -0.01814993 -0.0067153485\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.8068244   0.47681168  0.23712938\n0.23840584  0.14227761  0.07194484\n0.047425874 0.03597242  0.020078553\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nactivation = LogSigmoid()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.31326169 -0.12692802 -0.04858735]\n [-0.12692802 -0.04858735 -0.01814993]\n [-0.04858735 -0.01814993 -0.00671535]]\n\nprint grad\n[[ 0.80682439  0.47681168  0.23712938]\n [ 0.23840584  0.14227761  0.07194484]\n [ 0.04742587  0.03597242  0.03346425]]\n\n\n\n\n\n\nLogSoftMax\n\n\nScala:\n\n\nval model = LogSoftMax()\n\n\n\n\nPython:\n\n\nmodel = LogSoftMax()\n\n\n\n\nThe LogSoftMax module applies a LogSoftMax transformation to the input data\nwhich is defined as:\n\n\nf_i(x) = log(1 / a exp(x_i))\nwhere a = sum_j[exp(x_j)]\n\n\n\n\nThe input given in \nforward(input)\n must be either\na vector (1D tensor) or matrix (2D tensor).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = LogSoftMax()\nval input = Tensor(2, 5).rand()\nval output = model.forward(input)\n\nscala\n print(input)\n0.4434036   0.64535594  0.7516194   0.11752353  0.5216674   \n0.57294756  0.744955    0.62644184  0.0052207764    0.900162    \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x5]\n\nscala\n print(output)\n-1.6841899  -1.4822376  -1.3759742  -2.01007    -1.605926   \n-1.6479948  -1.4759872  -1.5945004  -2.2157214  -1.3207803  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]\n\n\n\n\nPython example:\n\n\nmodel = LogSoftMax()\ninput = np.random.randn(4, 10)\noutput = model.forward(input)\n\n\n print(input)\n[[ 0.10805365  0.11392282  1.31891713 -0.62910637 -0.80532589  0.57976863\n  -0.44454368  0.26292944  0.8338328   0.32305099]\n [-0.16443839  0.12010763  0.62978233 -1.57224143 -2.16133614 -0.60932395\n  -0.22722708  0.23268273  0.00313597  0.34585582]\n [ 0.55913444 -0.7560615   0.12170887  1.40628806  0.97614582  1.20417145\n  -1.60619173 -0.54483025  1.12227399 -0.79976189]\n [-0.05540945  0.86954458  0.34586427  2.52004267  0.6998163  -1.61315173\n  -0.76276874  0.38332142  0.66351792 -0.30111399]]\n\n\n print(output)\n[[-2.55674744 -2.55087829 -1.34588397 -3.2939074  -3.47012711 -2.08503246\n  -3.10934472 -2.40187168 -1.83096838 -2.34175014]\n [-2.38306785 -2.09852171 -1.58884704 -3.79087067 -4.37996578 -2.82795334\n  -2.44585633 -1.98594666 -2.21549344 -1.87277353]\n [-2.31549931 -3.63069534 -2.75292492 -1.46834576 -1.89848804 -1.67046237\n  -4.48082542 -3.41946411 -1.75235975 -3.67439556]\n [-3.23354769 -2.30859375 -2.83227396 -0.6580956  -2.47832203 -4.79128981\n  -3.940907   -2.79481697 -2.5146203  -3.47925234]]\n\n\n\n\n\n\nThreshold\n\n\nScala:\n\n\nval module = Threshold(threshold, value, ip)\n\n\n\n\nPython:\n\n\nmodule = Threshold(threshold, value, ip)\n\n\n\n\nThresholds each element of the input Tensor.\nThreshold is defined as:\n\n\n     \u23a7 x        if x \n= threshold\n y = \u23a8 \n     \u23a9 value    if x \n  threshold\n\n\n\n\n\n\nthreshold: The value to threshold at\n\n\nvalue: The value to replace with\n\n\nip: can optionally do the operation in-place\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Threshold(1, 0.8)\nval input = Tensor(2, 2, 2).randn()\nval output = module.forward(input)\n\n\n input\n(1,.,.) =\n2.0502799   -0.37522468\n-1.2704345  -0.22533786\n\n(2,.,.) =\n1.1959263   1.6670992\n-0.24333914 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n output\n(1,.,.) =\n(1,.,.) =\n2.0502799   0.8\n0.8 0.8\n\n(2,.,.) =\n1.1959263   1.6670992\n0.8 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Threshold(1.0, 0.8)\ninput = np.random.randn(2, 2, 2)\noutput = module.forward(input)\n\n\n input\n[[[-0.43226865 -1.09160093]\n  [-0.20280088  0.68196767]]\n\n [[ 2.32017942  1.00003307]\n  [-0.46618767  0.57057167]]]\n\n\n output\n[array([[[ 0.80000001,  0.80000001],\n        [ 0.80000001,  0.80000001]],\n\n       [[ 2.32017946,  1.00003302],\n        [ 0.80000001,  0.80000001]]], dtype=float32)]", 
            "title": "Activations"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#softsign", 
            "text": "Scala:  val softSign = SoftSign()  Python:  softSign = SoftSign()  SoftSign applies SoftSign function to the input tensor  SoftSign function:  f_i(x) = x_i / (1+|x_i|)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval softSign = SoftSign()\nval input = Tensor(3, 3).rand()  print(input)\n0.6733504   0.7566517   0.43793806  \n0.09683273  0.05829774  0.4567967   \n0.20021072  0.11158377  0.31668025  print(softSign.forward(input))\n0.40239656  0.4307352   0.30455974  \n0.08828395  0.05508633  0.31356242  \n0.16681297  0.10038269  0.24051417  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nsoftSign=SoftSign()  softSign.forward(np.array([[1, 2, 4],[-1, -2, -4]]))\n[array([[ 0.5       ,  0.66666669,  0.80000001],\n       [-0.5       , -0.66666669, -0.80000001]], dtype=float32)]", 
            "title": "SoftSign"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#relu6", 
            "text": "Scala:  val module = ReLU6(inplace = false)  Python:  module = ReLU6(inplace=False)  Same as ReLU except that the rectifying function f(x) saturates at x = 6 \nReLU6 is defined as: f(x) = min(max(0, x), 6)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = ReLU6()\n\nprintln(module.forward(Tensor.range(-2, 8, 1)))  Gives the output,  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n6.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 11]   Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = ReLU6()\n\nprint(module.forward(np.arange(-2, 9, 1)))  Gives the output,  [array([ 0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  6.,  6.], dtype=float32)]", 
            "title": "ReLU6"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#tanhshrink", 
            "text": "Scala:  val tanhShrink = TanhShrink()  Python:  tanhShrink = TanhShrink()  TanhShrink applies element-wise Tanh and Shrink function to the input  TanhShrink function :  f(x) = scala.math.tanh(x) - 1  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval tanhShrink = TanhShrink()\nval input = Tensor(3, 3).rand()  print(input)\n0.7056571   0.25239098  0.75746965  \n0.89736927  0.31193605  0.23842576  \n0.69492024  0.7512544   0.8386124   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(tanhShrink.forward(input))\n0.09771085  0.0052260756    0.11788553  \n0.18235475  0.009738684 0.004417494 \n0.09378672  0.1153577   0.153539    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\ntanhShrink = TanhShrink()   tanhShrink.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 0.23840582,  1.03597236,  2.00494528],\n       [ 3.00067067,  4.0000906 ,  5.0000124 ],\n       [ 6.00000191,  7.        ,  8.        ]], dtype=float32)]", 
            "title": "TanhShrink"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#softmax", 
            "text": "Scala:  val layer = SoftMax()  Python:  layer = SoftMax()  Applies the SoftMax function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0, 1) and sum to 1.\nSoftmax is defined as: f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift) \nwhere  shift = max_i(x_i) .  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SoftMax()\nval input = Tensor(3)\ninput.apply1(_ =  1.0f * 10)\nval gradOutput = Tensor(T(\n1.0f,\n0.0f,\n0.0f\n))\nval output = layer.forward(input)\nval gradient = layer.backward(input, gradOutput)\n-  print(output)\n0.33333334\n0.33333334\n0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n-  print(gradient)\n0.22222221\n-0.11111112\n-0.11111112\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nlayer = SoftMax()\ninput = np.ones(3)*10\ngrad_output = np.array([1.0, 0.0, 0.0])\noutput = layer.forward(input)\ngradient = layer.backward(input, grad_output)\n-  print output\n[ 0.33333334  0.33333334  0.33333334]\n-  print gradient\n[ 0.22222221 -0.11111112 -0.11111112]", 
            "title": "SoftMax"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#prelu", 
            "text": "Scala:  val module = PReLU(nOutputPlane = 0)  Python:  module = PReLU(nOutputPlane=0)  Applies parametric ReLU, which parameter varies the slope of the negative part.  PReLU: f(x) = max(0, x) + a * min(0, x)  nOutputPlane's default value is 0, that means using PReLU in shared version and has\nonly one parameters. nOutputPlane is the input map number(Default is 0).  Notice: Please don't use weight decay on this.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = PReLU(2)\nval input = Tensor(2, 2, 3).randn()\nval output = module.forward(input)  input\n(1,.,.) =\n-0.17810068 -0.69607687 0.25582042\n-1.2140307  -1.5410945  1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.16203058 -0.5643519  0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x3]  output\n(1,.,.) =\n-0.04452517 -0.17401922 0.25582042\n-0.3035077  -0.38527364 1.0209005\n\n(2,.,.) =\n0.2826971   0.6370953   0.21471702\n-0.040507644    -0.14108798 0.816576\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = PReLU(2)\ninput = np.random.randn(2, 2, 3)\noutput = module.forward(input)  input\n[[[ 2.50596953 -0.06593339 -1.90273409]\n  [ 0.2464341   0.45941315 -0.41977094]]\n\n [[-0.8584367   2.19389229  0.93136755]\n  [-0.39209027  0.16507514 -0.35850447]]]  output\n[array([[[ 2.50596952, -0.01648335, -0.47568351],\n         [ 0.24643411,  0.45941314, -0.10494273]],\n\n        [[-0.21460918,  2.19389224,  0.93136758],\n         [-0.09802257,  0.16507514, -0.08962612]]], dtype=float32)]", 
            "title": "PReLU"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#relu", 
            "text": "Scala:  val relu = ReLU(ip = false)  Python:  relu = ReLU(ip)  ReLU applies the element-wise rectified linear unit (ReLU) function to the input  ip  illustrate if the ReLU fuction is done on the origin input  ReLU function : f(x) = max(0, x)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval relu = ReLU(false)\n\nval input = Tensor(3, 3).rand()  print(input)\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(relu.forward(input))\n0.13486342  0.8986828   0.2648762   \n0.56467545  0.7727274   0.65959305  \n0.01554346  0.9552375   0.2434533   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nrelu = ReLU(False)  relu.forward(np.array([[-1, -2, -3], [0, 0, 0], [1, 2, 3]]))\n[array([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 1.,  2.,  3.]], dtype=float32)]", 
            "title": "ReLU"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#softmin", 
            "text": "Scala:  val sm = SoftMin()  Python:  sm = SoftMin()  Applies the SoftMin function to an n-dimensional input Tensor, rescaling them so that the\nelements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1.\nSoftmin is defined as:  f_i(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift) \nwhere  shift = max_i(-x_i) .  Scala example:  import com.intel.analytics.bigdl.nn.SoftMin\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval sm = SoftMin()\nval input = Tensor(3, 3).range(1, 3 * 3)\n\nval output = sm.forward(input)\n\nval gradOutput = Tensor(3, 3).range(1, 3 * 3).apply1(x =  (x / 10.0).toFloat)\nval gradInput = sm.backward(input, gradOutput)  Gives the output,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n0.66524094      0.24472848      0.09003057\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Gives the gradInput,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.02825874      -0.014077038    -0.014181711\n0.028258756     -0.01407703     -0.01418171\n0.028258756     -0.014077038    -0.014181707\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsm = SoftMin()\n\ninput = np.arange(1, 10, 1).astype( float32 )\ninput = input.reshape(3, 3)\n\noutput = sm.forward(input)\nprint output\n\ngradOutput = np.arange(1, 10, 1).astype( float32 )\ngradOutput = np.vectorize(lambda t: t / 10)(gradOutput)\ngradOutput = gradOutput.reshape(3, 3)\n\ngradInput = sm.backward(input, gradOutput)\nprint gradInput", 
            "title": "SoftMin"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#elu", 
            "text": "Scala:  val m = ELU(alpha = 1.0, inplace = false)  Python:  m = ELU(alpha=1.0, inplace=False)  Applies exponential linear unit ( ELU ), which parameter a varies the convergence value of the exponential function below zero:  ELU  is defined as:  f(x) = max(0, x) + min(0, alpha * (exp(x) - 1))  The output dimension is always equal to input dimension.  For reference see  Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) .  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval xs = Tensor(4).randn()\nprintln(xs)\nprintln(ELU(4).forward(xs))  1.0217569\n-0.17189966\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n1.0217569\n-0.63174534\n1.4164596\n0.69361746\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\nxs = np.linspace(-3, 3, num=200)\ngo = np.ones(200)\n\ndef f(a):\n    return ELU(a).forward(xs)[0]\ndef df(a):\n    m = ELU(a)\n    m.forward(xs)\n    return m.backward(xs, go)[0]\n\nplt.plot(xs, f(0.1), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, f(1.0), '-', label='fw ELU, alpha = 0.1')\nplt.plot(xs, df(0.1), '-', label='dw ELU, alpha = 0.1')\nplt.plot(xs, df(1.0), '-', label='dw ELU, alpha = 0.1')\n\nplt.legend(loc='best', shadow=True, fancybox=True)\nplt.show()", 
            "title": "ELU"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#softshrink", 
            "text": "Scala:  val layer = SoftShrink(lambda = 0.5)  Python:  layer = SoftShrink(the_lambda=0.5)  Apply the soft shrinkage function element-wise to the input Tensor  SoftShrinkage operator:         \u23a7 x - lambda, if x    lambda\nf(x) = \u23a8 x + lambda, if x   -lambda\n       \u23a9 0, otherwise  Parameters:\n*  lambda  a factor, default is 0.5  Scala example:  import com.intel.analytics.bigdl.nn.SoftShrink\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = SoftShrink()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.5    1.5 2.5\n-1.5    2.5 3.5\n-2.5    3.5 4.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n3.0 4.0 5.0\n2.0 3.0 4.0\n1.0 2.0 3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Scala example:  activation = SoftShrink()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n\nprint grad\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]", 
            "title": "SoftShrink"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#sigmoid", 
            "text": "Scala:  val module = Sigmoid()  Python:  module = Sigmoid()  Applies the Sigmoid function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.  Sigmoid is defined as:  f(x) = 1 / (1 + exp(-x))  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = new Sigmoid()\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =  {i += 1; i})  print(layer.forward(input))\n0.7310586   0.880797    0.95257413  \n0.98201376  0.9933072   0.9975274   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\nlayer = Sigmoid()\ninput = np.array([[1, 2, 3], [4, 5, 6]]) layer.forward(input)\narray([[ 0.7310586 ,  0.88079703,  0.95257413],\n       [ 0.98201376,  0.99330717,  0.99752742]], dtype=float32)", 
            "title": "Sigmoid"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#tanh", 
            "text": "Scala:  val activation = Tanh()  Python:  activation = Tanh()  Applies the Tanh function element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension.\nTanh is defined as  f(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x)).  Scala example:  import com.intel.analytics.bigdl.nn.Tanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = Tanh()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n0.7615942   0.9640276   0.9950548\n0.9640276   0.9950548   0.9993293\n0.9950548   0.9993293   0.9999092\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n1.259923    0.28260326  0.049329996\n0.14130163  0.029597998 0.0053634644\n0.009865999 0.0026817322    5.4466724E-4\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  activation = Tanh()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[ 0.76159418  0.96402758  0.99505478]\n [ 0.96402758  0.99505478  0.99932933]\n [ 0.99505478  0.99932933  0.99990922]]\n\nprint grad\n[[  1.25992298e+00   2.82603264e-01   4.93299961e-02]\n [  1.41301632e-01   2.95979977e-02   5.36346436e-03]\n [  9.86599922e-03   2.68173218e-03   9.07778740e-04]]", 
            "title": "Tanh"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#softplus", 
            "text": "Scala:  val model = SoftPlus(beta = 1.0)  Python:  model = SoftPlus(beta = 1.0)  Apply the SoftPlus function to an n-dimensional input tensor.\nSoftPlus function:   f_i(x) = 1/beta * log(1 + exp(beta * x_i))   param beta Controls sharpness of transfer function   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = SoftPlus()\nval input = Tensor(2, 3, 4).rand()\nval output = model.forward(input)\n\nscala  println(input)\n(1,.,.) =\n0.9812126   0.7044107   0.0657767   0.9173636   \n0.20853543  0.76482195  0.60774535  0.47837523  \n0.62954164  0.56440496  0.28893307  0.40742245  \n\n(2,.,.) =\n0.18701692  0.7700966   0.98496467  0.8958407   \n0.037015386 0.34626052  0.36459026  0.8460807   \n0.051016055 0.6742781   0.14469075  0.07565566  \n\nscala  println(output)\n(1,.,.) =\n1.2995617   1.1061354   0.7265762   1.2535294   \n0.80284095  1.1469617   1.0424956   0.9606715   \n1.0566612   1.0146512   0.8480129   0.91746557  \n\n(2,.,.) =\n0.7910212   1.1505641   1.3022922   1.2381986   \n0.71182615  0.88119024  0.8919668   1.203121    \n0.7189805   1.0860726   0.7681072   0.7316903   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodel = SoftPlus()\ninput = np.random.randn(2, 3, 4)\noutput = model.forward(input)  print(input)\n[[[ 0.82634972 -0.09853824  0.97570235  1.84464617]\n  [ 0.38466503  0.08963732  1.29438774  1.25204527]\n  [-0.01910449 -0.19560752 -0.81769143 -1.06365733]]\n\n [[-0.56284365 -0.28473239 -0.58206869 -1.97350909]\n  [-0.28303919 -0.59735361  0.73282102  0.0176838 ]\n  [ 0.63439133  1.84904987 -1.24073643  2.13275833]]]  print(output)\n[[[ 1.18935537  0.6450913   1.2955569   1.99141073]\n  [ 0.90386271  0.73896986  1.53660071  1.50351918]\n  [ 0.68364054  0.60011864  0.36564925  0.29653603]]\n\n [[ 0.45081255  0.56088102  0.44387865  0.1301229 ]\n  [ 0.56160825  0.43842646  1.12523568  0.70202816]\n  [ 1.0598278   1.99521446  0.2539995   2.24475574]]]", 
            "title": "SoftPlus"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#l1penalty", 
            "text": "Scala:  val l1Penalty = L1Penalty(l1weight, sizeAverage = false, provideOutput = true)  Python:  l1Penalty = L1Penalty( l1weight, size_average=False, provide_output=True)  L1Penalty adds an L1 penalty to an input \nFor forward, the output is the same as input and a L1 loss of the latent state will be calculated each time\nFor backward, gradInput = gradOutput + gradLoss  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval l1Penalty = L1Penalty(1, true, true)\nval input = Tensor(3, 3).rand()  print(input)\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(l1Penalty.forward(input))\n0.0370419   0.03080979  0.22083037  \n0.1547358   0.018475588 0.8102709   \n0.86393493  0.7081842   0.13717912  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]     Python example:  from bigdl.nn.layer import *\nl1Penalty = L1Penalty(1, True, True)  l1Penalty.forward(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n[array([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.],\n       [ 7.,  8.,  9.]], dtype=float32)]", 
            "title": "L1Penalty"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#hardshrink", 
            "text": "Scala:  val m = HardShrink(lambda = 0.5)  Python:  m = HardShrink(the_lambda=0.5)  Applies the hard shrinkage function element-wise to the input Tensor. lambda is set to 0.5 by default.  HardShrinkage operator is defined as:         \u23a7 x, if x    lambda\nf(x) = \u23a8 x, if x   -lambda\n       \u23a9 0, otherwise  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nimport com.intel.analytics.bigdl.utils._\n\ndef randomn(): Float = RandomGenerator.RNG.uniform(-10, 10)\nval input = Tensor(3, 4)\ninput.apply1(x =  randomn().toFloat)\n\nval layer = new HardShrink(8)\nprintln( input: )\nprintln(input)\nprintln( output: )\nprintln(layer.forward(input))  input:\n8.53746839798987    -2.25314284209162   2.838596091605723   0.7181660132482648  \n0.8278933027759194  8.986027473583817   -3.6885232804343104 -2.4018199276179075 \n-9.51015486381948   2.6402589259669185  5.438693333417177   -6.577442386187613  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]\noutput:\n8.53746839798987    0.0 0.0 0.0 \n0.0 8.986027473583817   0.0 0.0 \n-9.51015486381948   0.0 0.0 0.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\ninput = np.linspace(-5, 5, num=10)\nlayer = HardShrink(the_lambda=3.0)\nprint( input: )\nprint(input)\nprint( output:  )\nprint(layer.forward(input))  creating: createHardShrink\ninput:\n[-5.         -3.88888889 -2.77777778 -1.66666667 -0.55555556  0.55555556\n  1.66666667  2.77777778  3.88888889  5.        ]\noutput: \n[-5.         -3.88888884  0.          0.          0.          0.          0.\n  0.          3.88888884  5.        ]", 
            "title": "HardShrink"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#rrelu", 
            "text": "Scala:  val layer = RReLU(lower, upper, inPlace)  Python:  layer = RReLU(lower, upper, inPlace)  Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input Tensor,\nthus outputting a Tensor of the same dimension. Informally the RReLU is also known as 'insanity' layer.  RReLU is defined as:  f(x) = max(0,x) + a * min(0, x) where a ~ U(l, u) .  In training mode negative inputs are multiplied by a factor drawn from a uniform random\ndistribution U(l, u). In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean\nfactor  a = (l + u) / 2 .  By default,  l = 1/8  and  u = 1/3 . If  l == u  a RReLU effectively becomes a LeakyReLU.  Regardless of operating in in-place mode a RReLU will internally allocate an input-sized noise tensor to store random factors for negative inputs.  The backward() operation assumes that forward() has been called before.  For reference see  Empirical Evaluation of Rectified Activations in Convolutional Network .  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = RReLU()\nlayer.forward(Tensor(T(1.0f, 2.0f, -1.0f, -2.0f)))\nlayer.backward(Tensor(T(1.0f, 2.0f, -1.0f, -2.0f)),Tensor(T(0.1f, 0.2f, -0.1f, -0.2f)))  There's random factor. Gives the output,  1.0\n2.0\n-0.24342789\n-0.43175703\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n0.1\n0.2\n-0.024342788\n-0.043175705\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  from bigdl.nn.layer import RReLU\nimport numpy as np\n\nlayer = RReLU()\nlayer.forward(np.array([1.0, 2.0, -1.0, -2.0]))\nlayer.backward(np.array([1.0, 2.0, -1.0, -2.0]),\n  np.array([0.1, 0.2, -0.1, -0.2]))  There's random factor. Gives the ouput like  array([ 1.,  2., -0.15329693, -0.40423378], dtype=float32)\n\narray([ 0.1, 0.2, -0.01532969, -0.04042338], dtype=float32)", 
            "title": "RReLU"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#hardtanh", 
            "text": "Scala:  val activation = HardTanh(\n    minValue = -1,\n    maxValue = 1,\n    inplace = false)  Python:  activation = HardTanh(\n    min_value=-1.0,\n    max_value=1.0,\n    inplace=False)  Applies non-linear function HardTanh to each element of input, HardTanh is defined:             \u23a7  maxValue, if x   maxValue\n    f(x) = \u23a8  minValue, if x   minValue\n           \u23a9  x, otherwise  Parameters:   minValue  minValue in f(x), default is -1.   maxValue  maxValue in f(x), default is 1.\n*  inplace   weather inplace update output from input. default is false.  Scala example:  import com.intel.analytics.bigdl.nn.HardTanh\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = HardTanh()\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n-1.0    1.0 1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.0 0.0 0.0\n0.0 0.0 0.0\n0.0 0.0 0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  activation = HardTanh()\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-1.  1.  1.]\n [-1.  1.  1.]\n [-1.  1.  1.]]\n\nprint grad\n[[ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]", 
            "title": "HardTanh"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#leakyrelu", 
            "text": "Scala:  layer = LeakyReLU(negval=0.01,inplace=false)  Python:  layer = LeakyReLU(negval=0.01,inplace=False,bigdl_type= float )  It is a transfer module that applies LeakyReLU, which parameter\nnegval sets the slope of the negative part:\n LeakyReLU is defined as:\n   f(x) = max(0, x) + negval * min(0, x)   negval  sets the slope of the negative partl, default is 0.01  inplace  if it is true, doing the operation in-place without\n                using extra state memory, default is false   Scala example:  val layer = LeakyReLU(negval=0.01,inplace=false)\nval input = Tensor(3, 2).rand(-1, 1)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.6923256      -0.14086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nlayer.forward(input)\nres7: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.006923256    -0.0014086828\n0.029539397     0.477964\n0.5202874       0.10458552\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]  Python example:  layer = LeakyReLU(negval=0.01,inplace=False,bigdl_type= float )\ninput = np.random.rand(3, 2)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056004,  0.35643192],\n       [ 0.25075111,  0.18904582]])\n\nlayer.forward(input)\narray([[ 0.19502378,  0.40498206],\n       [ 0.97056001,  0.35643193],\n       [ 0.25075111,  0.18904583]], dtype=float32)", 
            "title": "LeakyReLU"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#logsigmoid", 
            "text": "Scala:  val activation = LogSigmoid()  Python:  activation = LogSigmoid()  This class is a activation layer corresponding to the non-linear function sigmoid function:  f(x) = Log(1 / (1 + e ^ (-x)))  Scala example:  import com.intel.analytics.bigdl.nn.LogSigmoid\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval activation = LogSigmoid()\nval input = Tensor(T(\n  T(1f, 2f, 3f),\n  T(2f, 3f, 4f),\n  T(3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = activation.forward(input)\nval grad = activation.backward(input, gradOutput)\n\nprintln(output)\n-0.3132617  -0.12692802 -0.04858735\n-0.12692802 -0.04858735 -0.01814993\n-0.04858735 -0.01814993 -0.0067153485\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n0.8068244   0.47681168  0.23712938\n0.23840584  0.14227761  0.07194484\n0.047425874 0.03597242  0.020078553\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  activation = LogSigmoid()\ninput = np.array([\n  [1.0, 2.0, 3.0],\n  [2.0, 3.0, 4.0],\n  [3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = activation.forward(input)\ngrad = activation.backward(input, gradOutput)\n\nprint output\n[[-0.31326169 -0.12692802 -0.04858735]\n [-0.12692802 -0.04858735 -0.01814993]\n [-0.04858735 -0.01814993 -0.00671535]]\n\nprint grad\n[[ 0.80682439  0.47681168  0.23712938]\n [ 0.23840584  0.14227761  0.07194484]\n [ 0.04742587  0.03597242  0.03346425]]", 
            "title": "LogSigmoid"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#logsoftmax", 
            "text": "Scala:  val model = LogSoftMax()  Python:  model = LogSoftMax()  The LogSoftMax module applies a LogSoftMax transformation to the input data\nwhich is defined as:  f_i(x) = log(1 / a exp(x_i))\nwhere a = sum_j[exp(x_j)]  The input given in  forward(input)  must be either\na vector (1D tensor) or matrix (2D tensor).  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = LogSoftMax()\nval input = Tensor(2, 5).rand()\nval output = model.forward(input)\n\nscala  print(input)\n0.4434036   0.64535594  0.7516194   0.11752353  0.5216674   \n0.57294756  0.744955    0.62644184  0.0052207764    0.900162    \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x5]\n\nscala  print(output)\n-1.6841899  -1.4822376  -1.3759742  -2.01007    -1.605926   \n-1.6479948  -1.4759872  -1.5945004  -2.2157214  -1.3207803  \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]  Python example:  model = LogSoftMax()\ninput = np.random.randn(4, 10)\noutput = model.forward(input)  print(input)\n[[ 0.10805365  0.11392282  1.31891713 -0.62910637 -0.80532589  0.57976863\n  -0.44454368  0.26292944  0.8338328   0.32305099]\n [-0.16443839  0.12010763  0.62978233 -1.57224143 -2.16133614 -0.60932395\n  -0.22722708  0.23268273  0.00313597  0.34585582]\n [ 0.55913444 -0.7560615   0.12170887  1.40628806  0.97614582  1.20417145\n  -1.60619173 -0.54483025  1.12227399 -0.79976189]\n [-0.05540945  0.86954458  0.34586427  2.52004267  0.6998163  -1.61315173\n  -0.76276874  0.38332142  0.66351792 -0.30111399]]  print(output)\n[[-2.55674744 -2.55087829 -1.34588397 -3.2939074  -3.47012711 -2.08503246\n  -3.10934472 -2.40187168 -1.83096838 -2.34175014]\n [-2.38306785 -2.09852171 -1.58884704 -3.79087067 -4.37996578 -2.82795334\n  -2.44585633 -1.98594666 -2.21549344 -1.87277353]\n [-2.31549931 -3.63069534 -2.75292492 -1.46834576 -1.89848804 -1.67046237\n  -4.48082542 -3.41946411 -1.75235975 -3.67439556]\n [-3.23354769 -2.30859375 -2.83227396 -0.6580956  -2.47832203 -4.79128981\n  -3.940907   -2.79481697 -2.5146203  -3.47925234]]", 
            "title": "LogSoftMax"
        }, 
        {
            "location": "/APIGuide/Layers/Activations/#threshold", 
            "text": "Scala:  val module = Threshold(threshold, value, ip)  Python:  module = Threshold(threshold, value, ip)  Thresholds each element of the input Tensor.\nThreshold is defined as:       \u23a7 x        if x  = threshold\n y = \u23a8 \n     \u23a9 value    if x    threshold   threshold: The value to threshold at  value: The value to replace with  ip: can optionally do the operation in-place   Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Threshold(1, 0.8)\nval input = Tensor(2, 2, 2).randn()\nval output = module.forward(input)  input\n(1,.,.) =\n2.0502799   -0.37522468\n-1.2704345  -0.22533786\n\n(2,.,.) =\n1.1959263   1.6670992\n-0.24333914 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  output\n(1,.,.) =\n(1,.,.) =\n2.0502799   0.8\n0.8 0.8\n\n(2,.,.) =\n1.1959263   1.6670992\n0.8 1.4424673\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Threshold(1.0, 0.8)\ninput = np.random.randn(2, 2, 2)\noutput = module.forward(input)  input\n[[[-0.43226865 -1.09160093]\n  [-0.20280088  0.68196767]]\n\n [[ 2.32017942  1.00003307]\n  [-0.46618767  0.57057167]]]  output\n[array([[[ 0.80000001,  0.80000001],\n        [ 0.80000001,  0.80000001]],\n\n       [[ 2.32017946,  1.00003302],\n        [ 0.80000001,  0.80000001]]], dtype=float32)]", 
            "title": "Threshold"
        }, 
        {
            "location": "/APIGuide/Layers/Embedding-Layers/", 
            "text": "LookupTable\n\n\nScala:\n\n\nval layer = LookupTable(nIndex: Int, nOutput: Int, paddingValue: Double = 0,\n                                 maxNorm: Double = Double.MaxValue,\n                                 normType: Double = 2.0,\n                                 shouldScaleGradByFreq: Boolean = false,\n                                 wRegularizer: Regularizer[T] = null)\n\n\n\n\nPython:\n\n\nlayer = LookupTable(nIndex, nOutput, paddingValue, maxNorm, normType, shouldScaleGradByFreq)\n\n\n\n\nThis layer is a particular case of a convolution, where the width of the convolution would be 1.\nInput should be a 1D or 2D tensor filled with indices. Indices are corresponding to the position\nin weight. For each index element of input, it outputs the selected index part of weight.\nThis layer is often used in word embedding. In collaborative filtering, it can be used together with Select to create embeddings for users or items. \n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval layer = LookupTable(9, 4, 2, 0.1, 2.0, true)\nval input = Tensor(Storage(Array(5.0f, 2.0f, 6.0f, 9.0f, 4.0f)), 1, Array(5))\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, output)\n\n\n println(layer.weight)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.2949163      -0.8240777      -0.9440595      -0.8326071\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n-1.7525806      0.7305201       0.3349018       0.03952092\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n0.014426847     -0.09052222     0.019132217     -0.035093457\n-0.7002858      1.1149521       0.9869375       1.2580993\n0.36649692      -0.6583153      0.90005803      0.12671651\n0.048913725     0.033388995     -0.07938445     0.01381052\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9x4]\n\n\n println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n5.0\n2.0\n6.0\n9.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n println(output)\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n0.014426847     -0.09052222     0.019132217     -0.035093457\n0.048913725     0.033388995     -0.07938445     0.01381052\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x4]\n\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\n\nPython example:\n\n\nlayer = LookupTable(9, 4, 2.0, 0.1, 2.0, True)\ninput = np.array([5.0, 2.0, 6.0, 9.0, 4.0]).astype(\nfloat32\n)\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, output)\n\n\n output\n[array([[-0.00704637,  0.07495038,  0.06465427,  0.01235369],\n        [ 0.00350313,  0.02751033, -0.02163727,  0.0936095 ],\n        [ 0.02330465, -0.05696457,  0.0081728 ,  0.07839092],\n        [ 0.06580321, -0.0743262 , -0.00414508, -0.01133001],\n        [-0.00382435, -0.04677011,  0.02839171, -0.08361723]], dtype=float32)]\n\n\n gradInput\n[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)]", 
            "title": "Embedding Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Embedding-Layers/#lookuptable", 
            "text": "Scala:  val layer = LookupTable(nIndex: Int, nOutput: Int, paddingValue: Double = 0,\n                                 maxNorm: Double = Double.MaxValue,\n                                 normType: Double = 2.0,\n                                 shouldScaleGradByFreq: Boolean = false,\n                                 wRegularizer: Regularizer[T] = null)  Python:  layer = LookupTable(nIndex, nOutput, paddingValue, maxNorm, normType, shouldScaleGradByFreq)  This layer is a particular case of a convolution, where the width of the convolution would be 1.\nInput should be a 1D or 2D tensor filled with indices. Indices are corresponding to the position\nin weight. For each index element of input, it outputs the selected index part of weight.\nThis layer is often used in word embedding. In collaborative filtering, it can be used together with Select to create embeddings for users or items.   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval layer = LookupTable(9, 4, 2, 0.1, 2.0, true)\nval input = Tensor(Storage(Array(5.0f, 2.0f, 6.0f, 9.0f, 4.0f)), 1, Array(5))\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, output)  println(layer.weight)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.2949163      -0.8240777      -0.9440595      -0.8326071\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n-1.7525806      0.7305201       0.3349018       0.03952092\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n0.014426847     -0.09052222     0.019132217     -0.035093457\n-0.7002858      1.1149521       0.9869375       1.2580993\n0.36649692      -0.6583153      0.90005803      0.12671651\n0.048913725     0.033388995     -0.07938445     0.01381052\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 9x4]  println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n5.0\n2.0\n6.0\n9.0\n4.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  println(output)\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.01619357     -0.030372608    0.07217587      -0.060049288\n-0.025108865    -0.025346711    0.09046136      -0.023320194\n0.014426847     -0.09052222     0.019132217     -0.035093457\n0.048913725     0.033388995     -0.07938445     0.01381052\n-0.0048129847   0.023922665     0.005595926     -0.09681542\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x4]  println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0\n0.0\n0.0\n0.0\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  layer = LookupTable(9, 4, 2.0, 0.1, 2.0, True)\ninput = np.array([5.0, 2.0, 6.0, 9.0, 4.0]).astype( float32 )\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, output)  output\n[array([[-0.00704637,  0.07495038,  0.06465427,  0.01235369],\n        [ 0.00350313,  0.02751033, -0.02163727,  0.0936095 ],\n        [ 0.02330465, -0.05696457,  0.0081728 ,  0.07839092],\n        [ 0.06580321, -0.0743262 , -0.00414508, -0.01133001],\n        [-0.00382435, -0.04677011,  0.02839171, -0.08361723]], dtype=float32)]  gradInput\n[array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)]", 
            "title": "LookupTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/", 
            "text": "Pack\n\n\nScala:\n\n\nval module = Pack(dim)\n\n\n\n\nPython:\n\n\nmodule = Pack(dim)\n\n\n\n\nPack is used to stack a list of n-dimensional tensors into one (n+1)-dimensional tensor.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Pack(2)\nval input1 = Tensor(2, 2).randn()\nval input2 = Tensor(2, 2).randn()\nval input = T()\ninput(1) = input1\ninput(2) = input2\n\nval output = module.forward(input)\n\n\n input\n {\n    2: -0.8737048   -0.7337217\n       0.7268678    -0.53470045\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: -1.3062215   -0.58756566\n       0.8921608    -1.8087773\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n\n\n output\n(1,.,.) =\n-1.3062215  -0.58756566\n-0.8737048  -0.7337217\n\n(2,.,.) =\n0.8921608   -1.8087773\n0.7268678   -0.53470045\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Pack(2)\ninput1 = np.random.randn(2, 2)\ninput2 = np.random.randn(2, 2)\ninput = [input1, input2]\noutput = module.forward(input)\n\n\n input\n[array([[ 0.92741416, -3.29826586],\n       [-0.03147819, -0.10049306]]), array([[-0.27146461, -0.25729802],\n       [ 0.1316149 ,  1.27620145]])]\n\n\n output\narray([[[ 0.92741418, -3.29826593],\n        [-0.27146462, -0.25729802]],\n\n       [[-0.03147819, -0.10049306],\n        [ 0.13161489,  1.27620149]]], dtype=float32)\n\n\n\n\n\n\nMM\n\n\nScala:\n\n\nval m = MM(transA=false,transB=false)\n\n\n\n\nPython:\n\n\nm = MM(trans_a=False,trans_b=False)\n\n\n\n\nMM is a module that performs matrix multiplication on two mini-batch inputs, producing one mini-batch.\n\n\nScala example:\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input = T(1 -\n Tensor(3, 3).randn(), 2 -\n Tensor(3, 3).randn())\nval m1 = MM()\nval output1 = m1.forward(input)\nval m2 = MM(true,true)\nval output2 = m2.forward(input)\n\nscala\n print(input)\n {\n        2: -0.62020904  -0.18690863     0.34132162\n           -0.5359324   -0.09937895     0.86147165\n           -2.6607985   -1.426654       2.3428898\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: -1.3087689   0.048720464     0.69583243\n           -0.52055264  -1.5275089      -1.1569321\n           0.28093573   -0.29353273     -0.9505267\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n }\n\nscala\n print(output1)\n-1.0658705      -0.7529337      1.225519\n4.2198563       1.8996398       -4.204146\n2.512235        1.3327343       -2.38396\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nscala\n print(output2)\n1.0048954       0.99516183      4.8832207\n0.15509865      -0.12717877     1.3618765\n-0.5397563      -1.0767963      -2.4279075\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput1=np.random.rand(3,3)\ninput2=np.random.rand(3,3)\ninput = [input1,input2]\nprint \ninput is :\n,input\nout = MM().forward(input)\nprint \noutput is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [array([[ 0.13696046,  0.92653165,  0.73585328],\n       [ 0.28167852,  0.06431783,  0.15710073],\n       [ 0.21896166,  0.00780161,  0.25780671]]), array([[ 0.11232797,  0.17023931,  0.92430042],\n       [ 0.86629537,  0.07630215,  0.08584417],\n       [ 0.47087278,  0.22992833,  0.59257503]])]\ncreating: createMM\noutput is : [array([[ 1.16452789,  0.26320592,  0.64217824],\n       [ 0.16133308,  0.08898225,  0.35897085],\n       [ 0.15274818,  0.09714822,  0.3558259 ]], dtype=float32)]\n\n\n\n\nCMaxTable\n\n\nScala:\n\n\nval m = CMaxTable()\n\n\n\n\nPython:\n\n\nm = CMaxTable()\n\n\n\n\nCMaxTable is a module that takes a table of Tensors and outputs the max of all of them.\n\n\nScala example:\n\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).randn()\nval input2 =  Tensor(3).randn()\nval input = T(input1, input2)\nval m = CMaxTable()\nval output = m.forward(input)\nval gradOut = Tensor(3).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala\n print(input)\n {\n        2: -0.38613814\n           0.74074316\n           -1.753783\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        1: -1.6037064\n           -2.3297918\n           -0.7160026\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\nscala\n print(output)\n-0.38613814\n0.74074316\n-0.7160026\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nscala\n print(gradOut)\n-1.4526331\n0.7070323\n0.29294914\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala\n print(gradIn)\n {\n        2: -1.4526331\n           0.7070323\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 0.0\n           0.0\n           0.29294914\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.rand(3)\ninput2 = np.random.rand(3)\nprint \ninput is :\n,input1,input2\n\nm = CMaxTable()\nout = m.forward([input1,input2])\nprint \noutput of m is :\n,out\n\ngrad_out = np.random.rand(3)\ngrad_in = m.backward([input1, input2],grad_out)\nprint \ngrad input of m is :\n,grad_in\n\n\n\n\nGives the output,\n\n\ninput is : [ 0.48649797  0.22131348  0.45667796] [ 0.73207053  0.74290136  0.03169769]\ncreating: createCMaxTable\noutput of m is : [array([ 0.73207051,  0.74290138,  0.45667794], dtype=float32)]\ngrad input of m is : [array([ 0.        ,  0.        ,  0.86938971], dtype=float32), array([ 0.04140199,  0.4787094 ,  0.        ], dtype=float32)]\n\n\n\n\n\n\nSplitTable\n\n\nScala:\n\n\nval layer = SplitTable(dim)\n\n\n\n\nPython:\n\n\nlayer = SplitTable(dim)\n\n\n\n\nSplitTable takes a Tensor as input and outputs several tables,\nsplitting the Tensor along the specified dimension \ndimension\n. Please note\nthe dimension starts from 1.\n\n\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user needs to specify the number of dimensions of each sample tensor in a\nbatch using \nnInputDims\n.\n\n\n    +----------+         +-----------+\n    | input[1] +---------\n {member1, |\n  +----------+-+         |           |\n  | input[2] +-----------\n  member2, |\n+----------+-+           |           |\n| input[3] +-------------\n  member3} |\n+----------+             +-----------+\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SplitTable(2)\nlayer.forward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\nlayer.backward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), T(\n  Tensor(T(0.1f, 0.2f, 0.3f)),\n  Tensor(T(0.4f, 0.5f, 0.6f)),\n  Tensor(T(0.7f, 0.8f, 0.9f))\n))\n\n\n\n\nGives the output,\n\n\n {\n        2: 2.0\n           5.0\n           8.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 1.0\n           4.0\n           7.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        3: 3.0\n           6.0\n           9.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n0.1     0.4     0.7\n0.2     0.5     0.8\n0.3     0.6     0.9\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import SplitTable\nimport numpy as np\n\nlayer = SplitTable(2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\n\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), [\n  np.array([0.1, 0.2, 0.3]),\n  np.array([0.4, 0.5, 0.6]),\n  np.array([0.7, 0.8, 0.9])\n])\n\n\n\n\nGives the output,\n\n\n[\n  array([ 1.,  4.,  7.], dtype=float32),\n  array([ 2.,  5.,  8.], dtype=float32),\n  array([ 3.,  6.,  9.], dtype=float32)\n]\n\narray([[ 0.1       ,  0.40000001,  0.69999999],\n       [ 0.2       ,  0.5       ,  0.80000001],\n       [ 0.30000001,  0.60000002,  0.89999998]], dtype=float32)\n\n\n\n\n\n\nDotProduct\n\n\nScala:\n\n\nval m = DotProduct()\n\n\n\n\nPython:\n\n\nm = DotProduct()\n\n\n\n\nOutputs the dot product (similarity) between inputs\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = DotProduct()\nval x = Tensor(3).fill(1f)\nval y = Tensor(3).fill(2f)\nprintln(\ninput:\n)\nprintln(x)\nprintln(y)\nprintln(\noutput:\n)\nprintln(mlp.forward(T(x, y)))\n\n\n\n\ninput:\n1.0\n1.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\noutput:\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\nmlp = DotProduct()\nx = np.array([1, 1, 1])\ny = np.array([2, 2, 2])\nprint(\ninput:\n)\nprint(x)\nprint(y)\nprint(\noutput:\n)\nprint(mlp.forward([x, y]))\n\n\n\n\n\ncreating: createDotProduct\ninput:\n[1 1 1]\n[2 2 2]\noutput:\n[ 6.]\n\n\n\n\n\n\nCSubTable\n\n\nScala:\n\n\nval model = CSubTable()\n\n\n\n\nPython:\n\n\nmodel = CSubTable()\n\n\n\n\nTakes a sequence with two Tensor and returns the component-wise subtraction between them.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CSubTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala\n print(input)\n {\n    2: 0.29122078\n       0.17347474\n       0.14127742\n       0.2249051\n       0.12171601\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.6202152\n       0.70417005\n       0.21334995\n       0.05191216\n       0.4209623\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\nscala\n print(output)\n0.3289944\n0.5306953\n0.072072536\n-0.17299294\n0.2992463\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\nPython example:\n\n\nmodel = CSubTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)\n\n\n\n\nGives the output,\n\n\narray([-1.15087152,  0.6169951 ,  2.41840839,  1.34374809,  1.39436531], dtype=float32)\n\n\n\n\n\n\nCDivTable\n\n\nScala:\n\n\nval module = CDivTable()\n\n\n\n\nPython:\n\n\nmodule = CDivTable()\n\n\n\n\nTakes a table with two Tensor and returns the component-wise division between them.\n\n\nScala example:\n\n\nval module = CDivTable()\nval input = T(1 -\n Tensor(2,3).rand(), 2 -\n Tensor(2,3).rand())\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.802295     0.7113872       0.29395157\n           0.6562403    0.06519115      0.20099664\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n        1: 0.7435388    0.59126955      0.10225375\n           0.46819785   0.10572237      0.9861797\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n }\n\nmodule.forward(input)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9267648       0.8311501       0.34785917\n0.7134549       1.6217289       4.906449\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nmodule = CDivTable()\ninput = [np.array([[1, 2, 3],[4, 5, 6]]), np.array([[1, 4, 9],[6, 10, 3]])]\nmodule.forward(input)\n[array([\n[ 1.,                   0.5     ,    0.33333334],\n[ 0.66666669, 0.5       ,  2.        ]], dtype=float32)]\n\n\n\n\n\n\nJoinTable\n\n\nScala:\n\n\nval layer = JoinTable(dimension, nInputDims)\n\n\n\n\nPython:\n\n\nlayer = JoinTable(dimension, n_input_dims)\n\n\n\n\nIt is a table module which takes a table of Tensors as input and\noutputs a Tensor by joining them together along the dimension \ndimension\n.\n\n\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using \nnInputDims\n.\n\n\nParameters:\n\n \ndimension\n  to be join in this dimension\n\n \nnInputDims\n specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size\n\n\n+----------+             +-----------+\n| {input1, +-------------\n output[1] |\n|          |           +-----------+-+\n|  input2, +-----------\n output[2] |\n|          |         +-----------+-+\n|  input3} +---------\n output[3] |\n+----------+         +-----------+\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.JoinTable\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = JoinTable(2, 2)\nval input1 = Tensor(T(\n  T(\n    T(1f, 2f, 3f),\n    T(2f, 3f, 4f),\n    T(3f, 4f, 5f))\n))\n\nval input2 = Tensor(T(\n  T(\n    T(3f, 4f, 5f),\n    T(2f, 3f, 4f),\n    T(1f, 2f, 3f))\n))\n\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(\n    T(1f, 2f, 3f, 3f, 4f, 5f),\n    T(2f, 3f, 4f, 2f, 3f, 4f),\n    T(3f, 4f, 5f, 1f, 2f, 3f)\n)))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n1.0 2.0 3.0 3.0 4.0 5.0\n2.0 3.0 4.0 2.0 3.0 4.0\n3.0 4.0 5.0 1.0 2.0 3.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x6]\n\nprintln(grad)\n {\n    2: (1,.,.) =\n       3.0  4.0 5.0\n       2.0  3.0 4.0\n       1.0  2.0 3.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n    1: (1,.,.) =\n       1.0  2.0 3.0\n       2.0  3.0 4.0\n       3.0  4.0 5.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }\n\n\n\n\nPython example:\n\n\nlayer = JoinTable(2, 2)\ninput1 = np.array([\n [\n    [1.0, 2.0, 3.0],\n    [2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0]\n  ]\n])\n\ninput2 = np.array([\n  [\n    [3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0],\n    [1.0, 2.0, 3.0]\n  ]\n])\n\ninput = [input1, input2]\n\ngradOutput = np.array([\n  [\n    [1.0, 2.0, 3.0, 3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0, 2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0, 1.0, 2.0, 3.0]\n  ]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 1.  2.  3.  3.  4.  5.]\n  [ 2.  3.  4.  2.  3.  4.]\n  [ 3.  4.  5.  1.  2.  3.]]]\n\nprint grad\n[array([[[ 1.,  2.,  3.],\n        [ 2.,  3.,  4.],\n        [ 3.,  4.,  5.]]], dtype=float32), array([[[ 3.,  4.,  5.],\n        [ 2.,  3.,  4.],\n        [ 1.,  2.,  3.]]], dtype=float32)]\n\n\n\n\n\n\nSelectTable\n\n\nScala:\n\n\nval m = SelectTable(index: Int)\n\n\n\n\nPython:\n\n\nm = SelectTable(dimension)\n\n\n\n\nSelect one element from a table by a given index.\nIn Scala API, table is kind of like HashMap with one-base index as the key.\nIn python, table is a just a list.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = T(Tensor(2,3).randn(), Tensor(2,3).randn())\n\nprintln(\ninput: \n)\nprintln(input)\nprintln(\noutput:\n)\nprintln(SelectTable(1).forward(input)) // Select and output the first element of the input which shape is (2, 3)\nprintln(SelectTable(2).forward(input)) // Select and output the second element of the input which shape is (2, 3)\n\n\n\n\n\ninput: \n {\n    2: 2.005436370849835    0.09670211785545313 1.186779895312918   \n       2.238415300857082    0.241626512721254   0.15765709974113828 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    1: 0.5668905654052705   -1.3205159007397167 -0.5431464848526197 \n       -0.11582559521074104 0.7671830693813515  -0.39992781407893574    \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\noutput:\n0.5668905654052705  -1.3205159007397167 -0.5431464848526197 \n-0.11582559521074104    0.7671830693813515  -0.39992781407893574    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n2.005436370849835   0.09670211785545313 1.186779895312918   \n2.238415300857082   0.241626512721254   0.15765709974113828 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\n\ninput = [np.random.random((2,3)), np.random.random((2, 1))]\nprint(\ninput:\n)\nprint(input)\nprint(\noutput:\n)\nprint(SelectTable(1).forward(input)) # Select and output the first element of the input which shape is (2, 3)\n\n\n\n\ninput:\n[array([[ 0.07185111,  0.26140439,  0.9437582 ],\n       [ 0.50278191,  0.83923974,  0.06396735]]), array([[ 0.84955122],\n       [ 0.16053703]])]\noutput:\ncreating: createSelectTable\n[[ 0.07185111  0.2614044   0.94375819]\n [ 0.50278193  0.83923972  0.06396735]]\n\n\n\n\n\n\n\nNarrowTable\n\n\nScala:\n\n\nval narrowTable = NarrowTable(offset, length = 1)\n\n\n\n\nPython:\n\n\nnarrowTable = NarrowTable(offset, length = 1)\n\n\n\n\nNarrowTable takes a table as input and returns a subtable starting from index \noffset\n having \nlength\n elements\n\n\nNegative \nlength\n means the last element is located at Abs|length| to the last element of input\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\nval narrowTable = NarrowTable(1, 1)\n\nval input = T()\ninput(1.0) = Tensor(2, 2).rand()\ninput(2.0) = Tensor(2, 2).rand()\ninput(3.0) = Tensor(2, 2).rand()\n\n print(input)\n {\n    2.0: 0.27686104 0.9040761   \n         0.75969505 0.8008061   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1.0: 0.94122535 0.46173728  \n         0.43302807 0.1670979   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    3.0: 0.43944374 0.49336782  \n         0.7274511  0.67777634  \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n  print(narrowTable.forward(input))\n {\n    1: 0.94122535   0.46173728  \n       0.43302807   0.1670979   \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nnarrowTable = NarrowTable(1, 1)\n\n narrowTable.forward([np.array([1, 2, 3]), np.array([4, 5, 6])])\n[array([ 1.,  2.,  3.], dtype=float32)]\n\n\n\n\n\n\n\nCAddTable\n\n\nScala:\n\n\nval module = CAddTable(inplace = false)\n\n\n\n\nPython:\n\n\nmodule = CAddTable(inplace=False)\n\n\n\n\nCAddTable merges the input tensors in the input table by element-wise adding. The input table is actually an array of tensor with same size.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprintln(mlp.forward(Tensor.range(1, 3, 1)))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.nn.abstractnn.Activity =\n2.0\n4.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprint(mlp.forward(np.arange(1, 4, 1)))\n\n\n\n\nGives the output,\n\n\n[array([ 2.,  4.,  6.], dtype=float32)]\n\n\n\n\n\n\nCMulTable\n\n\nScala:\n\n\nval model = CMulTable()\n\n\n\n\nPython:\n\n\nmodel = CMulTable()\n\n\n\n\nTakes a sequence of Tensors and outputs the multiplication of all of them.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CMulTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala\n print(input)\n {\n    2: 0.13224044\n       0.5460452\n       0.33032498\n       0.6317603\n       0.6665052\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.28694472\n       0.45169437\n       0.36891535\n       0.9126049\n       0.41318864\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }\n\nscala\n print(output)\n0.037945695\n0.24664554\n0.12186196\n0.57654756\n0.27539238\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n\n\n\n\nPython example:\n\n\nmodel = CMulTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)\n\n\n print(input)\n[array([ 0.28183274, -0.6477487 , -0.21279841,  0.22725124,  0.54748552]), array([-0.78673028, -1.08337196, -0.62710066,  0.37332587, -1.40708162])]\n\n\n print(output)\n[-0.22172636  0.70175284  0.13344601  0.08483877 -0.77035683]\n\n\n\n\n\n\nMV\n\n\nScala:\n\n\nval module = MV(trans = false)\n\n\n\n\nPython:\n\n\nmodule = MV(trans=False)\n\n\n\n\nIt is a module to perform matrix vector multiplication on two mini-batch inputs, producing a mini-batch.\n\n\ntrans\n means whether make matrix transpose before multiplication.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval module = MV()\n\nprintln(module.forward(T(Tensor.range(1, 12, 1).resize(2, 2, 3), Tensor.range(1, 6, 1).resize(2, 3))))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n14.0    32.0\n122.0   167.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nmodule = MV()\n\nprint(module.forward([np.arange(1, 13, 1).reshape(2, 2, 3), np.arange(1, 7, 1).reshape(2, 3)]))\n\n\n\n\nGives the output,\n\n\n[array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]\n\n\n\n\n\n\nFlattenTable\n\n\nScala:\n\n\nval module = FlattenTable()\n\n\n\n\nPython:\n\n\nmodule = FlattenTable()\n\n\n\n\nFlattenTable takes an arbitrarily deep table of Tensors (potentially nested) as input and a table of Tensors without any nested table will be produced\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = FlattenTable()\nval t1 = Tensor(3).randn()\nval t2 = Tensor(3).randn()\nval t3 = Tensor(3).randn()\nval input = T(t1, T(t2, T(t3)))\n\nval output = module.forward(input)\n\n\n input\n {\n    2:  {\n        2:  {\n            1: 0.5521984\n               -0.4160644\n               -0.698762\n               [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n            }\n        1: -1.7380241\n           0.60336906\n           -0.8751049\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        }\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\n\n\n output\n{\n    2: -1.7380241\n       0.60336906\n       -0.8751049\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    3: 0.5521984\n       -0.4160644\n       -0.698762\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Sequential()\n# this will create a nested table\nnested = ConcatTable().add(Identity()).add(Identity())\nmodule.add(nested).add(FlattenTable())\nt1 = np.random.randn(3)\nt2 = np.random.randn(3)\ninput = [t1, t2]\noutput = module.forward(input)\n\n\n input\n[array([-2.21080689, -0.48928043, -0.26122161]), array([-0.8499716 ,  1.63694575, -0.31109292])]\n\n\n output\n[array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32),\n array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32)]\n\n\n\n\n\n\n\nCMinTable\n\n\nScala:\n\n\nval layer = CMinTable()\n\n\n\n\nPython:\n\n\nlayer = CMinTable()\n\n\n\n\nCMinTable takes a bunch of tensors as inputs. These tensors must have\nsame shape. This layer will merge them by doing an element-wise comparision\nand use the min value.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = CMinTable()\nlayer.forward(T(\n  Tensor(T(1.0f, 5.0f, 2.0f)),\n  Tensor(T(3.0f, 4.0f, -1.0f)),\n  Tensor(T(5.0f, 7.0f, -5.0f))\n))\nlayer.backward(T(\n  Tensor(T(1.0f, 5.0f, 2.0f)),\n  Tensor(T(3.0f, 4.0f, -1.0f)),\n  Tensor(T(5.0f, 7.0f, -5.0f))\n), Tensor(T(0.1f, 0.2f, 0.3f)))\n\n\n\n\nGives the output,\n\n\n1.0\n4.0\n-5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n{\n  2: 0.0\n     0.2\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  1: 0.1\n     0.0\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  3: 0.0\n     0.0\n     0.3\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n}\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import CMinTable\nimport numpy as np\n\nlayer = CMinTable()\nlayer.forward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n])\n\nlayer.backward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n], np.array([0.1, 0.2, 0.3]))\n\n\n\n\n\nGives the output,\n\n\narray([ 1.,  4., -5.], dtype=float32)\n\n[array([ 0.1, 0., 0.], dtype=float32),\narray([ 0., 0.2, 0.], dtype=float32),\narray([ 0., 0., 0.30000001], dtype=float32)]", 
            "title": "Merge/Split Layers"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#pack", 
            "text": "Scala:  val module = Pack(dim)  Python:  module = Pack(dim)  Pack is used to stack a list of n-dimensional tensors into one (n+1)-dimensional tensor.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Pack(2)\nval input1 = Tensor(2, 2).randn()\nval input2 = Tensor(2, 2).randn()\nval input = T()\ninput(1) = input1\ninput(2) = input2\n\nval output = module.forward(input)  input\n {\n    2: -0.8737048   -0.7337217\n       0.7268678    -0.53470045\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: -1.3062215   -0.58756566\n       0.8921608    -1.8087773\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }  output\n(1,.,.) =\n-1.3062215  -0.58756566\n-0.8737048  -0.7337217\n\n(2,.,.) =\n0.8921608   -1.8087773\n0.7268678   -0.53470045\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Pack(2)\ninput1 = np.random.randn(2, 2)\ninput2 = np.random.randn(2, 2)\ninput = [input1, input2]\noutput = module.forward(input)  input\n[array([[ 0.92741416, -3.29826586],\n       [-0.03147819, -0.10049306]]), array([[-0.27146461, -0.25729802],\n       [ 0.1316149 ,  1.27620145]])]  output\narray([[[ 0.92741418, -3.29826593],\n        [-0.27146462, -0.25729802]],\n\n       [[-0.03147819, -0.10049306],\n        [ 0.13161489,  1.27620149]]], dtype=float32)", 
            "title": "Pack"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#mm", 
            "text": "Scala:  val m = MM(transA=false,transB=false)  Python:  m = MM(trans_a=False,trans_b=False)  MM is a module that performs matrix multiplication on two mini-batch inputs, producing one mini-batch.  Scala example:  scala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input = T(1 -  Tensor(3, 3).randn(), 2 -  Tensor(3, 3).randn())\nval m1 = MM()\nval output1 = m1.forward(input)\nval m2 = MM(true,true)\nval output2 = m2.forward(input)\n\nscala  print(input)\n {\n        2: -0.62020904  -0.18690863     0.34132162\n           -0.5359324   -0.09937895     0.86147165\n           -2.6607985   -1.426654       2.3428898\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n        1: -1.3087689   0.048720464     0.69583243\n           -0.52055264  -1.5275089      -1.1569321\n           0.28093573   -0.29353273     -0.9505267\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n }\n\nscala  print(output1)\n-1.0658705      -0.7529337      1.225519\n4.2198563       1.8996398       -4.204146\n2.512235        1.3327343       -2.38396\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nscala  print(output2)\n1.0048954       0.99516183      4.8832207\n0.15509865      -0.12717877     1.3618765\n-0.5397563      -1.0767963      -2.4279075\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]   Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput1=np.random.rand(3,3)\ninput2=np.random.rand(3,3)\ninput = [input1,input2]\nprint  input is : ,input\nout = MM().forward(input)\nprint  output is : ,out  produces output:  input is : [array([[ 0.13696046,  0.92653165,  0.73585328],\n       [ 0.28167852,  0.06431783,  0.15710073],\n       [ 0.21896166,  0.00780161,  0.25780671]]), array([[ 0.11232797,  0.17023931,  0.92430042],\n       [ 0.86629537,  0.07630215,  0.08584417],\n       [ 0.47087278,  0.22992833,  0.59257503]])]\ncreating: createMM\noutput is : [array([[ 1.16452789,  0.26320592,  0.64217824],\n       [ 0.16133308,  0.08898225,  0.35897085],\n       [ 0.15274818,  0.09714822,  0.3558259 ]], dtype=float32)]", 
            "title": "MM"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#cmaxtable", 
            "text": "Scala:  val m = CMaxTable()  Python:  m = CMaxTable()  CMaxTable is a module that takes a table of Tensors and outputs the max of all of them.  Scala example:  \nscala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval input1 = Tensor(3).randn()\nval input2 =  Tensor(3).randn()\nval input = T(input1, input2)\nval m = CMaxTable()\nval output = m.forward(input)\nval gradOut = Tensor(3).randn()\nval gradIn = m.backward(input,gradOut)\n\nscala  print(input)\n {\n        2: -0.38613814\n           0.74074316\n           -1.753783\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        1: -1.6037064\n           -2.3297918\n           -0.7160026\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\nscala  print(output)\n-0.38613814\n0.74074316\n-0.7160026\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nscala  print(gradOut)\n-1.4526331\n0.7070323\n0.29294914\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala  print(gradIn)\n {\n        2: -1.4526331\n           0.7070323\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 0.0\n           0.0\n           0.29294914\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput1 = np.random.rand(3)\ninput2 = np.random.rand(3)\nprint  input is : ,input1,input2\n\nm = CMaxTable()\nout = m.forward([input1,input2])\nprint  output of m is : ,out\n\ngrad_out = np.random.rand(3)\ngrad_in = m.backward([input1, input2],grad_out)\nprint  grad input of m is : ,grad_in  Gives the output,  input is : [ 0.48649797  0.22131348  0.45667796] [ 0.73207053  0.74290136  0.03169769]\ncreating: createCMaxTable\noutput of m is : [array([ 0.73207051,  0.74290138,  0.45667794], dtype=float32)]\ngrad input of m is : [array([ 0.        ,  0.        ,  0.86938971], dtype=float32), array([ 0.04140199,  0.4787094 ,  0.        ], dtype=float32)]", 
            "title": "CMaxTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#splittable", 
            "text": "Scala:  val layer = SplitTable(dim)  Python:  layer = SplitTable(dim)  SplitTable takes a Tensor as input and outputs several tables,\nsplitting the Tensor along the specified dimension  dimension . Please note\nthe dimension starts from 1.  The input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user needs to specify the number of dimensions of each sample tensor in a\nbatch using  nInputDims .      +----------+         +-----------+\n    | input[1] +---------  {member1, |\n  +----------+-+         |           |\n  | input[2] +-----------   member2, |\n+----------+-+           |           |\n| input[3] +-------------   member3} |\n+----------+             +-----------+  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SplitTable(2)\nlayer.forward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)))\nlayer.backward(Tensor(T(\n  T(1.0f, 2.0f, 3.0f),\n  T(4.0f, 5.0f, 6.0f),\n  T(7.0f, 8.0f, 9.0f)\n)), T(\n  Tensor(T(0.1f, 0.2f, 0.3f)),\n  Tensor(T(0.4f, 0.5f, 0.6f)),\n  Tensor(T(0.7f, 0.8f, 0.9f))\n))  Gives the output,   {\n        2: 2.0\n           5.0\n           8.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        1: 1.0\n           4.0\n           7.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n        3: 3.0\n           6.0\n           9.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n }\n\n0.1     0.4     0.7\n0.2     0.5     0.8\n0.3     0.6     0.9\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import SplitTable\nimport numpy as np\n\nlayer = SplitTable(2)\nlayer.forward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]))\n\nlayer.backward(np.array([\n  [1.0, 2.0, 3.0],\n  [4.0, 5.0, 6.0],\n  [7.0, 8.0, 9.0]\n]), [\n  np.array([0.1, 0.2, 0.3]),\n  np.array([0.4, 0.5, 0.6]),\n  np.array([0.7, 0.8, 0.9])\n])  Gives the output,  [\n  array([ 1.,  4.,  7.], dtype=float32),\n  array([ 2.,  5.,  8.], dtype=float32),\n  array([ 3.,  6.,  9.], dtype=float32)\n]\n\narray([[ 0.1       ,  0.40000001,  0.69999999],\n       [ 0.2       ,  0.5       ,  0.80000001],\n       [ 0.30000001,  0.60000002,  0.89999998]], dtype=float32)", 
            "title": "SplitTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#dotproduct", 
            "text": "Scala:  val m = DotProduct()  Python:  m = DotProduct()  Outputs the dot product (similarity) between inputs  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval mlp = DotProduct()\nval x = Tensor(3).fill(1f)\nval y = Tensor(3).fill(2f)\nprintln( input: )\nprintln(x)\nprintln(y)\nprintln( output: )\nprintln(mlp.forward(T(x, y)))  input:\n1.0\n1.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\noutput:\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\nmlp = DotProduct()\nx = np.array([1, 1, 1])\ny = np.array([2, 2, 2])\nprint( input: )\nprint(x)\nprint(y)\nprint( output: )\nprint(mlp.forward([x, y]))  creating: createDotProduct\ninput:\n[1 1 1]\n[2 2 2]\noutput:\n[ 6.]", 
            "title": "DotProduct"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#csubtable", 
            "text": "Scala:  val model = CSubTable()  Python:  model = CSubTable()  Takes a sequence with two Tensor and returns the component-wise subtraction between them.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CSubTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala  print(input)\n {\n    2: 0.29122078\n       0.17347474\n       0.14127742\n       0.2249051\n       0.12171601\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.6202152\n       0.70417005\n       0.21334995\n       0.05191216\n       0.4209623\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\nscala  print(output)\n0.3289944\n0.5306953\n0.072072536\n-0.17299294\n0.2992463\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  model = CSubTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)  Gives the output,  array([-1.15087152,  0.6169951 ,  2.41840839,  1.34374809,  1.39436531], dtype=float32)", 
            "title": "CSubTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#cdivtable", 
            "text": "Scala:  val module = CDivTable()  Python:  module = CDivTable()  Takes a table with two Tensor and returns the component-wise division between them.  Scala example:  val module = CDivTable()\nval input = T(1 -  Tensor(2,3).rand(), 2 -  Tensor(2,3).rand())\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.802295     0.7113872       0.29395157\n           0.6562403    0.06519115      0.20099664\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n        1: 0.7435388    0.59126955      0.10225375\n           0.46819785   0.10572237      0.9861797\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n }\n\nmodule.forward(input)\nres6: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9267648       0.8311501       0.34785917\n0.7134549       1.6217289       4.906449\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = CDivTable()\ninput = [np.array([[1, 2, 3],[4, 5, 6]]), np.array([[1, 4, 9],[6, 10, 3]])]\nmodule.forward(input)\n[array([\n[ 1.,                   0.5     ,    0.33333334],\n[ 0.66666669, 0.5       ,  2.        ]], dtype=float32)]", 
            "title": "CDivTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#jointable", 
            "text": "Scala:  val layer = JoinTable(dimension, nInputDims)  Python:  layer = JoinTable(dimension, n_input_dims)  It is a table module which takes a table of Tensors as input and\noutputs a Tensor by joining them together along the dimension  dimension .  The input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using  nInputDims .  Parameters:   dimension   to be join in this dimension   nInputDims  specify the number of dimensions that this module will receiveIf it is more than the dimension of input tensors, the first dimensionwould be considered as batch size  +----------+             +-----------+\n| {input1, +-------------  output[1] |\n|          |           +-----------+-+\n|  input2, +-----------  output[2] |\n|          |         +-----------+-+\n|  input3} +---------  output[3] |\n+----------+         +-----------+  Scala example:  import com.intel.analytics.bigdl.nn.JoinTable\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = JoinTable(2, 2)\nval input1 = Tensor(T(\n  T(\n    T(1f, 2f, 3f),\n    T(2f, 3f, 4f),\n    T(3f, 4f, 5f))\n))\n\nval input2 = Tensor(T(\n  T(\n    T(3f, 4f, 5f),\n    T(2f, 3f, 4f),\n    T(1f, 2f, 3f))\n))\n\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(\n    T(1f, 2f, 3f, 3f, 4f, 5f),\n    T(2f, 3f, 4f, 2f, 3f, 4f),\n    T(3f, 4f, 5f, 1f, 2f, 3f)\n)))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n(1,.,.) =\n1.0 2.0 3.0 3.0 4.0 5.0\n2.0 3.0 4.0 2.0 3.0 4.0\n3.0 4.0 5.0 1.0 2.0 3.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x6]\n\nprintln(grad)\n {\n    2: (1,.,.) =\n       3.0  4.0 5.0\n       2.0  3.0 4.0\n       1.0  2.0 3.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n    1: (1,.,.) =\n       1.0  2.0 3.0\n       2.0  3.0 4.0\n       3.0  4.0 5.0\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x3]\n }  Python example:  layer = JoinTable(2, 2)\ninput1 = np.array([\n [\n    [1.0, 2.0, 3.0],\n    [2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0]\n  ]\n])\n\ninput2 = np.array([\n  [\n    [3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0],\n    [1.0, 2.0, 3.0]\n  ]\n])\n\ninput = [input1, input2]\n\ngradOutput = np.array([\n  [\n    [1.0, 2.0, 3.0, 3.0, 4.0, 5.0],\n    [2.0, 3.0, 4.0, 2.0, 3.0, 4.0],\n    [3.0, 4.0, 5.0, 1.0, 2.0, 3.0]\n  ]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[[ 1.  2.  3.  3.  4.  5.]\n  [ 2.  3.  4.  2.  3.  4.]\n  [ 3.  4.  5.  1.  2.  3.]]]\n\nprint grad\n[array([[[ 1.,  2.,  3.],\n        [ 2.,  3.,  4.],\n        [ 3.,  4.,  5.]]], dtype=float32), array([[[ 3.,  4.,  5.],\n        [ 2.,  3.,  4.],\n        [ 1.,  2.,  3.]]], dtype=float32)]", 
            "title": "JoinTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#selecttable", 
            "text": "Scala:  val m = SelectTable(index: Int)  Python:  m = SelectTable(dimension)  Select one element from a table by a given index.\nIn Scala API, table is kind of like HashMap with one-base index as the key.\nIn python, table is a just a list.  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = T(Tensor(2,3).randn(), Tensor(2,3).randn())\n\nprintln( input:  )\nprintln(input)\nprintln( output: )\nprintln(SelectTable(1).forward(input)) // Select and output the first element of the input which shape is (2, 3)\nprintln(SelectTable(2).forward(input)) // Select and output the second element of the input which shape is (2, 3)  input: \n {\n    2: 2.005436370849835    0.09670211785545313 1.186779895312918   \n       2.238415300857082    0.241626512721254   0.15765709974113828 \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    1: 0.5668905654052705   -1.3205159007397167 -0.5431464848526197 \n       -0.11582559521074104 0.7671830693813515  -0.39992781407893574    \n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\noutput:\n0.5668905654052705  -1.3205159007397167 -0.5431464848526197 \n-0.11582559521074104    0.7671830693813515  -0.39992781407893574    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n2.005436370849835   0.09670211785545313 1.186779895312918   \n2.238415300857082   0.241626512721254   0.15765709974113828 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\n\ninput = [np.random.random((2,3)), np.random.random((2, 1))]\nprint( input: )\nprint(input)\nprint( output: )\nprint(SelectTable(1).forward(input)) # Select and output the first element of the input which shape is (2, 3)  input:\n[array([[ 0.07185111,  0.26140439,  0.9437582 ],\n       [ 0.50278191,  0.83923974,  0.06396735]]), array([[ 0.84955122],\n       [ 0.16053703]])]\noutput:\ncreating: createSelectTable\n[[ 0.07185111  0.2614044   0.94375819]\n [ 0.50278193  0.83923972  0.06396735]]", 
            "title": "SelectTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#narrowtable", 
            "text": "Scala:  val narrowTable = NarrowTable(offset, length = 1)  Python:  narrowTable = NarrowTable(offset, length = 1)  NarrowTable takes a table as input and returns a subtable starting from index  offset  having  length  elements  Negative  length  means the last element is located at Abs|length| to the last element of input  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\nval narrowTable = NarrowTable(1, 1)\n\nval input = T()\ninput(1.0) = Tensor(2, 2).rand()\ninput(2.0) = Tensor(2, 2).rand()\ninput(3.0) = Tensor(2, 2).rand()  print(input)\n {\n    2.0: 0.27686104 0.9040761   \n         0.75969505 0.8008061   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1.0: 0.94122535 0.46173728  \n         0.43302807 0.1670979   \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    3.0: 0.43944374 0.49336782  \n         0.7274511  0.67777634  \n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }   print(narrowTable.forward(input))\n {\n    1: 0.94122535   0.46173728  \n       0.43302807   0.1670979   \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }  Python example:  from bigdl.nn.layer import *\nnarrowTable = NarrowTable(1, 1)  narrowTable.forward([np.array([1, 2, 3]), np.array([4, 5, 6])])\n[array([ 1.,  2.,  3.], dtype=float32)]", 
            "title": "NarrowTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#caddtable", 
            "text": "Scala:  val module = CAddTable(inplace = false)  Python:  module = CAddTable(inplace=False)  CAddTable merges the input tensors in the input table by element-wise adding. The input table is actually an array of tensor with same size.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprintln(mlp.forward(Tensor.range(1, 3, 1)))  Gives the output,  com.intel.analytics.bigdl.nn.abstractnn.Activity =\n2.0\n4.0\n6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmlp = Sequential()\nmlp.add(ConcatTable().add(Identity()).add(Identity()))\nmlp.add(CAddTable())\n\nprint(mlp.forward(np.arange(1, 4, 1)))  Gives the output,  [array([ 2.,  4.,  6.], dtype=float32)]", 
            "title": "CAddTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#cmultable", 
            "text": "Scala:  val model = CMulTable()  Python:  model = CMulTable()  Takes a sequence of Tensors and outputs the multiplication of all of them.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\n\nval model = CMulTable()\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T(input1, input2)\nval output = model.forward(input)\n\nscala  print(input)\n {\n    2: 0.13224044\n       0.5460452\n       0.33032498\n       0.6317603\n       0.6665052\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1: 0.28694472\n       0.45169437\n       0.36891535\n       0.9126049\n       0.41318864\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }\n\nscala  print(output)\n0.037945695\n0.24664554\n0.12186196\n0.57654756\n0.27539238\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5]  Python example:  model = CMulTable()\ninput1 = np.random.randn(5)\ninput2 = np.random.randn(5)\ninput = [input1, input2]\noutput = model.forward(input)  print(input)\n[array([ 0.28183274, -0.6477487 , -0.21279841,  0.22725124,  0.54748552]), array([-0.78673028, -1.08337196, -0.62710066,  0.37332587, -1.40708162])]  print(output)\n[-0.22172636  0.70175284  0.13344601  0.08483877 -0.77035683]", 
            "title": "CMulTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#mv", 
            "text": "Scala:  val module = MV(trans = false)  Python:  module = MV(trans=False)  It is a module to perform matrix vector multiplication on two mini-batch inputs, producing a mini-batch.  trans  means whether make matrix transpose before multiplication.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.utils.T\n\nval module = MV()\n\nprintln(module.forward(T(Tensor.range(1, 12, 1).resize(2, 2, 3), Tensor.range(1, 6, 1).resize(2, 3))))  Gives the output,  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n14.0    32.0\n122.0   167.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  module = MV()\n\nprint(module.forward([np.arange(1, 13, 1).reshape(2, 2, 3), np.arange(1, 7, 1).reshape(2, 3)]))  Gives the output,  [array([ 0.31657887, -1.11062765, -1.16235781, -0.67723978,  0.74650359], dtype=float32)]", 
            "title": "MV"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#flattentable", 
            "text": "Scala:  val module = FlattenTable()  Python:  module = FlattenTable()  FlattenTable takes an arbitrarily deep table of Tensors (potentially nested) as input and a table of Tensors without any nested table will be produced  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = FlattenTable()\nval t1 = Tensor(3).randn()\nval t2 = Tensor(3).randn()\nval t3 = Tensor(3).randn()\nval input = T(t1, T(t2, T(t3)))\n\nval output = module.forward(input)  input\n {\n    2:  {\n        2:  {\n            1: 0.5521984\n               -0.4160644\n               -0.698762\n               [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n            }\n        1: -1.7380241\n           0.60336906\n           -0.8751049\n           [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n        }\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }  output\n{\n    2: -1.7380241\n       0.60336906\n       -0.8751049\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0529885\n       -0.792229\n       0.8395628\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    3: 0.5521984\n       -0.4160644\n       -0.698762\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Sequential()\n# this will create a nested table\nnested = ConcatTable().add(Identity()).add(Identity())\nmodule.add(nested).add(FlattenTable())\nt1 = np.random.randn(3)\nt2 = np.random.randn(3)\ninput = [t1, t2]\noutput = module.forward(input)  input\n[array([-2.21080689, -0.48928043, -0.26122161]), array([-0.8499716 ,  1.63694575, -0.31109292])]  output\n[array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32),\n array([-2.21080685, -0.48928043, -0.26122162], dtype=float32),\n array([-0.84997159,  1.63694572, -0.31109291], dtype=float32)]", 
            "title": "FlattenTable"
        }, 
        {
            "location": "/APIGuide/Layers/MergeSplit-Layers/#cmintable", 
            "text": "Scala:  val layer = CMinTable()  Python:  layer = CMinTable()  CMinTable takes a bunch of tensors as inputs. These tensors must have\nsame shape. This layer will merge them by doing an element-wise comparision\nand use the min value.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = CMinTable()\nlayer.forward(T(\n  Tensor(T(1.0f, 5.0f, 2.0f)),\n  Tensor(T(3.0f, 4.0f, -1.0f)),\n  Tensor(T(5.0f, 7.0f, -5.0f))\n))\nlayer.backward(T(\n  Tensor(T(1.0f, 5.0f, 2.0f)),\n  Tensor(T(3.0f, 4.0f, -1.0f)),\n  Tensor(T(5.0f, 7.0f, -5.0f))\n), Tensor(T(0.1f, 0.2f, 0.3f)))  Gives the output,  1.0\n4.0\n-5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n{\n  2: 0.0\n     0.2\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  1: 0.1\n     0.0\n     0.0\n     [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n  3: 0.0\n     0.0\n     0.3\n  [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n}  Python example:  from bigdl.nn.layer import CMinTable\nimport numpy as np\n\nlayer = CMinTable()\nlayer.forward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n])\n\nlayer.backward([\n  np.array([1.0, 5.0, 2.0]),\n  np.array([3.0, 4.0, -1.0]),\n  np.array([5.0, 7.0, -5.0])\n], np.array([0.1, 0.2, 0.3]))  Gives the output,  array([ 1.,  4., -5.], dtype=float32)\n\n[array([ 0.1, 0., 0.], dtype=float32),\narray([ 0., 0.2, 0.], dtype=float32),\narray([ 0., 0., 0.30000001], dtype=float32)]", 
            "title": "CMinTable"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/", 
            "text": "Scale\n\n\nScala:\n\n\nval m = Scale(Array(2, 1))\n\n\n\n\nPython:\n\n\nm = scale = Scale([2, 1])\n\n\n\n\nScale is the combination of cmul and cadd. \nScale(size).forward(input) == CAdd(size).forward(CMul(size).forward(input))\n\nComputes the elementwise product of input and weight, with the shape of the weight \"expand\" to\nmatch the shape of the input.Similarly, perform a expand cdd bias and perform an elementwise add.\n\noutput = input .* weight .+ bias (element wise)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).fill(1f)\nprintln(\ninput:\n)\nprintln(input)\nval scale = Scale(Array(2, 1))\nval weight = Tensor(2, 1).fill(2f)\nval bias = Tensor(2, 1).fill(3f)\nscale.setWeightsBias(Array(weight, bias))\nprintln(\nWeight:\n)\nprintln(weight)\nprintln(\nbias:\n)\nprintln(bias)\nprintln(\noutput:\n)\nprint(scale.forward(input))\n\n\n\n\ninput:\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\nWeight:\n2.0 \n2.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\nbias:\n3.0 \n3.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\noutput:\n5.0 5.0 5.0 \n5.0 5.0 5.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.layer import *\ninput = np.ones([2, 3])\nprint(\ninput:\n)\nprint(input)\nscale = Scale([2, 1])\nweight = np.full([2, 1], 2)\nbias = np.full([2, 1], 3)\nprint(\nweight: \n)\nprint(weight)\nprint(\nbias: \n)\nprint(bias)\nscale.set_weights([weight, bias])\nprint(\noutput: \n)\nprint(scale.forward(input))\n\n\n\n\n\ninput:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\ncreating: createScale\nweight: \n[[2]\n [2]]\nbias: \n[[3]\n [3]]\noutput: \n[[ 5.  5.  5.]\n [ 5.  5.  5.]]\n\n\n\n\n\n\nMin\n\n\nScala:\n\n\nval min = Min(dim, numInputDims)\n\n\n\n\nPython:\n\n\nmin = Min(dim, num_input_dims)\n\n\n\n\nApplies a min operation over dimension \ndim\n.\n\n\nParameters:\n\n \ndim\n A integer. The dimension to min along.\n\n \nnumInputDims\n An optional integer indicating the number of input dimensions.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval min = Min(2)\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n 1.0f,\n 1.0f\n))\nval output = min.forward(input)\nval gradient = min.backward(input, gradOutput)\n-\n print(output)\n1.0\n3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-\n print(gradient)\n1.0     0.0     \n1.0     0.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nmin = Min(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 4.0]\n])\n\ngrad_output = np.array([1.0, 1.0])\noutput = min.forward(input)\ngradient = min.backward(input, grad_output)\n-\n print output\n[ 1.  3.]\n-\n print gradient\n[[ 1.  0.]\n [ 1.  0.]]\n\n\n\n\n\n\nAdd\n\n\nScala:\n\n\nval addLayer = Add(inputSize)\n\n\n\n\nPython:\n\n\nadd_layer = Add(input_size)\n\n\n\n\nA.K.A BiasAdd. This layer adds input tensor with a parameter tensor and output the result.\nIf the input is 1D, this layer just do a element-wise add. If the input has multiple dimentions,\nthis layer will treat the first dimension as batch dimension, resize the input tensor to a 2D \ntensor(batch-dimension x input_size) and do a broadcast add between the 2D tensor and the \nparameter.\n\n\nPlease note that the parameter will be trained in the back propagation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval addLayer = Add(4)\naddLayer.bias.set(Tensor(T(1.0f, 2.0f, 3.0f, 4.0f)))\naddLayer.forward(Tensor(T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))))\naddLayer.backward(Tensor(T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))),\n    Tensor(T(T(0.1f, 0.1f, 0.1f, 0.1f), T(0.3f, 0.3f, 0.3f, 0.3f))))\n\n\n\n\nGives the output,\n\n\n2.0     3.0     4.0     5.0\n4.0     5.0     6.0     7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n0.1     0.1     0.1     0.1\n0.3     0.3     0.3     0.3\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import Add\nimport numpy as np\n\nadd_layer = Add(4)\nadd_layer.set_weights([np.array([1.0, 2.0, 3.0, 4.0])])\nadd_layer.forward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]))\nadd_layer.backward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]),\n    np.array([[0.1, 0.1, 0.1, 0.1], [0.3, 0.3, 0.3, 0.3]]))\n\n\n\n\nGives the output,\n\n\narray([[ 2.,  3.,  4.,  5.],\n       [ 4.,  5.,  6.,  7.]], dtype=float32)\n\narray([[ 0.1       ,  0.1       ,  0.1       ,  0.1       ],\n       [ 0.30000001,  0.30000001,  0.30000001,  0.30000001]], dtype=float32)   \n\n\n\n\n\n\nBiLinear\n\n\nScala:\n\n\nval layer = BiLinear(\n  inputSize1,\n  inputSize2,\n  outputSize,\n  biasRes = true,\n  wRegularizer = null,\n  bRegularizer = null)\n\n\n\n\nPython:\n\n\nlayer = BiLinear(\n    input_size1,\n    input_size2,\n    output_size,\n    bias_res=True,\n    wRegularizer=None,\n    bRegularizer=None)\n\n\n\n\nA bilinear transformation with sparse inputs.\nThe input tensor given in forward(input) is a table containing both inputs x_1 and x_2,\nwhich are tensors of size N x inputDimension1 and N x inputDimension2, respectively.\n\n\nParameters:\n\n\n\n\ninputSize1\n   dimension of input x_1\n\n\ninputSize2\n   dimension of input x_2\n\n\noutputSize\n   output dimension\n\n\nbiasRes\n The layer can be trained without biases by setting bias = false. otherwise true\n\n\nwRegularizer\n instance of \nRegularizer\n\n             (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nbRegularizer\n instance of \nRegularizer\n applied to the bias.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Bilinear\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Bilinear(3, 2, 3)\nval input1 = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\nval input2 = Tensor(T(\n  T(-2f, 3f),\n  T(-1f, 2f),\n  T(-3f, 4f)\n))\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n-0.14168167 -8.697224   -10.097688\n-0.20962894 -7.114827   -8.568602\n0.16706467  -19.751905  -24.516418\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n {\n    2: 13.411718    -18.695072\n       14.674414    -19.503393\n       13.9599  -17.271534\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n    1: -5.3747015   -17.803686  -17.558662\n       -2.413877    -8.373887   -8.346823\n       -2.239298    -11.249412  -14.537216\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }\n\n\n\n\nPython example:\n\n\nlayer = Bilinear(3, 2, 3)\ninput_1 = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ninput_2 = np.array([\n  [-3.0, 4.0],\n  [-2.0, 3.0],\n  [-1.0, 2.0]\n])\n\ninput = [input_1, input_2]\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]\n\nprint grad\n[array([[ 11.86168194, -14.02727222,  -6.16624403],\n       [  6.72984409,  -7.96572971,  -2.89302039],\n       [  5.52902842,  -5.76724434,  -1.46646953]], dtype=float32), array([[ 13.22105694,  -4.6879468 ],\n       [ 14.39296341,  -6.71434498],\n       [ 20.93929482, -13.02455521]], dtype=float32)]\n\n\n\n\n\n\nClamp\n\n\nScala:\n\n\nval model = Clamp(min, max)\n\n\n\n\nPython:\n\n\nmodel = Clamp(min, max)\n\n\n\n\nA kind of hard tanh activition function with integer min and max\n\n \nmin\n min value\n\n \nmax\n max value\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Clamp(-10, 10)\nval input = Tensor(2, 2, 2).rand()\nval output = model.forward(input)\n\nscala\n print(input)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala\n print(output)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]\n\n\n\n\n\nPython example:\n\n\nmodel = Clamp(-10, 10)\ninput = np.random.randn(2, 2, 2)\noutput = model.forward(input)\n\n\n print(input)\n[[[-0.66763755  1.15392566]\n  [-2.10846048  0.46931736]]\n\n [[ 1.74174638 -1.04323311]\n  [-1.91858729  0.12624046]]]\n\n\n print(output)\n[[[-0.66763753  1.15392566]\n  [-2.10846043  0.46931735]]\n\n [[ 1.74174643 -1.04323316]\n  [-1.91858733  0.12624046]]\n\n\n\n\n\n\nSquare\n\n\nScala:\n\n\nval module = Square()\n\n\n\n\nPython:\n\n\nmodule = Square()\n\n\n\n\nSquare apply an element-wise square operation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Square()\n\nprintln(module.forward(Tensor.range(1, 6, 1)))\n\n\n\n\nGives the output,\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n4.0\n9.0\n16.0\n25.0\n36.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 6]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Square()\nprint(module.forward(np.arange(1, 7, 1)))\n\n\n\n\nGives the output,\n\n\n[array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)]\n\n\n\n\n\n\nMean\n\n\nScala:\n\n\nval m = Mean(dimension=1, nInputDims=-1, squeeze=true)\n\n\n\n\nPython:\n\n\nm = Mean(dimension=1,n_input_dims=-1, squeeze=True)\n\n\n\n\nMean is a module that simply applies a mean operation over the given dimension - specified by \ndimension\n (starting from 1).\n\n\nThe input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using \nnInputDims\n.  When input is one tensor, do not specify \nnInputDims\n or set it = -1, otherwise input will be interpreted as batch of tensors. \n\n\nScala example:\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Mean()\nval output1 = m1.forward(input)\nval m2 = Mean(2,1,true)\nval output2 = m2.forward(input)\n\nscala\n print(input)\n(1,.,.) =\n-0.52021635     -1.8250599\n-0.2321481      -2.5672712\n\n(2,.,.) =\n4.007425        -0.8705412\n1.6506456       -0.2470611\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala\n print(output1)\n1.7436042       -1.3478005\n0.7092488       -1.4071661\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala\n print(output2)\n-0.37618223     -2.1961656\n2.8290353       -0.5588012\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(2,2,2)\nprint \ninput is :\n,input\n\nm1 = Mean()\nout = m1.forward(input)\nprint \noutput m1 is :\n,out\n\nm2 = Mean(2,1,True)\nout = m2.forward(input)\nprint \noutput m2 is :\n,out\n\n\n\n\nGives the output,\n\n\ninput is : [[[ 0.01990713  0.37740696]\n  [ 0.67689963  0.67715705]]\n\n [[ 0.45685026  0.58995121]\n  [ 0.33405769  0.86351324]]]\ncreating: createMean\noutput m1 is : [array([[ 0.23837869,  0.48367909],\n       [ 0.50547862,  0.77033514]], dtype=float32)]\ncreating: createMean\noutput m2 is : [array([[ 0.34840336,  0.527282  ],\n       [ 0.39545399,  0.72673225]], dtype=float32)]\n\n\n\n\n\n\nPower\n\n\nScala:\n\n\nval module = Power(power, scale=1, shift=0)\n\n\n\n\nPython:\n\n\nmodule = Power(power, scale=1.0, shift=0.0)\n\n\n\n\nApply an element-wise power operation with scale and shift.\n\n\nf(x) = (shift + scale * x)^power^\n\n\n\n\npower\n the exponent.\n\n\nscale\n Default is 1.\n\n\nshift\n Default is 0.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval power = Power(2, 1, 1)\nval input = Tensor(Storage(Array(0.0, 1, 2, 3, 4, 5)), 1, Array(2, 3))\n\n print(power.forward(input))\n1.0     4.0      9.0    \n16.0        25.0     36.0   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\npower = Power(2.0, 1.0, 1.0)\ninput = np.array([[0.0, 1, 2], [3, 4, 5]])\n\npower.forward(input)\narray([[  1.,   4.,   9.],\n       [ 16.,  25.,  36.]], dtype=float32)\n\n\n\n\n\nCMul\n\n\nScala:\n\n\nval module = CMul(size, wRegularizer = null)\n\n\n\n\nPython:\n\n\nmodule = CMul(size, wRegularizer=None)\n\n\n\n\nThis layer has a weight tensor with given size. The weight will be multiplied element wise to\nthe input tensor. If the element number of the weight tensor match the input tensor, a simply\nelement wise multiply will be done. Or the bias will be expanded to the same size of the input.\nThe expand means repeat on unmatched singleton dimension(if some unmatched dimension isn't\nsingleton dimension, it will report an error). If the input is a batch, a singleton dimension\nwill be add to the first dimension before the expand.\n\n\nsize\n the size of the bias, which is an array of bias shape\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = CMul(Array(2, 1))\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =\n {i += 1; i})\n\n print(layer.forward(input))\n-0.29362988     -0.58725977     -0.88088965\n1.9482219       2.4352775       2.9223328\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = CMul([2,1])\ninput = np.array([[1, 2, 3], [4, 5, 6]])\n\nlayer.forward(input)\narray([[-0.17618844, -0.35237688, -0.52856529],\n       [ 0.85603124,  1.07003903,  1.28404689]], dtype=float32)\n\n\n\n\nAddConstant\n\n\nScala:\n\n\nval module = AddConstant(constant_scalar,inplace= false)\n\n\n\n\nPython:\n\n\nmodule = AddConstant(constant_scalar,inplace=False,bigdl_type=\nfloat\n)\n\n\n\n\nElement wise add a constant scalar to input tensor\n\n \nconstant_scalar\n constant value\n\n \ninplace\n Can optionally do its operation in-place without using extra state memory\n\n\nScala example:\n\n\nval module = AddConstant(3.0)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.40684703      0.077655114     0.42314094\n0.55392265      0.8650696       0.3621729\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres11: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n3.406847        3.077655        3.423141\n3.5539227       3.8650696       3.3621728\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\n\nPython example:\n\n\nmodule = AddConstant(3.0,inplace=False,bigdl_type=\nfloat\n)\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 4.,  5.,  6.],\n[ 7.,  8.,  9.]], dtype=float32)]\n\n\n\n\n\n\nAbs\n\n\nScala:\n\n\nval m = Abs()\n\n\n\n\nPython:\n\n\nm = Abs()\n\n\n\n\nAn element-wise abs operation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval abs = new Abs\nval input = Tensor(2)\ninput(1) = 21f\ninput(2) = -29f\nprint(abs.forward(input))\n\n\n\n\noutput is:\u300021.0\u300029.0\n\n\nPython example:\n\n\nabs = Abs()\ninput = np.array([21, -29, 30])\nprint(abs.forward(input))\n\n\n\n\noutput is: [array([ 21.,  29.,  30.], dtype=float32)]\n\n\n\n\nLog\n\n\nScala:\n\n\nval log = Log()\n\n\n\n\nPython:\n\n\nlog = Log()\n\n\n\n\nThe Log module applies a log transformation to the input data\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval log = Log()\nval input = Tensor(T(1.0f, Math.E.toFloat))\nval gradOutput = Tensor(T(1.0f, 1.0f))\nval output = log.forward(input)\nval gradient = log.backward(input, gradOutput)\n-\n print(output)\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-\n print(gradient)\n1.0\n0.36787945\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nimport math\nlog = Log()\ninput = np.array([1.0, math.e])\ngrad_output = np.array([1.0, 1.0])\noutput = log.forward(input)\ngradient = log.backward(input, grad_output)\n\n-\n print output\n[ 0.  1.]\n\n-\n print gradient\n[ 1.          0.36787945]\n\n\n\n\n\n\nSum\n\n\nScala:\n\n\nval m = Sum(dimension=1,nInputDims=-1,sizeAverage=false,squeeze=true)\n\n\n\n\nPython:\n\n\nm = Sum(dimension=1,n_input_dims=-1,size_average=False,squeeze=True)\n\n\n\n\nSum is a module that simply applies a sum operation over the given dimension - specified by the argument \ndimension\n (starting from 1). \n\n\nThe input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using \nnInputDims\n.  When input is one tensor, do not specify \nnInputDims\n or set it = -1, otherwise input will be interpreted as batch of tensors. \n\n\nScala example:\n\n\n\nscala\n \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Sum(2)\nval output1 = m1.forward(input)\nval m2 = Sum(2, 1, true)\nval output2 = m2.forward(input)\n\nscala\n print(input)\n(1,.,.) =\n-0.003314678    0.96401167\n0.79000163      0.78624517\n\n(2,.,.) =\n-0.29975495     0.24742787\n0.8709072       0.4381108\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala\n print(output1)\n0.78668696      1.7502568\n0.5711522       0.68553865\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala\n print(output2)\n0.39334348      0.8751284\n0.2855761       0.34276932\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,2,2)\nprint \ninput is :\n,input\nmodule = Sum(2)\nout = module.forward(input)\nprint \noutput 1 is :\n,out\nmodule = Sum(2,1,True)\nout = module.forward(input)\nprint \noutput 2 is :\n,out\n\n\n\n\nproduces output:\n\n\ninput is : [[[ 0.7194801   0.99120677]\n  [ 0.07446639  0.056318  ]]\n\n [[ 0.08639016  0.17173268]\n  [ 0.71686986  0.30503663]]]\ncreating: createSum\noutput 1 is : [array([[ 0.7939465 ,  1.04752481],\n       [ 0.80325997,  0.47676933]], dtype=float32)]\ncreating: createSum\noutput 2 is : [array([[ 0.39697325,  0.5237624 ],\n       [ 0.40162998,  0.23838466]], dtype=float32)]\n\n\n\n\n\n\nSqrt\n\n\nApply an element-wise sqrt operation.\n\n\nScala:\n\n\nval sqrt = new Sqrt\n\n\n\n\nPython:\n\n\nsqrt = Sqrt()\n\n\n\n\nApply an element-wise sqrt operation.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Sqrt\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(3, 5).range(1, 15, 1)\nval sqrt = new Sqrt\nval output = sqrt.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(3, 5).range(2, 16, 1)\nval gradInput = sqrt.backward(input, gradOutput)\nprintln(gradOutput\n\n\n\n\nGives the output,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.4142135       1.7320508       2.0     2.236068\n2.4494898       2.6457512       2.828427        3.0     3.1622777\n3.3166249       3.4641016       3.6055512       3.7416575       3.8729835\n\n\n\n\nGives the gradInput\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0606601       1.1547005       1.25    1.3416407\n1.428869        1.5118579       1.5909902       1.6666667       1.7392527\n1.8090681       1.8763883       1.9414507       2.0044594       2.065591\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsqrt = Sqrt()\n\ninput = np.arange(1, 16, 1).astype(\nfloat32\n)\ninput = input.reshape(3, 5)\n\noutput = sqrt.forward(input)\nprint output\n\ngradOutput = np.arange(2, 17, 1).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(3, 5)\n\ngradInput = sqrt.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nGives the output,\n\n\n[array([[ 1.        ,  1.41421354,  1.73205078,  2.        ,  2.23606801],\n       [ 2.44948983,  2.64575124,  2.82842708,  3.        ,  3.1622777 ],\n       [ 3.31662488,  3.46410155,  3.60555124,  3.7416575 ,  3.87298346]], dtype=float32)]\n\n\n\n\nGives the gradInput:\n\n\n[array([[ 1.        ,  1.06066012,  1.15470052,  1.25      ,  1.34164071],\n       [ 1.42886901,  1.51185787,  1.59099019,  1.66666675,  1.73925269],\n       [ 1.80906808,  1.87638831,  1.94145072,  2.00445938,  2.0655911 ]], dtype=float32)]\n\n\n\n\n\n\nExp\n\n\nScala:\n\n\nval exp = Exp()\n\n\n\n\nPython:\n\n\nexp = Exp()\n\n\n\n\nExp applies element-wise exp operation to input tensor\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval exp = Exp()\nval input = Tensor(3, 3).rand()\n\n print(input)\n0.0858663   0.28117087  0.85724664  \n0.62026995  0.29137492  0.07581586  \n0.22099794  0.45131826  0.78286386  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n print(exp.forward(input))\n1.0896606   1.32468     2.356663    \n1.85943     1.3382663   1.078764    \n1.2473209   1.5703809   2.1877286   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nexp = Exp()\n\n exp.forward(np.array([[1, 2, 3],[1, 2, 3]]))\n[array([[  2.71828175,   7.38905621,  20.08553696],\n       [  2.71828175,   7.38905621,  20.08553696]], dtype=float32)]\n\n\n\n\n\n\n\nMax\n\n\nScala:\n\n\nval layer = Max(dim = 1, numInputDims = Int.MinValue)\n\n\n\n\nPython:\n\n\nlayer = Max(dim, num_input_dims=INTMIN)\n\n\n\n\nApplies a max operation over dimension \ndim\n.\n\n\nParameters:\n\n\n\n\n\n\ndim\n max along this dimension\n\n\n\n\n\n\nnumInputDims\n Optional. If in a batch model, set to the inputDims.\n\n\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.Max\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Max(1, 1)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nprintln(grad)\n0.0 0.0 3.0\n0.0 0.0 4.0\n0.0 0.0 5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nlayer = Max(1, 1)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[ 3.  4.  5.]\n\nprint grad\n[[ 0.  0.  3.]\n [ 0.  0.  4.]\n [ 0.  0.  5.]]\n\n\n\n\n\n\nCAdd\n\n\nScala:\n\n\nval module = CAdd(size,bRegularizer=null)\n\n\n\n\nPython:\n\n\nmodule = CAdd(size,bRegularizer=None,bigdl_type=\nfloat\n)\n\n\n\n\nThis layer has a bias tensor with given size. The bias will be added element wise to the input\ntensor. If the element number of the bias tensor match the input tensor, a simply element wise\nwill be done. Or the bias will be expanded to the same size of the input. The expand means\nrepeat on unmatched singleton dimension(if some unmatched dimension isn't singleton dimension,\nit will report an error). If the input is a batch, a singleton dimension will be add to the first\ndimension before the expand.\n\n\n\n\nsize\n the size of the bias \n\n\n\n\nScala example:\n\n\nval module = CAdd(Array(2, 1),bRegularizer=null)\nval input = Tensor(2, 3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.52146345      0.86262375      0.74210143\n0.15882674      0.026310394     0.28394955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres12: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.97027373      1.311434        1.1909117\n-0.047433108    -0.17994945     0.07768971\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nmodule = CAdd([2, 1],bRegularizer=None,bigdl_type=\nfloat\n)\ninput = np.random.rand(2, 3)\narray([[ 0.71239789,  0.65869477,  0.50425182],\n       [ 0.40333312,  0.64843273,  0.07286636]])\n\nmodule.forward(input)\narray([[ 0.89537328,  0.84167016,  0.68722725],\n       [ 0.1290929 ,  0.37419251, -0.20137388]], dtype=float32)\n\n\n\n\n\n\nCosine\n\n\nScala:\n\n\nval m = Cosine(inputSize, outputSize)\n\n\n\n\nPython:\n\n\nm = Cosine(input_size, output_size)\n\n\n\n\nCosine is a module used to  calculate the \ncosine similarity\n of the input to \noutputSize\n centers, i.e. this layer has the weights \nw_j\n, for \nj = 1,..,outputSize\n, where \nw_j\n are vectors of dimension \ninputSize\n.\n\n\nThe distance \ny_j\n between center \nj\n and input \nx\n is formulated as \ny_j = (x \u00b7 w_j) / ( || w_j || * || x || )\n.\n\n\nThe input given in \nforward(input)\n must be either a vector (1D tensor) or matrix (2D tensor). If the input is a\nvector, it must have the size of \ninputSize\n. If it is a matrix, then each row is assumed to be an input sample of given batch (the number of rows means the batch size and the number of columns should be equal to the \ninputSize\n).\n\n\nScala example:\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval m = Cosine(2, 3)\nval input = Tensor(3, 2).rand()\nval output = m.forward(input)\n\nscala\n print(input)\n0.48958543      0.38529378\n0.28814933      0.66979927\n0.3581584       0.67365724\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala\n print(output)\n0.998335        0.9098057       -0.71862763\n0.8496431       0.99756527      -0.2993874\n0.8901594       0.9999207       -0.37689084\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,3)\nprint \ninput is :\n,input\nmodule = Cosine(3,3)\nmodule.forward(input)\nprint \noutput is :\n,out\n\n\n\n\nGives the output,\n\n\ninput is : [[ 0.31156943  0.85577626  0.4274042 ]\n [ 0.79744055  0.66431136  0.05657437]]\ncreating: createCosine\noutput is : [array([[-0.73284394, -0.28076306, -0.51965958],\n       [-0.9563939 , -0.42036989, -0.08060561]], dtype=float32)]\n\n\n\n\n\n\n\n\nMul\n\n\nScala:\n\n\nval module = Mul()\n\n\n\n\nPython:\n\n\nmodule = Mul()\n\n\n\n\nMultiply a singla scalar factor to the incoming data\n\n\n                 +----Mul----+\n input -----+---\n input * weight -----+----\n output\n\n\n\n\nScala example:\n\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mul = Mul()\n\n\n print(mul.forward(Tensor(1, 5).rand()))\n-0.03212923     -0.019040342    -9.136753E-4    -0.014459004    -0.04096878\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmul = Mul()\ninput = np.random.uniform(0, 1, (1, 5)).astype(\nfloat32\n)\n\n\n mul.forward(input)\n[array([[ 0.72429317,  0.7377845 ,  0.09136307,  0.40439236,  0.29011244]], dtype=float32)]\n\n\n\n\n\n\n\nMulConstant\n\n\nScala:\n\n\nval layer = MulConstant(scalar, inplace)\n\n\n\n\nPython:\n\n\nlayer = MulConstant(const, inplace)\n\n\n\n\nMultiplies input Tensor by a (non-learnable) scalar constant.\nThis module is sometimes useful for debugging purposes.\n\n\nParameters:\n\n \nconstant\nscalar constant\n\n \ninplace\n Can optionally do its operation in-place without using extra state memory. Default: false\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n T(1.0f, 1.0f),\n T(1.0f, 1.0f))\n)\nval scalar = 2.0\nval module = MulConstant(scalar)\nval output = module.forward(input)\nval gradient = module.backward(input, gradOutput)\n-\n print(output)\n2.0     4.0     \n6.0     8.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n-\n print(gradient)\n2.0     2.0     \n2.0     2.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ngrad_output = np.array([\n           [1.0, 1.0],\n           [1.0, 1.0]\n         ])\nscalar = 2.0\nmodule = MulConstant(scalar)\noutput = module.forward(input)\ngradient = module.backward(input, grad_output)\n-\n print output\n[[ 2.  4.]\n [ 6.  8.]]\n-\n print gradient\n[[ 2.  2.]\n [ 2.  2.]]", 
            "title": "Math Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#scale", 
            "text": "Scala:  val m = Scale(Array(2, 1))  Python:  m = scale = Scale([2, 1])  Scale is the combination of cmul and cadd.  Scale(size).forward(input) == CAdd(size).forward(CMul(size).forward(input)) \nComputes the elementwise product of input and weight, with the shape of the weight \"expand\" to\nmatch the shape of the input.Similarly, perform a expand cdd bias and perform an elementwise add. output = input .* weight .+ bias (element wise)  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T, Table}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(2, 3).fill(1f)\nprintln( input: )\nprintln(input)\nval scale = Scale(Array(2, 1))\nval weight = Tensor(2, 1).fill(2f)\nval bias = Tensor(2, 1).fill(3f)\nscale.setWeightsBias(Array(weight, bias))\nprintln( Weight: )\nprintln(weight)\nprintln( bias: )\nprintln(bias)\nprintln( output: )\nprint(scale.forward(input))  input:\n1.0 1.0 1.0 \n1.0 1.0 1.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\nWeight:\n2.0 \n2.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\nbias:\n3.0 \n3.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1]\noutput:\n5.0 5.0 5.0 \n5.0 5.0 5.0 \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.layer import *\ninput = np.ones([2, 3])\nprint( input: )\nprint(input)\nscale = Scale([2, 1])\nweight = np.full([2, 1], 2)\nbias = np.full([2, 1], 3)\nprint( weight:  )\nprint(weight)\nprint( bias:  )\nprint(bias)\nscale.set_weights([weight, bias])\nprint( output:  )\nprint(scale.forward(input))  input:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\ncreating: createScale\nweight: \n[[2]\n [2]]\nbias: \n[[3]\n [3]]\noutput: \n[[ 5.  5.  5.]\n [ 5.  5.  5.]]", 
            "title": "Scale"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#min", 
            "text": "Scala:  val min = Min(dim, numInputDims)  Python:  min = Min(dim, num_input_dims)  Applies a min operation over dimension  dim .  Parameters:   dim  A integer. The dimension to min along.   numInputDims  An optional integer indicating the number of input dimensions.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval min = Min(2)\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n 1.0f,\n 1.0f\n))\nval output = min.forward(input)\nval gradient = min.backward(input, gradOutput)\n-  print(output)\n1.0\n3.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-  print(gradient)\n1.0     0.0     \n1.0     0.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nmin = Min(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 4.0]\n])\n\ngrad_output = np.array([1.0, 1.0])\noutput = min.forward(input)\ngradient = min.backward(input, grad_output)\n-  print output\n[ 1.  3.]\n-  print gradient\n[[ 1.  0.]\n [ 1.  0.]]", 
            "title": "Min"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#add", 
            "text": "Scala:  val addLayer = Add(inputSize)  Python:  add_layer = Add(input_size)  A.K.A BiasAdd. This layer adds input tensor with a parameter tensor and output the result.\nIf the input is 1D, this layer just do a element-wise add. If the input has multiple dimentions,\nthis layer will treat the first dimension as batch dimension, resize the input tensor to a 2D \ntensor(batch-dimension x input_size) and do a broadcast add between the 2D tensor and the \nparameter.  Please note that the parameter will be trained in the back propagation.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval addLayer = Add(4)\naddLayer.bias.set(Tensor(T(1.0f, 2.0f, 3.0f, 4.0f)))\naddLayer.forward(Tensor(T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))))\naddLayer.backward(Tensor(T(T(1.0f, 1.0f, 1.0f, 1.0f), T(3.0f, 3.0f, 3.0f, 3.0f))),\n    Tensor(T(T(0.1f, 0.1f, 0.1f, 0.1f), T(0.3f, 0.3f, 0.3f, 0.3f))))  Gives the output,  2.0     3.0     4.0     5.0\n4.0     5.0     6.0     7.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n0.1     0.1     0.1     0.1\n0.3     0.3     0.3     0.3\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]  Python example:  from bigdl.nn.layer import Add\nimport numpy as np\n\nadd_layer = Add(4)\nadd_layer.set_weights([np.array([1.0, 2.0, 3.0, 4.0])])\nadd_layer.forward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]))\nadd_layer.backward(np.array([[1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]]),\n    np.array([[0.1, 0.1, 0.1, 0.1], [0.3, 0.3, 0.3, 0.3]]))  Gives the output,  array([[ 2.,  3.,  4.,  5.],\n       [ 4.,  5.,  6.,  7.]], dtype=float32)\n\narray([[ 0.1       ,  0.1       ,  0.1       ,  0.1       ],\n       [ 0.30000001,  0.30000001,  0.30000001,  0.30000001]], dtype=float32)", 
            "title": "Add"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#bilinear", 
            "text": "Scala:  val layer = BiLinear(\n  inputSize1,\n  inputSize2,\n  outputSize,\n  biasRes = true,\n  wRegularizer = null,\n  bRegularizer = null)  Python:  layer = BiLinear(\n    input_size1,\n    input_size2,\n    output_size,\n    bias_res=True,\n    wRegularizer=None,\n    bRegularizer=None)  A bilinear transformation with sparse inputs.\nThe input tensor given in forward(input) is a table containing both inputs x_1 and x_2,\nwhich are tensors of size N x inputDimension1 and N x inputDimension2, respectively.  Parameters:   inputSize1    dimension of input x_1  inputSize2    dimension of input x_2  outputSize    output dimension  biasRes  The layer can be trained without biases by setting bias = false. otherwise true  wRegularizer  instance of  Regularizer \n             (eg. L1 or L2 regularization), applied to the input weights matrices.  bRegularizer  instance of  Regularizer  applied to the bias.   Scala example:  import com.intel.analytics.bigdl.nn.Bilinear\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Bilinear(3, 2, 3)\nval input1 = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\nval input2 = Tensor(T(\n  T(-2f, 3f),\n  T(-1f, 2f),\n  T(-3f, 4f)\n))\nval input = T(input1, input2)\n\nval gradOutput = Tensor(T(\n  T(3f, 4f, 5f),\n  T(2f, 3f, 4f),\n  T(1f, 2f, 3f)\n))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n-0.14168167 -8.697224   -10.097688\n-0.20962894 -7.114827   -8.568602\n0.16706467  -19.751905  -24.516418\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\nprintln(grad)\n {\n    2: 13.411718    -18.695072\n       14.674414    -19.503393\n       13.9599  -17.271534\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n    1: -5.3747015   -17.803686  -17.558662\n       -2.413877    -8.373887   -8.346823\n       -2.239298    -11.249412  -14.537216\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }  Python example:  layer = Bilinear(3, 2, 3)\ninput_1 = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ninput_2 = np.array([\n  [-3.0, 4.0],\n  [-2.0, 3.0],\n  [-1.0, 2.0]\n])\n\ninput = [input_1, input_2]\n\ngradOutput = np.array([\n  [3.0, 4.0, 5.0],\n  [2.0, 3.0, 4.0],\n  [1.0, 2.0, 5.0]\n])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[[-0.5  1.5  2.5]\n [-1.5  2.5  3.5]\n [-2.5  3.5  4.5]]\n[[ 3.  4.  5.]\n [ 2.  3.  4.]\n [ 1.  2.  5.]]\n\nprint grad\n[array([[ 11.86168194, -14.02727222,  -6.16624403],\n       [  6.72984409,  -7.96572971,  -2.89302039],\n       [  5.52902842,  -5.76724434,  -1.46646953]], dtype=float32), array([[ 13.22105694,  -4.6879468 ],\n       [ 14.39296341,  -6.71434498],\n       [ 20.93929482, -13.02455521]], dtype=float32)]", 
            "title": "BiLinear"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#clamp", 
            "text": "Scala:  val model = Clamp(min, max)  Python:  model = Clamp(min, max)  A kind of hard tanh activition function with integer min and max   min  min value   max  max value  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Clamp(-10, 10)\nval input = Tensor(2, 2, 2).rand()\nval output = model.forward(input)\n\nscala  print(input)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala  print(output)\n(1,.,.) =\n0.95979714  0.27654588  \n0.35592428  0.49355772  \n\n(2,.,.) =\n0.2624511   0.78833413  \n0.967827    0.59160346  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2]  Python example:  model = Clamp(-10, 10)\ninput = np.random.randn(2, 2, 2)\noutput = model.forward(input)  print(input)\n[[[-0.66763755  1.15392566]\n  [-2.10846048  0.46931736]]\n\n [[ 1.74174638 -1.04323311]\n  [-1.91858729  0.12624046]]]  print(output)\n[[[-0.66763753  1.15392566]\n  [-2.10846043  0.46931735]]\n\n [[ 1.74174643 -1.04323316]\n  [-1.91858733  0.12624046]]", 
            "title": "Clamp"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#square", 
            "text": "Scala:  val module = Square()  Python:  module = Square()  Square apply an element-wise square operation.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Square()\n\nprintln(module.forward(Tensor.range(1, 6, 1)))  Gives the output,  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n4.0\n9.0\n16.0\n25.0\n36.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 6]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Square()\nprint(module.forward(np.arange(1, 7, 1)))  Gives the output,  [array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)]", 
            "title": "Square"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#mean", 
            "text": "Scala:  val m = Mean(dimension=1, nInputDims=-1, squeeze=true)  Python:  m = Mean(dimension=1,n_input_dims=-1, squeeze=True)  Mean is a module that simply applies a mean operation over the given dimension - specified by  dimension  (starting from 1).  The input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using  nInputDims .  When input is one tensor, do not specify  nInputDims  or set it = -1, otherwise input will be interpreted as batch of tensors.   Scala example:  scala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Mean()\nval output1 = m1.forward(input)\nval m2 = Mean(2,1,true)\nval output2 = m2.forward(input)\n\nscala  print(input)\n(1,.,.) =\n-0.52021635     -1.8250599\n-0.2321481      -2.5672712\n\n(2,.,.) =\n4.007425        -0.8705412\n1.6506456       -0.2470611\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala  print(output1)\n1.7436042       -1.3478005\n0.7092488       -1.4071661\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala  print(output2)\n-0.37618223     -2.1961656\n2.8290353       -0.5588012\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput = np.random.rand(2,2,2)\nprint  input is : ,input\n\nm1 = Mean()\nout = m1.forward(input)\nprint  output m1 is : ,out\n\nm2 = Mean(2,1,True)\nout = m2.forward(input)\nprint  output m2 is : ,out  Gives the output,  input is : [[[ 0.01990713  0.37740696]\n  [ 0.67689963  0.67715705]]\n\n [[ 0.45685026  0.58995121]\n  [ 0.33405769  0.86351324]]]\ncreating: createMean\noutput m1 is : [array([[ 0.23837869,  0.48367909],\n       [ 0.50547862,  0.77033514]], dtype=float32)]\ncreating: createMean\noutput m2 is : [array([[ 0.34840336,  0.527282  ],\n       [ 0.39545399,  0.72673225]], dtype=float32)]", 
            "title": "Mean"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#power", 
            "text": "Scala:  val module = Power(power, scale=1, shift=0)  Python:  module = Power(power, scale=1.0, shift=0.0)  Apply an element-wise power operation with scale and shift.  f(x) = (shift + scale * x)^power^   power  the exponent.  scale  Default is 1.  shift  Default is 0.   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval power = Power(2, 1, 1)\nval input = Tensor(Storage(Array(0.0, 1, 2, 3, 4, 5)), 1, Array(2, 3))  print(power.forward(input))\n1.0     4.0      9.0    \n16.0        25.0     36.0   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\npower = Power(2.0, 1.0, 1.0)\ninput = np.array([[0.0, 1, 2], [3, 4, 5]]) power.forward(input)\narray([[  1.,   4.,   9.],\n       [ 16.,  25.,  36.]], dtype=float32)", 
            "title": "Power"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#cmul", 
            "text": "Scala:  val module = CMul(size, wRegularizer = null)  Python:  module = CMul(size, wRegularizer=None)  This layer has a weight tensor with given size. The weight will be multiplied element wise to\nthe input tensor. If the element number of the weight tensor match the input tensor, a simply\nelement wise multiply will be done. Or the bias will be expanded to the same size of the input.\nThe expand means repeat on unmatched singleton dimension(if some unmatched dimension isn't\nsingleton dimension, it will report an error). If the input is a batch, a singleton dimension\nwill be add to the first dimension before the expand.  size  the size of the bias, which is an array of bias shape  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = CMul(Array(2, 1))\nval input = Tensor(2, 3)\nvar i = 0\ninput.apply1(_ =  {i += 1; i})  print(layer.forward(input))\n-0.29362988     -0.58725977     -0.88088965\n1.9482219       2.4352775       2.9223328\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  from bigdl.nn.layer import *\n\nlayer = CMul([2,1])\ninput = np.array([[1, 2, 3], [4, 5, 6]]) layer.forward(input)\narray([[-0.17618844, -0.35237688, -0.52856529],\n       [ 0.85603124,  1.07003903,  1.28404689]], dtype=float32)", 
            "title": "CMul"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#addconstant", 
            "text": "Scala:  val module = AddConstant(constant_scalar,inplace= false)  Python:  module = AddConstant(constant_scalar,inplace=False,bigdl_type= float )  Element wise add a constant scalar to input tensor   constant_scalar  constant value   inplace  Can optionally do its operation in-place without using extra state memory  Scala example:  val module = AddConstant(3.0)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.40684703      0.077655114     0.42314094\n0.55392265      0.8650696       0.3621729\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres11: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n3.406847        3.077655        3.423141\n3.5539227       3.8650696       3.3621728\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = AddConstant(3.0,inplace=False,bigdl_type= float )\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 4.,  5.,  6.],\n[ 7.,  8.,  9.]], dtype=float32)]", 
            "title": "AddConstant"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#abs", 
            "text": "Scala:  val m = Abs()  Python:  m = Abs()  An element-wise abs operation.  Scala example:  import com.intel.analytics.bigdl.utils._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval abs = new Abs\nval input = Tensor(2)\ninput(1) = 21f\ninput(2) = -29f\nprint(abs.forward(input))  output is:\u300021.0\u300029.0  Python example:  abs = Abs()\ninput = np.array([21, -29, 30])\nprint(abs.forward(input))  output is: [array([ 21.,  29.,  30.], dtype=float32)]", 
            "title": "Abs"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#log", 
            "text": "Scala:  val log = Log()  Python:  log = Log()  The Log module applies a log transformation to the input data  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval log = Log()\nval input = Tensor(T(1.0f, Math.E.toFloat))\nval gradOutput = Tensor(T(1.0f, 1.0f))\nval output = log.forward(input)\nval gradient = log.backward(input, gradOutput)\n-  print(output)\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n-  print(gradient)\n1.0\n0.36787945\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nimport math\nlog = Log()\ninput = np.array([1.0, math.e])\ngrad_output = np.array([1.0, 1.0])\noutput = log.forward(input)\ngradient = log.backward(input, grad_output)\n\n-  print output\n[ 0.  1.]\n\n-  print gradient\n[ 1.          0.36787945]", 
            "title": "Log"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#sum", 
            "text": "Scala:  val m = Sum(dimension=1,nInputDims=-1,sizeAverage=false,squeeze=true)  Python:  m = Sum(dimension=1,n_input_dims=-1,size_average=False,squeeze=True)  Sum is a module that simply applies a sum operation over the given dimension - specified by the argument  dimension  (starting from 1).   The input is expected to be either one tensor, or a batch of tensors (in mini-batch processing). If the input is a batch of tensors, you need to specify the number of dimensions of each tensor in the batch using  nInputDims .  When input is one tensor, do not specify  nInputDims  or set it = -1, otherwise input will be interpreted as batch of tensors.   Scala example:  \nscala  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval input = Tensor(2, 2, 2).randn()\nval m1 = Sum(2)\nval output1 = m1.forward(input)\nval m2 = Sum(2, 1, true)\nval output2 = m2.forward(input)\n\nscala  print(input)\n(1,.,.) =\n-0.003314678    0.96401167\n0.79000163      0.78624517\n\n(2,.,.) =\n-0.29975495     0.24742787\n0.8709072       0.4381108\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2]\n\nscala  print(output1)\n0.78668696      1.7502568\n0.5711522       0.68553865\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\nscala  print(output2)\n0.39334348      0.8751284\n0.2855761       0.34276932\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,2,2)\nprint  input is : ,input\nmodule = Sum(2)\nout = module.forward(input)\nprint  output 1 is : ,out\nmodule = Sum(2,1,True)\nout = module.forward(input)\nprint  output 2 is : ,out  produces output:  input is : [[[ 0.7194801   0.99120677]\n  [ 0.07446639  0.056318  ]]\n\n [[ 0.08639016  0.17173268]\n  [ 0.71686986  0.30503663]]]\ncreating: createSum\noutput 1 is : [array([[ 0.7939465 ,  1.04752481],\n       [ 0.80325997,  0.47676933]], dtype=float32)]\ncreating: createSum\noutput 2 is : [array([[ 0.39697325,  0.5237624 ],\n       [ 0.40162998,  0.23838466]], dtype=float32)]", 
            "title": "Sum"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#sqrt", 
            "text": "Apply an element-wise sqrt operation.  Scala:  val sqrt = new Sqrt  Python:  sqrt = Sqrt()  Apply an element-wise sqrt operation.  Scala example:  import com.intel.analytics.bigdl.nn.Sqrt\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(3, 5).range(1, 15, 1)\nval sqrt = new Sqrt\nval output = sqrt.forward(input)\nprintln(output)\n\nval gradOutput = Tensor(3, 5).range(2, 16, 1)\nval gradInput = sqrt.backward(input, gradOutput)\nprintln(gradOutput  Gives the output,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.4142135       1.7320508       2.0     2.236068\n2.4494898       2.6457512       2.828427        3.0     3.1622777\n3.3166249       3.4641016       3.6055512       3.7416575       3.8729835  Gives the gradInput  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0606601       1.1547005       1.25    1.3416407\n1.428869        1.5118579       1.5909902       1.6666667       1.7392527\n1.8090681       1.8763883       1.9414507       2.0044594       2.065591  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nsqrt = Sqrt()\n\ninput = np.arange(1, 16, 1).astype( float32 )\ninput = input.reshape(3, 5)\n\noutput = sqrt.forward(input)\nprint output\n\ngradOutput = np.arange(2, 17, 1).astype( float32 )\ngradOutput = gradOutput.reshape(3, 5)\n\ngradInput = sqrt.backward(input, gradOutput)\nprint gradInput  Gives the output,  [array([[ 1.        ,  1.41421354,  1.73205078,  2.        ,  2.23606801],\n       [ 2.44948983,  2.64575124,  2.82842708,  3.        ,  3.1622777 ],\n       [ 3.31662488,  3.46410155,  3.60555124,  3.7416575 ,  3.87298346]], dtype=float32)]  Gives the gradInput:  [array([[ 1.        ,  1.06066012,  1.15470052,  1.25      ,  1.34164071],\n       [ 1.42886901,  1.51185787,  1.59099019,  1.66666675,  1.73925269],\n       [ 1.80906808,  1.87638831,  1.94145072,  2.00445938,  2.0655911 ]], dtype=float32)]", 
            "title": "Sqrt"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#exp", 
            "text": "Scala:  val exp = Exp()  Python:  exp = Exp()  Exp applies element-wise exp operation to input tensor  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval exp = Exp()\nval input = Tensor(3, 3).rand()  print(input)\n0.0858663   0.28117087  0.85724664  \n0.62026995  0.29137492  0.07581586  \n0.22099794  0.45131826  0.78286386  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]  print(exp.forward(input))\n1.0896606   1.32468     2.356663    \n1.85943     1.3382663   1.078764    \n1.2473209   1.5703809   2.1877286   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nexp = Exp()  exp.forward(np.array([[1, 2, 3],[1, 2, 3]]))\n[array([[  2.71828175,   7.38905621,  20.08553696],\n       [  2.71828175,   7.38905621,  20.08553696]], dtype=float32)]", 
            "title": "Exp"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#max", 
            "text": "Scala:  val layer = Max(dim = 1, numInputDims = Int.MinValue)  Python:  layer = Max(dim, num_input_dims=INTMIN)  Applies a max operation over dimension  dim .  Parameters:    dim  max along this dimension    numInputDims  Optional. If in a batch model, set to the inputDims.    Scala example:  import com.intel.analytics.bigdl.nn.Max\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval layer = Max(1, 1)\nval input = Tensor(T(\n  T(-1f, 2f, 3f),\n  T(-2f, 3f, 4f),\n  T(-3f, 4f, 5f)\n))\n\nval gradOutput = Tensor(T(3f, 4f, 5f))\n\nval output = layer.forward(input)\nval grad = layer.backward(input, gradOutput)\n\nprintln(output)\n3.0\n4.0\n5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\nprintln(grad)\n0.0 0.0 3.0\n0.0 0.0 4.0\n0.0 0.0 5.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  layer = Max(1, 1)\ninput = np.array([\n  [-1.0, 2.0, 3.0],\n  [-2.0, 3.0, 4.0],\n  [-3.0, 4.0, 5.0]\n])\n\ngradOutput = np.array([3.0, 4.0, 5.0])\n\noutput = layer.forward(input)\ngrad = layer.backward(input, gradOutput)\n\nprint output\n[ 3.  4.  5.]\n\nprint grad\n[[ 0.  0.  3.]\n [ 0.  0.  4.]\n [ 0.  0.  5.]]", 
            "title": "Max"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#cadd", 
            "text": "Scala:  val module = CAdd(size,bRegularizer=null)  Python:  module = CAdd(size,bRegularizer=None,bigdl_type= float )  This layer has a bias tensor with given size. The bias will be added element wise to the input\ntensor. If the element number of the bias tensor match the input tensor, a simply element wise\nwill be done. Or the bias will be expanded to the same size of the input. The expand means\nrepeat on unmatched singleton dimension(if some unmatched dimension isn't singleton dimension,\nit will report an error). If the input is a batch, a singleton dimension will be add to the first\ndimension before the expand.   size  the size of the bias    Scala example:  val module = CAdd(Array(2, 1),bRegularizer=null)\nval input = Tensor(2, 3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.52146345      0.86262375      0.74210143\n0.15882674      0.026310394     0.28394955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres12: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.97027373      1.311434        1.1909117\n-0.047433108    -0.17994945     0.07768971\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = CAdd([2, 1],bRegularizer=None,bigdl_type= float )\ninput = np.random.rand(2, 3)\narray([[ 0.71239789,  0.65869477,  0.50425182],\n       [ 0.40333312,  0.64843273,  0.07286636]])\n\nmodule.forward(input)\narray([[ 0.89537328,  0.84167016,  0.68722725],\n       [ 0.1290929 ,  0.37419251, -0.20137388]], dtype=float32)", 
            "title": "CAdd"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#cosine", 
            "text": "Scala:  val m = Cosine(inputSize, outputSize)  Python:  m = Cosine(input_size, output_size)  Cosine is a module used to  calculate the  cosine similarity  of the input to  outputSize  centers, i.e. this layer has the weights  w_j , for  j = 1,..,outputSize , where  w_j  are vectors of dimension  inputSize .  The distance  y_j  between center  j  and input  x  is formulated as  y_j = (x \u00b7 w_j) / ( || w_j || * || x || ) .  The input given in  forward(input)  must be either a vector (1D tensor) or matrix (2D tensor). If the input is a\nvector, it must have the size of  inputSize . If it is a matrix, then each row is assumed to be an input sample of given batch (the number of rows means the batch size and the number of columns should be equal to the  inputSize ).  Scala example:  scala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval m = Cosine(2, 3)\nval input = Tensor(3, 2).rand()\nval output = m.forward(input)\n\nscala  print(input)\n0.48958543      0.38529378\n0.28814933      0.66979927\n0.3581584       0.67365724\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala  print(output)\n0.998335        0.9098057       -0.71862763\n0.8496431       0.99756527      -0.2993874\n0.8901594       0.9999207       -0.37689084\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\ninput=np.random.rand(2,3)\nprint  input is : ,input\nmodule = Cosine(3,3)\nmodule.forward(input)\nprint  output is : ,out  Gives the output,  input is : [[ 0.31156943  0.85577626  0.4274042 ]\n [ 0.79744055  0.66431136  0.05657437]]\ncreating: createCosine\noutput is : [array([[-0.73284394, -0.28076306, -0.51965958],\n       [-0.9563939 , -0.42036989, -0.08060561]], dtype=float32)]", 
            "title": "Cosine"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#mul", 
            "text": "Scala:  val module = Mul()  Python:  module = Mul()  Multiply a singla scalar factor to the incoming data                   +----Mul----+\n input -----+---  input * weight -----+----  output  Scala example:  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval mul = Mul()  print(mul.forward(Tensor(1, 5).rand()))\n-0.03212923     -0.019040342    -9.136753E-4    -0.014459004    -0.04096878\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmul = Mul()\ninput = np.random.uniform(0, 1, (1, 5)).astype( float32 )  mul.forward(input)\n[array([[ 0.72429317,  0.7377845 ,  0.09136307,  0.40439236,  0.29011244]], dtype=float32)]", 
            "title": "Mul"
        }, 
        {
            "location": "/APIGuide/Layers/Math-Layers/#mulconstant", 
            "text": "Scala:  val layer = MulConstant(scalar, inplace)  Python:  layer = MulConstant(const, inplace)  Multiplies input Tensor by a (non-learnable) scalar constant.\nThis module is sometimes useful for debugging purposes.  Parameters:   constant scalar constant   inplace  Can optionally do its operation in-place without using extra state memory. Default: false  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval gradOutput = Tensor(T(\n T(1.0f, 1.0f),\n T(1.0f, 1.0f))\n)\nval scalar = 2.0\nval module = MulConstant(scalar)\nval output = module.forward(input)\nval gradient = module.backward(input, gradOutput)\n-  print(output)\n2.0     4.0     \n6.0     8.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n-  print(gradient)\n2.0     2.0     \n2.0     2.0     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ngrad_output = np.array([\n           [1.0, 1.0],\n           [1.0, 1.0]\n         ])\nscalar = 2.0\nmodule = MulConstant(scalar)\noutput = module.forward(input)\ngradient = module.backward(input, grad_output)\n-  print output\n[[ 2.  4.]\n [ 6.  8.]]\n-  print gradient\n[[ 2.  2.]\n [ 2.  2.]]", 
            "title": "MulConstant"
        }, 
        {
            "location": "/APIGuide/Layers/Padding-Layers/", 
            "text": "SpatialZeroPadding\n\n\nScala:\n\n\nval spatialZeroPadding = SpatialZeroPadding(padLeft, padRight, padTop, padBottom)\n\n\n\n\nPython:\n\n\nspatialZeroPadding = SpatialZeroPadding(pad_left, pad_right, pad_top, pad_bottom)\n\n\n\n\nEach feature map of a given input is padded with specified number of zeros.\n\n\nIf padding values are negative, then input will be cropped.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)\nval input = Tensor(3, 3, 3).rand()\n\n print(input)\n(1,.,.) =\n0.9494078   0.31556255  0.8432871   \n0.0064580487    0.6487367   0.151881    \n0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.32691675  0.07487922  0.08813124  \n0.4564806   0.37191486  0.05507739  \n0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.068939745 0.040364727 0.4893642   \n0.39481318  0.17923461  0.15748173  \n0.87117475  0.9933199   0.6097995\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3x3]\n\n\n  print(spatialZeroPadding.forward(input))\n(1,.,.) =\n0.0 0.0064580487    0.6487367   0.151881    \n0.0 0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.0 0.4564806   0.37191486  0.05507739  \n0.0 0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.0 0.39481318  0.17923461  0.15748173  \n0.0 0.87117475  0.9933199   0.6097995   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2x4]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nspatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)\n\n spatialZeroPadding.forward(np.array([[[1, 2],[3, 4]],[[1, 2],[3, 4]]]))\n[array([[[ 0.,  3.,  4.]],\n       [[ 0.,  3.,  4.]]], dtype=float32)]\n\n\n\n\n\nPadding\n\n\nScala:\n\n\nval module = Padding(dim,pad,nInputDim,value=0.0,nIndex=1)\n\n\n\n\nPython:\n\n\nmodule = Padding(dim,pad,n_input_dim,value=0.0,n_index=1,bigdl_type=\nfloat\n)\n\n\n\n\nThis module adds pad units of padding to dimension dim of the input. If pad is negative,\npadding is added to the left, otherwise, it is added to the right of the dimension.\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using nInputDims.\n\n\n\n\n@param dim the dimension to be applied padding operation\n\n\n@param pad num of the pad units\n\n\n@param nInputDim specify the number of dimensions that this module will receive\n                  If it is more than the dimension of input tensors, the first dimension\n                  would be considered as batch size\n\n\n@param value padding value, default is 0\n\n\n\n\nScala example:\n\n\nval module = Padding(1,-1,3,value=0.0,nIndex=1)\nval input = Tensor(3,2,1).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.673425\n0.9350421\n\n(2,.,.) =\n0.35407698\n0.52607465\n\n(3,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2x1]\n\nmodule.forward(input)\nres14: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.0\n0.0\n\n(2,.,.) =\n0.673425\n0.9350421\n\n(3,.,.) =\n0.35407698\n0.52607465\n\n(4,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x1]\n\n\n\n\n\nPython example:\n\n\nmodule = Padding(1, -1, 3, value=0.0,n_index=1,bigdl_type=\nfloat\n)\ninput = np.random.rand(3, 2, 1)\narray([[[ 0.81505274],\n        [ 0.55769512]],\n\n       [[ 0.13193961],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394154]]])\n\nmodule.forward(input)\narray([[[ 0.        ],\n        [ 0.        ]],\n\n       [[ 0.81505275],\n        [ 0.55769515]],\n\n       [[ 0.1319396 ],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394153]]], dtype=float32)", 
            "title": "Padding Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Padding-Layers/#spatialzeropadding", 
            "text": "Scala:  val spatialZeroPadding = SpatialZeroPadding(padLeft, padRight, padTop, padBottom)  Python:  spatialZeroPadding = SpatialZeroPadding(pad_left, pad_right, pad_top, pad_bottom)  Each feature map of a given input is padded with specified number of zeros.  If padding values are negative, then input will be cropped.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)\nval input = Tensor(3, 3, 3).rand()  print(input)\n(1,.,.) =\n0.9494078   0.31556255  0.8432871   \n0.0064580487    0.6487367   0.151881    \n0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.32691675  0.07487922  0.08813124  \n0.4564806   0.37191486  0.05507739  \n0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.068939745 0.040364727 0.4893642   \n0.39481318  0.17923461  0.15748173  \n0.87117475  0.9933199   0.6097995\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3x3]   print(spatialZeroPadding.forward(input))\n(1,.,.) =\n0.0 0.0064580487    0.6487367   0.151881    \n0.0 0.8822722   0.3634125   0.7034494   \n\n(2,.,.) =\n0.0 0.4564806   0.37191486  0.05507739  \n0.0 0.10097649  0.6589037   0.8721945   \n\n(3,.,.) =\n0.0 0.39481318  0.17923461  0.15748173  \n0.0 0.87117475  0.9933199   0.6097995   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2x4]  Python example:  from bigdl.nn.layer import *\nspatialZeroPadding = SpatialZeroPadding(1, 0, -1, 0)  spatialZeroPadding.forward(np.array([[[1, 2],[3, 4]],[[1, 2],[3, 4]]]))\n[array([[[ 0.,  3.,  4.]],\n       [[ 0.,  3.,  4.]]], dtype=float32)]", 
            "title": "SpatialZeroPadding"
        }, 
        {
            "location": "/APIGuide/Layers/Padding-Layers/#padding", 
            "text": "Scala:  val module = Padding(dim,pad,nInputDim,value=0.0,nIndex=1)  Python:  module = Padding(dim,pad,n_input_dim,value=0.0,n_index=1,bigdl_type= float )  This module adds pad units of padding to dimension dim of the input. If pad is negative,\npadding is added to the left, otherwise, it is added to the right of the dimension.\nThe input to this layer is expected to be a tensor, or a batch of tensors;\nwhen using mini-batch, a batch of sample tensors will be passed to the layer and\nthe user need to specify the number of dimensions of each sample tensor in the\nbatch using nInputDims.   @param dim the dimension to be applied padding operation  @param pad num of the pad units  @param nInputDim specify the number of dimensions that this module will receive\n                  If it is more than the dimension of input tensors, the first dimension\n                  would be considered as batch size  @param value padding value, default is 0   Scala example:  val module = Padding(1,-1,3,value=0.0,nIndex=1)\nval input = Tensor(3,2,1).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.673425\n0.9350421\n\n(2,.,.) =\n0.35407698\n0.52607465\n\n(3,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2x1]\n\nmodule.forward(input)\nres14: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.0\n0.0\n\n(2,.,.) =\n0.673425\n0.9350421\n\n(3,.,.) =\n0.35407698\n0.52607465\n\n(4,.,.) =\n0.7226349\n0.70645845\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x2x1]  Python example:  module = Padding(1, -1, 3, value=0.0,n_index=1,bigdl_type= float )\ninput = np.random.rand(3, 2, 1)\narray([[[ 0.81505274],\n        [ 0.55769512]],\n\n       [[ 0.13193961],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394154]]])\n\nmodule.forward(input)\narray([[[ 0.        ],\n        [ 0.        ]],\n\n       [[ 0.81505275],\n        [ 0.55769515]],\n\n       [[ 0.1319396 ],\n        [ 0.32610741]],\n\n       [[ 0.29855582],\n        [ 0.47394153]]], dtype=float32)", 
            "title": "Padding"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/", 
            "text": "BatchNormalization\n\n\nScala:\n\n\nval bn = BatchNormalization(nOutput, eps, momentum, affine)\n\n\n\n\nPython:\n\n\nbn = BatchNormalization(n_output, eps, momentum, affine)\n\n\n\n\nThis layer implements Batch Normalization as described in the paper:\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n\nby Sergey Ioffe, Christian Szegedy\n\n\nThis implementation is useful for inputs NOT coming from convolution layers. For convolution layers, use nn.SpatialBatchNormalization.\n\n\nThe operation implemented is:\n\n\n              ( x - mean(x) )\n      y =  -------------------- * gamma + beta\n              standard-deviation(x)\n\n\n\n\nwhere gamma and beta are learnable parameters.The learning of gamma and beta is optional.\n\n\nParameters:\n\n\n\n\nnOutput\n feature map number\n\n\neps\n avoid divide zero. Default: 1e-5\n\n\nmomentum\n momentum for weight update. Default: 0.1\n\n\naffine\n affine operation on output or not. Default: true\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval bn = BatchNormalization(2)\nval input = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n            )\nval gradOutput = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n)\nval output = bn.forward(input)\nval gradient = bn.backward(input, gradOutput)\n-\n print(output) \n# There's random factor. An output could be\n-0.46433213     -0.2762179      \n0.46433213      0.2762179       \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n-\n print(gradient)\n# There's random factor. An output could be\n-4.649627E-6    -6.585548E-7    \n4.649627E-6     6.585548E-7     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nbn = BatchNormalization(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 6.0]\n])\ngrad_output = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = bn.forward(input)\ngradient = bn.backward(input, grad_output)\n-\n print output\n# There's random factor. An output could be\n[[-0.99583918 -0.13030811]\n [ 0.99583918  0.13030811]]\n-\n print gradient\n# There's random factor. An output could be\n[[ -9.97191637e-06  -1.55339364e-07]\n [  9.97191637e-06   1.55339364e-07]]\n\n\n\n\n\n\nSpatialBatchNormalization\n\n\nScala:\n\n\nval module = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=true,\n                                           initWeight=null, initBias=null, initGradWeight=null, initGradBias=null)\n\n\n\n\nPython:\n\n\nmodule = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=True)\n\n\n\n\n\nThis file implements Batch Normalization as described in the paper:\n\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"\nby Sergey Ioffe, Christian Szegedy.\n\n\nThis implementation is useful for inputs coming from convolution layers.\nFor non-convolutional layers, see \nBatchNormalization\n\nThe operation implemented is:\n\n\n        ( x - mean(x) )\n  y = -------------------- * gamma + beta\n       standard-deviation(x)\n\n  where gamma and beta are learnable parameters.\n  The learning of gamma and beta is optional.\n\n\n\n\n\n\nnOutput\n output feature map number\n\n\neps\n avoid divide zero\n\n\nmomentum\n momentum for weight update\n\n\naffine\n affine operation on output or not\n\n\ninitWeight\n initial weight tensor\n\n\ninitBias\n  initial bias tensor\n\n\ninitGradWeight\n initial gradient weight \n\n\ninitGradBias\n initial gradient bias\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SpatialBatchNormalization(3, 1e-3)\nval input = Tensor(2, 3, 2, 2).randn()\n\n print(layer.forward(input))\n(1,1,.,.) =\n-0.21939678 -0.64394164 \n-0.03280549 0.13889995  \n\n(1,2,.,.) =\n0.48519397  0.40222475  \n-0.9339038  0.4131121   \n\n(1,3,.,.) =\n0.39790314  -0.040012743    \n-0.009540742    0.21598668  \n\n(2,1,.,.) =\n0.32008895  -0.23125978 \n0.4053611   0.26305377  \n\n(2,2,.,.) =\n-0.3810518  -0.34581286 \n0.14797378  0.21226381  \n\n(2,3,.,.) =\n0.2558251   -0.2211882  \n-0.59388477 -0.00508846 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\n\nlayer = SpatialBatchNormalization(3, 1e-3)\ninput = np.random.rand(2,3,2,2)\n\nlayer.forward(input)\narray([[[[  5.70826093e-03,   9.06338100e-05],\n         [ -3.49177676e-03,   1.10401707e-02]],\n\n        [[  1.80168569e-01,  -8.87815133e-02],\n         [  2.11335659e-01,   2.11817324e-01]],\n\n        [[ -1.02916014e+00,   4.02444333e-01],\n         [ -1.72453150e-01,   5.31806648e-01]]],\n\n\n       [[[ -3.46255396e-03,  -1.37512591e-02],\n         [  3.84721952e-03,   1.93112865e-05]],\n\n        [[  4.65962708e-01,  -5.29752195e-01],\n         [ -2.28064612e-01,  -2.22685724e-01]],\n\n        [[  8.49217057e-01,  -9.03094828e-01],\n         [  8.56826544e-01,  -5.35586655e-01]]]], dtype=float32)\n\n\n\n\n\n\nSpatialCrossMapLRN\n\n\nScala:\n\n\nval spatialCrossMapLRN = SpatialCrossMapLRN(size = 5, alpha  = 1.0, beta = 0.75, k = 1.0)\n\n\n\n\nPython:\n\n\nspatialCrossMapLRN = SpatialCrossMapLRN(size=5, alpha=1.0, beta=0.75, k=1.0)\n\n\n\n\nSpatialCrossMapLRN applies Spatial Local Response Normalization between different feature maps\n\n\n                             x_f\n  y_f =  -------------------------------------------------\n          (k+(alpha/size)* sum_{l=l1 to l2} (x_l^2^))^beta^\n\nwhere  l1 corresponds to `max(0,f-ceil(size/2))` and l2 to `min(F, f-ceil(size/2) + size)`, `F` is the number  of feature maps       \n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)\n\nval input = Tensor(2, 2, 2, 2).rand()\n\n\n print(input)\n(1,1,.,.) =\n0.42596373  0.20075735  \n0.10307904  0.7486494   \n\n(1,2,.,.) =\n0.9887414   0.3554662   \n0.6291069   0.53952795  \n\n(2,1,.,.) =\n0.41220918  0.5463298   \n0.40766734  0.08064394  \n\n(2,2,.,.) =\n0.58255607  0.027811589 \n0.47811228  0.3082057   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\n\n print(spatialCrossMapLRN.forward(input))\n(1,1,.,.) =\n0.42522463  0.20070718  \n0.10301625  0.74769455  \n\n(1,2,.,.) =\n0.98702586  0.35537735  \n0.6287237   0.5388398   \n\n(2,1,.,.) =\n0.41189456  0.5460847   \n0.4074261   0.08063166  \n\n(2,2,.,.) =\n0.5821114   0.02779911  \n0.47782937  0.3081588   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nspatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)\n\n spatialCrossMapLRN.forward(np.array([[[[1, 2],[3, 4]],[[5, 6],[7, 8]]],[[[9, 10],[11, 12]],[[13, 14],[15, 16]]]]))\n[array([[[[  0.96269381,   1.88782692],\n         [  2.76295042,   3.57862759]],\n\n        [[  4.81346893,   5.66348076],\n         [  6.44688463,   7.15725517]]],\n\n\n       [[[  6.6400919 ,   7.05574226],\n         [  7.41468   ,   7.72194815]],\n\n        [[  9.59124374,   9.87803936],\n         [ 10.11092758,  10.29593086]]]], dtype=float32)]\n\n\n\n\n\n\n\n\nSpatialWithinChannelLRN\n\n\nScala:\n\n\nval spatialWithinChannelLRN = SpatialWithinChannelLRN(size = 5, alpha  = 1.0, beta = 0.75)\n\n\n\n\nPython:\n\n\nspatialWithinChannelLRN = SpatialWithinChannelLRN(size=5, alpha=1.0, beta=0.75)\n\n\n\n\nSpatialWithinChannelLRN performs a kind of \u201clateral inhibition\u201d\nby normalizing over local input regions. the local regions extend spatially,\nin separate channels (i.e., they have shape 1 x size x size).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialWithinChannelLRN = SpatialWithinChannelLRN(5, 0.01, 0.75)\n\nval input = Tensor(2, 2, 2, 2).rand()\n\n\n print(input)\n(1,1,.,.) =\n0.8658837       0.1297312\n0.7559588       0.039047405\n\n(1,2,.,.) =\n0.79211944      0.84445393\n0.8854509       0.6596644\n\n(2,1,.,.) =\n0.96907943      0.7036902\n0.90358996      0.5719087\n\n(2,2,.,.) =\n0.52309155      0.8838519\n0.44981572      0.40950212\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]\n\n\n print(spatialWithinChannelLRN.forward(input))\n(1,1,.,.) =\n0.8655359       0.12967908      \n0.75565517      0.03903172      \n\n(1,2,.,.) =\n0.7915117       0.843806        \n0.8847715       0.6591583       \n\n(2,1,.,.) =\n0.9683307       0.70314646      \n0.9028918       0.5714668       \n\n(2,2,.,.) =\n0.52286804      0.8834743       \n0.44962353      0.40932715      \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nspatialWithinChannelLRN = SpatialWithinChannelLRN(5, 0.01, 0.75)\n\n spatialWithinChannelLRN.forward(np.array([[[[1, 2],[3, 4]],[[5, 6],[7, 8]]],[[[9, 10],[11, 12]],[[13, 14],[15, 16]]]]))\narray([[[[  0.99109352,   1.98218703],\n         [  2.97328043,   3.96437407]],\n\n        [[  4.75394297,   5.70473146],\n         [  6.65551996,   7.60630846]]],\n\n\n       [[[  7.95743227,   8.84159184],\n         [  9.72575092,  10.60991001]],\n\n        [[ 10.44729614,  11.2509346 ],\n         [ 12.05457211,  12.85821056]]]], dtype=float32)\n\n\n\n\n\n\n\n\nNormalize\n\n\nScala:\n\n\nval module = Normalize(p,eps=1e-10)\n\n\n\n\nPython:\n\n\nmodule = Normalize(p,eps=1e-10,bigdl_type=\nfloat\n)\n\n\n\n\nNormalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents\ndivision by zero when the input contains all zero elements (default = 1e-10).\nThe input can be 1d, 2d or 4d. If the input is 4d, it should follow the format (n, c, h, w) where n is the batch number,\nc is the channel number, h is the height and w is the width\n\n\n\n\np\n L_p norm\n\n\neps\n smoothing parameter\n\n\n\n\nScala example:\n\n\nval module = Normalize(2.0,eps=1e-10)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7075603       0.084298864     0.91339105\n0.22373432      0.8704987       0.6936567\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres8: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.6107763       0.072768        0.7884524\n0.19706465      0.76673317      0.61097115\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nmodule = Normalize(2.0,eps=1e-10,bigdl_type=\nfloat\n)\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 0.26726124,  0.53452247,  0.80178368],\n[ 0.45584232,  0.56980288,  0.68376344]], dtype=float32)]\n\n\n\n\nSpatialDivisiveNormalization\n\n\nScala:\n\n\nval layer = SpatialDivisiveNormalization()\n\n\n\n\nPython:\n\n\nlayer = SpatialDivisiveNormalization()\n\n\n\n\nApplies a spatial division operation on a series of 2D inputs using kernel for\ncomputing the weighted average in a neighborhood. The neighborhood is defined for\na local spatial region that is the size as kernel and across all features. For\nan input image, since there is only one feature, the region is only spatial. For\nan RGB image, the weighted average is taken over RGB channels and a spatial region.\n\n\nIf the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.\n\n\nThe kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.\n\n\nScala example:\n\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval layer = SpatialDivisiveNormalization()\nval input = Tensor(1, 5, 5).rand\nval gradOutput = Tensor(1, 5, 5).rand\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)\n\n\n println(input)\nres19: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.4022106       0.6872489       0.9712838       0.7769542       0.771034\n0.97930336      0.61022973      0.65092266      0.9507807       0.3158211\n0.12607759      0.320569        0.9267993       0.47579524      0.63989824\n0.713135        0.30836385      0.009723447     0.67723924      0.24405171\n0.51036286      0.115807846     0.123513035     0.28398398      0.271164\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n println(output)\nres20: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.37849638      0.6467289       0.91401714      0.73114514      0.725574\n0.9215639       0.57425076      0.6125444       0.89472294      0.29720038\n0.11864409      0.30166835      0.8721555       0.4477425       0.60217\n0.67108876      0.2901828       0.009150156     0.6373094       0.2296625\n0.480272        0.10897984      0.11623074      0.26724035      0.25517625\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n println(gradInput)\nres21: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.09343022     -0.25612304     0.25756648      -0.66132677     -0.44575396\n0.052990615     0.7899354       0.27205157      0.028260134     0.23150417\n-0.115425855    0.21133065      0.53093016      -0.36421964     -0.102551565\n0.7222408       0.46287358      0.0010696054    0.26336592      -0.050598443\n0.03733714      0.2775169       -0.21430963     0.3175013       0.6600435\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nlayer = SpatialDivisiveNormalization()\ninput = np.random.uniform(0, 1, (1, 5, 5)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (1, 5, 5)).astype(\nfloat32\n)\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)\n\n\n output\n[array([[[ 0.30657911,  0.75221181,  0.2318386 ,  0.84053135,  0.24818985],\n         [ 0.32852787,  0.43504578,  0.0219258 ,  0.47856906,  0.31112722],\n         [ 0.12381417,  0.61807972,  0.90043157,  0.57342309,  0.65450585],\n         [ 0.00401461,  0.33700454,  0.79859954,  0.64382601,  0.51768768],\n         [ 0.38087726,  0.8963666 ,  0.7982524 ,  0.78525543,  0.09658573]]], dtype=float32)]\n\n gradInput\n[array([[[ 0.08059166, -0.4616771 ,  0.11626807,  0.30253756,  0.7333734 ],\n         [ 0.2633073 , -0.01641282,  0.40653706,  0.07766753, -0.0237394 ],\n         [ 0.10733987,  0.23385212, -0.3291783 , -0.12808481,  0.4035565 ],\n         [ 0.56126803,  0.49945205, -0.40531909, -0.18559581,  0.27156472],\n         [ 0.28016835,  0.03791744, -0.17803842, -0.27817759,  0.42473239]]], dtype=float32)]\n\n\n\n\n\n\nSpatialSubtractiveNormalization\n\n\nScala:\n\n\nval spatialSubtractiveNormalization = SpatialSubtractiveNormalization(nInputPlane = 1, kernel = null)\n\n\n\n\nPython:\n\n\nspatialSubtractiveNormalization = SpatialSubtractiveNormalization(n_input_plane=1, kernel=None)\n\n\n\n\nSpatialSubtractiveNormalization applies a spatial subtraction operation on a series of 2D inputs using kernel for computing the weighted average in a neighborhood.The neighborhood is defined for a local spatial region that is the size as kernel and across all features. For an input image, since there is only one feature, the region is only spatial. For an RGB image, the weighted average is taken over RGB channels and a spatial region.\n\n\nIf the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.\n\n\nThe kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.\n\n\n\n\nnInputPlane\n  number of input plane, default is 1.\n\n\nkernel\n kernel tensor, default is a 9 x 9 tensor.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval kernel = Tensor(3, 3).rand()\n\n\n print(kernel)\n0.56141114  0.76815456  0.29409808  \n0.3599753   0.17142025  0.5243272   \n0.62450963  0.28084084  0.17154165  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\nval spatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)\n\nval input = Tensor(1, 1, 1, 5).rand()\n\n\n print(input)\n(1,1,.,.) =\n0.122356184 0.44442436  0.6394927   0.9349956   0.8226007   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x1x5]\n\n\n print(spatialSubtractiveNormalization.forward(input))\n(1,1,.,.) =\n-0.2427161  0.012936085 -0.08024883 0.15658027  -0.07613802 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1x5]\n\n\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nkernel=np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\nspatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)\n\n  spatialSubtractiveNormalization.forward(np.array([[[[1, 2, 3, 4, 5]]]]))\n[array([[[[ 0.,  0.,  0.,  0.,  0.]]]], dtype=float32)]", 
            "title": "Normalization Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#batchnormalization", 
            "text": "Scala:  val bn = BatchNormalization(nOutput, eps, momentum, affine)  Python:  bn = BatchNormalization(n_output, eps, momentum, affine)  This layer implements Batch Normalization as described in the paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift \nby Sergey Ioffe, Christian Szegedy  This implementation is useful for inputs NOT coming from convolution layers. For convolution layers, use nn.SpatialBatchNormalization.  The operation implemented is:                ( x - mean(x) )\n      y =  -------------------- * gamma + beta\n              standard-deviation(x)  where gamma and beta are learnable parameters.The learning of gamma and beta is optional.  Parameters:   nOutput  feature map number  eps  avoid divide zero. Default: 1e-5  momentum  momentum for weight update. Default: 0.1  affine  affine operation on output or not. Default: true   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval bn = BatchNormalization(2)\nval input = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n            )\nval gradOutput = Tensor(T(\n             T(1.0f, 2.0f),\n             T(3.0f, 6.0f))\n)\nval output = bn.forward(input)\nval gradient = bn.backward(input, gradOutput)\n-  print(output) \n# There's random factor. An output could be\n-0.46433213     -0.2762179      \n0.46433213      0.2762179       \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n-  print(gradient)\n# There's random factor. An output could be\n-4.649627E-6    -6.585548E-7    \n4.649627E-6     6.585548E-7     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nbn = BatchNormalization(2)\ninput = np.array([\n  [1.0, 2.0],\n  [3.0, 6.0]\n])\ngrad_output = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = bn.forward(input)\ngradient = bn.backward(input, grad_output)\n-  print output\n# There's random factor. An output could be\n[[-0.99583918 -0.13030811]\n [ 0.99583918  0.13030811]]\n-  print gradient\n# There's random factor. An output could be\n[[ -9.97191637e-06  -1.55339364e-07]\n [  9.97191637e-06   1.55339364e-07]]", 
            "title": "BatchNormalization"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#spatialbatchnormalization", 
            "text": "Scala:  val module = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=true,\n                                           initWeight=null, initBias=null, initGradWeight=null, initGradBias=null)  Python:  module = SpatialBatchNormalization(nOutput, eps=1e-5, momentum=0.1, affine=True)  This file implements Batch Normalization as described in the paper:\n\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"\nby Sergey Ioffe, Christian Szegedy.  This implementation is useful for inputs coming from convolution layers.\nFor non-convolutional layers, see  BatchNormalization \nThe operation implemented is:          ( x - mean(x) )\n  y = -------------------- * gamma + beta\n       standard-deviation(x)\n\n  where gamma and beta are learnable parameters.\n  The learning of gamma and beta is optional.   nOutput  output feature map number  eps  avoid divide zero  momentum  momentum for weight update  affine  affine operation on output or not  initWeight  initial weight tensor  initBias   initial bias tensor  initGradWeight  initial gradient weight   initGradBias  initial gradient bias   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = SpatialBatchNormalization(3, 1e-3)\nval input = Tensor(2, 3, 2, 2).randn()  print(layer.forward(input))\n(1,1,.,.) =\n-0.21939678 -0.64394164 \n-0.03280549 0.13889995  \n\n(1,2,.,.) =\n0.48519397  0.40222475  \n-0.9339038  0.4131121   \n\n(1,3,.,.) =\n0.39790314  -0.040012743    \n-0.009540742    0.21598668  \n\n(2,1,.,.) =\n0.32008895  -0.23125978 \n0.4053611   0.26305377  \n\n(2,2,.,.) =\n-0.3810518  -0.34581286 \n0.14797378  0.21226381  \n\n(2,3,.,.) =\n0.2558251   -0.2211882  \n-0.59388477 -0.00508846 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x2x2]  Python example:  from bigdl.nn.layer import *\n\nlayer = SpatialBatchNormalization(3, 1e-3)\ninput = np.random.rand(2,3,2,2) layer.forward(input)\narray([[[[  5.70826093e-03,   9.06338100e-05],\n         [ -3.49177676e-03,   1.10401707e-02]],\n\n        [[  1.80168569e-01,  -8.87815133e-02],\n         [  2.11335659e-01,   2.11817324e-01]],\n\n        [[ -1.02916014e+00,   4.02444333e-01],\n         [ -1.72453150e-01,   5.31806648e-01]]],\n\n\n       [[[ -3.46255396e-03,  -1.37512591e-02],\n         [  3.84721952e-03,   1.93112865e-05]],\n\n        [[  4.65962708e-01,  -5.29752195e-01],\n         [ -2.28064612e-01,  -2.22685724e-01]],\n\n        [[  8.49217057e-01,  -9.03094828e-01],\n         [  8.56826544e-01,  -5.35586655e-01]]]], dtype=float32)", 
            "title": "SpatialBatchNormalization"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#spatialcrossmaplrn", 
            "text": "Scala:  val spatialCrossMapLRN = SpatialCrossMapLRN(size = 5, alpha  = 1.0, beta = 0.75, k = 1.0)  Python:  spatialCrossMapLRN = SpatialCrossMapLRN(size=5, alpha=1.0, beta=0.75, k=1.0)  SpatialCrossMapLRN applies Spatial Local Response Normalization between different feature maps                               x_f\n  y_f =  -------------------------------------------------\n          (k+(alpha/size)* sum_{l=l1 to l2} (x_l^2^))^beta^\n\nwhere  l1 corresponds to `max(0,f-ceil(size/2))` and l2 to `min(F, f-ceil(size/2) + size)`, `F` is the number  of feature maps         Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)\n\nval input = Tensor(2, 2, 2, 2).rand()  print(input)\n(1,1,.,.) =\n0.42596373  0.20075735  \n0.10307904  0.7486494   \n\n(1,2,.,.) =\n0.9887414   0.3554662   \n0.6291069   0.53952795  \n\n(2,1,.,.) =\n0.41220918  0.5463298   \n0.40766734  0.08064394  \n\n(2,2,.,.) =\n0.58255607  0.027811589 \n0.47811228  0.3082057   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]  print(spatialCrossMapLRN.forward(input))\n(1,1,.,.) =\n0.42522463  0.20070718  \n0.10301625  0.74769455  \n\n(1,2,.,.) =\n0.98702586  0.35537735  \n0.6287237   0.5388398   \n\n(2,1,.,.) =\n0.41189456  0.5460847   \n0.4074261   0.08063166  \n\n(2,2,.,.) =\n0.5821114   0.02779911  \n0.47782937  0.3081588   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2]  Python example:  from bigdl.nn.layer import *\nspatialCrossMapLRN = SpatialCrossMapLRN(5, 0.01, 0.75, 1.0)  spatialCrossMapLRN.forward(np.array([[[[1, 2],[3, 4]],[[5, 6],[7, 8]]],[[[9, 10],[11, 12]],[[13, 14],[15, 16]]]]))\n[array([[[[  0.96269381,   1.88782692],\n         [  2.76295042,   3.57862759]],\n\n        [[  4.81346893,   5.66348076],\n         [  6.44688463,   7.15725517]]],\n\n\n       [[[  6.6400919 ,   7.05574226],\n         [  7.41468   ,   7.72194815]],\n\n        [[  9.59124374,   9.87803936],\n         [ 10.11092758,  10.29593086]]]], dtype=float32)]", 
            "title": "SpatialCrossMapLRN"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#spatialwithinchannellrn", 
            "text": "Scala:  val spatialWithinChannelLRN = SpatialWithinChannelLRN(size = 5, alpha  = 1.0, beta = 0.75)  Python:  spatialWithinChannelLRN = SpatialWithinChannelLRN(size=5, alpha=1.0, beta=0.75)  SpatialWithinChannelLRN performs a kind of \u201clateral inhibition\u201d\nby normalizing over local input regions. the local regions extend spatially,\nin separate channels (i.e., they have shape 1 x size x size).  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval spatialWithinChannelLRN = SpatialWithinChannelLRN(5, 0.01, 0.75)\n\nval input = Tensor(2, 2, 2, 2).rand()  print(input)\n(1,1,.,.) =\n0.8658837       0.1297312\n0.7559588       0.039047405\n\n(1,2,.,.) =\n0.79211944      0.84445393\n0.8854509       0.6596644\n\n(2,1,.,.) =\n0.96907943      0.7036902\n0.90358996      0.5719087\n\n(2,2,.,.) =\n0.52309155      0.8838519\n0.44981572      0.40950212\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2x2x2]  print(spatialWithinChannelLRN.forward(input))\n(1,1,.,.) =\n0.8655359       0.12967908      \n0.75565517      0.03903172      \n\n(1,2,.,.) =\n0.7915117       0.843806        \n0.8847715       0.6591583       \n\n(2,1,.,.) =\n0.9683307       0.70314646      \n0.9028918       0.5714668       \n\n(2,2,.,.) =\n0.52286804      0.8834743       \n0.44962353      0.40932715      \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2]  Python example:  from bigdl.nn.layer import *\nspatialWithinChannelLRN = SpatialWithinChannelLRN(5, 0.01, 0.75)  spatialWithinChannelLRN.forward(np.array([[[[1, 2],[3, 4]],[[5, 6],[7, 8]]],[[[9, 10],[11, 12]],[[13, 14],[15, 16]]]]))\narray([[[[  0.99109352,   1.98218703],\n         [  2.97328043,   3.96437407]],\n\n        [[  4.75394297,   5.70473146],\n         [  6.65551996,   7.60630846]]],\n\n\n       [[[  7.95743227,   8.84159184],\n         [  9.72575092,  10.60991001]],\n\n        [[ 10.44729614,  11.2509346 ],\n         [ 12.05457211,  12.85821056]]]], dtype=float32)", 
            "title": "SpatialWithinChannelLRN"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#normalize", 
            "text": "Scala:  val module = Normalize(p,eps=1e-10)  Python:  module = Normalize(p,eps=1e-10,bigdl_type= float )  Normalizes the input Tensor to have unit L_p norm. The smoothing parameter eps prevents\ndivision by zero when the input contains all zero elements (default = 1e-10).\nThe input can be 1d, 2d or 4d. If the input is 4d, it should follow the format (n, c, h, w) where n is the batch number,\nc is the channel number, h is the height and w is the width   p  L_p norm  eps  smoothing parameter   Scala example:  val module = Normalize(2.0,eps=1e-10)\nval input = Tensor(2,3).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.7075603       0.084298864     0.91339105\n0.22373432      0.8704987       0.6936567\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x3]\n\nmodule.forward(input)\nres8: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.6107763       0.072768        0.7884524\n0.19706465      0.76673317      0.61097115\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  module = Normalize(2.0,eps=1e-10,bigdl_type= float )\ninput = np.array([[1, 2, 3],[4, 5, 6]])\nmodule.forward(input)\n[array([\n[ 0.26726124,  0.53452247,  0.80178368],\n[ 0.45584232,  0.56980288,  0.68376344]], dtype=float32)]", 
            "title": "Normalize"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#spatialdivisivenormalization", 
            "text": "Scala:  val layer = SpatialDivisiveNormalization()  Python:  layer = SpatialDivisiveNormalization()  Applies a spatial division operation on a series of 2D inputs using kernel for\ncomputing the weighted average in a neighborhood. The neighborhood is defined for\na local spatial region that is the size as kernel and across all features. For\nan input image, since there is only one feature, the region is only spatial. For\nan RGB image, the weighted average is taken over RGB channels and a spatial region.  If the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.  The kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.  Scala example:  \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval layer = SpatialDivisiveNormalization()\nval input = Tensor(1, 5, 5).rand\nval gradOutput = Tensor(1, 5, 5).rand\n\nval output = layer.forward(input)\nval gradInput = layer.backward(input, gradOutput)  println(input)\nres19: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.4022106       0.6872489       0.9712838       0.7769542       0.771034\n0.97930336      0.61022973      0.65092266      0.9507807       0.3158211\n0.12607759      0.320569        0.9267993       0.47579524      0.63989824\n0.713135        0.30836385      0.009723447     0.67723924      0.24405171\n0.51036286      0.115807846     0.123513035     0.28398398      0.271164\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  println(output)\nres20: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.37849638      0.6467289       0.91401714      0.73114514      0.725574\n0.9215639       0.57425076      0.6125444       0.89472294      0.29720038\n0.11864409      0.30166835      0.8721555       0.4477425       0.60217\n0.67108876      0.2901828       0.009150156     0.6373094       0.2296625\n0.480272        0.10897984      0.11623074      0.26724035      0.25517625\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  println(gradInput)\nres21: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.09343022     -0.25612304     0.25756648      -0.66132677     -0.44575396\n0.052990615     0.7899354       0.27205157      0.028260134     0.23150417\n-0.115425855    0.21133065      0.53093016      -0.36421964     -0.102551565\n0.7222408       0.46287358      0.0010696054    0.26336592      -0.050598443\n0.03733714      0.2775169       -0.21430963     0.3175013       0.6600435\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nlayer = SpatialDivisiveNormalization()\ninput = np.random.uniform(0, 1, (1, 5, 5)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (1, 5, 5)).astype( float32 )\n\noutput = layer.forward(input)\ngradInput = layer.backward(input, gradOutput)  output\n[array([[[ 0.30657911,  0.75221181,  0.2318386 ,  0.84053135,  0.24818985],\n         [ 0.32852787,  0.43504578,  0.0219258 ,  0.47856906,  0.31112722],\n         [ 0.12381417,  0.61807972,  0.90043157,  0.57342309,  0.65450585],\n         [ 0.00401461,  0.33700454,  0.79859954,  0.64382601,  0.51768768],\n         [ 0.38087726,  0.8963666 ,  0.7982524 ,  0.78525543,  0.09658573]]], dtype=float32)]  gradInput\n[array([[[ 0.08059166, -0.4616771 ,  0.11626807,  0.30253756,  0.7333734 ],\n         [ 0.2633073 , -0.01641282,  0.40653706,  0.07766753, -0.0237394 ],\n         [ 0.10733987,  0.23385212, -0.3291783 , -0.12808481,  0.4035565 ],\n         [ 0.56126803,  0.49945205, -0.40531909, -0.18559581,  0.27156472],\n         [ 0.28016835,  0.03791744, -0.17803842, -0.27817759,  0.42473239]]], dtype=float32)]", 
            "title": "SpatialDivisiveNormalization"
        }, 
        {
            "location": "/APIGuide/Layers/Normalization-Layers/#spatialsubtractivenormalization", 
            "text": "Scala:  val spatialSubtractiveNormalization = SpatialSubtractiveNormalization(nInputPlane = 1, kernel = null)  Python:  spatialSubtractiveNormalization = SpatialSubtractiveNormalization(n_input_plane=1, kernel=None)  SpatialSubtractiveNormalization applies a spatial subtraction operation on a series of 2D inputs using kernel for computing the weighted average in a neighborhood.The neighborhood is defined for a local spatial region that is the size as kernel and across all features. For an input image, since there is only one feature, the region is only spatial. For an RGB image, the weighted average is taken over RGB channels and a spatial region.  If the kernel is 1D, then it will be used for constructing and separable 2D kernel.\nThe operations will be much more efficient in this case.  The kernel is generally chosen as a gaussian when it is believed that the correlation\nof two pixel locations decrease with increasing distance. On the feature dimension,\na uniform average is used since the weighting across features is not known.   nInputPlane   number of input plane, default is 1.  kernel  kernel tensor, default is a 9 x 9 tensor.   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval kernel = Tensor(3, 3).rand()  print(kernel)\n0.56141114  0.76815456  0.29409808  \n0.3599753   0.17142025  0.5243272   \n0.62450963  0.28084084  0.17154165  \n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x3]\n\n\nval spatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)\n\nval input = Tensor(1, 1, 1, 5).rand()  print(input)\n(1,1,.,.) =\n0.122356184 0.44442436  0.6394927   0.9349956   0.8226007   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x1x1x5]  print(spatialSubtractiveNormalization.forward(input))\n(1,1,.,.) =\n-0.2427161  0.012936085 -0.08024883 0.15658027  -0.07613802 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1x5]   Python example:  from bigdl.nn.layer import *\nkernel=np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\nspatialSubtractiveNormalization = SpatialSubtractiveNormalization(1, kernel)   spatialSubtractiveNormalization.forward(np.array([[[[1, 2, 3, 4, 5]]]]))\n[array([[[[ 0.,  0.,  0.,  0.,  0.]]]], dtype=float32)]", 
            "title": "SpatialSubtractiveNormalization"
        }, 
        {
            "location": "/APIGuide/Layers/Dropout-Layers/", 
            "text": "Dropout\n\n\nScala:\n\n\nval module = Dropout(\n  initP = 0.5,\n  inplace = false,\n  scale = true)\n\n\n\n\nPython:\n\n\nmodule = Dropout(\n  init_p=0.5,\n  inplace=False,\n  scale=True)\n\n\n\n\nDropout masks(set to zero) parts of input using a bernoulli distribution.\nEach input element has a probability \ninitP\n of being dropped. If \nscale\n is\ntrue(true by default), the outputs are scaled by a factor of \n1/(1-initP)\n during training.\nDuring evaluating, output is the same as input.\n\n\nIt has been proven an effective approach for regularization and preventing\nco-adaptation of feature detectors. For more details, plese see\n[Improving neural networks by preventing co-adaptation of feature detectors]\n(https://arxiv.org/abs/1207.0580)\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Dropout()\nval x = Tensor.range(1, 8, 1).resize(2, 4)\n\nprintln(module.forward(x))\nprintln(module.backward(x, x.clone().mul(0.5f))) // backward drops out the gradients at the same location.\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0     4.0     6.0     0.0\n10.0    12.0    0.0     16.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0    2.0    3.0    0.0\n5.0    6.0    0.0    8.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Dropout()\nx = np.arange(1, 9, 1).reshape(2, 4)\n\nprint(module.forward(x))\nprint(module.backward(x, x.copy() * 0.5)) # backward drops out the gradients at the same location.\n\n\n\n\nOutput is\n\n\n[array([[ 0.,  4.,  6.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]\n\n[array([[ 0.,  2.,  3.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]", 
            "title": "Dropout Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Dropout-Layers/#dropout", 
            "text": "Scala:  val module = Dropout(\n  initP = 0.5,\n  inplace = false,\n  scale = true)  Python:  module = Dropout(\n  init_p=0.5,\n  inplace=False,\n  scale=True)  Dropout masks(set to zero) parts of input using a bernoulli distribution.\nEach input element has a probability  initP  of being dropped. If  scale  is\ntrue(true by default), the outputs are scaled by a factor of  1/(1-initP)  during training.\nDuring evaluating, output is the same as input.  It has been proven an effective approach for regularization and preventing\nco-adaptation of feature detectors. For more details, plese see\n[Improving neural networks by preventing co-adaptation of feature detectors]\n(https://arxiv.org/abs/1207.0580)  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Dropout()\nval x = Tensor.range(1, 8, 1).resize(2, 4)\n\nprintln(module.forward(x))\nprintln(module.backward(x, x.clone().mul(0.5f))) // backward drops out the gradients at the same location.  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0     4.0     6.0     0.0\n10.0    12.0    0.0     16.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0    2.0    3.0    0.0\n5.0    6.0    0.0    8.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Dropout()\nx = np.arange(1, 9, 1).reshape(2, 4)\n\nprint(module.forward(x))\nprint(module.backward(x, x.copy() * 0.5)) # backward drops out the gradients at the same location.  Output is  [array([[ 0.,  4.,  6.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]\n\n[array([[ 0.,  2.,  3.,  0.],\n       [ 0.,  0.,  0.,  0.]], dtype=float32)]", 
            "title": "Dropout"
        }, 
        {
            "location": "/APIGuide/Layers/Distance-Layers/", 
            "text": "PairwiseDistance\n\n\nScala:\n\n\nval pd = PairwiseDistance(norm=2)\n\n\n\n\nPython:\n\n\npd = PairwiseDistance(norm=2)\n\n\n\n\nIt is a module that takes a table of two vectors as input and outputs\nthe distance between them using the p-norm.\nThe input given in \nforward(input)\n is a [[Table]] that contains two tensors which\nmust be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,\nit must have the size of \ninputSize\n. If it is a matrix, then each row is assumed to be\nan input sample of the given batch (the number of rows means the batch size and\nthe number of columns should be equal to the \ninputSize\n).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.PairwiseDistance\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval pd = PairwiseDistance()\nval input1 = Tensor(3, 3).randn()\nval input2 = Tensor(3, 3).randn()\nval input = T(1 -\n input1, 2 -\n input2)\n\nval output = pd.forward(input)\n\nval gradOutput = Tensor(3).randn()\nval gradInput = pd.backward(input, gradOutput)\n\n\n\n\n\nThe ouotput is,\n\n\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.155246\n1.1267666\n2.1415536\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.32565984\n-1.0108998\n-0.030873261\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\n\n\n\nThe gradInput is,\n\n\ngradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.012723052  0.31482473      0.08232752\n           0.7552968    -0.27292773     -0.6139655\n           0.0062761847 -0.018232936    -0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: -0.012723052 -0.31482473     -0.08232752\n           -0.7552968   0.27292773      0.6139655\n           -0.0062761847        0.018232936     0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npd = PairwiseDistance()\n\ninput1 = np.random.uniform(0, 1, [3, 3]).astype(\nfloat32\n)\ninput2 = np.random.uniform(0, 1, [3, 3]).astype(\nfloat32\n)\ninput1 = input1.reshape(3, 3)\ninput2 = input2.reshape(3, 3)\n\ninput = [input1, input2]\n\noutput = pd.forward(input)\nprint output\n\ngradOutput = np.random.uniform(0, 1, [3]).astype(\nfloat32\n)\ngradOutput = gradOutput.reshape(3)\n\ngradInput = pd.backward(input, gradOutput)\nprint gradInput\n\n\n\n\nThe output is,\n\n\n[ 0.99588805  0.65620303  1.11735415]\n\n\n\n\nThe gradInput is,\n\n\n[array([[-0.27412388,  0.32756016, -0.02032043],\n       [-0.16920818,  0.60189474,  0.21347123],\n       [ 0.57771122,  0.28602061,  0.58044904]], dtype=float32), array([[ 0.27412388, -0.32756016,  0.02032043],\n       [ 0.16920818, -0.60189474, -0.21347123],\n       [-0.57771122, -0.28602061, -0.58044904]], dtype=float32)]\n\n\n\n\nCosineDistance\n\n\nScala:\n\n\nval module = CosineDistance()\n\n\n\n\nPython:\n\n\nmodule = CosineDistance()\n\n\n\n\nCosineDistance creates a module that takes a table of two vectors (or matrices if in batch mode) as input and outputs the cosine distance between them.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = CosineDistance()\nval t1 = Tensor().range(1, 3)\nval t2 = Tensor().range(4, 6)\nval input = T(t1, t2)\nval output = module.forward(input)\n\n\n input\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n    2: 4.0\n       5.0\n       6.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }\n\n\n output\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9746319\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = CosineDistance()\nt1 = np.array([1.0, 2.0, 3.0])\nt2 = np.array([4.0, 5.0, 6.0])\ninput = [t1, t2]\noutput = module.forward(input)\n\n\n input\n[array([ 1.,  2.,  3.]), array([ 4.,  5.,  6.])]\n\n\n output\n[ 0.97463191]\n\n\n\n\nEuclidean\n\n\nScala:\n\n\nval module = Euclidean(\n  inputSize,\n  outputSize,\n  fastBackward = true)\n\n\n\n\nPython:\n\n\nmodule = Euclidean(\n  input_size,\n  output_size,\n  fast_backward=True)\n\n\n\n\nOutputs the Euclidean distance of the input to \noutputSize\n centers.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Euclidean(3, 3)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))\n\n\n\n\nOutput is\n\n\ncom.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.0323668\n3.7177157\n3.8736997\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Euclidean(3, 3)\n\nprint(module.forward(np.arange(1, 4, 1)))\n\n\n\n\nOutput is\n\n\n[array([ 3.86203027,  4.02212906,  3.2648952 ], dtype=float32)]", 
            "title": "Distance Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Distance-Layers/#pairwisedistance", 
            "text": "Scala:  val pd = PairwiseDistance(norm=2)  Python:  pd = PairwiseDistance(norm=2)  It is a module that takes a table of two vectors as input and outputs\nthe distance between them using the p-norm.\nThe input given in  forward(input)  is a [[Table]] that contains two tensors which\nmust be either a vector (1D tensor) or matrix (2D tensor). If the input is a vector,\nit must have the size of  inputSize . If it is a matrix, then each row is assumed to be\nan input sample of the given batch (the number of rows means the batch size and\nthe number of columns should be equal to the  inputSize ).  Scala example:  import com.intel.analytics.bigdl.nn.PairwiseDistance\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval pd = PairwiseDistance()\nval input1 = Tensor(3, 3).randn()\nval input2 = Tensor(3, 3).randn()\nval input = T(1 -  input1, 2 -  input2)\n\nval output = pd.forward(input)\n\nval gradOutput = Tensor(3).randn()\nval gradInput = pd.backward(input, gradOutput)  The ouotput is,  output: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.155246\n1.1267666\n2.1415536\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.32565984\n-1.0108998\n-0.030873261\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]  The gradInput is,  gradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.012723052  0.31482473      0.08232752\n           0.7552968    -0.27292773     -0.6139655\n           0.0062761847 -0.018232936    -0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n        1: -0.012723052 -0.31482473     -0.08232752\n           -0.7552968   0.27292773      0.6139655\n           -0.0062761847        0.018232936     0.024110721\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npd = PairwiseDistance()\n\ninput1 = np.random.uniform(0, 1, [3, 3]).astype( float32 )\ninput2 = np.random.uniform(0, 1, [3, 3]).astype( float32 )\ninput1 = input1.reshape(3, 3)\ninput2 = input2.reshape(3, 3)\n\ninput = [input1, input2]\n\noutput = pd.forward(input)\nprint output\n\ngradOutput = np.random.uniform(0, 1, [3]).astype( float32 )\ngradOutput = gradOutput.reshape(3)\n\ngradInput = pd.backward(input, gradOutput)\nprint gradInput  The output is,  [ 0.99588805  0.65620303  1.11735415]  The gradInput is,  [array([[-0.27412388,  0.32756016, -0.02032043],\n       [-0.16920818,  0.60189474,  0.21347123],\n       [ 0.57771122,  0.28602061,  0.58044904]], dtype=float32), array([[ 0.27412388, -0.32756016,  0.02032043],\n       [ 0.16920818, -0.60189474, -0.21347123],\n       [-0.57771122, -0.28602061, -0.58044904]], dtype=float32)]", 
            "title": "PairwiseDistance"
        }, 
        {
            "location": "/APIGuide/Layers/Distance-Layers/#cosinedistance", 
            "text": "Scala:  val module = CosineDistance()  Python:  module = CosineDistance()  CosineDistance creates a module that takes a table of two vectors (or matrices if in batch mode) as input and outputs the cosine distance between them.  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = CosineDistance()\nval t1 = Tensor().range(1, 3)\nval t2 = Tensor().range(4, 6)\nval input = T(t1, t2)\nval output = module.forward(input)  input\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n    2: 4.0\n       5.0\n       6.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n    1: 1.0\n       2.0\n       3.0\n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n }  output\noutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9746319\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = CosineDistance()\nt1 = np.array([1.0, 2.0, 3.0])\nt2 = np.array([4.0, 5.0, 6.0])\ninput = [t1, t2]\noutput = module.forward(input)  input\n[array([ 1.,  2.,  3.]), array([ 4.,  5.,  6.])]  output\n[ 0.97463191]", 
            "title": "CosineDistance"
        }, 
        {
            "location": "/APIGuide/Layers/Distance-Layers/#euclidean", 
            "text": "Scala:  val module = Euclidean(\n  inputSize,\n  outputSize,\n  fastBackward = true)  Python:  module = Euclidean(\n  input_size,\n  output_size,\n  fast_backward=True)  Outputs the Euclidean distance of the input to  outputSize  centers.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\n\nval module = Euclidean(3, 3)\n\nprintln(module.forward(Tensor.range(1, 3, 1)))  Output is  com.intel.analytics.bigdl.tensor.Tensor[Float] =\n4.0323668\n3.7177157\n3.8736997\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Euclidean(3, 3)\n\nprint(module.forward(np.arange(1, 4, 1)))  Output is  [array([ 3.86203027,  4.02212906,  3.2648952 ], dtype=float32)]", 
            "title": "Euclidean"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/", 
            "text": "Recurrent\n\n\nScala:\n\n\nval module = Recurrent()\n\n\n\n\nPython:\n\n\nmodule = Recurrent()\n\n\n\n\nRecurrent module is a container of rnn cells. Different types of rnn cells can be added using add() function.  \n\n\nRecurrent supports returning state and cell of its rnn cells at last time step by using getState. output of getState\nis an Activity and it can be directly used for setState function, which will set hidden state and cell at the first time step.  \n\n\nIf contained cell is simple rnn, getState return value is a tensor(hidden state) which is \nbatch x hiddenSize\n.\n\nIf contained cell is lstm, getState return value is a table [hidden state, cell], both size is \nbatch x hiddenSize\n.\n\nIf contained cell is convlstm, getState return value is a table [hidden state, cell], both size is \nbatch x outputPlane x height x width\n.\n\nIf contained cell is convlstm3D, getState return value is a table [hidden state, cell], both size is \nbatch x outputPlane x height x width x length\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 5\nval module = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\nval input = Tensor(Array(1, 5, inputSize))\nfor (i \n- 1 to 5) {\n  val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0)*inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n}\n\nval output = module.forward(input)\n\nval state = module.getState()\nmodule.setState(state)\n\n\n input\n(1,.,.) =\n0.0 0.0 0.0 1.0 0.0 \n0.0 0.0 0.0 0.0 1.0 \n0.0 1.0 0.0 0.0 0.0 \n0.0 1.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 1.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x5]\n\n\n output\n(1,.,.) =\n0.23312 -0.5702369  -0.29894134 -0.46780553 \n-0.020703634    -0.6821252  -0.71641463 -0.3367952  \n0.031236319 -0.29233444 -0.730908   0.13494356  \n-0.22310422 -0.25562853 -0.59091455 -0.25055867 \n0.007001166 -0.7096118  -0.778529   -0.47429603 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x4]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 5\nmodule = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\ninput = np.zeros((1, 5, 5))\ninput[0][0][4] = 1\ninput[0][1][0] = 1\ninput[0][2][4] = 1\ninput[0][3][3] = 1\ninput[0][4][0] = 1\n\noutput = module.forward(input)\n\nres = module.get_state()\nmodule.set_state(res)\n\n\n input\n[[[ 0.  0.  0.  0.  1.]\n  [ 1.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  1.]\n  [ 0.  0.  0.  1.  0.]\n  [ 1.  0.  0.  0.  0.]]]\n\n\n output\n[[[-0.43169451 -0.27838707  0.41472727  0.4450382 ]\n  [-0.10717546  0.59218317  0.67959404  0.62824875]\n  [-0.56745911 -0.31170678  0.44158491  0.31494498]\n  [ 0.13328044  0.41262615  0.37388939  0.10983802]\n  [-0.51452565  0.13222042  0.59192103  0.8393243 ]]]\n\n\n\n\n\n\n\nBiRecurrent\n\n\nScala:\n\n\nval module = BiRecurrent(merge=null)\n\n\n\n\nPython:\n\n\nmodule = BiRecurrent(merge=None,bigdl_type=\nfloat\n)\n\n\n\n\nThis layer implement a bidirectional recurrent neural network\n\n\n\n\nmerge\n concat or add the output tensor of the two RNNs. Default is add\n\n\n\n\nScala example:\n\n\nval module = BiRecurrent(CAddTable())\n.add(RnnCell(6, 4, Sigmoid()))\nval input = Tensor(Array(1, 2, 6)).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.55511624      0.44330198      0.9025551       0.26096714      0.3434667       0.20060952\n0.24903035      0.24026379      0.89252585      0.23025699      0.8131796       0.4013688\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x6]\n\nmodule.forward(input)\nres10: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.3577285       0.8861933       0.52908427      0.86278\n1.2850789       0.82549953      0.5560188       0.81468254\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4]\n\n\n\n\nPython example:\n\n\nmodule = BiRecurrent(CAddTable()).add(RnnCell(6, 4, Sigmoid()))\ninput = np.random.rand(1, 2, 6)\narray([[[ 0.75637438,  0.2642816 ,  0.61973312,  0.68565282,  0.73571443,\n          0.17167681],\n        [ 0.16439321,  0.06853251,  0.42257202,  0.42814042,  0.15706152,\n          0.57866659]]])\n\nmodule.forward(input)\narray([[[ 0.69091094,  0.97150528,  0.9562254 ,  1.14894259],\n        [ 0.83814102,  1.11358368,  0.96752423,  1.00913286]]], dtype=float32)\n\n\n\n\n\n\nRNN\n\n\nScala:\n\n\nval rnnCell = RnnCell[Double](inputSize, hiddenSize, activation, wRegularizer, uRegularizer, bRegularizer)\n\n\n\n\nPython:\n\n\nrnnCell = RnnCell(input_size, hidden_size, Tanh(), w_regularizer, u_regularizer, b_regularizer)\n\n\n\n\nImplementation of vanilla recurrent neural network cell\n\n\n\n\ni2h\n weight matrix of input to hidden units\n\n\nh2h\n weight matrix of hidden units to themselves through time\n\n\n\n\nThe updating is defined as:\n\n\nh_t = f(i2h * x_t + h2h * h_{t-1})\n\n\n\n\nParameters:\n\n\n\n\ninputSize\n input size. Default: 4\n\n\nhiddenSize\n  hidden layer size. Default: 3\n\n\nactivation\n activation function f for non-linearity\n\n\nwRegularizer\n instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the input weights matrices. Default: null\n\n\nuRegularizer\n instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null\n\n\nbRegularizer\n instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the bias. Default: null\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(RnnCell(inputSize, hiddenSize, Tanh())))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n-\n print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.41442442      0.1663357       \n0.5339842       0.57332826      \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n-\n print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n1.1512008       2.181274        \n-0.4805725      1.6620052       \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(RnnCell(input_size, hidden_size, Tanh()))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-\n print output\n# There's random factor. An output could be\n[[[-0.67860311  0.80307233]\n  [-0.77462083  0.97191858]]]\n\n-\n print gradient\n# There's random factor. An output could be\n[[[-0.90771425  1.24791598]\n  [-0.70141178  0.97821164]]]\n\n\n\n\n\n\nLSTM\n\n\nScala:\n\n\nval lstm = LSTM(inputSize, hiddenSize)\n\n\n\n\nPython:\n\n\nlstm = LSTM(input_size, hidden_size)\n\n\n\n\nLong Short Term Memory architecture.\n\n\nRef:\n\n\n\n\nhttp://arxiv.org/pdf/1303.5778v1 (blueprint for this module)\n\n\nhttp://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf\n\n\nhttp://arxiv.org/pdf/1503.04069v1.pdf\n\n\nhttps://github.com/wojzaremba/lstm\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.optim.SGD\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval seed = 100\n\nRNG.setSeed(seed)\nval input = Tensor(Array(1, seqLength, inputSize))\nval labels = Tensor(Array(1, seqLength))\nfor (i \n- 1 to seqLength) {\n  val rdmLabel = Math.ceil(RNG.uniform(0, 1) * outputSize).toInt\n  val rdmInput = Math.ceil(RNG.uniform(0, 1) * inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n  labels.setValue(1, i, rdmLabel)\n}\n\nprintln(input)\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(\n  rec.add(\n      LSTM(inputSize, hiddenSize))).add(\n        TimeDistributed(Linear(hiddenSize, outputSize)))\n\nval criterion = TimeDistributedCriterion(\n  CrossEntropyCriterion(), false)\n\nval sgd = new SGD(learningRate=0.1, learningRateDecay=5e-7, weightDecay=0.1, momentum=0.002)\n\nval (weight, grad) = model.getParameters()\n\nval output = model.forward(input).toTensor\nval _loss = criterion.forward(output, labels)\nmodel.zeroGradParameters()\nval gradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)\n\ndef feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n  val output = model.forward(input).toTensor\n  val _loss = criterion.forward(output, labels)\n  model.zeroGradParameters()\n  val gradInput = criterion.backward(output, labels)\n  model.backward(input, gradInput)\n  (_loss, grad)\n}\n\nvar loss: Array[Float] = null\nfor (i \n- 1 to 100) {\n  loss = sgd.optimize(feval, weight)._2\n  println(s\n${i}-th loss = ${loss(0)}\n)\n}\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nhidden_size = 4\ninput_size = 6\noutput_size = 5\nseq_length = 5\n\ninput = np.random.uniform(0, 1, [1, seq_length, input_size]).astype(\nfloat32\n)\nlabels = np.random.uniform(1, 5, [1, seq_length]).astype(\nint\n)\n\nprint labels\nprint input\n\nrec = Recurrent()\nrec.add(LSTM(input_size, hidden_size))\n\nmodel = Sequential()\nmodel.add(rec)\nmodel.add(TimeDistributed(Linear(hidden_size, output_size)))\n\ncriterion = TimeDistributedCriterion(CrossEntropyCriterion(), False)\n\nsgd = SGD(learningrate=0.1, learningrate_decay=5e-7)\n\nweight, grad = model.parameters()\n\noutput = model.forward(input)\nloss = criterion.forward(input, labels)\ngradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)\n\n\n\n\n\n\nLSTMPeephole\n\n\nScala:\n\n\nval model = LSTMPeephole(\n  inputSize = 4,\n  hiddenSize = 3,\n  p = 0.0,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null)\n\n\n\n\nPython:\n\n\nmodel = LSTMPeephole(\n  input_size,\n  hidden_size,\n  p=0.0,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None)\n\n\n\n\nLong Short Term Memory architecture with peephole.\nRef.\n\n\n\n\nhttp://arxiv.org/pdf/1303.5778v1 (blueprint for this module)\n\n\nhttp://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf\n\n\nhttp://arxiv.org/pdf/1503.04069v1.pdf\n\n\nhttps://github.com/wojzaremba/lstm\n\n\n\n\nParameters:\n\n \ninputSize\n the size of each input vector\n\n \nhiddenSize\n Hidden unit size in the LSTM\n\n \np\n is used for [[Dropout]] probability. For more details about\n           RNN dropouts, please refer to\n           [RnnDrop: A Novel Dropout for RNNs in ASR]\n           (http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)\n           [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]\n           (https://arxiv.org/pdf/1512.05287.pdf)\n\n \nwRegularizer\n instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n \nuRegularizer\n instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n \nbRegularizer\n instance of [[Regularizer]] applied to the bias.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize))\nfor (b \n- 1 to batchSize) {\n  for (i \n- 1 to seqLength) {\n    val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0) * inputSize).toInt\n    input.setValue(b, i, rdmInput, 1.0f)\n  }\n}\n\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input).toTensor\n\nscala\n print(input)\n(1,.,.) =\n1.0 0.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n0.0 1.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n1.0 0.0 0.0 0.0 0.0 0.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x6]\n\nscala\n print(output)\n(1,.,.) =\n0.34764957  -0.31453514 -0.45646006 -0.42966008 -0.13651063 \n0.3624894   -0.2926056  -0.4347164  -0.40951455 -0.1775867  \n0.33391106  -0.29304913 -0.4748538  -0.45285955 -0.14919288 \n0.35499972  -0.29385415 -0.4419502  -0.42135617 -0.17544147 \n0.32911295  -0.30237123 -0.47175884 -0.4409852  -0.15733294 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 6\noutputSize = 5\nseqLength = 5\nbatchSize = 1\n\ninput = np.random.randn(batchSize, seqLength, inputSize)\nrec = Recurrent(hiddenSize)\nmodel = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\noutput = model.forward(input)\n\n\n print(input)\n[[[ 0.73624017 -0.91135209 -0.30627796 -1.07902111 -1.13549159  0.52868762]\n  [-0.07251559 -0.45596589  1.64020513  0.53218623  1.37993166 -0.47724947]\n  [-1.24958366 -1.22220259 -0.52454306  0.17382396  1.77666173 -1.2961758 ]\n  [ 0.45407533  0.82944329  0.02155243  1.82168093 -0.06022129  2.23823013]\n  [ 1.09100802  0.28555387 -0.94312648  0.55774033 -0.54895792  0.79885853]]]\n\n\n print(output)\n[[[ 0.4034881  -0.26156989  0.46799076  0.06283229  0.11794794]\n  [ 0.37359846 -0.17925361  0.31623816  0.06038529  0.10813089]\n  [ 0.34150451 -0.16565879  0.25264332  0.1187657   0.05118144]\n  [ 0.40773875 -0.2028828   0.24765283  0.0986848   0.12132661]\n  [ 0.40263647 -0.22403356  0.38489845  0.04720671  0.1686969 ]]]\n\n\n\n\n\n\nGRU\n\n\nScala:\n\n\nval gru = GRU(inputSize, outputSize, p, wRegularizer, uRegularizer, bRegularizer)\n\n\n\n\nPython:\n\n\ngru = GRU(inputSize, outputSize, p, w_regularizer, u_regularizer, b_regularizer)\n\n\n\n\nGated Recurrent Units architecture. The first input in sequence uses zero value for cell and hidden state.\n\n\nRef.\n\n\n\n\nhttp://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n\n\nhttps://github.com/Element-Research/rnn/blob/master/GRU.lua\n\n\n\n\nParameters:\n\n\n\n\ninputSize\n the size of each input vector\n\n\noutputSize\n hidden unit size in GRU\n\n\np\n is used for [[Dropout]] probability. For more details about\n          RNN dropouts, please refer to\n           \nRnnDrop: A Novel Dropout for RNNs in ASR\n\n            and \nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n. Default: 0.0\n\n\nwRegularizer\n instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the input weights matrices. Default: null\n\n\nuRegularizer\n instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null\n\n\nbRegularizer\n instance of \nRegularizer\n(eg. L1 or L2 regularization), applied to the bias. Default: null\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(GRU(inputSize, hiddenSize)))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n\n-\n print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.3833429       0.0082434565    \n-0.041063666    -0.08152798     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n-\n print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n-0.7684499      -0.49320614     \n-0.98002595     -0.47857404     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(GRU(input_size, hidden_size))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-\n print output\n# There's random factor. An output could be\n[[[ 0.27857888  0.20263115]\n  [ 0.29470384  0.22594413]]]\n-\n print gradient\n[[[-0.32956457  0.27405274]\n  [-0.32718879  0.32963118]]]\n\n\n\n\n\n\nConvLSTMPeephole\n\n\nScala:\n\n\nval model = ConvLSTMPeephole(\n  inputSize = 2,\n  outputSize = 4,\n  kernelI = 3,\n  kernelC = 3,\n  stride = 1,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null,\n  cRegularizer = null,\n  withPeephole = true)\n\n\n\n\nPython:\n\n\nmodel = ConvLSTMPeephole(\n  input_size = 2,\n  output_size = 4,\n  kernel_i = 3,\n  kernel_c = 3,\n  stride = 1,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None,\n  cRegularizer = None,\n  with_peephole = True)\n\n\n\n\nConvolution Long Short Term Memory architecture with peephole for 2 dimension images.\nThe input tensor in \nforward(input)\n is expected to be a 5D tensor (\nbatch x time x nInputPlane x height x width\n). output of\n\nforward(input)\n is also expected to be a 5D tensor (\nbatch x time x outputPlane x height x width\n).\n\n\nRef.\n\n\n\n\nhttps://arxiv.org/abs/1506.04214 (blueprint for this module)\n\n\nhttps://github.com/viorik/ConvLSTM\n\n\n\n\nParameters:\n\n\n\n\ninputSize\n number of input planes in the image given into forward()\n\n\noutputSize\n number of output planes the convolution layer will produce\n\n\nkernelI\n convolutional filter size to convolve input\n\n\nkernelC\n convolutional filter size to convolve cell\n\n\nstride\n step of the convolution\n\n\nwRegularizer\n instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nuRegularizer\n instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n\nbRegularizer\n instance of [[Regularizer]]\n          applied to the bias.\n\n\ncRegularizer\n instance of [[Regularizer]]\n        applied to peephole.\n\n\nwithPeephole\n whether use last cell status control a gate\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval outputSize = 4\nval inputSize = 3\nval seqLength = 2\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize, 3, 3)).rand()\n\nval rec = Recurrent()\n    val model = Sequential()\n      .add(rec\n        .add(ConvLSTMPeephole(inputSize, outputSize, 3, 3, 1, withPeephole = false)))\n\nval output = model.forward(input).toTensor\n\nscala\n print(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.32810056      0.23436882      0.1387327\n0.98273766      0.76427716      0.73554766\n0.47947738      0.72805804      0.43982902\n\n(1,1,2,.,.) =\n0.58144385      0.7534736       0.94412255\n0.05087549      0.021427812     0.91333073\n0.6844351       0.62977004      0.68027127\n\n(1,1,3,.,.) =\n0.48504198      0.16233416      0.7612549\n0.5387952       0.8391377       0.3687795\n0.85271466      0.71726906      0.79466575\n\n(1,2,1,.,.) =\n0.727532        0.05341824      0.32531977\n0.79593664      0.60162276      0.99931896\n0.7534103       0.71214366      0.031062916\n\n(1,2,2,.,.) =\n0.7343414       0.053005006     0.7448063\n0.2277985       0.47414783      0.21945253\n0.0034818714    0.11545401      0.73085403\n\n(1,2,3,.,.) =\n0.9644807       0.30755267      0.42099005\n0.6831594       0.50683653      0.14237563\n0.65172654      0.86954886      0.5077393\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3x3]\n\nscala\n print(output)\n(1,1,1,.,.) =\n-0.04460164     -0.023752786    -0.014343993\n0.0067705153    0.08542874      0.020885356\n-0.042719357    -0.012113815    -0.030324051\n\n(1,1,2,.,.) =\n-0.038318213    -0.056998547    -0.02303889\n0.027873239     -0.040311974    -0.03261278\n0.015056128     0.11064132      0.0034682436\n\n(1,1,3,.,.) =\n0.006952648     0.011758738     -0.047590334\n0.052022297     0.040250845     -0.046224136\n-0.0084472215   -0.02629062     -0.0737972\n\n(1,1,4,.,.) =\n-0.087721705    0.0382758       0.027436329\n-0.030658737    -0.022953996    0.15838619\n0.055106055     0.004877564     0.098199464\n\n(1,2,1,.,.) =\n-0.069991425    -0.022071177    -0.06291955\n-0.006841902    0.010781053     0.05410414\n-0.03933395     -0.003422904    -0.106903486\n\n(1,2,2,.,.) =\n-0.059429795    -0.098534085    -0.068920344\n0.008100101     0.01948546      -0.040567685\n0.048763007     0.06001041      0.003068042\n\n(1,2,3,.,.) =\n0.02817994      0.006684172     -0.0962587\n0.022453573     0.014425971     -0.06118475\n-0.013392928    -0.04574135     -0.12722406\n\n(1,2,4,.,.) =\n-0.074006446    -0.028510522    0.06808455\n-0.021926142    0.036675904     0.18708621\n0.08240187      0.12469789      0.17341805\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\noutput_size = 4\ninput_size= 3\nseq_len = 2\nbatch_size = 1\n\ninput = np.random.randn(batch_size, seq_len, input_size, 3, 3)\nrec = Recurrent()\nmodel = Sequential().add(\n    rec.add(ConvLSTMPeephole(input_size, output_size, 3, 3, 1, with_peephole = False)))\noutput = model.forward(input)\n\n\n print(input)\n[[[[[ 2.39979422  0.75647109  0.88928214]\n    [-0.07132477 -0.4348564   0.38270011]\n    [-1.03522309  0.38399781  0.20369625]]\n\n   [[-0.48392771  0.54371842 -1.42064221]\n    [-0.3711481  -0.16019682  0.82116693]\n    [ 0.15922215  1.79676148  0.38362552]]\n\n   [[-0.69402482  1.11930766 -1.29138064]\n    [ 0.92755002 -0.31138235  0.34953374]\n    [-0.0176643   1.13839126  0.02133309]]]\n\n\n  [[[-0.40704988  0.1819258  -0.21400335]\n    [ 0.65717965  0.75912824  1.49077775]\n    [-0.74917913 -1.48460681  1.06098727]]\n\n   [[ 1.04942415  1.2558929  -1.24367776]\n    [-0.13452707  0.01485188  2.41215047]\n    [ 0.59776321 -0.38602613  0.57937933]]\n\n   [[ 0.55007301  1.22571134  0.11656841]\n    [-0.4722457   1.79801493  0.59698431]\n    [ 0.25119458 -0.27323404  1.5516505 ]]]]]\n\n\n print(output)\n[[[[[-0.22908808 -0.08243818 -0.10530333]\n    [ 0.04545299  0.0347576   0.06448466]\n    [ 0.00148075 -0.01422587 -0.04424585]]\n\n   [[-0.08625289  0.00121372  0.00961097]\n    [-0.08068027  0.2389598  -0.08875058]\n    [-0.10860988 -0.08109165  0.05274875]]\n\n   [[ 0.01545026 -0.14079301  0.0162897 ]\n    [ 0.0114354   0.01696588  0.09375648]\n    [ 0.06766916  0.16015787 -0.01530124]]\n\n   [[-0.00311095  0.07033439  0.05258823]\n    [-0.04846094 -0.11335927 -0.22434352]\n    [-0.09923813 -0.064981   -0.05341392]]]\n\n\n  [[[-0.01070079  0.01705431 -0.10199456]\n    [-0.19023973 -0.1359819   0.11552753]\n    [ 0.04331793  0.00603994 -0.19059387]]\n\n   [[-0.12100818 -0.01191896  0.08049219]\n    [-0.10134248  0.02910084 -0.00024394]\n    [-0.09548382 -0.18623565  0.18261637]]\n\n   [[-0.00644266  0.03494127  0.09105418]\n    [ 0.03467004 -0.1236406   0.23844369]\n    [ 0.12281432  0.09469442  0.04526915]]\n\n   [[ 0.00190313  0.01997324 -0.17609949]\n    [-0.0937     -0.03763293 -0.04860835]\n    [-0.15700462 -0.17341313 -0.06551415]]]]]\n\n\n\n\n\n\nConvLSTMPeephole3D\n\n\nScala:\n\n\nval model = ConvLSTMPeephole3D(\n  inputSize = 2,\n  outputSize = 4,\n  kernelI = 3,\n  kernelC = 3,\n  stride = 1,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null,\n  cRegularizer = null,\n  withPeephole = true)\n\n\n\n\nPython:\n\n\nmodel = ConvLSTMPeephole3D(\n  input_size = 2,\n  output_size = 4,\n  kernel_i = 3,\n  kernel_c = 3,\n  stride = 1,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None,\n  cRegularizer=None,\n  with_peephole = True)\n\n\n\n\nSimilar to Convlstm2D, it's a Convolution Long Short Term Memory architecture with peephole but for 3 spatial dimension images.\nThe input tensor in \nforward(input)\n is expected to be a 6D tensor (\nbatch x time x nInputPlane x height x width x length\n). output of\n\nforward(input)\n is also expected to be a 6D tensor (\nbatch x time x outputPlane x height x width x length\n).\n\n\nParameters:\n\n\n\n\ninputSize\n number of input planes in the image given into forward()\n\n\noutputSize\n number of output planes the convolution layer will produce\n\n\nkernelI\n convolutional filter size to convolve input\n\n\nkernelC\n convolutional filter size to convolve cell\n\n\nstride\n step of the convolution\n\n\nwRegularizer\n instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nuRegularizer\n instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n\nbRegularizer\n instance of [[Regularizer]]\n          applied to the bias.\n\n\ncRegularizer\n instance of [[Regularizer]]\n          applied to peephole.\n\n\nwithPeephole\n whether use last cell status control a gate\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval outputSize = 4\nval inputSize = 3\nval seqLength = 2\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize, 3, 3, 3)).rand()\n\nval rec = Recurrent()\n    val model = Sequential()\n      .add(rec\n        .add(ConvLSTMPeephole3D(inputSize, outputSize, 3, 3, 1, withPeephole = false)))\n\nval output = model.forward(input).toTensor\n\nscala\n print(input)\n(1,1,1,1,.,.) =\n0.42592695  0.32742274  0.7926296   \n0.21923159  0.7427106   0.31764257  \n0.121872835 0.54231954  0.32091624  \n\n(1,1,1,2,.,.) =\n0.06762145  0.8054027   0.8297814   \n0.95535785  0.20807801  0.46387103  \n0.90996957  0.7849159   0.79179865  \n\n(1,1,1,3,.,.) =\n0.22927228  0.29869995  0.1145133   \n0.12646529  0.8917339   0.7545332   \n0.8044227   0.5340327   0.9784876   \n\n(1,1,2,1,.,.) =\n0.68444395  0.47932255  0.28224406  \n0.5083046   0.9364489   0.27006733  \n0.24699332  0.55712855  0.50037974  \n\n(1,1,2,2,.,.) =\n0.46334672  0.10979338  0.6378528   \n0.8557069   0.10780747  0.73767877  \n0.12505454  0.72492164  0.5440267   \n\n(1,1,2,3,.,.) =\n0.15598479  0.52033675  0.64091414  \n0.15149859  0.64515823  0.6023936   \n0.31461328  0.1901752   0.98015004  \n\n(1,1,3,1,.,.) =\n0.9700778   0.24109624  0.23764393  \n0.16602103  0.97310185  0.072756775 \n0.849201    0.825025    0.2753475   \n\n(1,1,3,2,.,.) =\n0.8621034   0.24596989  0.56645423  \n0.004375741 0.9873366   0.89219636  \n0.56948274  0.291723    0.5503815   \n\n(1,1,3,3,.,.) =\n0.626368    0.9389012   0.8974684   \n0.8553843   0.39709046  0.372683    \n0.38087663  0.94703597  0.71530545  \n\n(1,2,1,1,.,.) =\n0.74050623  0.39862877  0.57509166  \n0.87832487  0.41345102  0.6262451   \n0.665165    0.49570015  0.8304163   \n\n(1,2,1,2,.,.) =\n0.30847755  0.51876235  0.10555197  \n0.10103849  0.9479695   0.11847988  \n0.60081536  0.003097216 0.22800316  \n\n(1,2,1,3,.,.) =\n0.113101795 0.76638913  0.091707565 \n0.30347276  0.029687135 0.37973404  \n0.67719024  0.02180517  0.12747364  \n\n(1,2,2,1,.,.) =\n0.12513511  0.74210113  0.82569206  \n0.1406212   0.7400157   0.041633762 \n0.26903376  0.6195371   0.618376    \n\n(1,2,2,2,.,.) =\n0.068732955 0.09746146  0.15479624  \n0.57418007  0.7181547   0.6494809   \n0.29213288  0.35022008  0.15421997  \n\n(1,2,2,3,.,.) =\n0.47196773  0.55650383  0.938309    \n0.70717365  0.68351734  0.32646814  \n0.99775004  0.2596666   0.6803594   \n\n(1,2,3,1,.,.) =\n0.6320722   0.105437785 0.36752152  \n0.8347324   0.38376364  0.641918    \n0.40254018  0.5421287   0.792421    \n\n(1,2,3,2,.,.) =\n0.2652298   0.6261154   0.21971565  \n0.31418183  0.44987184  0.43880364  \n0.76821107  0.17070894  0.47295105  \n\n(1,2,3,3,.,.) =\n0.16514553  0.37016368  0.23397927  \n0.19776458  0.07518195  0.48995376  \n0.13584352  0.23562871  0.41726747  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3x3x3]\n\nscala\n print(output)\n(1,1,1,1,.,.) =\n0.014528348 0.03160259  0.05313618  \n-0.011796958    0.027994404 0.028153816 \n-0.010374474    0.029486801 0.033610236 \n\n(1,1,1,2,.,.) =\n0.07966786  0.041255455 0.09181337  \n0.025984935 0.06594588  0.07572434  \n0.019637575 0.0068716113    0.03775029  \n\n(1,1,1,3,.,.) =\n0.07043511  0.044567406 0.08229201  \n0.10589862  0.109124646 0.0888148   \n0.018544039 0.04097363  0.09130414  \n\n(1,1,2,1,.,.) =\n0.1032162   -0.01981514 -0.0016546922   \n0.026028564 0.0100736385    0.009424217 \n-0.048695907    -0.009172593    -0.029458746    \n\n(1,1,2,2,.,.) =\n0.058081806 0.101963215 0.056670886 \n0.09300327  0.035424378 0.02410931  \n0.056604195 -0.0032351227   0.027961217 \n\n(1,1,2,3,.,.) =\n0.11710516  0.09371774  -0.013825272    \n0.02930173  0.06391968  0.04034334  \n0.010447707 -0.004905071    0.011929871 \n\n(1,1,3,1,.,.) =\n-0.020980358    0.08554982  -0.07644813 \n0.06367171  -0.06037125 0.019925931 \n0.0026421212    0.051610045 0.023478134 \n\n(1,1,3,2,.,.) =\n-0.033074334    -0.0381583  -0.019341394    \n-0.0625153  -0.06907081 -0.019746307    \n-0.010362335    0.0062695937    0.054116223 \n\n(1,1,3,3,.,.) =\n0.00461099  -0.03308314 -6.8137434E-4   \n-0.075023845    -0.024970314    0.008133534 \n0.019836657 0.051302493 0.043689556 \n\n(1,1,4,1,.,.) =\n0.027088374 0.008537832 -0.020948375    \n0.021569671 0.016515112 -0.019221392    \n-0.0074050943   -0.03274501 0.003256779 \n\n(1,1,4,2,.,.) =\n8.967657E-4 0.019020535 -0.05990117 \n0.06226491  -0.017516658    -0.028854925    \n0.048010994 0.031080479 -4.8373322E-4   \n\n(1,1,4,3,.,.) =\n0.03253352  -0.023469497    -0.047273926    \n-0.03765316 0.011091222 0.0036612307    \n0.050733108 0.01736545  0.0061482657    \n\n(1,2,1,1,.,.) =\n-0.0037416879   0.03895818  0.102294624 \n0.011019588 0.03201482  0.07654998  \n-0.015550408    0.009587483 0.027655594 \n\n(1,2,1,2,.,.) =\n0.089279816 0.03306113  0.11713534  \n0.07299529  0.057692382 0.11090511  \n-0.0031341386   0.091527686 0.07210587  \n\n(1,2,1,3,.,.) =\n0.080724075 0.07707712  0.07624206  \n0.06552311  0.104010254 0.09213451  \n0.07030998  0.0022800618    0.12461836  \n\n(1,2,2,1,.,.) =\n0.10180804  0.020320226 -0.0025817656   \n0.016294254 -0.024293585    -0.004399727    \n-0.032854877    1.1120379E-4    -0.02109197 \n\n(1,2,2,2,.,.) =\n0.0968586   0.07098973  0.07648221  \n0.0918679   0.10268471  0.056947876 \n0.027774762 -0.03927014 0.04663368  \n\n(1,2,2,3,.,.) =\n0.10225944  0.08460646  -8.393754E-4    \n0.051307157 0.011988232 0.037762236 \n0.029469138 0.023369621 0.037675448 \n\n(1,2,3,1,.,.) =\n-0.017874755    0.08561468  -0.066132575    \n0.010558257 -0.01448278 0.0073027355    \n-0.007930762    0.052643955 0.008378773 \n\n(1,2,3,2,.,.) =\n-0.009250246    -0.06543376 -0.025082456    \n-0.093004115    -0.08637037 -0.063408665    \n-0.06941878 0.010163672 0.07595171  \n\n(1,2,3,3,.,.) =\n0.014756428 -0.040423956    -0.011537984    \n-0.046337806    -0.008416044    0.068246834 \n3.5782385E-4    0.056929104 0.052956138 \n\n(1,2,4,1,.,.) =\n0.033539586 0.013915413 -0.024538055    \n0.042590756 0.034134552 0.021031722 \n-0.026687687    0.0012957935    -0.0053077694   \n\n(1,2,4,2,.,.) =\n0.0033482902    -0.037335612    -0.0956953  \n0.007350738 -0.05237038 -0.08849126 \n0.016356941 0.032067236 -0.0012172575   \n\n(1,2,4,3,.,.) =\n-0.020006038    -0.030038685    -0.054900024    \n-0.014171911    0.01270077  -0.004130667    \n0.04607582  0.040028486 0.011846061 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x3x3x3]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\noutput_size = 4\ninput_size= 3\nseq_len = 2\nbatch_size = 1\n\ninput = np.random.randn(batch_size, seq_len, input_size, 3, 3, 3)\nrec = Recurrent()\nmodel = Sequential().add(\n    rec.add(ConvLSTMPeephole3D(input_size, output_size, 3, 3, 1, with_peephole = False)))\noutput = model.forward(input)\n\n\n print(input)\n[[[[[[ -8.92954769e-02  -9.77685543e-03   1.97566296e+00]\n     [ -5.76910662e-01  -9.08404346e-01  -4.70799006e-01]\n     [ -9.86229768e-01   7.87303916e-01   2.29691167e+00]]\n\n    [[ -7.48240036e-01   4.12766483e-01  -3.88947296e-01]\n     [ -1.39879028e+00   2.43984720e+00  -2.43947000e-01]\n     [  1.86468980e-01   1.34599111e+00  -6.97932324e-01]]\n\n    [[  1.23278710e+00  -4.02661913e-01   8.50721265e-01]\n     [ -1.79452089e-01  -5.58813385e-01   1.10060751e+00]\n     [ -6.27181580e-01  -2.69531726e-01  -1.07857962e-01]]]\n\n\n   [[[ -1.01462355e+00   5.47520811e-02   3.06976674e-01]\n     [  9.64871158e-01  -1.16953916e+00   1.41880629e+00]\n     [  1.19127007e+00   1.71403439e-01  -1.30787798e+00]]\n\n    [[ -6.44313121e-01  -8.45131087e-01   6.99275525e-02]\n     [ -3.07656855e-01   1.25746926e+00   3.89980508e-02]\n     [ -2.59853355e-01   8.78915612e-01  -9.37204072e-02]]\n\n    [[  7.69958423e-02  -3.22523203e-01  -7.31295167e-01]\n     [  1.46184856e+00   1.88641278e+00   1.46645372e-01]\n     [  4.38390570e-01  -2.85102515e-01  -1.81269541e+00]]]\n\n\n   [[[  2.95126419e-01  -1.13715815e+00   9.36848777e-01]\n     [ -1.62071909e+00  -1.06018926e+00   1.88416944e+00]\n     [ -5.81248254e-01   1.05162543e+00  -3.58790528e-01]]\n\n    [[ -7.54710826e-01   2.29994522e+00   7.24276828e-01]\n     [  5.77031441e-01   7.36132125e-01   2.24719266e+00]\n     [ -4.53710071e-05   1.98478259e-01  -2.62825655e-01]]\n\n    [[  1.68124733e+00  -9.97417864e-01  -3.73490116e-01]\n     [ -1.12558844e+00   2.60032255e-01   9.67994680e-01]\n     [  1.78486852e+00   1.17514142e+00  -1.96871551e-01]]]]\n\n\n\n  [[[[  4.43156770e-01  -4.42279658e-01   8.00893010e-01]\n     [ -2.04817319e-01  -3.89658940e-01  -1.10950351e+00]\n     [  6.61008455e-01  -4.07251176e-01   1.14871901e+00]]\n\n    [[ -2.07785815e-01  -8.92450022e-01  -4.23830113e-02]\n     [ -5.26555807e-01   3.76671145e-02  -2.17877979e-01]\n     [ -7.68371469e-01   1.53052409e-01   1.02405949e+00]]\n\n    [[  5.75018628e-01  -9.47162716e-01   6.47917376e-01]\n     [  4.66967303e-01   1.00917068e-01  -1.60894238e+00]\n     [ -1.46491032e-01   3.17782758e+00   1.12581079e-01]]]\n\n\n   [[[  9.32343396e-01  -1.03853742e+00   5.67577254e-02]\n     [  1.25266813e+00   3.52463164e-01  -1.86783652e-01]\n     [ -1.20321270e+00   3.95144053e-01   2.09975625e-01]]\n\n    [[  2.68240844e-01  -1.34931544e+00   1.34259455e+00]\n     [  6.34339337e-01  -5.21231073e-02  -3.91895492e-01]\n     [  1.53872699e-01  -5.07236962e-02  -2.90772390e-01]]\n\n    [[ -5.07933749e-01   3.78036493e-01   7.41781186e-01]\n     [  1.62736825e+00   1.24125644e+00  -3.97490478e-01]\n     [  5.77762257e-01   1.10372911e+00   1.58060183e-01]]]\n\n\n   [[[  5.31859839e-01   1.72805654e+00  -3.77124271e-01]\n     [  1.24638369e+00  -1.54061928e+00   6.22001793e-01]\n     [  1.92447446e+00   7.71351435e-01  -1.59998400e+00]]\n\n    [[  1.44289958e+00   5.41433535e-01   9.19769038e-01]\n     [  9.92873720e-01  -9.05746035e-01   1.35906705e+00]\n     [  1.38994943e+00   2.11451648e+00  -1.58783119e-01]]\n\n    [[ -1.44024889e+00  -5.12269041e-01   8.56761529e-02]\n     [  1.16668889e+00   7.58164067e-01  -1.04304927e+00]\n     [  6.34138215e-01  -7.89939971e-01  -5.52376307e-01]]]]]]\n\n\n print(output)\n[[[[[[ 0.08801123 -0.15533912 -0.08897342]\n     [ 0.01158205 -0.01103314  0.02793931]\n     [-0.01269898 -0.09544773  0.03573112]]\n\n    [[-0.15603164 -0.16063154 -0.09672774]\n     [ 0.15531734  0.05808824 -0.01653268]\n     [-0.06348733 -0.10497692 -0.13086422]]\n\n    [[ 0.002062   -0.01604773 -0.14802884]\n     [-0.0934701  -0.06831796  0.07375477]\n     [-0.01157693  0.17962074  0.13433206]]]\n\n\n   [[[ 0.03571969 -0.20905718 -0.05286504]\n     [-0.18766534 -0.10728011  0.04605131]\n     [-0.07477143  0.02631984  0.02496208]]\n\n    [[ 0.06653454  0.06536704  0.01587131]\n     [-0.00348636 -0.04439256  0.12680793]\n     [ 0.00328905  0.01904229 -0.06607334]]\n\n    [[-0.04666118 -0.06754828  0.07643934]\n     [-0.05434367 -0.09878142  0.06385987]\n     [ 0.02643086 -0.01466259 -0.1031612 ]]]\n\n\n   [[[-0.0572568   0.13133277 -0.0435285 ]\n     [-0.11612531  0.09036689 -0.09608591]\n     [-0.01049453 -0.02091818 -0.00642477]]\n\n    [[ 0.1255362  -0.07545673 -0.07554446]\n     [ 0.07270454 -0.24932131 -0.13024282]\n     [ 0.05507039 -0.0109083   0.00408967]]\n\n    [[-0.1099453  -0.11417828  0.06235902]\n     [ 0.03701246 -0.02138007 -0.05719795]\n     [-0.02627739 -0.15853535 -0.01103899]]]\n\n\n   [[[ 0.10380347 -0.05826453 -0.00690799]\n     [ 0.01000955 -0.11808137 -0.039118  ]\n     [ 0.02591963 -0.03464907 -0.21320052]]\n\n    [[-0.03449376 -0.00601143  0.05562805]\n     [ 0.09242225  0.01035819  0.09432289]\n     [-0.12854564  0.189775   -0.06698175]]\n\n    [[ 0.03462109  0.02545513 -0.14716192]\n     [ 0.02003146 -0.03616474  0.04574323]\n     [ 0.04782774 -0.04594192  0.01773669]]]]\n\n\n\n  [[[[ 0.04205685 -0.05454008 -0.0389443 ]\n     [ 0.07172828  0.03370164  0.00703573]\n     [ 0.01299563 -0.06371058  0.02505058]]\n\n    [[-0.09191396  0.06227853 -0.15412274]\n     [ 0.09069916  0.01907965 -0.05783302]\n     [-0.03441796 -0.11438221 -0.1011953 ]]\n\n    [[-0.00837748 -0.06554071 -0.14735688]\n     [-0.04640726  0.01484136  0.14445931]\n     [-0.09255736 -0.12196805 -0.0444463 ]]]\n\n\n   [[[ 0.01632853  0.01925437  0.02539274]\n     [-0.09239745 -0.13713452  0.06149488]\n     [-0.01742462  0.06624916  0.01490385]]\n\n    [[ 0.03866836  0.19375585  0.06069621]\n     [-0.11291414 -0.29582706  0.11678439]\n     [-0.09451667  0.05238266 -0.05152772]]\n\n    [[-0.11206269  0.09128021  0.09243178]\n     [ 0.01127258 -0.05845089  0.09795895]\n     [ 0.00747248  0.02055444  0.0121724 ]]]\n\n\n   [[[-0.11144694 -0.0030012  -0.03507657]\n     [-0.15461211 -0.00992483  0.02500556]\n     [-0.07733752 -0.09037463  0.02955181]]\n\n    [[-0.00988597  0.0264726  -0.14286363]\n     [-0.06936073 -0.01345975 -0.16290392]\n     [-0.07821255 -0.02489748  0.05186536]]\n\n    [[-0.12142604  0.04658077  0.00509979]\n     [-0.16115788 -0.19458961 -0.04082467]\n     [ 0.10544231 -0.10425973  0.01532217]]]\n\n\n   [[[ 0.08169251  0.05370622  0.00506061]\n     [ 0.08195242  0.08890768  0.03178475]\n     [-0.03648232  0.02655745 -0.18274172]]\n\n    [[ 0.07358464 -0.09604233  0.06556321]\n     [-0.02229194  0.17364709  0.07240117]\n     [-0.18307404  0.04115544 -0.15400645]]\n\n    [[ 0.0156146  -0.15857749 -0.12837477]\n     [ 0.07957774  0.06684072  0.0719762 ]\n     [-0.13781127 -0.03935293 -0.096707  ]]]]]]\n\n\n\n\n\n\n\nTimeDistributed\n\n\nScala:\n\n\nval layer = TimeDistributed(layer)\n\n\n\n\nPython:\n\n\nlayer = TimeDistributed(layer)\n\n\n\n\nThis layer is intended to apply contained layer to each temporal time slice\nof input tensor.\n\n\nThe input data format is [Batch, Time, Other dims]. For the contained layer, it must not change\nthe Other dims length.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = TimeDistributed(Sum(1, squeeze = false, nInputDims = 2))\nval input = Tensor(T(T(\n  T(\n    T(1.0f, 2.0f),\n    T(3.0f, 4.0f)\n  ),\n  T(\n    T(2.0f, 3.0f),\n    T(4.0f, 5.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor(T(T(\n  T(\n    T(0.1f, 0.2f)\n  ),\n  T(\n    T(0.3f, 0.4f)\n  )\n))))\n\n\n\n\nGives the output,\n\n\n(1,1,.,.) =\n4.0     6.0\n\n(1,2,.,.) =\n6.0     8.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1x2]\n\n(1,1,.,.) =\n0.1     0.2\n0.1     0.2\n\n(1,2,.,.) =\n0.3     0.4\n0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import TimeDistributed,Sum\nimport numpy as np\n\nlayer = TimeDistributed(Sum(1, squeeze = False, n_input_dims = 2))\n\ninput = np.array([[\n  [\n    [1.0, 2.0],\n    [3.0, 4.0]\n  ],\n  [\n    [2.0, 3.0],\n    [4.0, 5.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[\n  [\n    [0.1, 0.2]\n  ],\n  [\n    [0.3, 0.4]\n  ]\n]]))\n\n\n\n\nGives the output,\n\n\narray([[[[ 4.,  6.]],\n\n        [[ 6.,  8.]]]], dtype=float32)\n\narray([[[[ 0.1       ,  0.2       ],\n         [ 0.1       ,  0.2       ]],\n\n        [[ 0.30000001,  0.40000001],\n         [ 0.30000001,  0.40000001]]]], dtype=float32)", 
            "title": "Recurrent Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#recurrent", 
            "text": "Scala:  val module = Recurrent()  Python:  module = Recurrent()  Recurrent module is a container of rnn cells. Different types of rnn cells can be added using add() function.    Recurrent supports returning state and cell of its rnn cells at last time step by using getState. output of getState\nis an Activity and it can be directly used for setState function, which will set hidden state and cell at the first time step.    If contained cell is simple rnn, getState return value is a tensor(hidden state) which is  batch x hiddenSize . \nIf contained cell is lstm, getState return value is a table [hidden state, cell], both size is  batch x hiddenSize . \nIf contained cell is convlstm, getState return value is a table [hidden state, cell], both size is  batch x outputPlane x height x width . \nIf contained cell is convlstm3D, getState return value is a table [hidden state, cell], both size is  batch x outputPlane x height x width x length .  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 5\nval module = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\nval input = Tensor(Array(1, 5, inputSize))\nfor (i  - 1 to 5) {\n  val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0)*inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n}\n\nval output = module.forward(input)\n\nval state = module.getState()\nmodule.setState(state)  input\n(1,.,.) =\n0.0 0.0 0.0 1.0 0.0 \n0.0 0.0 0.0 0.0 1.0 \n0.0 1.0 0.0 0.0 0.0 \n0.0 1.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 1.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x5]  output\n(1,.,.) =\n0.23312 -0.5702369  -0.29894134 -0.46780553 \n-0.020703634    -0.6821252  -0.71641463 -0.3367952  \n0.031236319 -0.29233444 -0.730908   0.13494356  \n-0.22310422 -0.25562853 -0.59091455 -0.25055867 \n0.007001166 -0.7096118  -0.778529   -0.47429603 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x4]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 5\nmodule = Recurrent().add(RnnCell(inputSize, hiddenSize, Tanh()))\ninput = np.zeros((1, 5, 5))\ninput[0][0][4] = 1\ninput[0][1][0] = 1\ninput[0][2][4] = 1\ninput[0][3][3] = 1\ninput[0][4][0] = 1\n\noutput = module.forward(input)\n\nres = module.get_state()\nmodule.set_state(res)  input\n[[[ 0.  0.  0.  0.  1.]\n  [ 1.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  1.]\n  [ 0.  0.  0.  1.  0.]\n  [ 1.  0.  0.  0.  0.]]]  output\n[[[-0.43169451 -0.27838707  0.41472727  0.4450382 ]\n  [-0.10717546  0.59218317  0.67959404  0.62824875]\n  [-0.56745911 -0.31170678  0.44158491  0.31494498]\n  [ 0.13328044  0.41262615  0.37388939  0.10983802]\n  [-0.51452565  0.13222042  0.59192103  0.8393243 ]]]", 
            "title": "Recurrent"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#birecurrent", 
            "text": "Scala:  val module = BiRecurrent(merge=null)  Python:  module = BiRecurrent(merge=None,bigdl_type= float )  This layer implement a bidirectional recurrent neural network   merge  concat or add the output tensor of the two RNNs. Default is add   Scala example:  val module = BiRecurrent(CAddTable())\n.add(RnnCell(6, 4, Sigmoid()))\nval input = Tensor(Array(1, 2, 6)).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.55511624      0.44330198      0.9025551       0.26096714      0.3434667       0.20060952\n0.24903035      0.24026379      0.89252585      0.23025699      0.8131796       0.4013688\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x6]\n\nmodule.forward(input)\nres10: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.3577285       0.8861933       0.52908427      0.86278\n1.2850789       0.82549953      0.5560188       0.81468254\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4]  Python example:  module = BiRecurrent(CAddTable()).add(RnnCell(6, 4, Sigmoid()))\ninput = np.random.rand(1, 2, 6)\narray([[[ 0.75637438,  0.2642816 ,  0.61973312,  0.68565282,  0.73571443,\n          0.17167681],\n        [ 0.16439321,  0.06853251,  0.42257202,  0.42814042,  0.15706152,\n          0.57866659]]])\n\nmodule.forward(input)\narray([[[ 0.69091094,  0.97150528,  0.9562254 ,  1.14894259],\n        [ 0.83814102,  1.11358368,  0.96752423,  1.00913286]]], dtype=float32)", 
            "title": "BiRecurrent"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#rnn", 
            "text": "Scala:  val rnnCell = RnnCell[Double](inputSize, hiddenSize, activation, wRegularizer, uRegularizer, bRegularizer)  Python:  rnnCell = RnnCell(input_size, hidden_size, Tanh(), w_regularizer, u_regularizer, b_regularizer)  Implementation of vanilla recurrent neural network cell   i2h  weight matrix of input to hidden units  h2h  weight matrix of hidden units to themselves through time   The updating is defined as:  h_t = f(i2h * x_t + h2h * h_{t-1})  Parameters:   inputSize  input size. Default: 4  hiddenSize   hidden layer size. Default: 3  activation  activation function f for non-linearity  wRegularizer  instance of  Regularizer (eg. L1 or L2 regularization), applied to the input weights matrices. Default: null  uRegularizer  instance of  Regularizer (eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null  bRegularizer  instance of  Regularizer (eg. L1 or L2 regularization), applied to the bias. Default: null   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(RnnCell(inputSize, hiddenSize, Tanh())))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n-  print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.41442442      0.1663357       \n0.5339842       0.57332826      \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n-  print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n1.1512008       2.181274        \n-0.4805725      1.6620052       \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(RnnCell(input_size, hidden_size, Tanh()))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-  print output\n# There's random factor. An output could be\n[[[-0.67860311  0.80307233]\n  [-0.77462083  0.97191858]]]\n\n-  print gradient\n# There's random factor. An output could be\n[[[-0.90771425  1.24791598]\n  [-0.70141178  0.97821164]]]", 
            "title": "RNN"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#lstm", 
            "text": "Scala:  val lstm = LSTM(inputSize, hiddenSize)  Python:  lstm = LSTM(input_size, hidden_size)  Long Short Term Memory architecture.  Ref:   http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)  http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf  http://arxiv.org/pdf/1503.04069v1.pdf  https://github.com/wojzaremba/lstm   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.optim.SGD\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval seed = 100\n\nRNG.setSeed(seed)\nval input = Tensor(Array(1, seqLength, inputSize))\nval labels = Tensor(Array(1, seqLength))\nfor (i  - 1 to seqLength) {\n  val rdmLabel = Math.ceil(RNG.uniform(0, 1) * outputSize).toInt\n  val rdmInput = Math.ceil(RNG.uniform(0, 1) * inputSize).toInt\n  input.setValue(1, i, rdmInput, 1.0f)\n  labels.setValue(1, i, rdmLabel)\n}\n\nprintln(input)\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(\n  rec.add(\n      LSTM(inputSize, hiddenSize))).add(\n        TimeDistributed(Linear(hiddenSize, outputSize)))\n\nval criterion = TimeDistributedCriterion(\n  CrossEntropyCriterion(), false)\n\nval sgd = new SGD(learningRate=0.1, learningRateDecay=5e-7, weightDecay=0.1, momentum=0.002)\n\nval (weight, grad) = model.getParameters()\n\nval output = model.forward(input).toTensor\nval _loss = criterion.forward(output, labels)\nmodel.zeroGradParameters()\nval gradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)\n\ndef feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n  val output = model.forward(input).toTensor\n  val _loss = criterion.forward(output, labels)\n  model.zeroGradParameters()\n  val gradInput = criterion.backward(output, labels)\n  model.backward(input, gradInput)\n  (_loss, grad)\n}\n\nvar loss: Array[Float] = null\nfor (i  - 1 to 100) {\n  loss = sgd.optimize(feval, weight)._2\n  println(s ${i}-th loss = ${loss(0)} )\n}  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nhidden_size = 4\ninput_size = 6\noutput_size = 5\nseq_length = 5\n\ninput = np.random.uniform(0, 1, [1, seq_length, input_size]).astype( float32 )\nlabels = np.random.uniform(1, 5, [1, seq_length]).astype( int )\n\nprint labels\nprint input\n\nrec = Recurrent()\nrec.add(LSTM(input_size, hidden_size))\n\nmodel = Sequential()\nmodel.add(rec)\nmodel.add(TimeDistributed(Linear(hidden_size, output_size)))\n\ncriterion = TimeDistributedCriterion(CrossEntropyCriterion(), False)\n\nsgd = SGD(learningrate=0.1, learningrate_decay=5e-7)\n\nweight, grad = model.parameters()\n\noutput = model.forward(input)\nloss = criterion.forward(input, labels)\ngradInput = criterion.backward(output, labels)\nmodel.backward(input, gradInput)", 
            "title": "LSTM"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#lstmpeephole", 
            "text": "Scala:  val model = LSTMPeephole(\n  inputSize = 4,\n  hiddenSize = 3,\n  p = 0.0,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null)  Python:  model = LSTMPeephole(\n  input_size,\n  hidden_size,\n  p=0.0,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None)  Long Short Term Memory architecture with peephole.\nRef.   http://arxiv.org/pdf/1303.5778v1 (blueprint for this module)  http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf  http://arxiv.org/pdf/1503.04069v1.pdf  https://github.com/wojzaremba/lstm   Parameters:   inputSize  the size of each input vector   hiddenSize  Hidden unit size in the LSTM   p  is used for [[Dropout]] probability. For more details about\n           RNN dropouts, please refer to\n           [RnnDrop: A Novel Dropout for RNNs in ASR]\n           (http://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf)\n           [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]\n           (https://arxiv.org/pdf/1512.05287.pdf)   wRegularizer  instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.   uRegularizer  instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.   bRegularizer  instance of [[Regularizer]] applied to the bias.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval hiddenSize = 4\nval inputSize = 6\nval outputSize = 5\nval seqLength = 5\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize))\nfor (b  - 1 to batchSize) {\n  for (i  - 1 to seqLength) {\n    val rdmInput = Math.ceil(RNG.uniform(0.0, 1.0) * inputSize).toInt\n    input.setValue(b, i, rdmInput, 1.0f)\n  }\n}\n\nval rec = Recurrent(hiddenSize)\nval model = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input).toTensor\n\nscala  print(input)\n(1,.,.) =\n1.0 0.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n0.0 1.0 0.0 0.0 0.0 0.0 \n0.0 0.0 0.0 0.0 0.0 1.0 \n1.0 0.0 0.0 0.0 0.0 0.0 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x6]\n\nscala  print(output)\n(1,.,.) =\n0.34764957  -0.31453514 -0.45646006 -0.42966008 -0.13651063 \n0.3624894   -0.2926056  -0.4347164  -0.40951455 -0.1775867  \n0.33391106  -0.29304913 -0.4748538  -0.45285955 -0.14919288 \n0.35499972  -0.29385415 -0.4419502  -0.42135617 -0.17544147 \n0.32911295  -0.30237123 -0.47175884 -0.4409852  -0.15733294 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x5]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nhiddenSize = 4\ninputSize = 6\noutputSize = 5\nseqLength = 5\nbatchSize = 1\n\ninput = np.random.randn(batchSize, seqLength, inputSize)\nrec = Recurrent(hiddenSize)\nmodel = Sequential().add(rec.add(LSTMPeephole(inputSize, hiddenSize))).add(TimeDistributed(Linear(hiddenSize, outputSize)))\noutput = model.forward(input)  print(input)\n[[[ 0.73624017 -0.91135209 -0.30627796 -1.07902111 -1.13549159  0.52868762]\n  [-0.07251559 -0.45596589  1.64020513  0.53218623  1.37993166 -0.47724947]\n  [-1.24958366 -1.22220259 -0.52454306  0.17382396  1.77666173 -1.2961758 ]\n  [ 0.45407533  0.82944329  0.02155243  1.82168093 -0.06022129  2.23823013]\n  [ 1.09100802  0.28555387 -0.94312648  0.55774033 -0.54895792  0.79885853]]]  print(output)\n[[[ 0.4034881  -0.26156989  0.46799076  0.06283229  0.11794794]\n  [ 0.37359846 -0.17925361  0.31623816  0.06038529  0.10813089]\n  [ 0.34150451 -0.16565879  0.25264332  0.1187657   0.05118144]\n  [ 0.40773875 -0.2028828   0.24765283  0.0986848   0.12132661]\n  [ 0.40263647 -0.22403356  0.38489845  0.04720671  0.1686969 ]]]", 
            "title": "LSTMPeephole"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#gru", 
            "text": "Scala:  val gru = GRU(inputSize, outputSize, p, wRegularizer, uRegularizer, bRegularizer)  Python:  gru = GRU(inputSize, outputSize, p, w_regularizer, u_regularizer, b_regularizer)  Gated Recurrent Units architecture. The first input in sequence uses zero value for cell and hidden state.  Ref.   http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/  https://github.com/Element-Research/rnn/blob/master/GRU.lua   Parameters:   inputSize  the size of each input vector  outputSize  hidden unit size in GRU  p  is used for [[Dropout]] probability. For more details about\n          RNN dropouts, please refer to\n            RnnDrop: A Novel Dropout for RNNs in ASR \n            and  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks . Default: 0.0  wRegularizer  instance of  Regularizer (eg. L1 or L2 regularization), applied to the input weights matrices. Default: null  uRegularizer  instance of  Regularizer (eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default: null  bRegularizer  instance of  Regularizer (eg. L1 or L2 regularization), applied to the bias. Default: null   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval hiddenSize = 2\nval inputSize = 2\nval outputSize = 2\nval seqLength = 2\nval input = Tensor(T(\n  T(1.0f, 2.0f),\n  T(2.0f, 3.0f)\n)).resize(Array(1, seqLength, inputSize))\nval gradOutput = Tensor(T(\n  T(2.0f, 3.0f),\n  T(4.0f, 5.0f)\n)).resize(Array(1, seqLength, inputSize))\nval rec = Recurrent()\n\nval model = Sequential()\n    .add(rec.add(GRU(inputSize, hiddenSize)))\n    .add(TimeDistributed(Linear(hiddenSize, outputSize)))\nval output = model.forward(input)\nval gradient = model.backward(input, gradOutput)\n\n-  print(output)\n# There's random factor. An output could be\n(1,.,.) =\n0.3833429       0.0082434565    \n-0.041063666    -0.08152798     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]\n\n\n-  print(gradient)\n# There's random factor. An output could be\n(1,.,.) =\n-0.7684499      -0.49320614     \n-0.98002595     -0.47857404     \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\nhidden_size = 2\ninput_size = 2\noutput_size = 2\nseq_length = 2\ninput = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0]\n]])\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\nrec = Recurrent()\n\nmodel = Sequential() \\\n    .add(rec.add(GRU(input_size, hidden_size))) \\\n    .add(TimeDistributed(Linear(hidden_size, output_size)))\noutput = model.forward(input)\ngradient = model.backward(input, grad_output)\n-  print output\n# There's random factor. An output could be\n[[[ 0.27857888  0.20263115]\n  [ 0.29470384  0.22594413]]]\n-  print gradient\n[[[-0.32956457  0.27405274]\n  [-0.32718879  0.32963118]]]", 
            "title": "GRU"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#convlstmpeephole", 
            "text": "Scala:  val model = ConvLSTMPeephole(\n  inputSize = 2,\n  outputSize = 4,\n  kernelI = 3,\n  kernelC = 3,\n  stride = 1,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null,\n  cRegularizer = null,\n  withPeephole = true)  Python:  model = ConvLSTMPeephole(\n  input_size = 2,\n  output_size = 4,\n  kernel_i = 3,\n  kernel_c = 3,\n  stride = 1,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None,\n  cRegularizer = None,\n  with_peephole = True)  Convolution Long Short Term Memory architecture with peephole for 2 dimension images.\nThe input tensor in  forward(input)  is expected to be a 5D tensor ( batch x time x nInputPlane x height x width ). output of forward(input)  is also expected to be a 5D tensor ( batch x time x outputPlane x height x width ).  Ref.   https://arxiv.org/abs/1506.04214 (blueprint for this module)  https://github.com/viorik/ConvLSTM   Parameters:   inputSize  number of input planes in the image given into forward()  outputSize  number of output planes the convolution layer will produce  kernelI  convolutional filter size to convolve input  kernelC  convolutional filter size to convolve cell  stride  step of the convolution  wRegularizer  instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.  uRegularizer  instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.  bRegularizer  instance of [[Regularizer]]\n          applied to the bias.  cRegularizer  instance of [[Regularizer]]\n        applied to peephole.  withPeephole  whether use last cell status control a gate   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval outputSize = 4\nval inputSize = 3\nval seqLength = 2\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize, 3, 3)).rand()\n\nval rec = Recurrent()\n    val model = Sequential()\n      .add(rec\n        .add(ConvLSTMPeephole(inputSize, outputSize, 3, 3, 1, withPeephole = false)))\n\nval output = model.forward(input).toTensor\n\nscala  print(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.32810056      0.23436882      0.1387327\n0.98273766      0.76427716      0.73554766\n0.47947738      0.72805804      0.43982902\n\n(1,1,2,.,.) =\n0.58144385      0.7534736       0.94412255\n0.05087549      0.021427812     0.91333073\n0.6844351       0.62977004      0.68027127\n\n(1,1,3,.,.) =\n0.48504198      0.16233416      0.7612549\n0.5387952       0.8391377       0.3687795\n0.85271466      0.71726906      0.79466575\n\n(1,2,1,.,.) =\n0.727532        0.05341824      0.32531977\n0.79593664      0.60162276      0.99931896\n0.7534103       0.71214366      0.031062916\n\n(1,2,2,.,.) =\n0.7343414       0.053005006     0.7448063\n0.2277985       0.47414783      0.21945253\n0.0034818714    0.11545401      0.73085403\n\n(1,2,3,.,.) =\n0.9644807       0.30755267      0.42099005\n0.6831594       0.50683653      0.14237563\n0.65172654      0.86954886      0.5077393\n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3x3]\n\nscala  print(output)\n(1,1,1,.,.) =\n-0.04460164     -0.023752786    -0.014343993\n0.0067705153    0.08542874      0.020885356\n-0.042719357    -0.012113815    -0.030324051\n\n(1,1,2,.,.) =\n-0.038318213    -0.056998547    -0.02303889\n0.027873239     -0.040311974    -0.03261278\n0.015056128     0.11064132      0.0034682436\n\n(1,1,3,.,.) =\n0.006952648     0.011758738     -0.047590334\n0.052022297     0.040250845     -0.046224136\n-0.0084472215   -0.02629062     -0.0737972\n\n(1,1,4,.,.) =\n-0.087721705    0.0382758       0.027436329\n-0.030658737    -0.022953996    0.15838619\n0.055106055     0.004877564     0.098199464\n\n(1,2,1,.,.) =\n-0.069991425    -0.022071177    -0.06291955\n-0.006841902    0.010781053     0.05410414\n-0.03933395     -0.003422904    -0.106903486\n\n(1,2,2,.,.) =\n-0.059429795    -0.098534085    -0.068920344\n0.008100101     0.01948546      -0.040567685\n0.048763007     0.06001041      0.003068042\n\n(1,2,3,.,.) =\n0.02817994      0.006684172     -0.0962587\n0.022453573     0.014425971     -0.06118475\n-0.013392928    -0.04574135     -0.12722406\n\n(1,2,4,.,.) =\n-0.074006446    -0.028510522    0.06808455\n-0.021926142    0.036675904     0.18708621\n0.08240187      0.12469789      0.17341805\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\noutput_size = 4\ninput_size= 3\nseq_len = 2\nbatch_size = 1\n\ninput = np.random.randn(batch_size, seq_len, input_size, 3, 3)\nrec = Recurrent()\nmodel = Sequential().add(\n    rec.add(ConvLSTMPeephole(input_size, output_size, 3, 3, 1, with_peephole = False)))\noutput = model.forward(input)  print(input)\n[[[[[ 2.39979422  0.75647109  0.88928214]\n    [-0.07132477 -0.4348564   0.38270011]\n    [-1.03522309  0.38399781  0.20369625]]\n\n   [[-0.48392771  0.54371842 -1.42064221]\n    [-0.3711481  -0.16019682  0.82116693]\n    [ 0.15922215  1.79676148  0.38362552]]\n\n   [[-0.69402482  1.11930766 -1.29138064]\n    [ 0.92755002 -0.31138235  0.34953374]\n    [-0.0176643   1.13839126  0.02133309]]]\n\n\n  [[[-0.40704988  0.1819258  -0.21400335]\n    [ 0.65717965  0.75912824  1.49077775]\n    [-0.74917913 -1.48460681  1.06098727]]\n\n   [[ 1.04942415  1.2558929  -1.24367776]\n    [-0.13452707  0.01485188  2.41215047]\n    [ 0.59776321 -0.38602613  0.57937933]]\n\n   [[ 0.55007301  1.22571134  0.11656841]\n    [-0.4722457   1.79801493  0.59698431]\n    [ 0.25119458 -0.27323404  1.5516505 ]]]]]  print(output)\n[[[[[-0.22908808 -0.08243818 -0.10530333]\n    [ 0.04545299  0.0347576   0.06448466]\n    [ 0.00148075 -0.01422587 -0.04424585]]\n\n   [[-0.08625289  0.00121372  0.00961097]\n    [-0.08068027  0.2389598  -0.08875058]\n    [-0.10860988 -0.08109165  0.05274875]]\n\n   [[ 0.01545026 -0.14079301  0.0162897 ]\n    [ 0.0114354   0.01696588  0.09375648]\n    [ 0.06766916  0.16015787 -0.01530124]]\n\n   [[-0.00311095  0.07033439  0.05258823]\n    [-0.04846094 -0.11335927 -0.22434352]\n    [-0.09923813 -0.064981   -0.05341392]]]\n\n\n  [[[-0.01070079  0.01705431 -0.10199456]\n    [-0.19023973 -0.1359819   0.11552753]\n    [ 0.04331793  0.00603994 -0.19059387]]\n\n   [[-0.12100818 -0.01191896  0.08049219]\n    [-0.10134248  0.02910084 -0.00024394]\n    [-0.09548382 -0.18623565  0.18261637]]\n\n   [[-0.00644266  0.03494127  0.09105418]\n    [ 0.03467004 -0.1236406   0.23844369]\n    [ 0.12281432  0.09469442  0.04526915]]\n\n   [[ 0.00190313  0.01997324 -0.17609949]\n    [-0.0937     -0.03763293 -0.04860835]\n    [-0.15700462 -0.17341313 -0.06551415]]]]]", 
            "title": "ConvLSTMPeephole"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#convlstmpeephole3d", 
            "text": "Scala:  val model = ConvLSTMPeephole3D(\n  inputSize = 2,\n  outputSize = 4,\n  kernelI = 3,\n  kernelC = 3,\n  stride = 1,\n  wRegularizer = null,\n  uRegularizer = null,\n  bRegularizer = null,\n  cRegularizer = null,\n  withPeephole = true)  Python:  model = ConvLSTMPeephole3D(\n  input_size = 2,\n  output_size = 4,\n  kernel_i = 3,\n  kernel_c = 3,\n  stride = 1,\n  wRegularizer=None,\n  uRegularizer=None,\n  bRegularizer=None,\n  cRegularizer=None,\n  with_peephole = True)  Similar to Convlstm2D, it's a Convolution Long Short Term Memory architecture with peephole but for 3 spatial dimension images.\nThe input tensor in  forward(input)  is expected to be a 6D tensor ( batch x time x nInputPlane x height x width x length ). output of forward(input)  is also expected to be a 6D tensor ( batch x time x outputPlane x height x width x length ).  Parameters:   inputSize  number of input planes in the image given into forward()  outputSize  number of output planes the convolution layer will produce  kernelI  convolutional filter size to convolve input  kernelC  convolutional filter size to convolve cell  stride  step of the convolution  wRegularizer  instance of [[Regularizer]]\n                   (eg. L1 or L2 regularization), applied to the input weights matrices.  uRegularizer  instance [[Regularizer]]\n          (eg. L1 or L2 regularization), applied to the recurrent weights matrices.  bRegularizer  instance of [[Regularizer]]\n          applied to the bias.  cRegularizer  instance of [[Regularizer]]\n          applied to peephole.  withPeephole  whether use last cell status control a gate   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.RandomGenerator._\n\nval outputSize = 4\nval inputSize = 3\nval seqLength = 2\nval batchSize = 1\n\nval input = Tensor(Array(batchSize, seqLength, inputSize, 3, 3, 3)).rand()\n\nval rec = Recurrent()\n    val model = Sequential()\n      .add(rec\n        .add(ConvLSTMPeephole3D(inputSize, outputSize, 3, 3, 1, withPeephole = false)))\n\nval output = model.forward(input).toTensor\n\nscala  print(input)\n(1,1,1,1,.,.) =\n0.42592695  0.32742274  0.7926296   \n0.21923159  0.7427106   0.31764257  \n0.121872835 0.54231954  0.32091624  \n\n(1,1,1,2,.,.) =\n0.06762145  0.8054027   0.8297814   \n0.95535785  0.20807801  0.46387103  \n0.90996957  0.7849159   0.79179865  \n\n(1,1,1,3,.,.) =\n0.22927228  0.29869995  0.1145133   \n0.12646529  0.8917339   0.7545332   \n0.8044227   0.5340327   0.9784876   \n\n(1,1,2,1,.,.) =\n0.68444395  0.47932255  0.28224406  \n0.5083046   0.9364489   0.27006733  \n0.24699332  0.55712855  0.50037974  \n\n(1,1,2,2,.,.) =\n0.46334672  0.10979338  0.6378528   \n0.8557069   0.10780747  0.73767877  \n0.12505454  0.72492164  0.5440267   \n\n(1,1,2,3,.,.) =\n0.15598479  0.52033675  0.64091414  \n0.15149859  0.64515823  0.6023936   \n0.31461328  0.1901752   0.98015004  \n\n(1,1,3,1,.,.) =\n0.9700778   0.24109624  0.23764393  \n0.16602103  0.97310185  0.072756775 \n0.849201    0.825025    0.2753475   \n\n(1,1,3,2,.,.) =\n0.8621034   0.24596989  0.56645423  \n0.004375741 0.9873366   0.89219636  \n0.56948274  0.291723    0.5503815   \n\n(1,1,3,3,.,.) =\n0.626368    0.9389012   0.8974684   \n0.8553843   0.39709046  0.372683    \n0.38087663  0.94703597  0.71530545  \n\n(1,2,1,1,.,.) =\n0.74050623  0.39862877  0.57509166  \n0.87832487  0.41345102  0.6262451   \n0.665165    0.49570015  0.8304163   \n\n(1,2,1,2,.,.) =\n0.30847755  0.51876235  0.10555197  \n0.10103849  0.9479695   0.11847988  \n0.60081536  0.003097216 0.22800316  \n\n(1,2,1,3,.,.) =\n0.113101795 0.76638913  0.091707565 \n0.30347276  0.029687135 0.37973404  \n0.67719024  0.02180517  0.12747364  \n\n(1,2,2,1,.,.) =\n0.12513511  0.74210113  0.82569206  \n0.1406212   0.7400157   0.041633762 \n0.26903376  0.6195371   0.618376    \n\n(1,2,2,2,.,.) =\n0.068732955 0.09746146  0.15479624  \n0.57418007  0.7181547   0.6494809   \n0.29213288  0.35022008  0.15421997  \n\n(1,2,2,3,.,.) =\n0.47196773  0.55650383  0.938309    \n0.70717365  0.68351734  0.32646814  \n0.99775004  0.2596666   0.6803594   \n\n(1,2,3,1,.,.) =\n0.6320722   0.105437785 0.36752152  \n0.8347324   0.38376364  0.641918    \n0.40254018  0.5421287   0.792421    \n\n(1,2,3,2,.,.) =\n0.2652298   0.6261154   0.21971565  \n0.31418183  0.44987184  0.43880364  \n0.76821107  0.17070894  0.47295105  \n\n(1,2,3,3,.,.) =\n0.16514553  0.37016368  0.23397927  \n0.19776458  0.07518195  0.48995376  \n0.13584352  0.23562871  0.41726747  \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x2x3x3x3x3]\n\nscala  print(output)\n(1,1,1,1,.,.) =\n0.014528348 0.03160259  0.05313618  \n-0.011796958    0.027994404 0.028153816 \n-0.010374474    0.029486801 0.033610236 \n\n(1,1,1,2,.,.) =\n0.07966786  0.041255455 0.09181337  \n0.025984935 0.06594588  0.07572434  \n0.019637575 0.0068716113    0.03775029  \n\n(1,1,1,3,.,.) =\n0.07043511  0.044567406 0.08229201  \n0.10589862  0.109124646 0.0888148   \n0.018544039 0.04097363  0.09130414  \n\n(1,1,2,1,.,.) =\n0.1032162   -0.01981514 -0.0016546922   \n0.026028564 0.0100736385    0.009424217 \n-0.048695907    -0.009172593    -0.029458746    \n\n(1,1,2,2,.,.) =\n0.058081806 0.101963215 0.056670886 \n0.09300327  0.035424378 0.02410931  \n0.056604195 -0.0032351227   0.027961217 \n\n(1,1,2,3,.,.) =\n0.11710516  0.09371774  -0.013825272    \n0.02930173  0.06391968  0.04034334  \n0.010447707 -0.004905071    0.011929871 \n\n(1,1,3,1,.,.) =\n-0.020980358    0.08554982  -0.07644813 \n0.06367171  -0.06037125 0.019925931 \n0.0026421212    0.051610045 0.023478134 \n\n(1,1,3,2,.,.) =\n-0.033074334    -0.0381583  -0.019341394    \n-0.0625153  -0.06907081 -0.019746307    \n-0.010362335    0.0062695937    0.054116223 \n\n(1,1,3,3,.,.) =\n0.00461099  -0.03308314 -6.8137434E-4   \n-0.075023845    -0.024970314    0.008133534 \n0.019836657 0.051302493 0.043689556 \n\n(1,1,4,1,.,.) =\n0.027088374 0.008537832 -0.020948375    \n0.021569671 0.016515112 -0.019221392    \n-0.0074050943   -0.03274501 0.003256779 \n\n(1,1,4,2,.,.) =\n8.967657E-4 0.019020535 -0.05990117 \n0.06226491  -0.017516658    -0.028854925    \n0.048010994 0.031080479 -4.8373322E-4   \n\n(1,1,4,3,.,.) =\n0.03253352  -0.023469497    -0.047273926    \n-0.03765316 0.011091222 0.0036612307    \n0.050733108 0.01736545  0.0061482657    \n\n(1,2,1,1,.,.) =\n-0.0037416879   0.03895818  0.102294624 \n0.011019588 0.03201482  0.07654998  \n-0.015550408    0.009587483 0.027655594 \n\n(1,2,1,2,.,.) =\n0.089279816 0.03306113  0.11713534  \n0.07299529  0.057692382 0.11090511  \n-0.0031341386   0.091527686 0.07210587  \n\n(1,2,1,3,.,.) =\n0.080724075 0.07707712  0.07624206  \n0.06552311  0.104010254 0.09213451  \n0.07030998  0.0022800618    0.12461836  \n\n(1,2,2,1,.,.) =\n0.10180804  0.020320226 -0.0025817656   \n0.016294254 -0.024293585    -0.004399727    \n-0.032854877    1.1120379E-4    -0.02109197 \n\n(1,2,2,2,.,.) =\n0.0968586   0.07098973  0.07648221  \n0.0918679   0.10268471  0.056947876 \n0.027774762 -0.03927014 0.04663368  \n\n(1,2,2,3,.,.) =\n0.10225944  0.08460646  -8.393754E-4    \n0.051307157 0.011988232 0.037762236 \n0.029469138 0.023369621 0.037675448 \n\n(1,2,3,1,.,.) =\n-0.017874755    0.08561468  -0.066132575    \n0.010558257 -0.01448278 0.0073027355    \n-0.007930762    0.052643955 0.008378773 \n\n(1,2,3,2,.,.) =\n-0.009250246    -0.06543376 -0.025082456    \n-0.093004115    -0.08637037 -0.063408665    \n-0.06941878 0.010163672 0.07595171  \n\n(1,2,3,3,.,.) =\n0.014756428 -0.040423956    -0.011537984    \n-0.046337806    -0.008416044    0.068246834 \n3.5782385E-4    0.056929104 0.052956138 \n\n(1,2,4,1,.,.) =\n0.033539586 0.013915413 -0.024538055    \n0.042590756 0.034134552 0.021031722 \n-0.026687687    0.0012957935    -0.0053077694   \n\n(1,2,4,2,.,.) =\n0.0033482902    -0.037335612    -0.0956953  \n0.007350738 -0.05237038 -0.08849126 \n0.016356941 0.032067236 -0.0012172575   \n\n(1,2,4,3,.,.) =\n-0.020006038    -0.030038685    -0.054900024    \n-0.014171911    0.01270077  -0.004130667    \n0.04607582  0.040028486 0.011846061 \n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x3x3x3]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\noutput_size = 4\ninput_size= 3\nseq_len = 2\nbatch_size = 1\n\ninput = np.random.randn(batch_size, seq_len, input_size, 3, 3, 3)\nrec = Recurrent()\nmodel = Sequential().add(\n    rec.add(ConvLSTMPeephole3D(input_size, output_size, 3, 3, 1, with_peephole = False)))\noutput = model.forward(input)  print(input)\n[[[[[[ -8.92954769e-02  -9.77685543e-03   1.97566296e+00]\n     [ -5.76910662e-01  -9.08404346e-01  -4.70799006e-01]\n     [ -9.86229768e-01   7.87303916e-01   2.29691167e+00]]\n\n    [[ -7.48240036e-01   4.12766483e-01  -3.88947296e-01]\n     [ -1.39879028e+00   2.43984720e+00  -2.43947000e-01]\n     [  1.86468980e-01   1.34599111e+00  -6.97932324e-01]]\n\n    [[  1.23278710e+00  -4.02661913e-01   8.50721265e-01]\n     [ -1.79452089e-01  -5.58813385e-01   1.10060751e+00]\n     [ -6.27181580e-01  -2.69531726e-01  -1.07857962e-01]]]\n\n\n   [[[ -1.01462355e+00   5.47520811e-02   3.06976674e-01]\n     [  9.64871158e-01  -1.16953916e+00   1.41880629e+00]\n     [  1.19127007e+00   1.71403439e-01  -1.30787798e+00]]\n\n    [[ -6.44313121e-01  -8.45131087e-01   6.99275525e-02]\n     [ -3.07656855e-01   1.25746926e+00   3.89980508e-02]\n     [ -2.59853355e-01   8.78915612e-01  -9.37204072e-02]]\n\n    [[  7.69958423e-02  -3.22523203e-01  -7.31295167e-01]\n     [  1.46184856e+00   1.88641278e+00   1.46645372e-01]\n     [  4.38390570e-01  -2.85102515e-01  -1.81269541e+00]]]\n\n\n   [[[  2.95126419e-01  -1.13715815e+00   9.36848777e-01]\n     [ -1.62071909e+00  -1.06018926e+00   1.88416944e+00]\n     [ -5.81248254e-01   1.05162543e+00  -3.58790528e-01]]\n\n    [[ -7.54710826e-01   2.29994522e+00   7.24276828e-01]\n     [  5.77031441e-01   7.36132125e-01   2.24719266e+00]\n     [ -4.53710071e-05   1.98478259e-01  -2.62825655e-01]]\n\n    [[  1.68124733e+00  -9.97417864e-01  -3.73490116e-01]\n     [ -1.12558844e+00   2.60032255e-01   9.67994680e-01]\n     [  1.78486852e+00   1.17514142e+00  -1.96871551e-01]]]]\n\n\n\n  [[[[  4.43156770e-01  -4.42279658e-01   8.00893010e-01]\n     [ -2.04817319e-01  -3.89658940e-01  -1.10950351e+00]\n     [  6.61008455e-01  -4.07251176e-01   1.14871901e+00]]\n\n    [[ -2.07785815e-01  -8.92450022e-01  -4.23830113e-02]\n     [ -5.26555807e-01   3.76671145e-02  -2.17877979e-01]\n     [ -7.68371469e-01   1.53052409e-01   1.02405949e+00]]\n\n    [[  5.75018628e-01  -9.47162716e-01   6.47917376e-01]\n     [  4.66967303e-01   1.00917068e-01  -1.60894238e+00]\n     [ -1.46491032e-01   3.17782758e+00   1.12581079e-01]]]\n\n\n   [[[  9.32343396e-01  -1.03853742e+00   5.67577254e-02]\n     [  1.25266813e+00   3.52463164e-01  -1.86783652e-01]\n     [ -1.20321270e+00   3.95144053e-01   2.09975625e-01]]\n\n    [[  2.68240844e-01  -1.34931544e+00   1.34259455e+00]\n     [  6.34339337e-01  -5.21231073e-02  -3.91895492e-01]\n     [  1.53872699e-01  -5.07236962e-02  -2.90772390e-01]]\n\n    [[ -5.07933749e-01   3.78036493e-01   7.41781186e-01]\n     [  1.62736825e+00   1.24125644e+00  -3.97490478e-01]\n     [  5.77762257e-01   1.10372911e+00   1.58060183e-01]]]\n\n\n   [[[  5.31859839e-01   1.72805654e+00  -3.77124271e-01]\n     [  1.24638369e+00  -1.54061928e+00   6.22001793e-01]\n     [  1.92447446e+00   7.71351435e-01  -1.59998400e+00]]\n\n    [[  1.44289958e+00   5.41433535e-01   9.19769038e-01]\n     [  9.92873720e-01  -9.05746035e-01   1.35906705e+00]\n     [  1.38994943e+00   2.11451648e+00  -1.58783119e-01]]\n\n    [[ -1.44024889e+00  -5.12269041e-01   8.56761529e-02]\n     [  1.16668889e+00   7.58164067e-01  -1.04304927e+00]\n     [  6.34138215e-01  -7.89939971e-01  -5.52376307e-01]]]]]]  print(output)\n[[[[[[ 0.08801123 -0.15533912 -0.08897342]\n     [ 0.01158205 -0.01103314  0.02793931]\n     [-0.01269898 -0.09544773  0.03573112]]\n\n    [[-0.15603164 -0.16063154 -0.09672774]\n     [ 0.15531734  0.05808824 -0.01653268]\n     [-0.06348733 -0.10497692 -0.13086422]]\n\n    [[ 0.002062   -0.01604773 -0.14802884]\n     [-0.0934701  -0.06831796  0.07375477]\n     [-0.01157693  0.17962074  0.13433206]]]\n\n\n   [[[ 0.03571969 -0.20905718 -0.05286504]\n     [-0.18766534 -0.10728011  0.04605131]\n     [-0.07477143  0.02631984  0.02496208]]\n\n    [[ 0.06653454  0.06536704  0.01587131]\n     [-0.00348636 -0.04439256  0.12680793]\n     [ 0.00328905  0.01904229 -0.06607334]]\n\n    [[-0.04666118 -0.06754828  0.07643934]\n     [-0.05434367 -0.09878142  0.06385987]\n     [ 0.02643086 -0.01466259 -0.1031612 ]]]\n\n\n   [[[-0.0572568   0.13133277 -0.0435285 ]\n     [-0.11612531  0.09036689 -0.09608591]\n     [-0.01049453 -0.02091818 -0.00642477]]\n\n    [[ 0.1255362  -0.07545673 -0.07554446]\n     [ 0.07270454 -0.24932131 -0.13024282]\n     [ 0.05507039 -0.0109083   0.00408967]]\n\n    [[-0.1099453  -0.11417828  0.06235902]\n     [ 0.03701246 -0.02138007 -0.05719795]\n     [-0.02627739 -0.15853535 -0.01103899]]]\n\n\n   [[[ 0.10380347 -0.05826453 -0.00690799]\n     [ 0.01000955 -0.11808137 -0.039118  ]\n     [ 0.02591963 -0.03464907 -0.21320052]]\n\n    [[-0.03449376 -0.00601143  0.05562805]\n     [ 0.09242225  0.01035819  0.09432289]\n     [-0.12854564  0.189775   -0.06698175]]\n\n    [[ 0.03462109  0.02545513 -0.14716192]\n     [ 0.02003146 -0.03616474  0.04574323]\n     [ 0.04782774 -0.04594192  0.01773669]]]]\n\n\n\n  [[[[ 0.04205685 -0.05454008 -0.0389443 ]\n     [ 0.07172828  0.03370164  0.00703573]\n     [ 0.01299563 -0.06371058  0.02505058]]\n\n    [[-0.09191396  0.06227853 -0.15412274]\n     [ 0.09069916  0.01907965 -0.05783302]\n     [-0.03441796 -0.11438221 -0.1011953 ]]\n\n    [[-0.00837748 -0.06554071 -0.14735688]\n     [-0.04640726  0.01484136  0.14445931]\n     [-0.09255736 -0.12196805 -0.0444463 ]]]\n\n\n   [[[ 0.01632853  0.01925437  0.02539274]\n     [-0.09239745 -0.13713452  0.06149488]\n     [-0.01742462  0.06624916  0.01490385]]\n\n    [[ 0.03866836  0.19375585  0.06069621]\n     [-0.11291414 -0.29582706  0.11678439]\n     [-0.09451667  0.05238266 -0.05152772]]\n\n    [[-0.11206269  0.09128021  0.09243178]\n     [ 0.01127258 -0.05845089  0.09795895]\n     [ 0.00747248  0.02055444  0.0121724 ]]]\n\n\n   [[[-0.11144694 -0.0030012  -0.03507657]\n     [-0.15461211 -0.00992483  0.02500556]\n     [-0.07733752 -0.09037463  0.02955181]]\n\n    [[-0.00988597  0.0264726  -0.14286363]\n     [-0.06936073 -0.01345975 -0.16290392]\n     [-0.07821255 -0.02489748  0.05186536]]\n\n    [[-0.12142604  0.04658077  0.00509979]\n     [-0.16115788 -0.19458961 -0.04082467]\n     [ 0.10544231 -0.10425973  0.01532217]]]\n\n\n   [[[ 0.08169251  0.05370622  0.00506061]\n     [ 0.08195242  0.08890768  0.03178475]\n     [-0.03648232  0.02655745 -0.18274172]]\n\n    [[ 0.07358464 -0.09604233  0.06556321]\n     [-0.02229194  0.17364709  0.07240117]\n     [-0.18307404  0.04115544 -0.15400645]]\n\n    [[ 0.0156146  -0.15857749 -0.12837477]\n     [ 0.07957774  0.06684072  0.0719762 ]\n     [-0.13781127 -0.03935293 -0.096707  ]]]]]]", 
            "title": "ConvLSTMPeephole3D"
        }, 
        {
            "location": "/APIGuide/Layers/Recurrent-Layers/#timedistributed", 
            "text": "Scala:  val layer = TimeDistributed(layer)  Python:  layer = TimeDistributed(layer)  This layer is intended to apply contained layer to each temporal time slice\nof input tensor.  The input data format is [Batch, Time, Other dims]. For the contained layer, it must not change\nthe Other dims length.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval layer = TimeDistributed(Sum(1, squeeze = false, nInputDims = 2))\nval input = Tensor(T(T(\n  T(\n    T(1.0f, 2.0f),\n    T(3.0f, 4.0f)\n  ),\n  T(\n    T(2.0f, 3.0f),\n    T(4.0f, 5.0f)\n  )\n)))\nlayer.forward(input)\nlayer.backward(input, Tensor(T(T(\n  T(\n    T(0.1f, 0.2f)\n  ),\n  T(\n    T(0.3f, 0.4f)\n  )\n))))  Gives the output,  (1,1,.,.) =\n4.0     6.0\n\n(1,2,.,.) =\n6.0     8.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x1x2]\n\n(1,1,.,.) =\n0.1     0.2\n0.1     0.2\n\n(1,2,.,.) =\n0.3     0.4\n0.3     0.4\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]  Python example:  from bigdl.nn.layer import TimeDistributed,Sum\nimport numpy as np\n\nlayer = TimeDistributed(Sum(1, squeeze = False, n_input_dims = 2))\n\ninput = np.array([[\n  [\n    [1.0, 2.0],\n    [3.0, 4.0]\n  ],\n  [\n    [2.0, 3.0],\n    [4.0, 5.0]\n  ]\n]])\nlayer.forward(input)\nlayer.backward(input, np.array([[\n  [\n    [0.1, 0.2]\n  ],\n  [\n    [0.3, 0.4]\n  ]\n]]))  Gives the output,  array([[[[ 4.,  6.]],\n\n        [[ 6.,  8.]]]], dtype=float32)\n\narray([[[[ 0.1       ,  0.2       ],\n         [ 0.1       ,  0.2       ]],\n\n        [[ 0.30000001,  0.40000001],\n         [ 0.30000001,  0.40000001]]]], dtype=float32)", 
            "title": "TimeDistributed"
        }, 
        {
            "location": "/APIGuide/Layers/Recursive-Layers/", 
            "text": "TensorTree\n\n\nTensorTree class is used to decode a tensor to a tree structure.\nThe given input \ncontent\n is a tensor which encodes a constituency parse tree.\nThe tensor should have the following structure:\n\n\nEach row of the tensor represents a tree node and the row number is node number.\nFor each row, except the last column, all other columns represent the children\nnode number of this node. Assume the value of a certain column of the row is not zero,\nthe value \np\n means this node has a child whose node number is \np\n (lies in the \np\n-th)\nrow. Each leaf has a leaf number, in the tensor, the last column represents the leaf number.\nEach leaf does not have any children, so all the columns of a leaf except the last should\nbe zero. If a node is the root, the last column should equal to \n-1\n.\n\n\nNote: if any row for padding, the padding rows should be placed at the last rows with all\nelements equal to \n-1\n.\n\n\neg. a tensor represents a binary tree:\n\n\n[11, 10, -1;\n 0, 0, 1;\n 0, 0, 2;\n 0, 0, 3;\n 0, 0, 4;\n 0, 0, 5;\n 0, 0, 6;\n 4, 5, 0;\n 6, 7, 0;\n 8, 9, 0;\n 2, 3, 0;\n -1, -1, -1;\n -1, -1, -1]\n\n\n\n\nParameters:\n* \ncontent\n the tensor to be encoded\n\n\nTreeLSTM\n\n\nTreeLSTM is a base class of all other kinds of tree lstms,\n, as described in the paper \n\nImproved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\n\n by Kai Sheng Tai, Richard Socher, and Christopher Manning.\n\n\n\n\nBinaryTreeLSTM\n\n\nScala:\n\n\nval treeLSTM = BinaryTreeLSTM(\n  inputSize,\n  hiddenSize,\n  gateOutput,\n  withGraph)\n\n\n\n\nPython:\n\n\ntree_lstm = BinaryTreeLSTM(\n  input_size,\n  hidden_size,\n  gate_output,\n  with_graph)\n\n\n\n\nThis class is an implementation of Binary TreeLSTM (Constituency Tree LSTM)\nreceiving \nConstituency-based parse trees\n.\nTree-LSTM is a kind of recursive neural networks, as described in the paper \n\nImproved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\n\n by Kai Sheng Tai, Richard Socher, and Christopher Manning.\n\n\nParameters:\n\n \ninputSize\n the size of each input vector\n\n \nhiddenSize\n hidden unit size in GRU\n\n \ngateOutput\n whether gate the output. Default is \ntrue\n\n\n \nwithGraph\n whether create lstms with \ncom.intel.analytics.bigdl.nn.Graph\n. Default is \ntrue\n.\n\n\nScala example:\n\n\n    import com.intel.analytics.bigdl.numeric.NumericFloat\n    import com.intel.analytics.bigdl.utils.RandomGenerator.RNG\n\n    RNG.setSeed(100)\n\n    val hiddenSize = 2\n    val inputSize = 2\n\n    val inputs =\n      Tensor(\n        T(T(T(1f, 2f),\n          T(2f, 3f),\n          T(4f, 5f))))\n\n    val tree =\n      Tensor(\n        T(T(T(2f, 5f, -1f),\n          T(0f, 0f, 1f),\n          T(0f, 0f, 2f),\n          T(0f, 0f, 3f),\n          T(3f, 4f, 0f))))\n\n    val input = T(inputs, tree)\n\n    val gradOutput =\n      Tensor(\n        T(T(T(2f, 5f),\n          T(2f, 3f),\n          T(4f, 5f),\n          T(2f, 3f),\n          T(4f, 5f),\n          T(6f, 7f))))\n\n    val model = BinaryTreeLSTM(inputSize, hiddenSize)\n\n    val output = model.forward(input)\n    println(output)\n    (1,.,.) =\n    -0.07799375 -0.14419462 \n    -0.23495524 -0.04679072 \n    -0.15945151 -0.026039641    \n    -0.0454074  -0.007066241    \n    -0.058696028    -0.13559057 \n\n    [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x2]\n\n    val gradInput = model.backward(input, gradOutput)\n    println(gradInput)\n      {\n        2: (1,.,.) =\n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x3]\n        1: (1,.,.) =\n           0.56145966   -0.3383652  \n           0.81720364   -0.46767634 \n           0.37739626   -0.23355529 \n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x2]\n      }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\nhidden_size = 2\ninput_size = 2\ninputs = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\n\ntree = np.array([[\n  [2.0, 5.0, -1.0],\n  [0.0, 0.0, 1.0],\n  [0.0, 0.0, 2.0],\n  [0.0, 0.0, 3.0],\n  [3.0, 4.0, 0.0]\n]])\n\ninput = [inputs, tree]\n\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0],\n  [2.0, 3.0],\n  [4.0, 5.0],\n  [6.0, 7.0]\n]])\n\nmodel = BinaryTreeLSTM(input_size, hidden_size)\noutput = model.forward(input)\nprint output\n[[[-0.08113038 -0.0289295 ]\n  [ 0.1378704   0.00550814]\n  [ 0.33053339 -0.02395477]\n  [ 0.26895314 -0.02019646]\n  [ 0.34085754 -0.12480961]]]\n\ngradient = model.backward(input, grad_output)\nprint gradient\n[array([[[ 0.43623093,  0.97416967],\n        [-0.02283204,  0.99245077],\n        [-1.11290622,  0.84173977]]], dtype=float32), array([[[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]]], dtype=float32)]", 
            "title": "Recursive Layers"
        }, 
        {
            "location": "/APIGuide/Layers/Recursive-Layers/#tensortree", 
            "text": "TensorTree class is used to decode a tensor to a tree structure.\nThe given input  content  is a tensor which encodes a constituency parse tree.\nThe tensor should have the following structure:  Each row of the tensor represents a tree node and the row number is node number.\nFor each row, except the last column, all other columns represent the children\nnode number of this node. Assume the value of a certain column of the row is not zero,\nthe value  p  means this node has a child whose node number is  p  (lies in the  p -th)\nrow. Each leaf has a leaf number, in the tensor, the last column represents the leaf number.\nEach leaf does not have any children, so all the columns of a leaf except the last should\nbe zero. If a node is the root, the last column should equal to  -1 .  Note: if any row for padding, the padding rows should be placed at the last rows with all\nelements equal to  -1 .  eg. a tensor represents a binary tree:  [11, 10, -1;\n 0, 0, 1;\n 0, 0, 2;\n 0, 0, 3;\n 0, 0, 4;\n 0, 0, 5;\n 0, 0, 6;\n 4, 5, 0;\n 6, 7, 0;\n 8, 9, 0;\n 2, 3, 0;\n -1, -1, -1;\n -1, -1, -1]  Parameters:\n*  content  the tensor to be encoded", 
            "title": "TensorTree"
        }, 
        {
            "location": "/APIGuide/Layers/Recursive-Layers/#treelstm", 
            "text": "TreeLSTM is a base class of all other kinds of tree lstms,\n, as described in the paper  Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks \n by Kai Sheng Tai, Richard Socher, and Christopher Manning.", 
            "title": "TreeLSTM"
        }, 
        {
            "location": "/APIGuide/Layers/Recursive-Layers/#binarytreelstm", 
            "text": "Scala:  val treeLSTM = BinaryTreeLSTM(\n  inputSize,\n  hiddenSize,\n  gateOutput,\n  withGraph)  Python:  tree_lstm = BinaryTreeLSTM(\n  input_size,\n  hidden_size,\n  gate_output,\n  with_graph)  This class is an implementation of Binary TreeLSTM (Constituency Tree LSTM)\nreceiving  Constituency-based parse trees .\nTree-LSTM is a kind of recursive neural networks, as described in the paper  Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks \n by Kai Sheng Tai, Richard Socher, and Christopher Manning.  Parameters:   inputSize  the size of each input vector   hiddenSize  hidden unit size in GRU   gateOutput  whether gate the output. Default is  true    withGraph  whether create lstms with  com.intel.analytics.bigdl.nn.Graph . Default is  true .  Scala example:      import com.intel.analytics.bigdl.numeric.NumericFloat\n    import com.intel.analytics.bigdl.utils.RandomGenerator.RNG\n\n    RNG.setSeed(100)\n\n    val hiddenSize = 2\n    val inputSize = 2\n\n    val inputs =\n      Tensor(\n        T(T(T(1f, 2f),\n          T(2f, 3f),\n          T(4f, 5f))))\n\n    val tree =\n      Tensor(\n        T(T(T(2f, 5f, -1f),\n          T(0f, 0f, 1f),\n          T(0f, 0f, 2f),\n          T(0f, 0f, 3f),\n          T(3f, 4f, 0f))))\n\n    val input = T(inputs, tree)\n\n    val gradOutput =\n      Tensor(\n        T(T(T(2f, 5f),\n          T(2f, 3f),\n          T(4f, 5f),\n          T(2f, 3f),\n          T(4f, 5f),\n          T(6f, 7f))))\n\n    val model = BinaryTreeLSTM(inputSize, hiddenSize)\n\n    val output = model.forward(input)\n    println(output)\n    (1,.,.) =\n    -0.07799375 -0.14419462 \n    -0.23495524 -0.04679072 \n    -0.15945151 -0.026039641    \n    -0.0454074  -0.007066241    \n    -0.058696028    -0.13559057 \n\n    [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x2]\n\n    val gradInput = model.backward(input, gradOutput)\n    println(gradInput)\n      {\n        2: (1,.,.) =\n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n           0.0  0.0 0.0 \n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x5x3]\n        1: (1,.,.) =\n           0.56145966   -0.3383652  \n           0.81720364   -0.46767634 \n           0.37739626   -0.23355529 \n\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x3x2]\n      }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\nhidden_size = 2\ninput_size = 2\ninputs = np.array([[\n  [1.0, 2.0],\n  [2.0, 3.0],\n  [4.0, 5.0]\n]])\n\ntree = np.array([[\n  [2.0, 5.0, -1.0],\n  [0.0, 0.0, 1.0],\n  [0.0, 0.0, 2.0],\n  [0.0, 0.0, 3.0],\n  [3.0, 4.0, 0.0]\n]])\n\ninput = [inputs, tree]\n\ngrad_output = np.array([[\n  [2.0, 3.0],\n  [4.0, 5.0],\n  [2.0, 3.0],\n  [4.0, 5.0],\n  [6.0, 7.0]\n]])\n\nmodel = BinaryTreeLSTM(input_size, hidden_size)\noutput = model.forward(input)\nprint output\n[[[-0.08113038 -0.0289295 ]\n  [ 0.1378704   0.00550814]\n  [ 0.33053339 -0.02395477]\n  [ 0.26895314 -0.02019646]\n  [ 0.34085754 -0.12480961]]]\n\ngradient = model.backward(input, grad_output)\nprint gradient\n[array([[[ 0.43623093,  0.97416967],\n        [-0.02283204,  0.99245077],\n        [-1.11290622,  0.84173977]]], dtype=float32), array([[[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]]], dtype=float32)]", 
            "title": "BinaryTreeLSTM"
        }, 
        {
            "location": "/APIGuide/Layers/Utilities/", 
            "text": "Input\n\n\nScala:\n\n\nval input = Input()\n\n\n\n\nPython:\n\n\ninput = Input()\n\n\n\n\nInput layer do nothing to the input tensors, just passing them through.\nIt is used as input to the \nGraph container\n when the first layer of the graph container accepts multiple tensors as inputs.\n\n\nEach input node of the graph container should accept one tensor as input. If you want a module\naccepting multiple tensors as input, you should add some Input module before it and connect\nthe outputs of the Input nodes to it. Please see the example of the Graph document.\n\n\nPlease note that the return is not a layer but a Node containing input layer.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Input()\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.93366385      0.82551944\n0.71642804      0.4798109\n0.83710635      0.068483874\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nmodule.element.forward(input)\n com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.93366385      0.82551944\n0.71642804      0.4798109\n0.83710635      0.068483874\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Input()\ninput = np.random.rand(3,2)\narray([[ 0.7006678 ,  0.29719472],\n       [ 0.76668255,  0.59518023],\n       [ 0.65543809,  0.41172803]])\n\nmodule.element().forward(input)\narray([[ 0.7006678 ,  0.29719472],\n       [ 0.76668257,  0.59518021],\n       [ 0.65543807,  0.41172802]], dtype=float32)\n\n\n\n\n\nEcho\n\n\nScala:\n\n\nval module = Echo()\n\n\n\n\nPython:\n\n\nmodule = Echo()\n\n\n\n\nThis module is for debug purpose, which can print activation and gradient size in your model topology\n\n\nScala example:\n\n\nval module = Echo()\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nmodule.forward(input)\nres13: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\n\n\n\nPython example:\n\n\nmodule = Echo()\ninput = np.random.rand(3,2)\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]\n\nmodule.forward(input)\ncom.intel.analytics.bigdl.nn.Echo@535c681 : Activation size is 3x2\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]", 
            "title": "Utilities"
        }, 
        {
            "location": "/APIGuide/Layers/Utilities/#input", 
            "text": "Scala:  val input = Input()  Python:  input = Input()  Input layer do nothing to the input tensors, just passing them through.\nIt is used as input to the  Graph container  when the first layer of the graph container accepts multiple tensors as inputs.  Each input node of the graph container should accept one tensor as input. If you want a module\naccepting multiple tensors as input, you should add some Input module before it and connect\nthe outputs of the Input nodes to it. Please see the example of the Graph document.  Please note that the return is not a layer but a Node containing input layer.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval module = Input()\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.93366385      0.82551944\n0.71642804      0.4798109\n0.83710635      0.068483874\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nmodule.element.forward(input)\n com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.93366385      0.82551944\n0.71642804      0.4798109\n0.83710635      0.068483874\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]  Python example:  from bigdl.nn.layer import *\nimport numpy as np\n\nmodule = Input()\ninput = np.random.rand(3,2)\narray([[ 0.7006678 ,  0.29719472],\n       [ 0.76668255,  0.59518023],\n       [ 0.65543809,  0.41172803]])\n\nmodule.element().forward(input)\narray([[ 0.7006678 ,  0.29719472],\n       [ 0.76668257,  0.59518021],\n       [ 0.65543807,  0.41172802]], dtype=float32)", 
            "title": "Input"
        }, 
        {
            "location": "/APIGuide/Layers/Utilities/#echo", 
            "text": "Scala:  val module = Echo()  Python:  module = Echo()  This module is for debug purpose, which can print activation and gradient size in your model topology  Scala example:  val module = Echo()\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nmodule.forward(input)\nres13: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.24058184      0.22737113\n0.0028103297    0.18359558\n0.80443156      0.07047854\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]  Python example:  module = Echo()\ninput = np.random.rand(3,2)\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]\n\nmodule.forward(input)\ncom.intel.analytics.bigdl.nn.Echo@535c681 : Activation size is 3x2\n[array([\n[ 0.87273163,  0.59974301],\n[ 0.09416127,  0.135765  ],\n[ 0.11577505,  0.46095625]], dtype=float32)]", 
            "title": "Echo"
        }, 
        {
            "location": "/APIGuide/Losses/", 
            "text": "L1Cost\n\n\nScala:\n\n\nval layer = L1Cost[Float]()\n\n\n\n\nPython:\n\n\nlayer = L1Cost()\n\n\n\n\nCompute L1 norm for input, and sign of input\n\n\nScala example:\n\n\nval layer = L1Cost[Float]()\nval input = Tensor[Float](2, 2).rand\nval target = Tensor[Float](2, 2).rand\n\nval output = layer.forward(input, target)\nval gradInput = layer.backward(input, target)\n\n\n println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.48145306      0.476887\n0.23729686      0.5169516\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n println(target)\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.42999148      0.22272833\n0.49723643      0.17884709\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n println(output)\noutput: Float = 1.7125885\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nlayer = L1Cost()\n\ninput = np.random.uniform(0, 1, (2, 2)).astype(\nfloat32\n)\ntarget = np.random.uniform(0, 1, (2, 2)).astype(\nfloat32\n)\n\noutput = layer.forward(input, target)\ngradInput = layer.backward(input, target)\n\n\n output\n2.522411\n\n gradInput\n[array([[ 1.,  1.],\n        [ 1.,  1.]], dtype=float32)]\n\n\n\n\n\n\nTimeDistributedCriterion\n\n\nScala:\n\n\nval module = TimeDistributedCriterion(critrn, sizeAverage)\n\n\n\n\nPython:\n\n\nmodule = TimeDistributedCriterion(critrn, sizeAverage)\n\n\n\n\nThis class is intended to support inputs with 3 or more dimensions.\nApply Any Provided Criterion to every temporal slice of an input.\n\n\n\n\ncritrn\n embedded criterion\n\n\nsizeAverage\n whether to divide the sequence length. Default is false.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval criterion = ClassNLLCriterion[Double]()\nval layer = TimeDistributedCriterion[Double](criterion, true)\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404))).resize(3, 2, 3)\nval target = Tensor[Double](3, 2)\n    target(Array(1, 1)) = 1\n    target(Array(1, 2)) = 1\n    target(Array(2, 1)) = 2\n    target(Array(2, 2)) = 2\n    target(Array(3, 1)) = 3\n    target(Array(3, 2)) = 3\n\n print(layer.forward(input, target))\n0.8793184268272332\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\nlayer = TimeDistributedCriterion(criterion, True)\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404]).reshape(3,2,3)\ntarget = np.array([[1,1],[2,2],[3,3]])                      \n\nlayer.forward(input, target)\n0.8793184\n\n\n\n\n\n\nMarginRankingCriterion\n\n\nScala:\n\n\nval mse = new MarginRankingCriterion(margin=1.0, sizeAverage=true)\n\n\n\n\nPython:\n\n\nmse = MarginRankingCriterion(margin=1.0, size_average=true)\n\n\n\n\nCreates a criterion that measures the loss given an input \nx = {x1, x2}\n,\na table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).\nIn batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size\nbatchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.\nIf \ny == 1\n then it assumed the first input should be ranked higher (have a larger value) than\nthe second input, and vice-versa for \ny == -1\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.MarginRankingCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nimport scala.util.Random\n\nval input1Arr = Array(1, 2, 3, 4, 5)\nval input2Arr = Array(5, 4, 3, 2, 1)\n\nval target1Arr = Array(-1, 1, -1, 1, 1)\n\nval input1 = Tensor(Storage(input1Arr.map(x =\n x.toFloat)))\nval input2 = Tensor(Storage(input2Arr.map(x =\n x.toFloat)))\n\nval input = T((1.toFloat, input1), (2.toFloat, input2))\n\nval target1 = Tensor(Storage(target1Arr.map(x =\n x.toFloat)))\nval target = T((1.toFloat, target1))\n\nval mse = new MarginRankingCriterion()\n\nval output = mse.forward(input, target)\nval gradInput = mse.backward(input, target)\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\nGives the output\n\n\noutput: Float = 0.8                                                                                                                                                                    [21/154]\n\n\n\n\nGives the gradInput,\n\n\ngradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: -0.0\n           0.2\n           -0.2\n           0.0\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n        1: 0.0\n           -0.2\n           0.2\n           -0.0\n           -0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmse = MarginRankingCriterion()\n\ninput1 = np.array([1, 2, 3, 4, 5]).astype(\nfloat32\n)\ninput2 = np.array([5, 4, 3, 2, 1]).astype(\nfloat32\n)\ninput = [input1, input2]\n\ntarget1 = np.array([-1, 1, -1, 1, 1]).astype(\nfloat32\n)\ntarget = [target1, target1]\n\noutput = mse.forward(input, target)\ngradInput = mse.backward(input, target)\n\nprint output\nprint gradInput\n\n\n\n\nGives the output,\n\n\n0.8\n\n\n\n\nGives the gradInput,\n\n\n[array([ 0. , -0.2,  0.2, -0. , -0. ], dtype=float32), array([-0. ,  0.2, -0.2,  0. ,  0. ], dtype=float32)] \n\n\n\n\n\n\nClassNLLCriterion\n\n\nScala:\n\n\nval criterion = ClassNLLCriterion(weights = null, sizeAverage = true)\n\n\n\n\nPython:\n\n\ncriterion = ClassNLLCriterion(weights=None, size_average=True)\n\n\n\n\nThe negative log likelihood criterion. It is useful to train a classification problem with n\nclasses. If provided, the optional argument weights should be a 1D Tensor assigning weight to\neach of the classes. This is particularly useful when you have an unbalanced training set.\n\n\nThe input given through a \nforward()\n is expected to contain log-probabilities of each class:\ninput has to be a 1D Tensor of size \nn\n. Obtaining log-probabilities in a neural network is easily\nachieved by adding a \nLogSoftMax\n layer in the last layer of your neural network. You may use\n\nCrossEntropyCriterion\n instead, if you prefer not to add an extra layer to your network. This\ncriterion expects a class index (1 to the number of class) as target when calling\n\nforward(input, target)\n and \nbackward(input, target)\n.\n\n\nThe loss can be described as:\n     \nloss(x, class) = -x[class]\n\n or in the case of the weights argument it is specified as follows:\n     \nloss(x, class) = -weights[class] * x[class]\n\n Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when\n calculating losses in non-batch mode.\n\n\nNote that if the target is \n-1\n, the training process will skip this sample.\n In other words, the forward process will return zero output and the backward process\n will also return zero \ngradInput\n.\n\n\nBy default, the losses are averaged over observations for each minibatch. However, if the field\n \nsizeAverage\n is set to false, the losses are instead summed for each minibatch.\n\n\nParameters:\n\n\n\n\nweights\n weights of each element of the input\n\n\nsizeAverage\n A boolean indicating whether normalizing by the number of elements in the input.\n                  Default: true\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.ClassNLLCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval criterion = ClassNLLCriterion()\nval input = Tensor(T(\n              T(1f, 2f, 3f),\n              T(2f, 3f, 4f),\n              T(3f, 4f, 5f)\n          ))\n\nval target = Tensor(T(1f, 2f, 3f))\n\nval loss = criterion.forward(input, target)\nval grad = criterion.backward(input, target)\n\nprint(loss)\n-3.0\nprintln(grad)\n-0.33333334 0.0 0.0\n0.0 -0.33333334 0.0\n0.0 0.0 -0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\ninput = np.array([\n              [1.0, 2.0, 3.0],\n              [2.0, 3.0, 4.0],\n              [3.0, 4.0, 5.0]\n          ])\n\ntarget = np.array([1.0, 2.0, 3.0])\n\nloss = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n\nprint loss\n-3.0\nprint gradient\n-3.0\n[[-0.33333334  0.          0.        ]\n [ 0.         -0.33333334  0.        ]\n [ 0.          0.         -0.33333334]]\n\n\n\n\n\n\nSoftmaxWithCriterion\n\n\nScala:\n\n\nval model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)\n\n\n\n\nPython:\n\n\nmodel = SoftmaxWithCriterion(ignoreLabel, normalizeMode)\n\n\n\n\nComputes the multinomial logistic loss for a one-of-many classification task, passing real-valued predictions through a softmax to\nget a probability distribution over classes. It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as \nits gradient computation is more numerically stable.\n\n\n\n\nignoreLabel\n   (optional) Specify a label value that should be ignored when computing the loss.\n\n\nnormalizeMode\n How to normalize the output loss.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\n\nval input = Tensor(1, 5, 2, 3).rand()\nval target = Tensor(Storage(Array(2.0f, 4.0f, 2.0f, 4.0f, 1.0f, 2.0f))).resize(1, 1, 2, 3)\n\nval model = SoftmaxWithCriterion[Float]()\nval output = model.forward(input, target)\n\nscala\n print(input)\n(1,1,.,.) =\n0.65131104  0.9332143   0.5618989   \n0.9965054   0.9370902   0.108070895 \n\n(1,2,.,.) =\n0.46066576  0.9636703   0.8123812   \n0.31076035  0.16386998  0.37894428  \n\n(1,3,.,.) =\n0.49111295  0.3704862   0.9938375   \n0.87996656  0.8695406   0.53354675  \n\n(1,4,.,.) =\n0.8502225   0.9033509   0.8518651   \n0.0692618   0.10121379  0.970959    \n\n(1,5,.,.) =\n0.9397213   0.49688303  0.75739735  \n0.25074655  0.11416598  0.6594504   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x2x3]\n\nscala\n print(output)\n1.6689054\n\n\n\n\nPython example:\n\n\ninput = np.random.randn(1, 5, 2, 3)\ntarget = np.array([[[[2.0, 4.0, 2.0], [4.0, 1.0, 2.0]]]])\n\nmodel = SoftmaxWithCriterion()\noutput = model.forward(input, target)\n\n\n print input\n[[[[ 0.78455689  0.01402084  0.82539628]\n   [-1.06448238  2.58168413  0.60053703]]\n\n  [[-0.48617618  0.44538094  0.46611658]\n   [-1.41509329  0.40038991 -0.63505732]]\n\n  [[ 0.91266769  1.68667933  0.92423611]\n   [ 0.1465411   0.84637557  0.14917515]]\n\n  [[-0.7060493  -2.02544114  0.89070726]\n   [ 0.14535539  0.73980064 -0.33130613]]\n\n  [[ 0.64538791 -0.44384233 -0.40112523]\n   [ 0.44346658 -2.22303621  0.35715986]]]]\n\n\n print output\n2.1002123\n\n\n\n\n\n\n\nSmoothL1Criterion\n\n\nScala:\n\n\nval slc = SmoothL1Criterion(sizeAverage=true)\n\n\n\n\nPython:\n\n\nslc = SmoothL1Criterion(size_average=True)\n\n\n\n\nCreates a criterion that can be thought of as a smooth version of the AbsCriterion.\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some\ncases prevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.SmoothL1Criterion\n\nval slc = SmoothL1Criterion()\n\nval inputArr = Array(\n  0.17503996845335,\n  0.83220188552514,\n  0.48450597329065,\n  0.64701424003579,\n  0.62694586534053,\n  0.34398410236463,\n  0.55356747563928,\n  0.20383032318205\n)\nval targetArr = Array(\n  0.69956525065936,\n  0.86074831243604,\n  0.54923197557218,\n  0.57388074393384,\n  0.63334444304928,\n  0.99680578662083,\n  0.49997645849362,\n  0.23869121982716\n)\n\nval input = Tensor(Storage(inputArr.map(x =\n x.toFloat))).reshape(Array(2, 2, 2))\nval target = Tensor(Storage(targetArr.map(x =\n x.toFloat))).reshape(Array(2, 2, 2))\n\nval output = slc.forward(input, target)\nval gradInput = slc.backward(input, target)\n\n\n\n\nGives the output,\n\n\noutput: Float = 0.0447365\n\n\n\n\nGives the gradInput,\n\n\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.06556566     -0.003568299\n-0.008090746    0.009141691\n\n(2,.,.) =\n-7.998273E-4    -0.08160271\n0.0066988766    -0.0043576136\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nslc = SmoothL1Criterion()\n\ninput = np.array([\n    0.17503996845335,\n    0.83220188552514,\n    0.48450597329065,\n    0.64701424003579,\n    0.62694586534053,\n    0.34398410236463,\n    0.55356747563928,\n    0.20383032318205\n])\ninput.reshape(2, 2, 2)\n\ntarget = np.array([\n    0.69956525065936,\n    0.86074831243604,\n    0.54923197557218,\n    0.57388074393384,\n    0.63334444304928,\n    0.99680578662083,\n    0.49997645849362,\n    0.23869121982716\n])\n\ntarget.reshape(2, 2, 2)\n\noutput = slc.forward(input, target)\ngradInput = slc.backward(input, target)\n\nprint output\nprint gradInput\n\n\n\n\n\n\nSmoothL1CriterionWithWeights\n\n\nScala:\n\n\nval smcod = SmoothL1CriterionWithWeights[Float](sigma: Float = 2.4f, num: Int = 2)\n\n\n\n\nPython:\n\n\nsmcod = SmoothL1CriterionWithWeights(sigma, num)\n\n\n\n\na smooth version of the AbsCriterion\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some cases\nprevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).\n\n\n   d = (x - y) * w_in\n\n  loss(x, y, w_in, w_out)\n              | 0.5 * (sigma * d_i)^2 * w_out          if |d_i| \n 1 / sigma / sigma\n   = 1/n \\sum |\n              | (|d_i| - 0.5 / sigma / sigma) * w_out   otherwise\n\n\n\n\nScala example:\n\n\nval smcod = SmoothL1CriterionWithWeights[Float](2.4f, 2)\n\nval inputArr = Array(1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03)\nval targetArr = Array(0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58)\nval inWArr = Array(-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8)\nval outWArr = Array(-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1)\n\nval input = Tensor(Storage(inputArr.map(x =\n x.toFloat)))\nval target = T()\ntarget.insert(Tensor(Storage(targetArr.map(x =\n x.toFloat))))\ntarget.insert(Tensor(Storage(inWArr.map(x =\n x.toFloat))))\ntarget.insert(Tensor(Storage(outWArr.map(x =\n x.toFloat))))\n\nval output = smcod.forward(input, target)\nval gradInput = smcod.backward(input, target)\n\n\n println(output)\n  output: Float = -2.17488\n\n println(gradInput)\n-0.010944003\n0.425\n0.63037443\n-0.95\n-0.1\n0.07\n0.120000005\n-0.44000003\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 8]\n\n\n\n\nPython example:\n\n\nsmcod = SmoothL1CriterionWithWeights(2.4, 2)\n\ninput = np.array([1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03]).astype(\nfloat32\n)\ntargetArr = np.array([0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58]).astype(\nfloat32\n)\ninWArr = np.array([-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8]).astype(\nfloat32\n)\noutWArr = np.array([-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1]).astype(\nfloat32\n)\ntarget = [targetArr, inWArr, outWArr]\n\noutput = smcod.forward(input, target)\ngradInput = smcod.backward(input, target)\n\n\n output\n-2.17488\n\n gradInput\n[array([-0.010944  ,  0.42500001,  0.63037443, -0.94999999, -0.1       ,\n         0.07      ,  0.12      , -0.44000003], dtype=float32)]\n\n\n\n\n\n\nMultiMarginCriterion\n\n\nScala:\n\n\nval loss = MultiMarginCriterion(p=1,weights=null,margin=1.0,sizeAverage=true)\n\n\n\n\nPython:\n\n\nloss = MultiMarginCriterion(p=1,weights=None,margin=1.0,size_average=True)\n\n\n\n\nMultiMarginCriterion is a loss function that optimizes a multi-class classification hinge loss (margin-based loss) between input \nx\n and output \ny\n (\ny\n is the target class index).\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(3,2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f, 2.0f)))\nval loss = MultiMarginCriterion(1)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala\n print(input)\n-0.45896783     -0.80141246\n0.22560088      -0.13517438\n0.2601126       0.35492152\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala\n print(target)\n2.0\n1.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala\n print(output)\n0.4811434\n\nscala\n print(grad)\n0.16666667      -0.16666667\n-0.16666667     0.16666667\n0.16666667      -0.16666667\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(3,2)\ntarget = np.array([2,1,2])\nprint \ninput=\n,input\nprint \ntarget=\n,target\n\nloss = MultiMarginCriterion(1)\nout = loss.forward(input, target)\nprint \noutput of loss is : \n,out\n\ngrad_out = loss.backward(input,target)\nprint \ngrad out of loss is : \n,grad_out\n\n\n\n\nGives the output,\n\n\ninput= [[ 0.46868305 -2.28562261]\n [ 0.8076243  -0.67809689]\n [-0.20342555 -0.66264743]]\ntarget= [2 1 2]\ncreating: createMultiMarginCriterion\noutput of loss is :  0.8689213\ngrad out of loss is :  [[ 0.16666667 -0.16666667]\n [ 0.          0.        ]\n [ 0.16666667 -0.16666667]]\n\n\n\n\n\n\n\n\nHingeEmbeddingCriterion\n\n\nScala:\n\n\nval m = HingeEmbeddingCriterion(margin = 1, sizeAverage = true)\n\n\n\n\nPython:\n\n\nm = HingeEmbeddingCriterion(margin=1, size_average=True)\n\n\n\n\nCreates a criterion that measures the loss given an input \nx\n which is a 1-dimensional vector and a label \ny\n (\n1\n or \n-1\n).\nThis is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.\n\n\n                 \u23a7 x_i,                  if y_i ==  1\nloss(x, y) = 1/n \u23a8\n                 \u23a9 max(0, margin - x_i), if y_i == -1\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval loss = HingeEmbeddingCriterion(1, sizeAverage = false)\nval input = Tensor(T(0.1f, 2.0f, 2.0f, 2.0f))\nprintln(\ninput: \\n\n + input)\nprintln(\nouput: \n)\n\nprintln(\nTarget=1: \n + loss.forward(input, Tensor(4, 1).fill(1f)))\n\nprintln(\nTarget=-1: \n + loss.forward(input, Tensor(4, 1).fill(-1f)))\n\n\n\n\ninput: \n0.1\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\nouput: \nTarget=1: 6.1\nTarget=-1: 0.9\n\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.criterion import *\ninput = np.array([0.1, 2.0, 2.0, 2.0])\ntarget = np.full(4, 1)\nprint(\ninput: \n )\nprint(input)\nprint(\ntarget: \n)\nprint(target)\nprint(\noutput: \n)\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, target))\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, np.full(4, -1)))\n\n\n\n\ninput: \n[ 0.1  2.   2.   2. ]\ntarget: \n[1 1 1 1]\noutput: \ncreating: createHingeEmbeddingCriterion\n6.1\ncreating: createHingeEmbeddingCriterion\n0.9\n\n\n\n\n\n\nMarginCriterion\n\n\nScala:\n\n\ncriterion = MarginCriterion(margin=1.0, sizeAverage=true)\n\n\n\n\nPython:\n\n\ncriterion = MarginCriterion(margin=1.0, sizeAverage=true, bigdl_type=\nfloat\n)\n\n\n\n\nCreates a criterion that optimizes a two-class classification hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.\n * \nmargin\n if unspecified, is by default 1.\n * \nsizeAverage\n whether to average the loss, is by default true\n\n\nScala example:\n\n\nval criterion = MarginCriterion(margin=1.0, sizeAverage=true)\n\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.33753583      0.3575501\n0.23477706      0.7240361\n0.92835575      0.4737949\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nval target = Tensor(3, 2).rand()\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.27280563      0.7022703\n0.3348442       0.43332106\n0.08935371      0.17876455\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\ncriterion.forward(input, target)\nres5: Float = 0.84946966\n\n\n\n\nPython example:\n\n\ncriterion = MarginCriterion(margin=1.0,size_average=True,bigdl_type=\nfloat\n)\ninput = np.random.rand(3, 2)\narray([[ 0.20824672,  0.67299837],\n       [ 0.80561452,  0.19564743],\n       [ 0.42501441,  0.19408184]])\n\ntarget = np.random.rand(3, 2)\narray([[ 0.67882632,  0.61257846],\n       [ 0.10111138,  0.75225082],\n       [ 0.60404296,  0.31373273]])\n\ncriterion.forward(input, target)\n0.8166871\n\n\n\n\n\n\nCosineEmbeddingCriterion\n\n\nScala:\n\n\nval cosineEmbeddingCriterion = CosineEmbeddingCriterion(margin  = 0.0, sizeAverage = true)\n\n\n\n\nPython:\n\n\ncosineEmbeddingCriterion = CosineEmbeddingCriterion( margin=0.0,size_average=True)\n\n\n\n\nCosineEmbeddingCriterion creates a criterion that measures the loss given an input x = {x1, x2},\na table of two Tensors, and a Tensor label y with values 1 or -1.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimpot com.intel.analytics.bigdl.utils.T\nval cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, false)\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T()\ninput(1.0) = input1\ninput(2.0) = input2\nval target1 = Tensor(Storage(Array(-0.5f)))\nval target = T()\ntarget(1.0) = target1\n\n\n print(input)\n {\n    2.0: 0.4110882\n         0.57726574\n         0.1949834\n         0.67670715\n         0.16984987\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1.0: 0.16878392\n         0.24124223\n         0.8964794\n         0.11156334\n         0.5101486\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }\n\n\n print(cosineEmbeddingCriterion.forward(input, target))\n0.49919847\n\n\n print(cosineEmbeddingCriterion.backward(input, target))\n {\n    2: -0.045381278\n       -0.059856333\n       0.72547954\n       -0.2268434\n       0.3842142\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n    1: 0.30369008\n       0.42463788\n       -0.20637506\n       0.5712836\n       -0.06355385\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\ncosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, False)\n\n cosineEmbeddingCriterion.forward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n0.6363636\n\n cosineEmbeddingCriterion.backward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n[array([ 0.07933884,  0.04958678,  0.01983471, -0.00991735, -0.03966942], dtype=float32), array([-0.03966942, -0.00991735,  0.01983471,  0.04958678,  0.07933884], dtype=float32)]\n\n\n\n\n\n\n\nBCECriterion\n\n\nScala:\n\n\nval criterion = BCECriterion[Float]()\n\n\n\n\nPython:\n\n\ncriterion = BCECriterion()\n\n\n\n\nThis loss function measures the Binary Cross Entropy between the target and the output\n\n\n loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\n\n\n\n\nor in the case of the weights argument being specified:\n\n\n loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\n\n\n\n\nBy default, the losses are averaged for each mini-batch over observations as well as over\n dimensions. However, if the field sizeAverage is set to false, the losses are instead summed.\n\n\nScala example:\n\n\n\nval criterion = BCECriterion[Float]()\nval input = Tensor[Float](3, 1).rand\n\nval target = Tensor[Float](3)\ntarget(1) = 1\ntarget(2) = 0\ntarget(3) = 1\n\nval output = criterion.forward(input, target)\nval gradInput = criterion.backward(input, target)\n\n\n println(target)\nres25: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n\n\n println(output)\noutput: Float = 0.9009579\n\n\n println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-1.5277504\n1.0736246\n-0.336957\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]\n\n\n\n\n\nPython example:\n\n\n\ncriterion = BCECriterion()\ninput = np.random.uniform(0, 1, (3, 1)).astype(\nfloat32\n)\ntarget = np.array([1, 0, 1])\noutput = criterion.forward(input, target)\ngradInput = criterion.backward(input, target)\n\n\n output\n1.9218739\n\n gradInput\n[array([[-4.3074522 ],\n        [ 2.24244714],\n        [-1.22368968]], dtype=float32)]\n\n\n\n\n\n\n\nDiceCoefficientCriterion\n\n\nScala:\n\n\nval loss = DiceCoefficientCriterion(sizeAverage=true, epsilon=1.0f)\n\n\n\n\nPython:\n\n\nloss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)\n\n\n\n\nDiceCoefficientCriterion is the Dice-Coefficient objective function. \n\n\nBoth \nforward\n and \nbackward\n accept two tensors : input and target. The \nforward\n result is formulated as \n          \n1 - (2 * (input intersection target) / (input union target))\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DiceCoefficientCriterion(epsilon = 1.0f)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala\n print(input)\n-0.50278\n0.51387966\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(output)\n0.9958517\n\nscala\n print(grad)\n-0.99619853     -0.49758217\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1],dtype='float64')\n\nprint \ninput=\n, input\nprint \ntarget=\n, target\nloss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)\nout = loss.forward(input,target)\nprint \noutput of loss is :\n,out\n\ngrad_out = loss.backward(input,target)\nprint \ngrad out of loss is :\n,grad_out\n\n\n\n\nproduces output:\n\n\ninput= [ 0.4440505  2.9430301]\ntarget= [ 2.  1.]\ncreating: createDiceCoefficientCriterion\noutput of loss is : -0.17262316\ngrad out of loss is : [[-0.38274616 -0.11200322]]\n\n\n\n\n\n\nMSECriterion\n\n\nScala:\n\n\nval criterion = MSECriterion()\n\n\n\n\nPython:\n\n\ncriterion = MSECriterion()\n\n\n\n\nThe mean squared error criterion e.g. input: a, target: b, total elements: n\n\n\nloss(a, b) = 1/n * sum(|a_i - b_i|^2)\n\n\n\n\nParameters:\n\n\n\n\nsizeAverage\n a boolean indicating whether to divide the sum of squared error by n. \n                 Default: true\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MSECriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(2.0f, 3.0f),\n T(4.0f, 5.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-\n print(output)\n1.0\n-\n print(gradient)\n-0.5    -0.5    \n-0.5    -0.5    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = MSECriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n-\n print output\n1.0\n-\n print gradient\n[[-0.5 -0.5]\n [-0.5 -0.5]]\n\n\n\n\n\n\nSoftMarginCriterion\n\n\nScala:\n\n\nval criterion = SoftMarginCriterion(sizeAverage)\n\n\n\n\nPython:\n\n\ncriterion = SoftMarginCriterion(size_average)\n\n\n\n\nCreates a criterion that optimizes a two-class classification logistic loss between\ninput x (a Tensor of dimension 1) and output y (which is a tensor containing either\n1s or -1s).\n\n\nloss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()\n\n\n\n\nParameters:\n* \nsizeAverage\n A boolean indicating whether normalizing by the number of elements in the input.\n                    Default: true\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = SoftMarginCriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(1.0f, -1.0f),\n T(-1.0f, 1.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-\n print(output)\n1.3767318\n-\n print(gradient)\n-0.06723536     0.22019927      \n0.23814353      -0.0044965525   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = SoftMarginCriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-\n print output\n1.3767318\n-\n print gradient\n[[-0.06723536  0.22019927]\n [ 0.23814353 -0.00449655]]\n\n\n\n\n\n\nDistKLDivCriterion\n\n\nScala:\n\n\nval loss = DistKLDivCriterion[T](sizeAverage=true)\n\n\n\n\nPython:\n\n\nloss = DistKLDivCriterion(size_average=True)\n\n\n\n\nDistKLDivCriterion is the Kullback\u2013Leibler divergence loss.\n\n\nScala example:\n\n\n\nscala\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DistKLDivCriterion()\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala\n print(input)\n-0.3854126\n-0.7707398\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala\n print(output)\n1.4639297\n\nscala\n print(grad)\n-1.0\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1])\n\nprint \ninput=\n, input\nprint \ntarget=\n, target\nloss = DistKLDivCriterion()\nout = loss.forward(input,target)\nprint \noutput of loss is :\n,out\n\ngrad_out = loss.backward(input,target)\nprint \ngrad out of loss is :\n,grad_out\n\n\n\n\nGives the output\n\n\ninput= [-1.14333924  0.97662296]\ntarget= [2 1]\ncreating: createDistKLDivCriterion\noutput of loss is : 1.348175\ngrad out of loss is : [-1.  -0.5]\n\n\n\n\n\n\nClassSimplexCriterion\n\n\nScala:\n\n\nval criterion = ClassSimplexCriterion(nClasses)\n\n\n\n\nPython:\n\n\ncriterion = ClassSimplexCriterion(nClasses)\n\n\n\n\nClassSimplexCriterion implements a criterion for classification.\nIt learns an embedding per class, where each class' embedding is a\npoint on an (N-1)-dimensional simplex, where N is the number of classes.\n\n\nParameters:\n* \nnClasses\n An integer, the number of classes.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = ClassSimplexCriterion(5)\nval input = Tensor(T(\n T(1.0f, 2.0f, 3.0f, 4.0f, 5.0f),\n T(4.0f, 5.0f, 6.0f, 7.0f, 8.0f)\n))\nval target = Tensor(2)\ntarget(1) = 2.0f\ntarget(2) = 1.0f\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-\n print(output)\n23.562702\n-\n print(gradient)\n0.25    0.20635083      0.6     0.8     1.0     \n0.6     1.0     1.2     1.4     1.6     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = ClassSimplexCriterion(5)\ninput = np.array([\n   [1.0, 2.0, 3.0, 4.0, 5.0],\n   [4.0, 5.0, 6.0, 7.0, 8.0]\n])\ntarget = np.array([2.0, 1.0])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-\n print output\n23.562702\n-\n print gradient\n[[ 0.25        0.20635083  0.60000002  0.80000001  1.        ]\n [ 0.60000002  1.          1.20000005  1.39999998  1.60000002]]\n\n\n\n\n\n\nL1HingeEmbeddingCriterion\n\n\nScala:\n\n\nval model = L1HingeEmbeddingCriterion(margin)\n\n\n\n\nPython:\n\n\nmodel = L1HingeEmbeddingCriterion(margin)\n\n\n\n\nCreates a criterion that measures the loss given an input \nx = {x1, x2}\n, a table of two Tensors, and a label y (1 or -1).\nThis is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.\n\n\n             \u23a7 ||x1 - x2||_1,                  if y ==  1\nloss(x, y) = \u23a8\n             \u23a9 max(0, margin - ||x1 - x2||_1), if y == -1\n\n\n\n\nThe margin has a default value of 1, or can be set in the constructor.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = L1HingeEmbeddingCriterion(0.6)\nval input1 = Tensor(T(1.0f, -0.1f))\nval input2 = Tensor(T(2.0f, -0.2f))\nval input = T(input1, input2)\nval target = Tensor(1)\ntarget(Array(1)) = 1.0f\n\nval output = model.forward(input, target)\n\nscala\n print(output)\n1.1\n\n\n\n\nPython example:\n\n\nmodel = L1HingeEmbeddingCriterion(0.6)\ninput1 = np.array(1.0, -0.1)\ninput2 = np.array(2.0, -0.2)\ninput = [input1, input2]\ntarget = np.array([1.0])\n\noutput = model.forward(input, target)\n\n\n print output\n1.1\n\n\n\n\n\n\nCrossEntropyCriterion\n\n\nScala:\n\n\nval module = CrossEntropyCriterion(weights, sizeAverage)\n\n\n\n\nPython:\n\n\nmodule = CrossEntropyCriterion(weights, sizeAverage)\n\n\n\n\nThis criterion combines LogSoftMax and ClassNLLCriterion in one single class.\n\n\n\n\nweights\n A tensor assigning weight to each of the classes\n\n\nsizeAverage\n whether to divide the sequence length. Default is true.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval layer = CrossEntropyCriterion[Double]()\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404\n    ))).resize(3, 3)\nval target = Tensor[Double](3)\n    target(Array(1)) = 1\n    target(Array(2)) = 2\n    target(Array(3)) = 3\n\n print(layer.forward(input, target))\n0.9483051199107635\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\n\nlayer = CrossEntropyCriterion()\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404\n                      ]).reshape(3,3)\ntarget = np.array([1, 2, 3])                      \n\nlayer.forward(input, target)\n0.94830513\n\n\n\n\n\n\nParallelCriterion\n\n\nScala:\n\n\nval pc = ParallelCriterion(repeatTarget=false)\n\n\n\n\nPython:\n\n\npc = ParallelCriterion(repeat_target=False)\n\n\n\n\nParallelCriterion is a weighted sum of other criterions each applied to a different input\nand target. Set repeatTarget = true to share the target for criterions.\nUse add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.{ParallelCriterion, ClassNLLCriterion, MSECriterion}\n\nval pc = ParallelCriterion()\n\nval input = T(Tensor(2, 10), Tensor(2, 10))\nvar i = 0\ninput[Tensor](1).apply1(_ =\n {i += 1; i})\ninput[Tensor](2).apply1(_ =\n {i -= 1; i})\nval target = T(Tensor(Storage(Array(1.0f, 8.0f))), Tensor(2, 10).fill(1.0f))\n\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\npc.add(nll, 0.5).add(mse)\n\nval output = pc.forward(input, target)\nval gradInput = pc.backward(input, target)\n\nprintln(output)\nprintln(gradInput)\n\n\n\n\n\nGives the output,\n\n\n100.75\n\n\n\n\n\nGives the gradInput,\n\n\n {\n        2: 1.8000001    1.7     1.6     1.5     1.4     1.3000001       1.2     1.1     1.0     0.90000004\n           0.8  0.7     0.6     0.5     0.4     0.3     0.2     0.1     0.0     -0.1\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n        1: -0.25        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n           0.0  0.0     0.0     0.0     0.0     0.0     0.0     -0.25   0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npc = ParallelCriterion()\n\ninput1 = np.arange(1, 21, 1).astype(\nfloat32\n)\ninput2 = np.arange(0, 20, 1).astype(\nfloat32\n)[::-1]\ninput1 = input1.reshape(2, 10)\ninput2 = input2.reshape(2, 10)\n\ninput = [input1, input2]\n\ntarget1 = np.array([1.0, 8.0]).astype(\nfloat32\n)\ntarget1 = target1.reshape(2)\ntarget2 = np.full([2, 10], 1).astype(\nfloat32\n)\ntarget2 = target2.reshape(2, 10)\ntarget = [target1, target2]\n\nnll = ClassNLLCriterion()\nmse = MSECriterion()\n\npc.add(nll, weight = 0.5).add(mse)\n\nprint \ninput = \\n %s \n % input\nprint \ntarget = \\n %s\n % target\n\noutput = pc.forward(input, target)\ngradInput = pc.backward(input, target)\n\nprint \noutput = %s \n % output\nprint \ngradInput = %s \n % gradInput\n\n\n\n\nGives the output,\n\n\ninput = \n [array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n       [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]], dtype=float32), array([[ 19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,  10.],\n       [  9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.]], dtype=float32)] \ntarget = \n [array([ 1.,  8.], dtype=float32), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)]\noutput = 100.75 \ngradInput = [array([[-0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ]], dtype=float32), array([[ 1.80000007,  1.70000005,  1.60000002,  1.5       ,  1.39999998,\n         1.30000007,  1.20000005,  1.10000002,  1.        ,  0.90000004],\n       [ 0.80000001,  0.69999999,  0.60000002,  0.5       ,  0.40000001,\n         0.30000001,  0.2       ,  0.1       ,  0.        , -0.1       ]], dtype=float32)]\n\n\n\n\n\n\nMultiLabelMarginCriterion\n\n\nScala:\n\n\nval multiLabelMarginCriterion = MultiLabelMarginCriterion(sizeAverage = true)\n\n\n\n\nPython:\n\n\nmultiLabelMarginCriterion = MultiLabelMarginCriterion(size_average=True)\n\n\n\n\nMultiLabelMarginCriterion creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x and output y \n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval multiLabelMarginCriterion = MultiLabelMarginCriterion(false)\nval input = Tensor(4).rand()\nval target = Tensor(4)\ntarget(Array(1)) = 3\ntarget(Array(2)) = 2\ntarget(Array(3)) = 1\ntarget(Array(4)) = 0\n\n\n print(input)\n0.40267515\n0.5913795\n0.84936756\n0.05999674\n\n\n  print(multiLabelMarginCriterion.forward(input, target))\n0.33414197\n\n\n print(multiLabelMarginCriterion.backward(input, target))\n-0.25\n-0.25\n-0.25\n0.75\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nmultiLabelMarginCriterion = MultiLabelMarginCriterion(False)\n\n\n multiLabelMarginCriterion.forward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n0.975\n\n\n multiLabelMarginCriterion.backward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n[array([-0.25, -0.25, -0.25,  0.75], dtype=float32)]\n\n\n\n\n\n\n\nMultiLabelSoftMarginCriterion\n\n\nScala:\n\n\nval criterion = MultiLabelSoftMarginCriterion(weights = null, sizeAverage = true)\n\n\n\n\nPython:\n\n\ncriterion = MultiLabelSoftMarginCriterion(weights=None, size_average=True)\n\n\n\n\nMultiLabelSoftMarginCriterion is a multiLabel multiclass criterion based on sigmoid:\n\n\nl(x,y) = - sum_i y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i])\n\n\n\n\nwhere \np[i] = exp(x[i]) / (1 + exp(x[i]))\n\n\nIf with weights,\n \nl(x,y) = - sum_i weights[i] (y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i]))\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiLabelSoftMarginCriterion()\nval input = Tensor(3)\ninput(Array(1)) = 0.4f\ninput(Array(2)) = 0.5f\ninput(Array(3)) = 0.6f\nval target = Tensor(3)\ntarget(Array(1)) = 0\ntarget(Array(2)) = 1\ntarget(Array(3)) = 1\n\n\n criterion.forward(input, target)\nres0: Float = 0.6081934\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiLabelSoftMarginCriterion()\ninput = np.array([0.4, 0.5, 0.6])\ntarget = np.array([0, 1, 1])\n\n\n criterion.forward(input, target)\n0.6081934\n\n\n\n\n\n\nAbsCriterion\n\n\nScala:\n\n\nval criterion = AbsCriterion(sizeAverage)\n\n\n\n\nPython:\n\n\ncriterion = AbsCriterion(sizeAverage)\n\n\n\n\nMeasures the mean absolute value of the element-wise difference between input and target\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = AbsCriterion()\nval input = Tensor(T(1.0f, 2.0f, 3.0f))\nval target = Tensor(T(4.0f, 5.0f, 6.0f))\nval output = criterion.forward(input, target)\n\nscala\n print(output)\n3.0\n\n\n\n\nPython example:\n\n\ncriterion = AbsCriterion()\ninput = np.array([1.0, 2.0, 3.0])\ntarget = np.array([4.0, 5.0, 6.0])\noutput=criterion.forward(input, target)\n\n\n print output\n3.0\n\n\n\n\n\n\nMultiCriterion\n\n\nScala:\n\n\nval criterion = MultiCriterion()\n\n\n\n\nPython:\n\n\ncriterion = MultiCriterion()\n\n\n\n\nMultiCriterion is a weighted sum of other criterions each applied to the same input and target\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiCriterion()\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\nval input = Tensor(5).randn()\nval target = Tensor(5)\ntarget(Array(1)) = 1\ntarget(Array(2)) = 2\ntarget(Array(3)) = 3\ntarget(Array(4)) = 2\ntarget(Array(5)) = 1\n\nval output = criterion.forward(input, target)\n\n\n input\n1.0641425\n-0.33507252\n1.2345984\n0.08065767\n0.531199\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n\n\n\n output\nres7: Float = 1.9633228\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiCriterion()\nnll = ClassNLLCriterion()\nmse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\ninput = np.array([0.9682213801388531,\n0.35258855644097503,\n0.04584479998452568,\n-0.21781499692588918,\n-1.02721844006879])\ntarget = np.array([1, 2, 3, 2, 1])\n\noutput = criterion.forward(input, target)\n\n\n output\n3.6099546", 
            "title": "Losses"
        }, 
        {
            "location": "/APIGuide/Losses/#l1cost", 
            "text": "Scala:  val layer = L1Cost[Float]()  Python:  layer = L1Cost()  Compute L1 norm for input, and sign of input  Scala example:  val layer = L1Cost[Float]()\nval input = Tensor[Float](2, 2).rand\nval target = Tensor[Float](2, 2).rand\n\nval output = layer.forward(input, target)\nval gradInput = layer.backward(input, target)  println(input)\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.48145306      0.476887\n0.23729686      0.5169516\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  println(target)\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.42999148      0.22272833\n0.49723643      0.17884709\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  println(output)\noutput: Float = 1.7125885  println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  layer = L1Cost()\n\ninput = np.random.uniform(0, 1, (2, 2)).astype( float32 )\ntarget = np.random.uniform(0, 1, (2, 2)).astype( float32 )\n\noutput = layer.forward(input, target)\ngradInput = layer.backward(input, target)  output\n2.522411  gradInput\n[array([[ 1.,  1.],\n        [ 1.,  1.]], dtype=float32)]", 
            "title": "L1Cost"
        }, 
        {
            "location": "/APIGuide/Losses/#timedistributedcriterion", 
            "text": "Scala:  val module = TimeDistributedCriterion(critrn, sizeAverage)  Python:  module = TimeDistributedCriterion(critrn, sizeAverage)  This class is intended to support inputs with 3 or more dimensions.\nApply Any Provided Criterion to every temporal slice of an input.   critrn  embedded criterion  sizeAverage  whether to divide the sequence length. Default is false.   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval criterion = ClassNLLCriterion[Double]()\nval layer = TimeDistributedCriterion[Double](criterion, true)\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404))).resize(3, 2, 3)\nval target = Tensor[Double](3, 2)\n    target(Array(1, 1)) = 1\n    target(Array(1, 2)) = 1\n    target(Array(2, 1)) = 2\n    target(Array(2, 2)) = 2\n    target(Array(3, 1)) = 3\n    target(Array(3, 2)) = 3  print(layer.forward(input, target))\n0.8793184268272332  Python example:  from bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\nlayer = TimeDistributedCriterion(criterion, True)\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404]).reshape(3,2,3)\ntarget = np.array([[1,1],[2,2],[3,3]])                       layer.forward(input, target)\n0.8793184", 
            "title": "TimeDistributedCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#marginrankingcriterion", 
            "text": "Scala:  val mse = new MarginRankingCriterion(margin=1.0, sizeAverage=true)  Python:  mse = MarginRankingCriterion(margin=1.0, size_average=true)  Creates a criterion that measures the loss given an input  x = {x1, x2} ,\na table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).\nIn batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size\nbatchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.\nIf  y == 1  then it assumed the first input should be ranked higher (have a larger value) than\nthe second input, and vice-versa for  y == -1 .  Scala example:  import com.intel.analytics.bigdl.nn.MarginRankingCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nimport scala.util.Random\n\nval input1Arr = Array(1, 2, 3, 4, 5)\nval input2Arr = Array(5, 4, 3, 2, 1)\n\nval target1Arr = Array(-1, 1, -1, 1, 1)\n\nval input1 = Tensor(Storage(input1Arr.map(x =  x.toFloat)))\nval input2 = Tensor(Storage(input2Arr.map(x =  x.toFloat)))\n\nval input = T((1.toFloat, input1), (2.toFloat, input2))\n\nval target1 = Tensor(Storage(target1Arr.map(x =  x.toFloat)))\nval target = T((1.toFloat, target1))\n\nval mse = new MarginRankingCriterion()\n\nval output = mse.forward(input, target)\nval gradInput = mse.backward(input, target)\n\nprintln(output)\nprintln(gradInput)  Gives the output  output: Float = 0.8                                                                                                                                                                    [21/154]  Gives the gradInput,  gradInput: com.intel.analytics.bigdl.utils.Table =\n {\n        2: -0.0\n           0.2\n           -0.2\n           0.0\n           0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n        1: 0.0\n           -0.2\n           0.2\n           -0.0\n           -0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nmse = MarginRankingCriterion()\n\ninput1 = np.array([1, 2, 3, 4, 5]).astype( float32 )\ninput2 = np.array([5, 4, 3, 2, 1]).astype( float32 )\ninput = [input1, input2]\n\ntarget1 = np.array([-1, 1, -1, 1, 1]).astype( float32 )\ntarget = [target1, target1]\n\noutput = mse.forward(input, target)\ngradInput = mse.backward(input, target)\n\nprint output\nprint gradInput  Gives the output,  0.8  Gives the gradInput,  [array([ 0. , -0.2,  0.2, -0. , -0. ], dtype=float32), array([-0. ,  0.2, -0.2,  0. ,  0. ], dtype=float32)]", 
            "title": "MarginRankingCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#classnllcriterion", 
            "text": "Scala:  val criterion = ClassNLLCriterion(weights = null, sizeAverage = true)  Python:  criterion = ClassNLLCriterion(weights=None, size_average=True)  The negative log likelihood criterion. It is useful to train a classification problem with n\nclasses. If provided, the optional argument weights should be a 1D Tensor assigning weight to\neach of the classes. This is particularly useful when you have an unbalanced training set.  The input given through a  forward()  is expected to contain log-probabilities of each class:\ninput has to be a 1D Tensor of size  n . Obtaining log-probabilities in a neural network is easily\nachieved by adding a  LogSoftMax  layer in the last layer of your neural network. You may use CrossEntropyCriterion  instead, if you prefer not to add an extra layer to your network. This\ncriterion expects a class index (1 to the number of class) as target when calling forward(input, target)  and  backward(input, target) .  The loss can be described as:\n      loss(x, class) = -x[class] \n or in the case of the weights argument it is specified as follows:\n      loss(x, class) = -weights[class] * x[class] \n Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when\n calculating losses in non-batch mode.  Note that if the target is  -1 , the training process will skip this sample.\n In other words, the forward process will return zero output and the backward process\n will also return zero  gradInput .  By default, the losses are averaged over observations for each minibatch. However, if the field\n  sizeAverage  is set to false, the losses are instead summed for each minibatch.  Parameters:   weights  weights of each element of the input  sizeAverage  A boolean indicating whether normalizing by the number of elements in the input.\n                  Default: true   Scala example:  import com.intel.analytics.bigdl.nn.ClassNLLCriterion\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval criterion = ClassNLLCriterion()\nval input = Tensor(T(\n              T(1f, 2f, 3f),\n              T(2f, 3f, 4f),\n              T(3f, 4f, 5f)\n          ))\n\nval target = Tensor(T(1f, 2f, 3f))\n\nval loss = criterion.forward(input, target)\nval grad = criterion.backward(input, target)\n\nprint(loss)\n-3.0\nprintln(grad)\n-0.33333334 0.0 0.0\n0.0 -0.33333334 0.0\n0.0 0.0 -0.33333334\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\n\ncriterion = ClassNLLCriterion()\ninput = np.array([\n              [1.0, 2.0, 3.0],\n              [2.0, 3.0, 4.0],\n              [3.0, 4.0, 5.0]\n          ])\n\ntarget = np.array([1.0, 2.0, 3.0])\n\nloss = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n\nprint loss\n-3.0\nprint gradient\n-3.0\n[[-0.33333334  0.          0.        ]\n [ 0.         -0.33333334  0.        ]\n [ 0.          0.         -0.33333334]]", 
            "title": "ClassNLLCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#softmaxwithcriterion", 
            "text": "Scala:  val model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)  Python:  model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)  Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued predictions through a softmax to\nget a probability distribution over classes. It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as \nits gradient computation is more numerically stable.   ignoreLabel    (optional) Specify a label value that should be ignored when computing the loss.  normalizeMode  How to normalize the output loss.   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.{Storage, Tensor}\n\nval input = Tensor(1, 5, 2, 3).rand()\nval target = Tensor(Storage(Array(2.0f, 4.0f, 2.0f, 4.0f, 1.0f, 2.0f))).resize(1, 1, 2, 3)\n\nval model = SoftmaxWithCriterion[Float]()\nval output = model.forward(input, target)\n\nscala  print(input)\n(1,1,.,.) =\n0.65131104  0.9332143   0.5618989   \n0.9965054   0.9370902   0.108070895 \n\n(1,2,.,.) =\n0.46066576  0.9636703   0.8123812   \n0.31076035  0.16386998  0.37894428  \n\n(1,3,.,.) =\n0.49111295  0.3704862   0.9938375   \n0.87996656  0.8695406   0.53354675  \n\n(1,4,.,.) =\n0.8502225   0.9033509   0.8518651   \n0.0692618   0.10121379  0.970959    \n\n(1,5,.,.) =\n0.9397213   0.49688303  0.75739735  \n0.25074655  0.11416598  0.6594504   \n\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x2x3]\n\nscala  print(output)\n1.6689054  Python example:  input = np.random.randn(1, 5, 2, 3)\ntarget = np.array([[[[2.0, 4.0, 2.0], [4.0, 1.0, 2.0]]]])\n\nmodel = SoftmaxWithCriterion()\noutput = model.forward(input, target)  print input\n[[[[ 0.78455689  0.01402084  0.82539628]\n   [-1.06448238  2.58168413  0.60053703]]\n\n  [[-0.48617618  0.44538094  0.46611658]\n   [-1.41509329  0.40038991 -0.63505732]]\n\n  [[ 0.91266769  1.68667933  0.92423611]\n   [ 0.1465411   0.84637557  0.14917515]]\n\n  [[-0.7060493  -2.02544114  0.89070726]\n   [ 0.14535539  0.73980064 -0.33130613]]\n\n  [[ 0.64538791 -0.44384233 -0.40112523]\n   [ 0.44346658 -2.22303621  0.35715986]]]]  print output\n2.1002123", 
            "title": "SoftmaxWithCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#smoothl1criterion", 
            "text": "Scala:  val slc = SmoothL1Criterion(sizeAverage=true)  Python:  slc = SmoothL1Criterion(size_average=True)  Creates a criterion that can be thought of as a smooth version of the AbsCriterion.\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some\ncases prevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).  Scala example:  import com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.SmoothL1Criterion\n\nval slc = SmoothL1Criterion()\n\nval inputArr = Array(\n  0.17503996845335,\n  0.83220188552514,\n  0.48450597329065,\n  0.64701424003579,\n  0.62694586534053,\n  0.34398410236463,\n  0.55356747563928,\n  0.20383032318205\n)\nval targetArr = Array(\n  0.69956525065936,\n  0.86074831243604,\n  0.54923197557218,\n  0.57388074393384,\n  0.63334444304928,\n  0.99680578662083,\n  0.49997645849362,\n  0.23869121982716\n)\n\nval input = Tensor(Storage(inputArr.map(x =  x.toFloat))).reshape(Array(2, 2, 2))\nval target = Tensor(Storage(targetArr.map(x =  x.toFloat))).reshape(Array(2, 2, 2))\n\nval output = slc.forward(input, target)\nval gradInput = slc.backward(input, target)  Gives the output,  output: Float = 0.0447365  Gives the gradInput,  gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.06556566     -0.003568299\n-0.008090746    0.009141691\n\n(2,.,.) =\n-7.998273E-4    -0.08160271\n0.0066988766    -0.0043576136  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\nslc = SmoothL1Criterion()\n\ninput = np.array([\n    0.17503996845335,\n    0.83220188552514,\n    0.48450597329065,\n    0.64701424003579,\n    0.62694586534053,\n    0.34398410236463,\n    0.55356747563928,\n    0.20383032318205\n])\ninput.reshape(2, 2, 2)\n\ntarget = np.array([\n    0.69956525065936,\n    0.86074831243604,\n    0.54923197557218,\n    0.57388074393384,\n    0.63334444304928,\n    0.99680578662083,\n    0.49997645849362,\n    0.23869121982716\n])\n\ntarget.reshape(2, 2, 2)\n\noutput = slc.forward(input, target)\ngradInput = slc.backward(input, target)\n\nprint output\nprint gradInput", 
            "title": "SmoothL1Criterion"
        }, 
        {
            "location": "/APIGuide/Losses/#smoothl1criterionwithweights", 
            "text": "Scala:  val smcod = SmoothL1CriterionWithWeights[Float](sigma: Float = 2.4f, num: Int = 2)  Python:  smcod = SmoothL1CriterionWithWeights(sigma, num)  a smooth version of the AbsCriterion\nIt uses a squared term if the absolute element-wise error falls below 1.\nIt is less sensitive to outliers than the MSECriterion and in some cases\nprevents exploding gradients (e.g. see \"Fast R-CNN\" paper by Ross Girshick).     d = (x - y) * w_in\n\n  loss(x, y, w_in, w_out)\n              | 0.5 * (sigma * d_i)^2 * w_out          if |d_i|   1 / sigma / sigma\n   = 1/n \\sum |\n              | (|d_i| - 0.5 / sigma / sigma) * w_out   otherwise  Scala example:  val smcod = SmoothL1CriterionWithWeights[Float](2.4f, 2)\n\nval inputArr = Array(1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03)\nval targetArr = Array(0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58)\nval inWArr = Array(-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8)\nval outWArr = Array(-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1)\n\nval input = Tensor(Storage(inputArr.map(x =  x.toFloat)))\nval target = T()\ntarget.insert(Tensor(Storage(targetArr.map(x =  x.toFloat))))\ntarget.insert(Tensor(Storage(inWArr.map(x =  x.toFloat))))\ntarget.insert(Tensor(Storage(outWArr.map(x =  x.toFloat))))\n\nval output = smcod.forward(input, target)\nval gradInput = smcod.backward(input, target)  println(output)\n  output: Float = -2.17488  println(gradInput)\n-0.010944003\n0.425\n0.63037443\n-0.95\n-0.1\n0.07\n0.120000005\n-0.44000003\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 8]  Python example:  smcod = SmoothL1CriterionWithWeights(2.4, 2)\n\ninput = np.array([1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03]).astype( float32 )\ntargetArr = np.array([0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58]).astype( float32 )\ninWArr = np.array([-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8]).astype( float32 )\noutWArr = np.array([-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1]).astype( float32 )\ntarget = [targetArr, inWArr, outWArr]\n\noutput = smcod.forward(input, target)\ngradInput = smcod.backward(input, target)  output\n-2.17488  gradInput\n[array([-0.010944  ,  0.42500001,  0.63037443, -0.94999999, -0.1       ,\n         0.07      ,  0.12      , -0.44000003], dtype=float32)]", 
            "title": "SmoothL1CriterionWithWeights"
        }, 
        {
            "location": "/APIGuide/Losses/#multimargincriterion", 
            "text": "Scala:  val loss = MultiMarginCriterion(p=1,weights=null,margin=1.0,sizeAverage=true)  Python:  loss = MultiMarginCriterion(p=1,weights=None,margin=1.0,size_average=True)  MultiMarginCriterion is a loss function that optimizes a multi-class classification hinge loss (margin-based loss) between input  x  and output  y  ( y  is the target class index).  Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(3,2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f, 2.0f)))\nval loss = MultiMarginCriterion(1)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala  print(input)\n-0.45896783     -0.80141246\n0.22560088      -0.13517438\n0.2601126       0.35492152\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nscala  print(target)\n2.0\n1.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala  print(output)\n0.4811434\n\nscala  print(grad)\n0.16666667      -0.16666667\n-0.16666667     0.16666667\n0.16666667      -0.16666667\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(3,2)\ntarget = np.array([2,1,2])\nprint  input= ,input\nprint  target= ,target\n\nloss = MultiMarginCriterion(1)\nout = loss.forward(input, target)\nprint  output of loss is :  ,out\n\ngrad_out = loss.backward(input,target)\nprint  grad out of loss is :  ,grad_out  Gives the output,  input= [[ 0.46868305 -2.28562261]\n [ 0.8076243  -0.67809689]\n [-0.20342555 -0.66264743]]\ntarget= [2 1 2]\ncreating: createMultiMarginCriterion\noutput of loss is :  0.8689213\ngrad out of loss is :  [[ 0.16666667 -0.16666667]\n [ 0.          0.        ]\n [ 0.16666667 -0.16666667]]", 
            "title": "MultiMarginCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#hingeembeddingcriterion", 
            "text": "Scala:  val m = HingeEmbeddingCriterion(margin = 1, sizeAverage = true)  Python:  m = HingeEmbeddingCriterion(margin=1, size_average=True)  Creates a criterion that measures the loss given an input  x  which is a 1-dimensional vector and a label  y  ( 1  or  -1 ).\nThis is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.                   \u23a7 x_i,                  if y_i ==  1\nloss(x, y) = 1/n \u23a8\n                 \u23a9 max(0, margin - x_i), if y_i == -1  Scala example:  import com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.{T}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval loss = HingeEmbeddingCriterion(1, sizeAverage = false)\nval input = Tensor(T(0.1f, 2.0f, 2.0f, 2.0f))\nprintln( input: \\n  + input)\nprintln( ouput:  )\n\nprintln( Target=1:   + loss.forward(input, Tensor(4, 1).fill(1f)))\n\nprintln( Target=-1:   + loss.forward(input, Tensor(4, 1).fill(-1f)))  input: \n0.1\n2.0\n2.0\n2.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]\nouput: \nTarget=1: 6.1\nTarget=-1: 0.9  Python example:  import numpy as np\nfrom bigdl.nn.criterion import *\ninput = np.array([0.1, 2.0, 2.0, 2.0])\ntarget = np.full(4, 1)\nprint( input:   )\nprint(input)\nprint( target:  )\nprint(target)\nprint( output:  )\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, target))\nprint(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, np.full(4, -1)))  input: \n[ 0.1  2.   2.   2. ]\ntarget: \n[1 1 1 1]\noutput: \ncreating: createHingeEmbeddingCriterion\n6.1\ncreating: createHingeEmbeddingCriterion\n0.9", 
            "title": "HingeEmbeddingCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#margincriterion", 
            "text": "Scala:  criterion = MarginCriterion(margin=1.0, sizeAverage=true)  Python:  criterion = MarginCriterion(margin=1.0, sizeAverage=true, bigdl_type= float )  Creates a criterion that optimizes a two-class classification hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.\n *  margin  if unspecified, is by default 1.\n *  sizeAverage  whether to average the loss, is by default true  Scala example:  val criterion = MarginCriterion(margin=1.0, sizeAverage=true)\n\nval input = Tensor(3, 2).rand()\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.33753583      0.3575501\n0.23477706      0.7240361\n0.92835575      0.4737949\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\nval target = Tensor(3, 2).rand()\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.27280563      0.7022703\n0.3348442       0.43332106\n0.08935371      0.17876455\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]\n\ncriterion.forward(input, target)\nres5: Float = 0.84946966  Python example:  criterion = MarginCriterion(margin=1.0,size_average=True,bigdl_type= float )\ninput = np.random.rand(3, 2)\narray([[ 0.20824672,  0.67299837],\n       [ 0.80561452,  0.19564743],\n       [ 0.42501441,  0.19408184]])\n\ntarget = np.random.rand(3, 2)\narray([[ 0.67882632,  0.61257846],\n       [ 0.10111138,  0.75225082],\n       [ 0.60404296,  0.31373273]])\n\ncriterion.forward(input, target)\n0.8166871", 
            "title": "MarginCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#cosineembeddingcriterion", 
            "text": "Scala:  val cosineEmbeddingCriterion = CosineEmbeddingCriterion(margin  = 0.0, sizeAverage = true)  Python:  cosineEmbeddingCriterion = CosineEmbeddingCriterion( margin=0.0,size_average=True)  CosineEmbeddingCriterion creates a criterion that measures the loss given an input x = {x1, x2},\na table of two Tensors, and a Tensor label y with values 1 or -1.  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimpot com.intel.analytics.bigdl.utils.T\nval cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, false)\nval input1 = Tensor(5).rand()\nval input2 = Tensor(5).rand()\nval input = T()\ninput(1.0) = input1\ninput(2.0) = input2\nval target1 = Tensor(Storage(Array(-0.5f)))\nval target = T()\ntarget(1.0) = target1  print(input)\n {\n    2.0: 0.4110882\n         0.57726574\n         0.1949834\n         0.67670715\n         0.16984987\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n    1.0: 0.16878392\n         0.24124223\n         0.8964794\n         0.11156334\n         0.5101486\n         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]\n }  print(cosineEmbeddingCriterion.forward(input, target))\n0.49919847  print(cosineEmbeddingCriterion.backward(input, target))\n {\n    2: -0.045381278\n       -0.059856333\n       0.72547954\n       -0.2268434\n       0.3842142\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n    1: 0.30369008\n       0.42463788\n       -0.20637506\n       0.5712836\n       -0.06355385\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]\n }  Python example:  from bigdl.nn.layer import *\ncosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, False)  cosineEmbeddingCriterion.forward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n0.6363636  cosineEmbeddingCriterion.backward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])\n[array([ 0.07933884,  0.04958678,  0.01983471, -0.00991735, -0.03966942], dtype=float32), array([-0.03966942, -0.00991735,  0.01983471,  0.04958678,  0.07933884], dtype=float32)]", 
            "title": "CosineEmbeddingCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#bcecriterion", 
            "text": "Scala:  val criterion = BCECriterion[Float]()  Python:  criterion = BCECriterion()  This loss function measures the Binary Cross Entropy between the target and the output   loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))  or in the case of the weights argument being specified:   loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))  By default, the losses are averaged for each mini-batch over observations as well as over\n dimensions. However, if the field sizeAverage is set to false, the losses are instead summed.  Scala example:  \nval criterion = BCECriterion[Float]()\nval input = Tensor[Float](3, 1).rand\n\nval target = Tensor[Float](3)\ntarget(1) = 1\ntarget(2) = 0\ntarget(3) = 1\n\nval output = criterion.forward(input, target)\nval gradInput = criterion.backward(input, target)  println(target)\nres25: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0\n0.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]  println(output)\noutput: Float = 0.9009579  println(gradInput)\ngradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-1.5277504\n1.0736246\n-0.336957\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]  Python example:  \ncriterion = BCECriterion()\ninput = np.random.uniform(0, 1, (3, 1)).astype( float32 )\ntarget = np.array([1, 0, 1])\noutput = criterion.forward(input, target)\ngradInput = criterion.backward(input, target)  output\n1.9218739  gradInput\n[array([[-4.3074522 ],\n        [ 2.24244714],\n        [-1.22368968]], dtype=float32)]", 
            "title": "BCECriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#dicecoefficientcriterion", 
            "text": "Scala:  val loss = DiceCoefficientCriterion(sizeAverage=true, epsilon=1.0f)  Python:  loss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)  DiceCoefficientCriterion is the Dice-Coefficient objective function.   Both  forward  and  backward  accept two tensors : input and target. The  forward  result is formulated as \n           1 - (2 * (input intersection target) / (input union target))  Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DiceCoefficientCriterion(epsilon = 1.0f)\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala  print(input)\n-0.50278\n0.51387966\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(output)\n0.9958517\n\nscala  print(grad)\n-0.99619853     -0.49758217\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1],dtype='float64')\n\nprint  input= , input\nprint  target= , target\nloss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)\nout = loss.forward(input,target)\nprint  output of loss is : ,out\n\ngrad_out = loss.backward(input,target)\nprint  grad out of loss is : ,grad_out  produces output:  input= [ 0.4440505  2.9430301]\ntarget= [ 2.  1.]\ncreating: createDiceCoefficientCriterion\noutput of loss is : -0.17262316\ngrad out of loss is : [[-0.38274616 -0.11200322]]", 
            "title": "DiceCoefficientCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#msecriterion", 
            "text": "Scala:  val criterion = MSECriterion()  Python:  criterion = MSECriterion()  The mean squared error criterion e.g. input: a, target: b, total elements: n  loss(a, b) = 1/n * sum(|a_i - b_i|^2)  Parameters:   sizeAverage  a boolean indicating whether to divide the sum of squared error by n. \n                 Default: true   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MSECriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(2.0f, 3.0f),\n T(4.0f, 5.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-  print(output)\n1.0\n-  print(gradient)\n-0.5    -0.5    \n-0.5    -0.5    \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = MSECriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient= criterion.backward(input, target)\n-  print output\n1.0\n-  print gradient\n[[-0.5 -0.5]\n [-0.5 -0.5]]", 
            "title": "MSECriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#softmargincriterion", 
            "text": "Scala:  val criterion = SoftMarginCriterion(sizeAverage)  Python:  criterion = SoftMarginCriterion(size_average)  Creates a criterion that optimizes a two-class classification logistic loss between\ninput x (a Tensor of dimension 1) and output y (which is a tensor containing either\n1s or -1s).  loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()  Parameters:\n*  sizeAverage  A boolean indicating whether normalizing by the number of elements in the input.\n                    Default: true  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = SoftMarginCriterion()\nval input = Tensor(T(\n T(1.0f, 2.0f),\n T(3.0f, 4.0f))\n)\nval target = Tensor(T(\n T(1.0f, -1.0f),\n T(-1.0f, 1.0f))\n)\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-  print(output)\n1.3767318\n-  print(gradient)\n-0.06723536     0.22019927      \n0.23814353      -0.0044965525   \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = SoftMarginCriterion()\ninput = np.array([\n          [1.0, 2.0],\n          [3.0, 4.0]\n        ])\ntarget = np.array([\n           [2.0, 3.0],\n           [4.0, 5.0]\n         ])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-  print output\n1.3767318\n-  print gradient\n[[-0.06723536  0.22019927]\n [ 0.23814353 -0.00449655]]", 
            "title": "SoftMarginCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#distkldivcriterion", 
            "text": "Scala:  val loss = DistKLDivCriterion[T](sizeAverage=true)  Python:  loss = DistKLDivCriterion(size_average=True)  DistKLDivCriterion is the Kullback\u2013Leibler divergence loss.  Scala example:  \nscala \nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval input = Tensor(2).randn()\nval target = Tensor(Storage(Array(2.0f, 1.0f)))\nval loss = DistKLDivCriterion()\nval output = loss.forward(input,target)\nval grad = loss.backward(input,target)\n\nscala  print(input)\n-0.3854126\n-0.7707398\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(target)\n2.0\n1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\nscala  print(output)\n1.4639297\n\nscala  print(grad)\n-1.0\n-0.5\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ninput  = np.random.randn(2)\ntarget = np.array([2,1])\n\nprint  input= , input\nprint  target= , target\nloss = DistKLDivCriterion()\nout = loss.forward(input,target)\nprint  output of loss is : ,out\n\ngrad_out = loss.backward(input,target)\nprint  grad out of loss is : ,grad_out  Gives the output  input= [-1.14333924  0.97662296]\ntarget= [2 1]\ncreating: createDistKLDivCriterion\noutput of loss is : 1.348175\ngrad out of loss is : [-1.  -0.5]", 
            "title": "DistKLDivCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#classsimplexcriterion", 
            "text": "Scala:  val criterion = ClassSimplexCriterion(nClasses)  Python:  criterion = ClassSimplexCriterion(nClasses)  ClassSimplexCriterion implements a criterion for classification.\nIt learns an embedding per class, where each class' embedding is a\npoint on an (N-1)-dimensional simplex, where N is the number of classes.  Parameters:\n*  nClasses  An integer, the number of classes.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = ClassSimplexCriterion(5)\nval input = Tensor(T(\n T(1.0f, 2.0f, 3.0f, 4.0f, 5.0f),\n T(4.0f, 5.0f, 6.0f, 7.0f, 8.0f)\n))\nval target = Tensor(2)\ntarget(1) = 2.0f\ntarget(2) = 1.0f\nval output = criterion.forward(input, target)\nval gradient = criterion.backward(input, target)\n-  print(output)\n23.562702\n-  print(gradient)\n0.25    0.20635083      0.6     0.8     1.0     \n0.6     1.0     1.2     1.4     1.6     \n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np\ncriterion = ClassSimplexCriterion(5)\ninput = np.array([\n   [1.0, 2.0, 3.0, 4.0, 5.0],\n   [4.0, 5.0, 6.0, 7.0, 8.0]\n])\ntarget = np.array([2.0, 1.0])\noutput = criterion.forward(input, target)\ngradient = criterion.backward(input, target)\n-  print output\n23.562702\n-  print gradient\n[[ 0.25        0.20635083  0.60000002  0.80000001  1.        ]\n [ 0.60000002  1.          1.20000005  1.39999998  1.60000002]]", 
            "title": "ClassSimplexCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#l1hingeembeddingcriterion", 
            "text": "Scala:  val model = L1HingeEmbeddingCriterion(margin)  Python:  model = L1HingeEmbeddingCriterion(margin)  Creates a criterion that measures the loss given an input  x = {x1, x2} , a table of two Tensors, and a label y (1 or -1).\nThis is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.               \u23a7 ||x1 - x2||_1,                  if y ==  1\nloss(x, y) = \u23a8\n             \u23a9 max(0, margin - ||x1 - x2||_1), if y == -1  The margin has a default value of 1, or can be set in the constructor.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = L1HingeEmbeddingCriterion(0.6)\nval input1 = Tensor(T(1.0f, -0.1f))\nval input2 = Tensor(T(2.0f, -0.2f))\nval input = T(input1, input2)\nval target = Tensor(1)\ntarget(Array(1)) = 1.0f\n\nval output = model.forward(input, target)\n\nscala  print(output)\n1.1  Python example:  model = L1HingeEmbeddingCriterion(0.6)\ninput1 = np.array(1.0, -0.1)\ninput2 = np.array(2.0, -0.2)\ninput = [input1, input2]\ntarget = np.array([1.0])\n\noutput = model.forward(input, target)  print output\n1.1", 
            "title": "L1HingeEmbeddingCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#crossentropycriterion", 
            "text": "Scala:  val module = CrossEntropyCriterion(weights, sizeAverage)  Python:  module = CrossEntropyCriterion(weights, sizeAverage)  This criterion combines LogSoftMax and ClassNLLCriterion in one single class.   weights  A tensor assigning weight to each of the classes  sizeAverage  whether to divide the sequence length. Default is true.   Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Storage\n\nval layer = CrossEntropyCriterion[Double]()\nval input = Tensor[Double](Storage(Array(\n    1.0262627674932,\n    -1.2412600935171,\n    -1.0423174168648,\n    -0.90330565804228,\n    -1.3686840144413,\n    -1.0778380454479,\n    -0.99131220658219,\n    -1.0559142847536,\n    -1.2692712660404\n    ))).resize(3, 3)\nval target = Tensor[Double](3)\n    target(Array(1)) = 1\n    target(Array(2)) = 2\n    target(Array(3)) = 3  print(layer.forward(input, target))\n0.9483051199107635  Python example:  from bigdl.nn.criterion import *\n\nlayer = CrossEntropyCriterion()\ninput = np.array([1.0262627674932,\n                      -1.2412600935171,\n                      -1.0423174168648,\n                      -0.90330565804228,\n                      -1.3686840144413,\n                      -1.0778380454479,\n                      -0.99131220658219,\n                      -1.0559142847536,\n                      -1.2692712660404\n                      ]).reshape(3,3)\ntarget = np.array([1, 2, 3])                       layer.forward(input, target)\n0.94830513", 
            "title": "CrossEntropyCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#parallelcriterion", 
            "text": "Scala:  val pc = ParallelCriterion(repeatTarget=false)  Python:  pc = ParallelCriterion(repeat_target=False)  ParallelCriterion is a weighted sum of other criterions each applied to a different input\nand target. Set repeatTarget = true to share the target for criterions.\nUse add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).  Scala example:  import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.{Tensor, Storage}\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.{ParallelCriterion, ClassNLLCriterion, MSECriterion}\n\nval pc = ParallelCriterion()\n\nval input = T(Tensor(2, 10), Tensor(2, 10))\nvar i = 0\ninput[Tensor](1).apply1(_ =  {i += 1; i})\ninput[Tensor](2).apply1(_ =  {i -= 1; i})\nval target = T(Tensor(Storage(Array(1.0f, 8.0f))), Tensor(2, 10).fill(1.0f))\n\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\npc.add(nll, 0.5).add(mse)\n\nval output = pc.forward(input, target)\nval gradInput = pc.backward(input, target)\n\nprintln(output)\nprintln(gradInput)  Gives the output,  100.75  Gives the gradInput,   {\n        2: 1.8000001    1.7     1.6     1.5     1.4     1.3000001       1.2     1.1     1.0     0.90000004\n           0.8  0.7     0.6     0.5     0.4     0.3     0.2     0.1     0.0     -0.1\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n        1: -0.25        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n           0.0  0.0     0.0     0.0     0.0     0.0     0.0     -0.25   0.0     0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]\n }  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\npc = ParallelCriterion()\n\ninput1 = np.arange(1, 21, 1).astype( float32 )\ninput2 = np.arange(0, 20, 1).astype( float32 )[::-1]\ninput1 = input1.reshape(2, 10)\ninput2 = input2.reshape(2, 10)\n\ninput = [input1, input2]\n\ntarget1 = np.array([1.0, 8.0]).astype( float32 )\ntarget1 = target1.reshape(2)\ntarget2 = np.full([2, 10], 1).astype( float32 )\ntarget2 = target2.reshape(2, 10)\ntarget = [target1, target2]\n\nnll = ClassNLLCriterion()\nmse = MSECriterion()\n\npc.add(nll, weight = 0.5).add(mse)\n\nprint  input = \\n %s   % input\nprint  target = \\n %s  % target\n\noutput = pc.forward(input, target)\ngradInput = pc.backward(input, target)\n\nprint  output = %s   % output\nprint  gradInput = %s   % gradInput  Gives the output,  input = \n [array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n       [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]], dtype=float32), array([[ 19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,  10.],\n       [  9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.]], dtype=float32)] \ntarget = \n [array([ 1.,  8.], dtype=float32), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)]\noutput = 100.75 \ngradInput = [array([[-0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ]], dtype=float32), array([[ 1.80000007,  1.70000005,  1.60000002,  1.5       ,  1.39999998,\n         1.30000007,  1.20000005,  1.10000002,  1.        ,  0.90000004],\n       [ 0.80000001,  0.69999999,  0.60000002,  0.5       ,  0.40000001,\n         0.30000001,  0.2       ,  0.1       ,  0.        , -0.1       ]], dtype=float32)]", 
            "title": "ParallelCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#multilabelmargincriterion", 
            "text": "Scala:  val multiLabelMarginCriterion = MultiLabelMarginCriterion(sizeAverage = true)  Python:  multiLabelMarginCriterion = MultiLabelMarginCriterion(size_average=True)  MultiLabelMarginCriterion creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x and output y   Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor._\nval multiLabelMarginCriterion = MultiLabelMarginCriterion(false)\nval input = Tensor(4).rand()\nval target = Tensor(4)\ntarget(Array(1)) = 3\ntarget(Array(2)) = 2\ntarget(Array(3)) = 1\ntarget(Array(4)) = 0  print(input)\n0.40267515\n0.5913795\n0.84936756\n0.05999674   print(multiLabelMarginCriterion.forward(input, target))\n0.33414197  print(multiLabelMarginCriterion.backward(input, target))\n-0.25\n-0.25\n-0.25\n0.75\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]  Python example:  from bigdl.nn.layer import *\nmultiLabelMarginCriterion = MultiLabelMarginCriterion(False)  multiLabelMarginCriterion.forward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n0.975  multiLabelMarginCriterion.backward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))\n[array([-0.25, -0.25, -0.25,  0.75], dtype=float32)]", 
            "title": "MultiLabelMarginCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#multilabelsoftmargincriterion", 
            "text": "Scala:  val criterion = MultiLabelSoftMarginCriterion(weights = null, sizeAverage = true)  Python:  criterion = MultiLabelSoftMarginCriterion(weights=None, size_average=True)  MultiLabelSoftMarginCriterion is a multiLabel multiclass criterion based on sigmoid:  l(x,y) = - sum_i y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i])  where  p[i] = exp(x[i]) / (1 + exp(x[i]))  If with weights,\n  l(x,y) = - sum_i weights[i] (y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i]))  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiLabelSoftMarginCriterion()\nval input = Tensor(3)\ninput(Array(1)) = 0.4f\ninput(Array(2)) = 0.5f\ninput(Array(3)) = 0.6f\nval target = Tensor(3)\ntarget(Array(1)) = 0\ntarget(Array(2)) = 1\ntarget(Array(3)) = 1  criterion.forward(input, target)\nres0: Float = 0.6081934  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiLabelSoftMarginCriterion()\ninput = np.array([0.4, 0.5, 0.6])\ntarget = np.array([0, 1, 1])  criterion.forward(input, target)\n0.6081934", 
            "title": "MultiLabelSoftMarginCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#abscriterion", 
            "text": "Scala:  val criterion = AbsCriterion(sizeAverage)  Python:  criterion = AbsCriterion(sizeAverage)  Measures the mean absolute value of the element-wise difference between input and target  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = AbsCriterion()\nval input = Tensor(T(1.0f, 2.0f, 3.0f))\nval target = Tensor(T(4.0f, 5.0f, 6.0f))\nval output = criterion.forward(input, target)\n\nscala  print(output)\n3.0  Python example:  criterion = AbsCriterion()\ninput = np.array([1.0, 2.0, 3.0])\ntarget = np.array([4.0, 5.0, 6.0])\noutput=criterion.forward(input, target)  print output\n3.0", 
            "title": "AbsCriterion"
        }, 
        {
            "location": "/APIGuide/Losses/#multicriterion", 
            "text": "Scala:  val criterion = MultiCriterion()  Python:  criterion = MultiCriterion()  MultiCriterion is a weighted sum of other criterions each applied to the same input and target  Scala example:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval criterion = MultiCriterion()\nval nll = ClassNLLCriterion()\nval mse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\nval input = Tensor(5).randn()\nval target = Tensor(5)\ntarget(Array(1)) = 1\ntarget(Array(2)) = 2\ntarget(Array(3)) = 3\ntarget(Array(4)) = 2\ntarget(Array(5)) = 1\n\nval output = criterion.forward(input, target)  input\n1.0641425\n-0.33507252\n1.2345984\n0.08065767\n0.531199\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]  output\nres7: Float = 1.9633228  Python example:  from bigdl.nn.criterion import *\nimport numpy as np\n\ncriterion = MultiCriterion()\nnll = ClassNLLCriterion()\nmse = MSECriterion()\ncriterion.add(nll, 0.5)\ncriterion.add(mse)\n\ninput = np.array([0.9682213801388531,\n0.35258855644097503,\n0.04584479998452568,\n-0.21781499692588918,\n-1.02721844006879])\ntarget = np.array([1, 2, 3, 2, 1])\n\noutput = criterion.forward(input, target)  output\n3.6099546", 
            "title": "MultiCriterion"
        }, 
        {
            "location": "/APIGuide/Initializers/", 
            "text": "Zeros\n\n\nScala:\n\n\nval initMethod = Zeros\n\n\n\n\n\nPython:\n\n\ninit_method = Zeros()\n\n\n\n\nInitialization method that set tensor to zeros.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Zeros\nval biasInitMethod = Zeros\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n\n {\n    weight: 0.0 0.0 0.0 \n            0.0 0.0 0.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = Zeros()\nbias_init = Zeros()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createZeros\ncreating: createZeros\ncreating: createLinear\nweight:\n[[ 0.  0.  0.]\n [ 0.  0.  0.]]\nbias: \n[ 0.  0.]\n\n\n\n\n\n\nOnes\n\n\nScala:\n\n\nval initMethod = Ones\n\n\n\n\n\nPython:\n\n\ninit_method = Ones()\n\n\n\n\nInitialization method that set tensor to be ones.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Ones\nval biasInitMethod = Ones\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: 1.0 1.0 1.0 \n            1.0 1.0 1.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0\n          1.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = Ones()\nbias_init = Ones()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createOnes\ncreating: createOnes\ncreating: createLinear\nweight:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\nbias: \n[ 1.  1.]\n\n\n\n\n\nConstInitMethod\n\n\nScala:\n\n\nval initMethod = ConstInitMethod(value: Double)\n\n\n\n\n\nPython:\n\n\ninit_method = ConstInitMethod(value)\n\n\n\n\nInitialization method that set tensor to the specified constant value.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\n\nval weightInitMethod = ConstInitMethod(0.2)\nval biasInitMethod = ConstInitMethod(0.2)\nval linear = Linear(3, 2).setName(\nlinear1\n)\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: 0.2 0.2 0.2\n            0.2 0.2 0.2\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.2\n          0.2\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0\n                0.0 0.0 0.0\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = ConstInitMethod(0.2)\nbias_init = ConstInitMethod(0.2)\nlinear = Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(linear.get_weights()[0])\nprint(\nbias: \n)\nprint(linear.get_weights()[1])\n\n\n\n\ncreating: createConstInitMethod\ncreating: createConstInitMethod\ncreating: createLinear\nweight:\n[[ 0.2  0.2  0.2]\n [ 0.2  0.2  0.2]]\nbias:\n[ 0.2  0.2]\n\n\n\n\n\nXavier\n\n\nScala:\n\n\nval initMethod = Xavier\n\n\n\n\n\nPython:\n\n\ninit_method = Xavier()\n\n\n\n\nThe Xavier initialization method draws samples from a uniform distribution\nbounded by [-limit, limit) where limit = sqrt(6.0/(fanIn+fanOut)). The rationale\nbehind this formula can be found in the paper\n\nUnderstanding the difficulty of training deep feedforward neural networks\n.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Xavier\nval biasInitMethod = Xavier\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: -0.78095555 -0.09939616 0.12034761  \n            -0.3019594  0.11734331  0.80369484  \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0727772\n          -0.6703765\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = Xavier()\nbias_init = Xavier()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createXavier\ncreating: createXavier\ncreating: createLinear\nweight:\n[[ 0.00580597 -0.73662472  0.13767919]\n [ 0.16802482 -0.49394709 -0.74967551]]\nbias: \n[-1.12355328  0.0779365 ]\n\n\n\n\nBilinearFiller\n\n\nScala:\n\n\nval initMethod = BilinearFiller\n\n\n\n\n\nPython:\n\n\ninit_method = BilinearFiller()\n\n\n\n\nInitialize the weight with coefficients for bilinear interpolation. A common use case is with the DeconvolutionLayer acting as upsampling. This initialization method can only be used in the weight initialization of SpatialFullConvolution.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = BilinearFiller\nval biasInitMethod - Zeros\nval model = SpatialFullConvolution(2, 3, 2, 2).setName(\nsfconv\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nsfconv\n).get)\n\n\n\n\n{\n    weight: (1,1,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n    bias: 0.0\n          0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradBias: 0.0\n              0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradWeight: (1,1,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n }\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = BilinearFiller()\nbias_init = Zeros()\nmodel =  SpatialFullConvolution(2, 3, 2, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createBilinearFiller\ncreating: createZeros\ncreating: createSpatialFullConvolution\nweight:\n[[[[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]\n\n\n  [[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]]]\nbias: \n[ 0.  0.  0.]\n\n\n\n\n\n\nRandomNormal\n\n\nScala:\n\n\nval initMethod = RandomNormal(mean, stdv)\n\n\n\n\n\nPython:\n\n\ninit_method = RandomNormal(mean, stdv)\n\n\n\n\nThis initialization method draws samples from a normal distribution.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomNormal(0, 1)\nval biasInitMethod = RandomNormal(0, 1)\nval linear = Linear(3, 2).setName(\nlinear1\n)\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: -0.5908564  0.32844943  -0.845019   \n            0.21550806  1.2037253   0.6807024   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.5345903\n          -0.76420456\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n  }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nfrom bigdl.nn.layer import *\n\nweight_init = RandomNormal(0, 1)\nbias_init = RandomNormal(0, 1)\nlinear= Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(linear.get_weights()[0])\nprint(\nbias: \n)\nprint(linear.get_weights()[1])\n\n\n\n\ncreating: createRandomNormal\ncreating: createRandomNormal\ncreating: createLinear\nweight:\n[[-0.00784962  0.77845585 -1.16250944]\n [ 0.03195094 -0.15211993  0.6254822 ]]\nbias: \n[-0.37883148 -0.81106091]\n\n\n\n\n\nRandomUniform\n\n\nScala:\n\n\nval initMethod = RandomUniform(lower, upper)\n\n\n\n\n\nPython:\n\n\ninit_method = RandomUniform(upper=None, lower=None)\n\n\n\n\nThis initialization method draws samples from a uniform distribution. If the lower bound and upper bound of this uniform distribution is not specified, it will be set to [-limit, limit) where limit = 1/sqrt(fanIn).\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomUniform\nval biasInitMethod = RandomUniform(0, 1)\nval model = Linear(3, 2).setName(\nlinear1\n)\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get(\nlinear1\n).get)\n\n\n\n\n {\n    weight: -0.572536   0.13046022  -0.040449623    \n            -0.547542   0.19093458  0.5632484   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.785292\n          0.63280666\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }\n\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.initialization_method import *\nweight_init = RandomUniform()\nbias_init = RandomUniform()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint(\nweight:\n)\nprint(model.get_weights()[0])\nprint(\nbias: \n)\nprint(model.get_weights()[1])\n\n\n\n\ncreating: createRandomUniform\ncreating: createRandomUniform\ncreating: createLinear\nweight:\n[[ 0.53153235  0.53016287  0.32831791]\n [-0.45736417 -0.16206641  0.21758588]]\nbias: \n[ 0.32058391  0.26307678]\n\n\n\n\n\nDefine your own Initializer\n\n\nAll customizedInitializer should implement the \nInitializationMethod\n trait\n\n\n/**\n * Initialization method to initialize bias and weight.\n * The init method will be called in Module.reset()\n */\n\ntrait InitializationMethod {\n\n  type Shape = Array[Int]\n\n  /**\n   * Initialize the given variable\n   *\n   * @param variable    the variable to initialize\n   * @param dataFormat  describe the meaning of each dimension of the variable\n   */\n  def init[T](variable: Tensor[T], dataFormat: VariableFormat)\n             (implicit ev: TensorNumeric[T]): Unit\n}\n\n\n\n\nThe \nRandomUniform\n\ncode should give you a good sense of how to implement this trait.\n\n\n_\nPython\n\nCustom initialization method in python is not supported right now.", 
            "title": "Initalizers"
        }, 
        {
            "location": "/APIGuide/Initializers/#zeros", 
            "text": "Scala:  val initMethod = Zeros  Python:  init_method = Zeros()  Initialization method that set tensor to zeros.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Zeros\nval biasInitMethod = Zeros\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)  \n {\n    weight: 0.0 0.0 0.0 \n            0.0 0.0 0.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = Zeros()\nbias_init = Zeros()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createZeros\ncreating: createZeros\ncreating: createLinear\nweight:\n[[ 0.  0.  0.]\n [ 0.  0.  0.]]\nbias: \n[ 0.  0.]", 
            "title": "Zeros"
        }, 
        {
            "location": "/APIGuide/Initializers/#ones", 
            "text": "Scala:  val initMethod = Ones  Python:  init_method = Ones()  Initialization method that set tensor to be ones.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Ones\nval biasInitMethod = Ones\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)   {\n    weight: 1.0 1.0 1.0 \n            1.0 1.0 1.0 \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0\n          1.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = Ones()\nbias_init = Ones()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createOnes\ncreating: createOnes\ncreating: createLinear\nweight:\n[[ 1.  1.  1.]\n [ 1.  1.  1.]]\nbias: \n[ 1.  1.]", 
            "title": "Ones"
        }, 
        {
            "location": "/APIGuide/Initializers/#constinitmethod", 
            "text": "Scala:  val initMethod = ConstInitMethod(value: Double)  Python:  init_method = ConstInitMethod(value)  Initialization method that set tensor to the specified constant value.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\n\nval weightInitMethod = ConstInitMethod(0.2)\nval biasInitMethod = ConstInitMethod(0.2)\nval linear = Linear(3, 2).setName( linear1 )\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get( linear1 ).get)   {\n    weight: 0.2 0.2 0.2\n            0.2 0.2 0.2\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.2\n          0.2\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0\n                0.0 0.0 0.0\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = ConstInitMethod(0.2)\nbias_init = ConstInitMethod(0.2)\nlinear = Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(linear.get_weights()[0])\nprint( bias:  )\nprint(linear.get_weights()[1])  creating: createConstInitMethod\ncreating: createConstInitMethod\ncreating: createLinear\nweight:\n[[ 0.2  0.2  0.2]\n [ 0.2  0.2  0.2]]\nbias:\n[ 0.2  0.2]", 
            "title": "ConstInitMethod"
        }, 
        {
            "location": "/APIGuide/Initializers/#xavier", 
            "text": "Scala:  val initMethod = Xavier  Python:  init_method = Xavier()  The Xavier initialization method draws samples from a uniform distribution\nbounded by [-limit, limit) where limit = sqrt(6.0/(fanIn+fanOut)). The rationale\nbehind this formula can be found in the paper Understanding the difficulty of training deep feedforward neural networks .  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = Xavier\nval biasInitMethod = Xavier\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)   {\n    weight: -0.78095555 -0.09939616 0.12034761  \n            -0.3019594  0.11734331  0.80369484  \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 1.0727772\n          -0.6703765\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = Xavier()\nbias_init = Xavier()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createXavier\ncreating: createXavier\ncreating: createLinear\nweight:\n[[ 0.00580597 -0.73662472  0.13767919]\n [ 0.16802482 -0.49394709 -0.74967551]]\nbias: \n[-1.12355328  0.0779365 ]", 
            "title": "Xavier"
        }, 
        {
            "location": "/APIGuide/Initializers/#bilinearfiller", 
            "text": "Scala:  val initMethod = BilinearFiller  Python:  init_method = BilinearFiller()  Initialize the weight with coefficients for bilinear interpolation. A common use case is with the DeconvolutionLayer acting as upsampling. This initialization method can only be used in the weight initialization of SpatialFullConvolution.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = BilinearFiller\nval biasInitMethod - Zeros\nval model = SpatialFullConvolution(2, 3, 2, 2).setName( sfconv )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( sfconv ).get)  {\n    weight: (1,1,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,1,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,1,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,2,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            (1,2,3,.,.) =\n            1.0 0.0 \n            0.0 0.0 \n\n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n    bias: 0.0\n          0.0\n          0.0\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradBias: 0.0\n              0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 3]\n    gradWeight: (1,1,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,1,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,1,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,2,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                (1,2,3,.,.) =\n                0.0 0.0 \n                0.0 0.0 \n\n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x2x2]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = BilinearFiller()\nbias_init = Zeros()\nmodel =  SpatialFullConvolution(2, 3, 2, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createBilinearFiller\ncreating: createZeros\ncreating: createSpatialFullConvolution\nweight:\n[[[[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]\n\n\n  [[[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]\n\n   [[ 1.  0.]\n    [ 0.  0.]]]]]\nbias: \n[ 0.  0.  0.]", 
            "title": "BilinearFiller"
        }, 
        {
            "location": "/APIGuide/Initializers/#randomnormal", 
            "text": "Scala:  val initMethod = RandomNormal(mean, stdv)  Python:  init_method = RandomNormal(mean, stdv)  This initialization method draws samples from a normal distribution.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomNormal(0, 1)\nval biasInitMethod = RandomNormal(0, 1)\nval linear = Linear(3, 2).setName( linear1 )\nlinear.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(linear.getParametersTable().get( linear1 ).get)   {\n    weight: -0.5908564  0.32844943  -0.845019   \n            0.21550806  1.2037253   0.6807024   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.5345903\n          -0.76420456\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n  }  Python example:  from bigdl.nn.initialization_method import *\nfrom bigdl.nn.layer import *\n\nweight_init = RandomNormal(0, 1)\nbias_init = RandomNormal(0, 1)\nlinear= Linear(3, 2)\nlinear.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(linear.get_weights()[0])\nprint( bias:  )\nprint(linear.get_weights()[1])  creating: createRandomNormal\ncreating: createRandomNormal\ncreating: createLinear\nweight:\n[[-0.00784962  0.77845585 -1.16250944]\n [ 0.03195094 -0.15211993  0.6254822 ]]\nbias: \n[-0.37883148 -0.81106091]", 
            "title": "RandomNormal"
        }, 
        {
            "location": "/APIGuide/Initializers/#randomuniform", 
            "text": "Scala:  val initMethod = RandomUniform(lower, upper)  Python:  init_method = RandomUniform(upper=None, lower=None)  This initialization method draws samples from a uniform distribution. If the lower bound and upper bound of this uniform distribution is not specified, it will be set to [-limit, limit) where limit = 1/sqrt(fanIn).  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval weightInitMethod = RandomUniform\nval biasInitMethod = RandomUniform(0, 1)\nval model = Linear(3, 2).setName( linear1 )\nmodel.setInitMethod(weightInitMethod, biasInitMethod)\nprintln(model.getParametersTable().get( linear1 ).get)   {\n    weight: -0.572536   0.13046022  -0.040449623    \n            -0.547542   0.19093458  0.5632484   \n            [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n    bias: 0.785292\n          0.63280666\n          [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradBias: 0.0\n              0.0\n              [com.intel.analytics.bigdl.tensor.DenseTensor of size 2]\n    gradWeight: 0.0 0.0 0.0 \n                0.0 0.0 0.0 \n                [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n }  Python example:  from bigdl.nn.initialization_method import *\nweight_init = RandomUniform()\nbias_init = RandomUniform()\nmodel = Linear(3, 2)\nmodel.set_init_method(weight_init, bias_init)\nprint( weight: )\nprint(model.get_weights()[0])\nprint( bias:  )\nprint(model.get_weights()[1])  creating: createRandomUniform\ncreating: createRandomUniform\ncreating: createLinear\nweight:\n[[ 0.53153235  0.53016287  0.32831791]\n [-0.45736417 -0.16206641  0.21758588]]\nbias: \n[ 0.32058391  0.26307678]", 
            "title": "RandomUniform"
        }, 
        {
            "location": "/APIGuide/Initializers/#define-your-own-initializer", 
            "text": "All customizedInitializer should implement the  InitializationMethod  trait  /**\n * Initialization method to initialize bias and weight.\n * The init method will be called in Module.reset()\n */\n\ntrait InitializationMethod {\n\n  type Shape = Array[Int]\n\n  /**\n   * Initialize the given variable\n   *\n   * @param variable    the variable to initialize\n   * @param dataFormat  describe the meaning of each dimension of the variable\n   */\n  def init[T](variable: Tensor[T], dataFormat: VariableFormat)\n             (implicit ev: TensorNumeric[T]): Unit\n}  The  RandomUniform \ncode should give you a good sense of how to implement this trait.  _ Python \nCustom initialization method in python is not supported right now.", 
            "title": "Define your own Initializer"
        }, 
        {
            "location": "/APIGuide/Regularizers/", 
            "text": "L1 Regularizer\n\n\nScala:\n\n\nval l1Regularizer = L1Regularizer(rate)\n\n\n\n\nPython:\n\n\nregularizerl1 = L1Regularizer(rate)\n\n\n\n\nL1 regularizer is used to add penalty to the gradWeight to avoid overfitting.\n\n\nIn our code implmenation, gradWeight = gradWeight + alpha * abs(weight)\n\n\nFor more details, please refer to \nwiki\n.\n\n\nScala example:\n\n\n\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nRNG.setSeed(100)\n\nval input = Tensor(3, 5).rand\nval gradOutput = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L1Regularizer(0.2), bRegularizer = L1Regularizer(0.2))\n\nval output = linear.forward(input)\nval gradInput = linear.backward(input, gradOutput)\n\nscala\n input\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala\n gradOutput\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.4527399       0.13670659      0.87014264      0.5750933       0.063681036\n0.89132196      0.62431186      0.20920213      0.52334774      0.18532822\n0.5622963       0.10837689      0.0058171963    0.21969749      0.3074232\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala\n linear.gradWeight\nres2: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9835552       1.3616763       0.83564335      0.108898684     0.59625006\n0.21608911      0.8393639       0.0035243928    -0.11795368     0.4453743\n0.38366735      0.9618148       0.47721142      0.5607486       0.6069793\n0.81469804      0.6690552       0.18522228      0.08559488      0.7075894\n-0.030468717    0.056625083     0.051471338     0.2917061       0.109963015\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x5]\n\n\n\n\n\nPython example:\n\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\nlinear = Linear(5, 5, wRegularizer = L1Regularizer(0.2), bRegularizer = L1Regularizer(0.2))\noutput = linear.forward(input)\ngradInput = linear.backward(input, gradOutput)\n\n\n linear.parameters()\n{u'Linear@596d857b': {u'bias': array([ 0.3185505 , -0.02004393,  0.34620118, -0.09206461,  0.40776938], dtype=float32),\n  u'gradBias': array([ 2.14087653,  1.82181644,  1.90674937,  1.37307787,  0.81534696], dtype=float32),\n  u'gradWeight': array([[ 0.34909648,  0.85083449,  1.44904375,  0.90150446,  0.57136625],\n         [ 0.3745544 ,  0.42218602,  1.53656614,  1.1836741 ,  1.00702667],\n         [ 0.30529332,  0.26813674,  0.85559171,  0.61224306,  0.34721529],\n         [ 0.22859855,  0.8535381 ,  1.19809723,  1.37248564,  0.50041491],\n         [ 0.36197871,  0.03069445,  0.64837945,  0.12765063,  0.12872688]], dtype=float32),\n  u'weight': array([[-0.12423037,  0.35694697,  0.39038274, -0.34970999, -0.08283543],\n         [-0.4186025 , -0.33235055,  0.34948507,  0.39953214,  0.16294235],\n         [-0.25171402, -0.28955361, -0.32243955, -0.19771226, -0.29320192],\n         [-0.39263198,  0.37766701,  0.14673658,  0.24882999, -0.0779015 ],\n         [ 0.0323218 , -0.31266898,  0.31543773, -0.0898933 , -0.33485892]], dtype=float32)}}\n\n\n\n\nL2 Regularizer\n\n\nScala:\n\n\nval l2Regularizer = L2Regularizer(rate)\n\n\n\n\nPython:\n\n\nregularizerl2 = L2Regularizer(rate)\n\n\n\n\nL2 regularizer is used to add penalty to the gradWeight to avoid overfitting.\n\n\nIn our code implmenation, gradWeight = gradWeight + alpha * weight * weight\n\n\nFor more details, please refer to \nwiki\n.\n\n\nScala example:\n\n\n\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nRNG.setSeed(100)linear.updateParameters\n\nval input = Tensor(3, 5).rand\nval gradOutput = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L2Regularizer(0.2), bRegularizer = L2Regularizer(0.2))\n\nval output = linear.forward(input)\nval gradInput = linear.backward(input, gradOutput)\n\nscala\n input\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala\n gradOutput\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.4527399       0.13670659      0.87014264      0.5750933       0.063681036\n0.89132196      0.62431186      0.20920213      0.52334774      0.18532822\n0.5622963       0.10837689      0.0058171963    0.21969749      0.3074232\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala\n linear.gradWeight\nres0: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0329735       0.047239657     0.8979603       0.53614384      1.2781229\n0.5621818       0.29772854      0.69706535      0.30559152      0.8352279\n1.3044653       0.43065858      0.9896795       0.7435816       1.6003494\n0.94218314      0.6793372       0.97101355      0.62892824      1.3458569\n0.73134506      0.5975239       0.9109101       0.59374434      1.1656629\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x5]\n\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\nlinear = Linear(5, 5, wRegularizer = L2Regularizer(0.2), bRegularizer = L2Regularizer(0.2))\noutput = linear.forward(input)\ngradInput = linear.backward(input, gradOutput)\n\n\n linear.parameters()\n{u'Linear@787aab5e': {u'bias': array([-0.43960261, -0.12444571,  0.22857292, -0.43216187,  0.27770036], dtype=float32),\n  u'gradBias': array([ 0.51726723,  1.32883406,  0.57567948,  1.7791357 ,  1.2887038 ], dtype=float32),\n  u'gradWeight': array([[ 0.45477036,  0.22262168,  0.21923628,  0.26152173,  0.19836383],\n         [ 1.12261093,  0.72921795,  0.08405925,  0.78192139,  0.48798928],\n         [ 0.34581488,  0.21195598,  0.26357424,  0.18987852,  0.2465664 ],\n         [ 1.18659711,  1.11271608,  0.72589797,  1.19098675,  0.33769298],\n         [ 0.82314551,  0.71177536,  0.4428404 ,  0.764337  ,  0.3500182 ]], dtype=float32),\n  u'weight': array([[ 0.03727285, -0.39697152,  0.42733836, -0.34291714, -0.13833708],\n         [ 0.09232076, -0.09720675, -0.33625153,  0.06477787, -0.34739712],\n         [ 0.17145753,  0.10128133,  0.16679128, -0.33541158,  0.40437087],\n         [-0.03005157, -0.36412898,  0.0629965 ,  0.13443278, -0.38414535],\n         [-0.16630849,  0.06934392,  0.40328237,  0.22299488, -0.1178569 ]], dtype=float32)}}\n\n\n\n\nL1L2 Regularizer\n\n\nScala:\n\n\nval l1l2Regularizer = L1L2Regularizer(l1rate, l2rate)\n\n\n\n\nPython:\n\n\nregularizerl1l2 = L1L2Regularizer(l1rate, l2rate)\n\n\n\n\nL1L2 regularizer is used to add penalty to the gradWeight to avoid overfitting.\n\n\nIn our code implmenation, we will apply L1regularizer and L2regularizer sequentially.\n\n\nFor more details, please refer to \nwiki\n.\n\n\nScala example:\n\n\n\nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nRNG.setSeed(100)\n\nval input = Tensor(3, 5).rand\nval gradOutput = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L1L2Regularizer(0.2, 0.2), bRegularizer = L1L2Regularizer(0.2, 0.2))\n\nval output = linear.forward(input)\nval gradInput = linear.backward(input, gradOutput)\n\nscala\n input\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala\n gradOutput\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.4527399       0.13670659      0.87014264      0.5750933       0.063681036\n0.89132196      0.62431186      0.20920213      0.52334774      0.18532822\n0.5622963       0.10837689      0.0058171963    0.21969749      0.3074232\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala\n linear.gradWeight\nres1: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.069174        1.4422078       0.8913989       0.042112567     0.53756505\n0.14077617      0.8959319       -0.030221784    -0.1583686      0.4690558\n0.37145022      0.99747723      0.5559263       0.58614403      0.66380215\n0.88983417      0.639738        0.14924419      0.027530536     0.71988696\n-0.053217214    -8.643427E-4    -0.036953792    0.29753304      0.06567569\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x5]\n\n\n\n\nPython example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\ngradOutput = np.random.uniform(0, 1, (3, 5)).astype(\nfloat32\n)\nlinear = Linear(5, 5, wRegularizer = L1L2Regularizer(0.2, 0.2), bRegularizer = L1L2Regularizer(0.2, 0.2))\noutput = linear.forward(input)\ngradInput = linear.backward(input, gradOutput)\n\n\n linear.parameters()\n{u'Linear@1356aa91': {u'bias': array([-0.05799473, -0.0548001 ,  0.00408955, -0.22004321, -0.07143869], dtype=float32),\n  u'gradBias': array([ 0.89119786,  1.09953558,  1.03394508,  1.19511735,  2.02241182], dtype=float32),\n  u'gradWeight': array([[ 0.89061081,  0.58810186, -0.10087357,  0.19108151,  0.60029608],\n         [ 0.95275503,  0.2333075 ,  0.46897018,  0.74429053,  1.16038764],\n         [ 0.22894514,  0.60031962,  0.3836292 ,  0.15895618,  0.83136207],\n         [ 0.49079862,  0.80913013,  0.55491877,  0.69608945,  0.80458677],\n         [ 0.98890561,  0.49226439,  0.14861123,  1.37666655,  1.47615671]], dtype=float32),\n  u'weight': array([[ 0.44654208,  0.16320795, -0.36029238, -0.25365737, -0.41974261],\n         [ 0.18809238, -0.28065765,  0.27677274, -0.29904234,  0.41338971],\n         [-0.03731538,  0.22493915,  0.10021331, -0.19495697,  0.25470355],\n         [-0.30836752,  0.12083009,  0.3773002 ,  0.24059358, -0.40325543],\n         [-0.13601269, -0.39310011, -0.05292636,  0.20001481, -0.08444868]], dtype=float32)}}", 
            "title": "Regularizers"
        }, 
        {
            "location": "/APIGuide/Regularizers/#l1-regularizer", 
            "text": "Scala:  val l1Regularizer = L1Regularizer(rate)  Python:  regularizerl1 = L1Regularizer(rate)  L1 regularizer is used to add penalty to the gradWeight to avoid overfitting.  In our code implmenation, gradWeight = gradWeight + alpha * abs(weight)  For more details, please refer to  wiki .  Scala example:  \nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nRNG.setSeed(100)\n\nval input = Tensor(3, 5).rand\nval gradOutput = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L1Regularizer(0.2), bRegularizer = L1Regularizer(0.2))\n\nval output = linear.forward(input)\nval gradInput = linear.backward(input, gradOutput)\n\nscala  input\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala  gradOutput\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.4527399       0.13670659      0.87014264      0.5750933       0.063681036\n0.89132196      0.62431186      0.20920213      0.52334774      0.18532822\n0.5622963       0.10837689      0.0058171963    0.21969749      0.3074232\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala  linear.gradWeight\nres2: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.9835552       1.3616763       0.83564335      0.108898684     0.59625006\n0.21608911      0.8393639       0.0035243928    -0.11795368     0.4453743\n0.38366735      0.9618148       0.47721142      0.5607486       0.6069793\n0.81469804      0.6690552       0.18522228      0.08559488      0.7075894\n-0.030468717    0.056625083     0.051471338     0.2917061       0.109963015\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x5]  Python example:  \nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\nlinear = Linear(5, 5, wRegularizer = L1Regularizer(0.2), bRegularizer = L1Regularizer(0.2))\noutput = linear.forward(input)\ngradInput = linear.backward(input, gradOutput)  linear.parameters()\n{u'Linear@596d857b': {u'bias': array([ 0.3185505 , -0.02004393,  0.34620118, -0.09206461,  0.40776938], dtype=float32),\n  u'gradBias': array([ 2.14087653,  1.82181644,  1.90674937,  1.37307787,  0.81534696], dtype=float32),\n  u'gradWeight': array([[ 0.34909648,  0.85083449,  1.44904375,  0.90150446,  0.57136625],\n         [ 0.3745544 ,  0.42218602,  1.53656614,  1.1836741 ,  1.00702667],\n         [ 0.30529332,  0.26813674,  0.85559171,  0.61224306,  0.34721529],\n         [ 0.22859855,  0.8535381 ,  1.19809723,  1.37248564,  0.50041491],\n         [ 0.36197871,  0.03069445,  0.64837945,  0.12765063,  0.12872688]], dtype=float32),\n  u'weight': array([[-0.12423037,  0.35694697,  0.39038274, -0.34970999, -0.08283543],\n         [-0.4186025 , -0.33235055,  0.34948507,  0.39953214,  0.16294235],\n         [-0.25171402, -0.28955361, -0.32243955, -0.19771226, -0.29320192],\n         [-0.39263198,  0.37766701,  0.14673658,  0.24882999, -0.0779015 ],\n         [ 0.0323218 , -0.31266898,  0.31543773, -0.0898933 , -0.33485892]], dtype=float32)}}", 
            "title": "L1 Regularizer"
        }, 
        {
            "location": "/APIGuide/Regularizers/#l2-regularizer", 
            "text": "Scala:  val l2Regularizer = L2Regularizer(rate)  Python:  regularizerl2 = L2Regularizer(rate)  L2 regularizer is used to add penalty to the gradWeight to avoid overfitting.  In our code implmenation, gradWeight = gradWeight + alpha * weight * weight  For more details, please refer to  wiki .  Scala example:  \nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nRNG.setSeed(100)linear.updateParameters\n\nval input = Tensor(3, 5).rand\nval gradOutput = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L2Regularizer(0.2), bRegularizer = L2Regularizer(0.2))\n\nval output = linear.forward(input)\nval gradInput = linear.backward(input, gradOutput)\n\nscala  input\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala  gradOutput\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.4527399       0.13670659      0.87014264      0.5750933       0.063681036\n0.89132196      0.62431186      0.20920213      0.52334774      0.18532822\n0.5622963       0.10837689      0.0058171963    0.21969749      0.3074232\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala  linear.gradWeight\nres0: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0329735       0.047239657     0.8979603       0.53614384      1.2781229\n0.5621818       0.29772854      0.69706535      0.30559152      0.8352279\n1.3044653       0.43065858      0.9896795       0.7435816       1.6003494\n0.94218314      0.6793372       0.97101355      0.62892824      1.3458569\n0.73134506      0.5975239       0.9109101       0.59374434      1.1656629\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\nlinear = Linear(5, 5, wRegularizer = L2Regularizer(0.2), bRegularizer = L2Regularizer(0.2))\noutput = linear.forward(input)\ngradInput = linear.backward(input, gradOutput)  linear.parameters()\n{u'Linear@787aab5e': {u'bias': array([-0.43960261, -0.12444571,  0.22857292, -0.43216187,  0.27770036], dtype=float32),\n  u'gradBias': array([ 0.51726723,  1.32883406,  0.57567948,  1.7791357 ,  1.2887038 ], dtype=float32),\n  u'gradWeight': array([[ 0.45477036,  0.22262168,  0.21923628,  0.26152173,  0.19836383],\n         [ 1.12261093,  0.72921795,  0.08405925,  0.78192139,  0.48798928],\n         [ 0.34581488,  0.21195598,  0.26357424,  0.18987852,  0.2465664 ],\n         [ 1.18659711,  1.11271608,  0.72589797,  1.19098675,  0.33769298],\n         [ 0.82314551,  0.71177536,  0.4428404 ,  0.764337  ,  0.3500182 ]], dtype=float32),\n  u'weight': array([[ 0.03727285, -0.39697152,  0.42733836, -0.34291714, -0.13833708],\n         [ 0.09232076, -0.09720675, -0.33625153,  0.06477787, -0.34739712],\n         [ 0.17145753,  0.10128133,  0.16679128, -0.33541158,  0.40437087],\n         [-0.03005157, -0.36412898,  0.0629965 ,  0.13443278, -0.38414535],\n         [-0.16630849,  0.06934392,  0.40328237,  0.22299488, -0.1178569 ]], dtype=float32)}}", 
            "title": "L2 Regularizer"
        }, 
        {
            "location": "/APIGuide/Regularizers/#l1l2-regularizer", 
            "text": "Scala:  val l1l2Regularizer = L1L2Regularizer(l1rate, l2rate)  Python:  regularizerl1l2 = L1L2Regularizer(l1rate, l2rate)  L1L2 regularizer is used to add penalty to the gradWeight to avoid overfitting.  In our code implmenation, we will apply L1regularizer and L2regularizer sequentially.  For more details, please refer to  wiki .  Scala example:  \nimport com.intel.analytics.bigdl.utils.RandomGenerator.RNG\nimport com.intel.analytics.bigdl.tensor._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nRNG.setSeed(100)\n\nval input = Tensor(3, 5).rand\nval gradOutput = Tensor(3, 5).rand\nval linear = Linear(5, 5, wRegularizer = L1L2Regularizer(0.2, 0.2), bRegularizer = L1L2Regularizer(0.2, 0.2))\n\nval output = linear.forward(input)\nval gradInput = linear.backward(input, gradOutput)\n\nscala  input\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.54340494      0.67115563      0.2783694       0.4120464       0.4245176\n0.52638245      0.84477615      0.14860484      0.004718862     0.15671109\n0.12156912      0.18646719      0.67074907      0.21010774      0.82585275\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala  gradOutput\ngradOutput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.4527399       0.13670659      0.87014264      0.5750933       0.063681036\n0.89132196      0.62431186      0.20920213      0.52334774      0.18532822\n0.5622963       0.10837689      0.0058171963    0.21969749      0.3074232\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x5]\n\nscala  linear.gradWeight\nres1: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.069174        1.4422078       0.8913989       0.042112567     0.53756505\n0.14077617      0.8959319       -0.030221784    -0.1583686      0.4690558\n0.37145022      0.99747723      0.5559263       0.58614403      0.66380215\n0.88983417      0.639738        0.14924419      0.027530536     0.71988696\n-0.053217214    -8.643427E-4    -0.036953792    0.29753304      0.06567569\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 5x5]  Python example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\n\ninput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\ngradOutput = np.random.uniform(0, 1, (3, 5)).astype( float32 )\nlinear = Linear(5, 5, wRegularizer = L1L2Regularizer(0.2, 0.2), bRegularizer = L1L2Regularizer(0.2, 0.2))\noutput = linear.forward(input)\ngradInput = linear.backward(input, gradOutput)  linear.parameters()\n{u'Linear@1356aa91': {u'bias': array([-0.05799473, -0.0548001 ,  0.00408955, -0.22004321, -0.07143869], dtype=float32),\n  u'gradBias': array([ 0.89119786,  1.09953558,  1.03394508,  1.19511735,  2.02241182], dtype=float32),\n  u'gradWeight': array([[ 0.89061081,  0.58810186, -0.10087357,  0.19108151,  0.60029608],\n         [ 0.95275503,  0.2333075 ,  0.46897018,  0.74429053,  1.16038764],\n         [ 0.22894514,  0.60031962,  0.3836292 ,  0.15895618,  0.83136207],\n         [ 0.49079862,  0.80913013,  0.55491877,  0.69608945,  0.80458677],\n         [ 0.98890561,  0.49226439,  0.14861123,  1.37666655,  1.47615671]], dtype=float32),\n  u'weight': array([[ 0.44654208,  0.16320795, -0.36029238, -0.25365737, -0.41974261],\n         [ 0.18809238, -0.28065765,  0.27677274, -0.29904234,  0.41338971],\n         [-0.03731538,  0.22493915,  0.10021331, -0.19495697,  0.25470355],\n         [-0.30836752,  0.12083009,  0.3773002 ,  0.24059358, -0.40325543],\n         [-0.13601269, -0.39310011, -0.05292636,  0.20001481, -0.08444868]], dtype=float32)}}", 
            "title": "L1L2 Regularizer"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/", 
            "text": "Optimizer\n\n\nAn optimizer is in general to minimize any function with respect to a set of parameters. In case of training a neural network, an optimizer tries to minimize the loss of the neural net with respect to its weights/biases, over the training set.\n\n\nScala API\n\n\nFactory method\n\n\nval optimizer = Opimizer[T: ClassTag](\n  model: Module[T],\n  sampleRDD: RDD[Sample[T]],\n  criterion: Criterion[T],\n  batchSize: Int)\n\n\n\n\nT\n: the numeric type(Float/Double).\n\n\nmodel\n: the model will be optimized.\n\n\nsampleRDD\n: an RDD of training Sample.\n\n\ncriterion\n: the Loss function.\n\n\nbatchSize\n: size of minibatch. \n\n\nval optimizer = Opimizer[T: ClassTag, D](\n  model: Module[T],\n  dataset: DataSet[D],\n  criterion: Criterion[T])\n\n\n\n\nT\n: the numeric type(Float/Double).\n\n\nD\n: should be a kind of MiniBatch.\n\n\nmodel\n: the model will be optimized.\n\n\ndataset\n: the training DataSet.\n\n\ncriterion\n: the Loss function.\n\n\ndef apply[T: ClassTag](\n      model: Module[T],\n      sampleRDD: RDD[Sample[T]],\n      criterion: Criterion[T],\n      batchSize: Int,\n      featurePaddingParam: PaddingParam[T],\n      labelPaddingParam: PaddingParam[T])\n\n\n\n\nApply an Optimizer who could apply padding to the Samples with a padding strategy.\n\n\nmodel\n: model will be optimizied.\n\n\nsampleRDD\n: training Samples.\n\n\ncriterion\n: loss function.\n\n\nbatchSize\n: mini batch size.\n\n\nfeaturePaddingParam\n: feature padding strategy.\n\n\nlabelPaddingParam\n: label padding strategy.\n\n\ndef apply[T: ClassTag](\n      model: Module[T],\n      sampleRDD: RDD[Sample[T]],\n      criterion: Criterion[T],\n      batchSize: Int,\n      miniBatch: MiniBatch[T])\n\n\n\n\nApply an optimizer with User-Defined \nMiniBatch\n.\n\n\nmodel\n: model will be optimizied.\n\n\nsampleRDD\n: training Samples.\n\n\ncriterion\n: loss function.\n\n\nbatchSize\n: mini batch size.\n\n\nminiBatch\n: An User-Defined MiniBatch to construct a mini batch.\n\n\nValidation\n\n\nFunction setValidation is to set a validate evaluation in the \noptimizer\n.\n\n\noptimizer.setValidation(\n  trigger: Trigger,\n  dataset: DataSet[MiniBatch[T]],\n  vMethods : Array[ValidationMethod[T])\n\n\n\n\ntrigger\n: how often to evaluation validation set.\n\n\ndataset\n: validate data set in type of DataSet[MiniBatch].\n\n\nvMethods\n: a set of ValidationMethod.\n\n \n\n\noptimizer.setValidation(\n  trigger: Trigger,\n  sampleRDD: RDD[Sample[T]],\n  vMethods: Array[ValidationMethod[T]],\n  batchSize: Int)\n\n\n\n\ntrigger\n: how often to evaluation validation set.\n\n\nsampleRDD\n: validate data set in type of RDD[Sample].\n\n\nvMethods\n: a set of ValidationMethod.\n\n\nbatchSize\n: size of mini batch.\n\n\nCheckpoint\n\n\noptimizer.setCheckpoint(path: String, trigger: Trigger)\n\n\n\n\nFunction setCheckPoint is used to set a check point saved at \npath\n triggered by \ntrigger\n.\n\n\npath\n: a local/HDFS directory to save checkpoint.\n\n\ntrigger\n: how often to save the check point.\n\n \n\n\nval path = optimizer.getCheckpointPath()\n\n\n\n\nFunction getCheckpointPath is used to get the directory of saving checkpoint.\n\n \n\n\noptimizer.overWriteCheckpoint()\n\n\n\n\nFunction overWriteCheckpoint is enable overwrite saving checkpoint.  \n\n\nSummary\n\n\noptimizer.setTrainSummary(trainSummary: TrainSummary)\n\n\n\n\nFunction setTrainSummary is used to enable train summary in this optimizer.\n\n\ntrainSummary\n: an instance of TrainSummary.\n\n \n\n\noptimizer.setValidationSummary(validationSummary: ValidationSummary)\n\n\n\n\nFunction setValidationSummary is used to enable validation summary in this optimizer.\n\n\nvalidationSummary\n: an instance of ValidationSummary.  \n\n\nOther important API\n\n\nval trainedModel = optimizer.optimize()\n\n\n\n\nFunction optimize will start the training.\n\n \n\n\noptimizer.setModel(newModel: Module[T])\n\n\n\n\nFunction setModel will set a new model to the optimizer.\n\n\nnewModel\n: a model will replace the old model in optimizer.\n\n \n\n\noptimizer.setState(state: Table)\n\n\n\n\nFunction setState is used to set a state(learning rate, epochs...) to the \noptimizer\n.\n\n\nstate\n: the state to be saved.\n\n \n\n\noptimizer.setOptimMethod(method : OptimMethod[T])\n\n\n\n\nFunction setOptimMethod is used to set an optimization method in this \noptimizer\n.\n\n\nmethod\n: the method the optimize the model in this \noptimizer\n.\n\n \n\n\noptimizer.setEndWhen(endWhen: Trigger)\n\n\n\n\nFunction setEndWhen is used to declare when to stop the training invoked by \noptimize()\n.\n\n\nendWhen\n: a trigger to stop the training.\n\n\nScala example\n\n\nHere is an example to new an Optimizer with SGD for optimizing LeNet5 model.\n\n\nval trainingRDD = ...\nval valRDD = ...\nval batchSize = 12\n// Create an optimizer\nval optimizer = Optimizer(\n  model = LeNet5(classNum = 10),\n  sampleRDD = trainingRDD,\n  criterion = ClassNLLCriterion(),\n  batchSize = batchSize\n).setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy), batchSize) // set validation method\n  .setEndWhen(Trigger.maxEpoch(15)) // set end trigger\n  .setOptimMethod(new SGD(learningRate = 0.05)) // set optimize method, Since 0.2.0. Older version should use optimizer.setOptimMethod(new SGD()).setState(T(\nlearningRate\n -\n 0.05))\n\nval trainedModel = optimizer.optimize()\n\n\n\n\nPython API\n\n\nFactory method\n\n\noptimizer =  Optimizer(model,\n                 training_rdd,\n                 criterion,\n                 end_trigger,\n                 batch_size,\n                 optim_method=None,\n                 bigdl_type=\nfloat\n)\n\n\n\n\nmodel\n: the model will be optimized.\n\n\ntraining_rdd\n: the training dataset.\n\n\ncriterion\n: the Loss function.\n\n\nend_trigger\n: when to end the optimization.\n\n\nbatch_size\n: size of minibatch.\n\n\noptim_method\n:  the algorithm to use for optimization, e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.\n\n\nbigdl_type\n: the numeric type(Float/Double).  \n\n\nValidation\n\n\nFunction setValidation is to set a validate evaluation in the \noptimizer\n.\n\n\noptimizer.set_validation(batch_size, val_rdd, trigger, val_method=[\nTop1Accuracy\n])\n\n\n\n\ntrigger\n: how often to evaluation validation set.\n\n\nval_rdd\n: validate data set in type of RDD[Sample].\n\n\nval_method\n: a list of ValidationMethod, e.g. \"Top1Accuracy\", \"Top5Accuracy\", \"Loss\".\n\n\nbatch_size\n: size of mini batch.\n\n\nCheckpoint\n\n\noptimizer.set_checkpoint(checkpoint_trigger,\n                      checkpoint_path, isOverWrite=True)\n\n\n\n\nFunction setCheckPoint is used to set a check point saved at \npath\n triggered by \ntrigger\n.\n\n\ncheckpoint_trigger\n: how often to save the check point.\n\ncheckpoint_path\n: a local/HDFS directory to save checkpoint.\n\n\nisOverWrite\n: whether to overwrite existing snapshots in path.default is True\n\n\nSummary\n\n\noptimizer.set_train_summary(summary)\n\n\n\n\nSet train summary. A TrainSummary object contains information necessary for the optimizer to know how often the logs are recorded, where to store the logs and how to retrieve them, etc. For details, refer to the docs of TrainSummary.\n\nsummary\n: an instance of TrainSummary.\n\n\noptimizer.set_validation_summary(summary)\n\n\n\n\nFunction setValidationSummary is used to set validation summary. A ValidationSummary object contains information necessary for the optimizer to know how often the logs are recorded, where to store the logs and how to retrieve them, etc. For details, refer to the docs of ValidationSummary.\n\nsummary\n: an instance of ValidationSummary.\n\n\nStart Training\n\n\ntrained_model = optimizer.optimize()\n\n\n\n\nFunction optimize will start the training.\n\n\nSet Model\n\n\noptimizer.set_model(model)\n\n\n\n\nFunction setModel will set a new model to the optimizer.\n\n\nmodel\n: a model will replace the old model in optimizer.\n\n\nPython example\n\n\nHere is an example to new an Optimizer with SGD for optimizing LeNet5 model.\n\n\ntrain_data = ...\ntest_data = ...\nbatch_size = 12\n# Create an Optimizer, Since 0.2.0\noptimizer = Optimizer(\n  model=lenet_model,\n  training_rdd=train_data,\n  criterion=ClassNLLCriterion(),\n  optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002), # set optim method\n  end_trigger=MaxEpoch(15),\n  batch_size=batch_size)\n\n# Older version, before 0.2.0, use following code: \n# optimizer = Optimizer(\n#   model=model,\n#   training_rdd=train_data,\n#   criterion=ClassNLLCriterion(),\n#   optim_method=\nSGD\n,\n#   state={\nlearningRate\n: 0.05},\n#   end_trigger=MaxEpoch(training_epochs),\n#   batch_size=batch_size)\n\noptimizer.set_validation(\n    batch_size=2048,\n    val_rdd=test_data,\n    trigger=EveryEpoch(),\n    val_method=[Top1Accuracy()]\n)\n\n# Older version, before 0.2.0, use following code: \n#optimizer.set_validation(\n#    batch_size=2048,\n#    val_rdd=test_data,\n#    trigger=EveryEpoch(),\n#    val_method=[\nTop1Accuracy\n]\n#)\n\ntrained_model = optimizer.optimize()\n\n\n\n\n\nHow BigDL train models in a distributed cluster?\n\n\nBigDL distributed training is data parallelism. The training data is split among workers and cached in memory. A complete model is also cached on each worker. The model only uses the data of the same worker in the training.\n\n\nBigDL employs a synchronous distributed training. In each iteration, each worker will sync the latest weights, calculate gradients with local data and local model, sync the gradients and update the weights with a given optimization method(e.g. SGD, Adagrad).\n\n\nIn gradients and weights sync, BigDL doesn't use the RDD APIs like(broadcast, reduce, aggregate, treeAggregate). The problem of these methods is every worker needs to communicate with driver, so the driver will become the bottleneck if the parameter is too large or the workers are too many. Instead, BigDL implement a P2P algorithm for parameter sync to remove the bottleneck. For detail of the algorithm, please see the \ncode", 
            "title": "Optimizer"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/#optimizer", 
            "text": "An optimizer is in general to minimize any function with respect to a set of parameters. In case of training a neural network, an optimizer tries to minimize the loss of the neural net with respect to its weights/biases, over the training set.", 
            "title": "Optimizer"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/#scala-api", 
            "text": "Factory method  val optimizer = Opimizer[T: ClassTag](\n  model: Module[T],\n  sampleRDD: RDD[Sample[T]],\n  criterion: Criterion[T],\n  batchSize: Int)  T : the numeric type(Float/Double).  model : the model will be optimized.  sampleRDD : an RDD of training Sample.  criterion : the Loss function.  batchSize : size of minibatch.   val optimizer = Opimizer[T: ClassTag, D](\n  model: Module[T],\n  dataset: DataSet[D],\n  criterion: Criterion[T])  T : the numeric type(Float/Double).  D : should be a kind of MiniBatch.  model : the model will be optimized.  dataset : the training DataSet.  criterion : the Loss function.  def apply[T: ClassTag](\n      model: Module[T],\n      sampleRDD: RDD[Sample[T]],\n      criterion: Criterion[T],\n      batchSize: Int,\n      featurePaddingParam: PaddingParam[T],\n      labelPaddingParam: PaddingParam[T])  Apply an Optimizer who could apply padding to the Samples with a padding strategy.  model : model will be optimizied.  sampleRDD : training Samples.  criterion : loss function.  batchSize : mini batch size.  featurePaddingParam : feature padding strategy.  labelPaddingParam : label padding strategy.  def apply[T: ClassTag](\n      model: Module[T],\n      sampleRDD: RDD[Sample[T]],\n      criterion: Criterion[T],\n      batchSize: Int,\n      miniBatch: MiniBatch[T])  Apply an optimizer with User-Defined  MiniBatch .  model : model will be optimizied.  sampleRDD : training Samples.  criterion : loss function.  batchSize : mini batch size.  miniBatch : An User-Defined MiniBatch to construct a mini batch.  Validation  Function setValidation is to set a validate evaluation in the  optimizer .  optimizer.setValidation(\n  trigger: Trigger,\n  dataset: DataSet[MiniBatch[T]],\n  vMethods : Array[ValidationMethod[T])  trigger : how often to evaluation validation set.  dataset : validate data set in type of DataSet[MiniBatch].  vMethods : a set of ValidationMethod. \n   optimizer.setValidation(\n  trigger: Trigger,\n  sampleRDD: RDD[Sample[T]],\n  vMethods: Array[ValidationMethod[T]],\n  batchSize: Int)  trigger : how often to evaluation validation set.  sampleRDD : validate data set in type of RDD[Sample].  vMethods : a set of ValidationMethod.  batchSize : size of mini batch.  Checkpoint  optimizer.setCheckpoint(path: String, trigger: Trigger)  Function setCheckPoint is used to set a check point saved at  path  triggered by  trigger .  path : a local/HDFS directory to save checkpoint.  trigger : how often to save the check point. \n   val path = optimizer.getCheckpointPath()  Function getCheckpointPath is used to get the directory of saving checkpoint. \n   optimizer.overWriteCheckpoint()  Function overWriteCheckpoint is enable overwrite saving checkpoint.    Summary  optimizer.setTrainSummary(trainSummary: TrainSummary)  Function setTrainSummary is used to enable train summary in this optimizer.  trainSummary : an instance of TrainSummary. \n   optimizer.setValidationSummary(validationSummary: ValidationSummary)  Function setValidationSummary is used to enable validation summary in this optimizer.  validationSummary : an instance of ValidationSummary.    Other important API  val trainedModel = optimizer.optimize()  Function optimize will start the training. \n   optimizer.setModel(newModel: Module[T])  Function setModel will set a new model to the optimizer.  newModel : a model will replace the old model in optimizer. \n   optimizer.setState(state: Table)  Function setState is used to set a state(learning rate, epochs...) to the  optimizer .  state : the state to be saved. \n   optimizer.setOptimMethod(method : OptimMethod[T])  Function setOptimMethod is used to set an optimization method in this  optimizer .  method : the method the optimize the model in this  optimizer . \n   optimizer.setEndWhen(endWhen: Trigger)  Function setEndWhen is used to declare when to stop the training invoked by  optimize() .  endWhen : a trigger to stop the training.", 
            "title": "Scala API"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/#scala-example", 
            "text": "Here is an example to new an Optimizer with SGD for optimizing LeNet5 model.  val trainingRDD = ...\nval valRDD = ...\nval batchSize = 12\n// Create an optimizer\nval optimizer = Optimizer(\n  model = LeNet5(classNum = 10),\n  sampleRDD = trainingRDD,\n  criterion = ClassNLLCriterion(),\n  batchSize = batchSize\n).setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy), batchSize) // set validation method\n  .setEndWhen(Trigger.maxEpoch(15)) // set end trigger\n  .setOptimMethod(new SGD(learningRate = 0.05)) // set optimize method, Since 0.2.0. Older version should use optimizer.setOptimMethod(new SGD()).setState(T( learningRate  -  0.05))\n\nval trainedModel = optimizer.optimize()", 
            "title": "Scala example"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/#python-api", 
            "text": "Factory method  optimizer =  Optimizer(model,\n                 training_rdd,\n                 criterion,\n                 end_trigger,\n                 batch_size,\n                 optim_method=None,\n                 bigdl_type= float )  model : the model will be optimized.  training_rdd : the training dataset.  criterion : the Loss function.  end_trigger : when to end the optimization.  batch_size : size of minibatch.  optim_method :  the algorithm to use for optimization, e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.  bigdl_type : the numeric type(Float/Double).    Validation  Function setValidation is to set a validate evaluation in the  optimizer .  optimizer.set_validation(batch_size, val_rdd, trigger, val_method=[ Top1Accuracy ])  trigger : how often to evaluation validation set.  val_rdd : validate data set in type of RDD[Sample].  val_method : a list of ValidationMethod, e.g. \"Top1Accuracy\", \"Top5Accuracy\", \"Loss\".  batch_size : size of mini batch.  Checkpoint  optimizer.set_checkpoint(checkpoint_trigger,\n                      checkpoint_path, isOverWrite=True)  Function setCheckPoint is used to set a check point saved at  path  triggered by  trigger .  checkpoint_trigger : how often to save the check point. checkpoint_path : a local/HDFS directory to save checkpoint.  isOverWrite : whether to overwrite existing snapshots in path.default is True  Summary  optimizer.set_train_summary(summary)  Set train summary. A TrainSummary object contains information necessary for the optimizer to know how often the logs are recorded, where to store the logs and how to retrieve them, etc. For details, refer to the docs of TrainSummary. summary : an instance of TrainSummary.  optimizer.set_validation_summary(summary)  Function setValidationSummary is used to set validation summary. A ValidationSummary object contains information necessary for the optimizer to know how often the logs are recorded, where to store the logs and how to retrieve them, etc. For details, refer to the docs of ValidationSummary. summary : an instance of ValidationSummary.  Start Training  trained_model = optimizer.optimize()  Function optimize will start the training.  Set Model  optimizer.set_model(model)  Function setModel will set a new model to the optimizer.  model : a model will replace the old model in optimizer.", 
            "title": "Python API"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/#python-example", 
            "text": "Here is an example to new an Optimizer with SGD for optimizing LeNet5 model.  train_data = ...\ntest_data = ...\nbatch_size = 12\n# Create an Optimizer, Since 0.2.0\noptimizer = Optimizer(\n  model=lenet_model,\n  training_rdd=train_data,\n  criterion=ClassNLLCriterion(),\n  optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002), # set optim method\n  end_trigger=MaxEpoch(15),\n  batch_size=batch_size)\n\n# Older version, before 0.2.0, use following code: \n# optimizer = Optimizer(\n#   model=model,\n#   training_rdd=train_data,\n#   criterion=ClassNLLCriterion(),\n#   optim_method= SGD ,\n#   state={ learningRate : 0.05},\n#   end_trigger=MaxEpoch(training_epochs),\n#   batch_size=batch_size)\n\noptimizer.set_validation(\n    batch_size=2048,\n    val_rdd=test_data,\n    trigger=EveryEpoch(),\n    val_method=[Top1Accuracy()]\n)\n\n# Older version, before 0.2.0, use following code: \n#optimizer.set_validation(\n#    batch_size=2048,\n#    val_rdd=test_data,\n#    trigger=EveryEpoch(),\n#    val_method=[ Top1Accuracy ]\n#)\n\ntrained_model = optimizer.optimize()", 
            "title": "Python example"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optimizer/#how-bigdl-train-models-in-a-distributed-cluster", 
            "text": "BigDL distributed training is data parallelism. The training data is split among workers and cached in memory. A complete model is also cached on each worker. The model only uses the data of the same worker in the training.  BigDL employs a synchronous distributed training. In each iteration, each worker will sync the latest weights, calculate gradients with local data and local model, sync the gradients and update the weights with a given optimization method(e.g. SGD, Adagrad).  In gradients and weights sync, BigDL doesn't use the RDD APIs like(broadcast, reduce, aggregate, treeAggregate). The problem of these methods is every worker needs to communicate with driver, so the driver will become the bottleneck if the parameter is too large or the workers are too many. Instead, BigDL implement a P2P algorithm for parameter sync to remove the bottleneck. For detail of the algorithm, please see the  code", 
            "title": "How BigDL train models in a distributed cluster?"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/", 
            "text": "OptimMethod\n\n\nOptimMethod is used to update model gradient parameters.We have defined SGD method, Adagrad method, etc.\nDetails about those optim methods, you can refer to \nOptim-Methods\n.\nNow, method construct parameters(e.g.\"learningRate\") and internal training parameters(e.g.\"epoch\") store in optim method instead of state(since version 0.2.0)\nHere is mainly to describe how to use those methods when training\n\n\nSet method\n\n\nscala\n\n\noptimizer.setOptimMethod(method : OptimMethod[T])\n\n\n\n\npython\n\n\noptimizer = Optimizer(\n    model,\n    training_rdd,\n    criterion,\n    optim_method,\n    end_trigger,\n    batch_size)\n\n\n\n\nin python, you can set optim method when creating an optimizer\n\n\nSave method\n\n\nmethod.save(path: String, overWrite: Boolean = false)\n\n\n\n\nT\n: path to save method\n\n\noverWrite\n: whether to overwrite or not\n\n\nWhen training, you can use optimizer.setCheckPoint(for scala) or optimizer.set_checkpoint(for python) to save methods at regular intervals.\n\n\nLoad method\n\n\nscala\n\n\nval method = OptimMethod.load(path : String)\n\n\n\n\npath\n: file of optim method path\n\n\npython\n\n\noptimizer = OptimMethod.load(path, bigdl_type=\nfloat\n)\n\n\n\n\nbigdl_type\n: type of optim method, default is \"float\"\n\n\nScala example\n\n\nHere is an example to train LeNet5 model with a loading method.\n\n\nval trainingRDD = ...\nval valRDD = ...\nval batchSize = 12\nval methodPath = ...\n// Load optim method\nval method = OptimMethod.load(methodPath)\n// Create an optimizer\nval optimizer = Optimizer(\n  model = LeNet5(classNum = 10),\n  sampleRDD = trainingRDD,\n  criterion = ClassNLLCriterion(),\n  batchSize = batchSize\n).setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy), batchSize)\n  .setEndWhen(Trigger.maxEpoch(15))\n\noptimizer.setOptimMethod(method) // set optim method\n\noptimizer.setCheckpoint(param.checkpoint.get, checkpointTrigger) // set checkpoint to save model and optim method\n\nval trainedModel = optimizer.optimize()\n\n\n\n\nPython example\n\n\nHere is an example to train LeNet5 model with SGD method.\n\n\ntrain_data = ...\ntest_data = ...\nbatch_size = 12\noptimizer = Optimizer(\n  model=lenet_model,\n  training_rdd=train_data,\n  criterion=ClassNLLCriterion(),\n  optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002), # set optim method\n  end_trigger=MaxEpoch(15),\n  batch_size=batch_size)\n\noptimizer.set_validation(\n    batch_size=32,\n    val_rdd=test_data,\n    trigger=EveryEpoch(),\n    val_method=[Top1Accuracy()]\n)\n\noptimizer.set_checkpoint(EveryEpoch(), checkpointPath) # set checkpoint to save model and optim method\n\ntrained_model = optimizer.optimize()", 
            "title": "OptimMethod"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/#optimmethod", 
            "text": "OptimMethod is used to update model gradient parameters.We have defined SGD method, Adagrad method, etc.\nDetails about those optim methods, you can refer to  Optim-Methods .\nNow, method construct parameters(e.g.\"learningRate\") and internal training parameters(e.g.\"epoch\") store in optim method instead of state(since version 0.2.0)\nHere is mainly to describe how to use those methods when training", 
            "title": "OptimMethod"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/#set-method", 
            "text": "scala  optimizer.setOptimMethod(method : OptimMethod[T])  python  optimizer = Optimizer(\n    model,\n    training_rdd,\n    criterion,\n    optim_method,\n    end_trigger,\n    batch_size)  in python, you can set optim method when creating an optimizer", 
            "title": "Set method"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/#save-method", 
            "text": "method.save(path: String, overWrite: Boolean = false)  T : path to save method  overWrite : whether to overwrite or not  When training, you can use optimizer.setCheckPoint(for scala) or optimizer.set_checkpoint(for python) to save methods at regular intervals.", 
            "title": "Save method"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/#load-method", 
            "text": "scala  val method = OptimMethod.load(path : String)  path : file of optim method path  python  optimizer = OptimMethod.load(path, bigdl_type= float )  bigdl_type : type of optim method, default is \"float\"", 
            "title": "Load method"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/#scala-example", 
            "text": "Here is an example to train LeNet5 model with a loading method.  val trainingRDD = ...\nval valRDD = ...\nval batchSize = 12\nval methodPath = ...\n// Load optim method\nval method = OptimMethod.load(methodPath)\n// Create an optimizer\nval optimizer = Optimizer(\n  model = LeNet5(classNum = 10),\n  sampleRDD = trainingRDD,\n  criterion = ClassNLLCriterion(),\n  batchSize = batchSize\n).setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy), batchSize)\n  .setEndWhen(Trigger.maxEpoch(15))\n\noptimizer.setOptimMethod(method) // set optim method\n\noptimizer.setCheckpoint(param.checkpoint.get, checkpointTrigger) // set checkpoint to save model and optim method\n\nval trainedModel = optimizer.optimize()", 
            "title": "Scala example"
        }, 
        {
            "location": "/APIGuide/Optimizers/OptimMethod/#python-example", 
            "text": "Here is an example to train LeNet5 model with SGD method.  train_data = ...\ntest_data = ...\nbatch_size = 12\noptimizer = Optimizer(\n  model=lenet_model,\n  training_rdd=train_data,\n  criterion=ClassNLLCriterion(),\n  optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002), # set optim method\n  end_trigger=MaxEpoch(15),\n  batch_size=batch_size)\n\noptimizer.set_validation(\n    batch_size=32,\n    val_rdd=test_data,\n    trigger=EveryEpoch(),\n    val_method=[Top1Accuracy()]\n)\n\noptimizer.set_checkpoint(EveryEpoch(), checkpointPath) # set checkpoint to save model and optim method\n\ntrained_model = optimizer.optimize()", 
            "title": "Python example"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/", 
            "text": "Adam\n\n\nScala:\n\n\nval optim = new Adam(learningRate=1e-3, learningRateDecay=0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8)\n\n\n\n\nPython:\n\n\noptim = Adam(learningRate=1e-3, learningRateDecay-0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8, bigdl_type=\nfloat\n)\n\n\n\n\nAn implementation of Adam optimization, first-order gradient-based optimization of stochastic  objective  functions. http://arxiv.org/pdf/1412.6980.pdf\n\n\nlearningRate\n learning rate. Default value is 1e-3. \n\n\nlearningRateDecay\n learning rate decay. Default value is 0.0.\n\n\nbeta1\n first moment coefficient. Default value is 0.9.\n\n\nbeta2\n second moment coefficient. Default value is 0.999.\n\n\nEpsilon\n for numerical stability. Default value is 1e-8.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval optm = new Adam(learningRate=0.002)\ndef rosenBrock(x: Tensor[Float]): (Float, Tensor[Float]) = {\n    // (1) compute f(x)\n    val d = x.size(1)\n\n    // x1 = x(i)\n    val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // x(i + 1) - x(i)^2\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n    // 100 * (x(i + 1) - x(i)^2)^2\n    x1.cmul(x1).mul(100)\n\n    // x0 = x(i)\n    val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // 1-x(i)\n    x0.mul(-1).add(1)\n    x0.cmul(x0)\n    // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n    x1.add(x0)\n\n    val fout = x1.sum()\n\n    // (2) compute f(x)/dx\n    val dxout = Tensor[Float]().resizeAs(x).zero()\n    // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n    x1.copy(x.narrow(1, 1, d - 1))\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n    x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n    x1.add(x0)\n    dxout.narrow(1, 1, d - 1).copy(x1)\n\n    // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n    x0.copy(x.narrow(1, 1, d - 1))\n    x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n    dxout.narrow(1, 2, d - 1).add(x0)\n\n    (fout, dxout)\n  }  \nval x = Tensor(2).fill(0)\n\n print(optm.optimize(rosenBrock, x))\n(0.0019999996\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcD$sp of size 2],[D@302d88d8)\n\n\n\n\nPython example:\n\n\noptim_method = Adam(learningrate=0.002)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)\n\n\n\n\n\nSGD\n\n\nScala:\n\n\nval optimMethod = SGD(learningRate= 1e-3,learningRateDecay=0.0,\n                      weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                      nesterov=false,learningRateSchedule=Default(),\n                      learningRates=null,weightDecays=null)\n\n\n\n\nPython:\n\n\noptim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                   momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                   leaningrate_schedule=None,learningrates=None,\n                   weightdecays=None,bigdl_type=\nfloat\n)\n\n\n\n\nA plain implementation of SGD which provides optimize method. After setting \noptimization method when create Optimize, Optimize will call optimization method at the end of \neach iteration.\n\n\nScala example:\n\n\nval optimMethod = new SGD[Float](learningRate= 1e-3,learningRateDecay=0.0,\n                               weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                               nesterov=false,learningRateSchedule=Default(),\n                               learningRates=null,weightDecays=null)\noptimizer.setOptimMethod(optimMethod)\n\n\n\n\nPython example:\n\n\noptim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                  momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                  leaningrate_schedule=None,learningrates=None,\n                  weightdecays=None,bigdl_type=\nfloat\n)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)\n\n\n\n\nAdadelta\n\n\nAdaDelta\n implementation for \nSGD\n \nIt has been proposed in \nADADELTA: An Adaptive Learning Rate Method\n.\nhttp://arxiv.org/abs/1212.5701.\n\n\nScala:\n\n\nval optimMethod = Adadelta(decayRate = 0.9, Epsilon = 1e-10)\n\n\n\n\nPython:\n\n\noptim_method = AdaDelta(decayrate = 0.9, epsilon = 1e-10)\n\n\n\n\nScala example:\n\n\noptimizer.setOptimMethod(new Adadelta(0.9, 1e-10))\n\n\n\n\n\nPython example:\n\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adadelta(0.9, 0.00001),\n    end_trigger=MaxEpoch(20),\n    batch_size=32)\n\n\n\n\nRMSprop\n\n\nAn implementation of RMSprop (Reference: http://arxiv.org/pdf/1308.0850v5.pdf, Sec 4.2)\n\n learningRate : learning rate\n\n learningRateDecaye : learning rate decay\n\n decayRatee : decayRate, also called rho\n\n Epsilone : for numerical stability\n\n\nAdamax\n\n\nAn implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf\n\n\nArguments:\n\n\n\n\nlearningRate : learning rate\n\n\nbeta1 : first moment coefficient\n\n\nbeta2 : second moment coefficient\n\n\nEpsilon : for numerical stability\n\n\n\n\nReturns:\n\n\nthe new x vector and the function list {fx}, evaluated before the update\n\n\nAdagrad\n\n\nScala:\n\n\nval adagrad = new Adagrad(learningRate = 1e-3,\n                          learningRateDecay = 0.0,\n                          weightDecay = 0.0)\n\n\n\n\n\nAn implementation of Adagrad. See the original paper:\n \nhttp://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor._\nval adagrad = Adagrad(0.01, 0.0, 0.0)\n    def feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n      // (1) compute f(x)\n      val d = x.size(1)\n      // x1 = x(i)\n      val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // x(i + 1) - x(i)^2\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n      // 100 * (x(i + 1) - x(i)^2)^2\n      x1.cmul(x1).mul(100)\n      // x0 = x(i)\n      val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // 1-x(i)\n      x0.mul(-1).add(1)\n      x0.cmul(x0)\n      // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n      x1.add(x0)\n      val fout = x1.sum()\n      // (2) compute f(x)/dx\n      val dxout = Tensor[Float]().resizeAs(x).zero()\n      // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n      x1.copy(x.narrow(1, 1, d - 1))\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n      x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n      x1.add(x0)\n      dxout.narrow(1, 1, d - 1).copy(x1)\n      // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n      x0.copy(x.narrow(1, 1, d - 1))\n      x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n      dxout.narrow(1, 2, d - 1).add(x0)\n      (fout, dxout)\n    }\nval x = Tensor(2).fill(0)\nval config = T(\nlearningRate\n -\n 1e-1)\nfor (i \n- 1 to 10) {\n  adagrad.optimize(feval, x, config, config)\n}\nx after optimize: 0.27779138\n0.07226955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]\n\n\n\n\nScala:\n\n\nval optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)\n\n\n\n\nPython:\n\n\noptim_method = LBFGS(max_iter=20, max_eval=Double.MaxValue, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)\n\n\n\n\nThis implementation of L-BFGS relies on a user-provided line search function\n(state.lineSearch). If this function is not provided, then a simple learningRate\nis used to produce fixed size steps. Fixed size steps are much less costly than line\nsearches, and can be useful for stochastic problems.\n\n\nThe learning rate is used even when a line search is provided.This is also useful for\nlarge-scale stochastic problems, where opfunc is a noisy approximation of f(x). In that\ncase, the learning rate allows a reduction of confidence in the step size.\n\n\nParameters:\n\n\n \nmaxIter\n - Maximum number of iterations allowed. Default: 20\n\n \nmaxEval\n - Maximum number of function evaluations. Default: Double.MaxValue\n\n \ntolFun\n - Termination tolerance on the first-order optimality. Default: 1e-5\n\n \ntolX\n - Termination tol on progress in terms of func/param changes. Default: 1e-9\n\n \nlearningRate\n - the learning rate. Default: 1.0\n\n \nlineSearch\n - A line search function. Default: None\n* \nlineSearchOptions\n - If no line search provided, then a fixed step size is used. Default: None\n\n\nScala example:\n\n\nval optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)\noptimizer.setOptimMethod(optimMethod)\n\n\n\n\nPython example:\n\n\noptim_method = LBFGS(max_iter=20, max_eval=DOUBLEMAX, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Optimization Algorithms"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/#adam", 
            "text": "Scala:  val optim = new Adam(learningRate=1e-3, learningRateDecay=0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8)  Python:  optim = Adam(learningRate=1e-3, learningRateDecay-0.0, beta1=0.9, beta2=0.999, Epsilon=1e-8, bigdl_type= float )  An implementation of Adam optimization, first-order gradient-based optimization of stochastic  objective  functions. http://arxiv.org/pdf/1412.6980.pdf  learningRate  learning rate. Default value is 1e-3.   learningRateDecay  learning rate decay. Default value is 0.0.  beta1  first moment coefficient. Default value is 0.9.  beta2  second moment coefficient. Default value is 0.999.  Epsilon  for numerical stability. Default value is 1e-8.  Scala example:  import com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.utils.T\n\nval optm = new Adam(learningRate=0.002)\ndef rosenBrock(x: Tensor[Float]): (Float, Tensor[Float]) = {\n    // (1) compute f(x)\n    val d = x.size(1)\n\n    // x1 = x(i)\n    val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // x(i + 1) - x(i)^2\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n    // 100 * (x(i + 1) - x(i)^2)^2\n    x1.cmul(x1).mul(100)\n\n    // x0 = x(i)\n    val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n    // 1-x(i)\n    x0.mul(-1).add(1)\n    x0.cmul(x0)\n    // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n    x1.add(x0)\n\n    val fout = x1.sum()\n\n    // (2) compute f(x)/dx\n    val dxout = Tensor[Float]().resizeAs(x).zero()\n    // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n    x1.copy(x.narrow(1, 1, d - 1))\n    x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n    x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n    x1.add(x0)\n    dxout.narrow(1, 1, d - 1).copy(x1)\n\n    // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n    x0.copy(x.narrow(1, 1, d - 1))\n    x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n    dxout.narrow(1, 2, d - 1).add(x0)\n\n    (fout, dxout)\n  }  \nval x = Tensor(2).fill(0)  print(optm.optimize(rosenBrock, x))\n(0.0019999996\n0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcD$sp of size 2],[D@302d88d8)  Python example:  optim_method = Adam(learningrate=0.002)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Adam"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/#sgd", 
            "text": "Scala:  val optimMethod = SGD(learningRate= 1e-3,learningRateDecay=0.0,\n                      weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                      nesterov=false,learningRateSchedule=Default(),\n                      learningRates=null,weightDecays=null)  Python:  optim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                   momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                   leaningrate_schedule=None,learningrates=None,\n                   weightdecays=None,bigdl_type= float )  A plain implementation of SGD which provides optimize method. After setting \noptimization method when create Optimize, Optimize will call optimization method at the end of \neach iteration.  Scala example:  val optimMethod = new SGD[Float](learningRate= 1e-3,learningRateDecay=0.0,\n                               weightDecay=0.0,momentum=0.0,dampening=Double.MaxValue,\n                               nesterov=false,learningRateSchedule=Default(),\n                               learningRates=null,weightDecays=null)\noptimizer.setOptimMethod(optimMethod)  Python example:  optim_method = SGD(learningrate=1e-3,learningrate_decay=0.0,weightdecay=0.0,\n                  momentum=0.0,dampening=DOUBLEMAX,nesterov=False,\n                  leaningrate_schedule=None,learningrates=None,\n                  weightdecays=None,bigdl_type= float )\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "SGD"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/#adadelta", 
            "text": "AdaDelta  implementation for  SGD  \nIt has been proposed in  ADADELTA: An Adaptive Learning Rate Method .\nhttp://arxiv.org/abs/1212.5701.  Scala:  val optimMethod = Adadelta(decayRate = 0.9, Epsilon = 1e-10)  Python:  optim_method = AdaDelta(decayrate = 0.9, epsilon = 1e-10)  Scala example:  optimizer.setOptimMethod(new Adadelta(0.9, 1e-10))  Python example:  optimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adadelta(0.9, 0.00001),\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Adadelta"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/#rmsprop", 
            "text": "An implementation of RMSprop (Reference: http://arxiv.org/pdf/1308.0850v5.pdf, Sec 4.2)  learningRate : learning rate  learningRateDecaye : learning rate decay  decayRatee : decayRate, also called rho  Epsilone : for numerical stability", 
            "title": "RMSprop"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/#adamax", 
            "text": "An implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf  Arguments:   learningRate : learning rate  beta1 : first moment coefficient  beta2 : second moment coefficient  Epsilon : for numerical stability   Returns:  the new x vector and the function list {fx}, evaluated before the update", 
            "title": "Adamax"
        }, 
        {
            "location": "/APIGuide/Optimizers/Optim-Methods/#adagrad", 
            "text": "Scala:  val adagrad = new Adagrad(learningRate = 1e-3,\n                          learningRateDecay = 0.0,\n                          weightDecay = 0.0)  An implementation of Adagrad. See the original paper:\n  http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf  Scala example:  import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.tensor._\nval adagrad = Adagrad(0.01, 0.0, 0.0)\n    def feval(x: Tensor[Float]): (Float, Tensor[Float]) = {\n      // (1) compute f(x)\n      val d = x.size(1)\n      // x1 = x(i)\n      val x1 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // x(i + 1) - x(i)^2\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1))\n      // 100 * (x(i + 1) - x(i)^2)^2\n      x1.cmul(x1).mul(100)\n      // x0 = x(i)\n      val x0 = Tensor[Float](d - 1).copy(x.narrow(1, 1, d - 1))\n      // 1-x(i)\n      x0.mul(-1).add(1)\n      x0.cmul(x0)\n      // 100*(x(i+1) - x(i)^2)^2 + (1-x(i))^2\n      x1.add(x0)\n      val fout = x1.sum()\n      // (2) compute f(x)/dx\n      val dxout = Tensor[Float]().resizeAs(x).zero()\n      // df(1:D-1) = - 400*x(1:D-1).*(x(2:D)-x(1:D-1).^2) - 2*(1-x(1:D-1));\n      x1.copy(x.narrow(1, 1, d - 1))\n      x1.cmul(x1).mul(-1).add(x.narrow(1, 2, d - 1)).cmul(x.narrow(1, 1, d - 1)).mul(-400)\n      x0.copy(x.narrow(1, 1, d - 1)).mul(-1).add(1).mul(-2)\n      x1.add(x0)\n      dxout.narrow(1, 1, d - 1).copy(x1)\n      // df(2:D) = df(2:D) + 200*(x(2:D)-x(1:D-1).^2);\n      x0.copy(x.narrow(1, 1, d - 1))\n      x0.cmul(x0).mul(-1).add(x.narrow(1, 2, d - 1)).mul(200)\n      dxout.narrow(1, 2, d - 1).add(x0)\n      (fout, dxout)\n    }\nval x = Tensor(2).fill(0)\nval config = T( learningRate  -  1e-1)\nfor (i  - 1 to 10) {\n  adagrad.optimize(feval, x, config, config)\n}\nx after optimize: 0.27779138\n0.07226955\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]  Scala:  val optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)  Python:  optim_method = LBFGS(max_iter=20, max_eval=Double.MaxValue, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)  This implementation of L-BFGS relies on a user-provided line search function\n(state.lineSearch). If this function is not provided, then a simple learningRate\nis used to produce fixed size steps. Fixed size steps are much less costly than line\nsearches, and can be useful for stochastic problems.  The learning rate is used even when a line search is provided.This is also useful for\nlarge-scale stochastic problems, where opfunc is a noisy approximation of f(x). In that\ncase, the learning rate allows a reduction of confidence in the step size.  Parameters:    maxIter  - Maximum number of iterations allowed. Default: 20   maxEval  - Maximum number of function evaluations. Default: Double.MaxValue   tolFun  - Termination tolerance on the first-order optimality. Default: 1e-5   tolX  - Termination tol on progress in terms of func/param changes. Default: 1e-9   learningRate  - the learning rate. Default: 1.0   lineSearch  - A line search function. Default: None\n*  lineSearchOptions  - If no line search provided, then a fixed step size is used. Default: None  Scala example:  val optimMethod = new LBFGS(maxIter=20, maxEval=Double.MaxValue,\n                            tolFun=1e-5, tolX=1e-9, nCorrection=100,\n                            learningRate=1.0, lineSearch=None, lineSearchOptions=None)\noptimizer.setOptimMethod(optimMethod)  Python example:  optim_method = LBFGS(max_iter=20, max_eval=DOUBLEMAX, \\\n                 tol_fun=1e-5, tol_x=1e-9, n_correction=100, \\\n                 learning_rate=1.0, line_search=None, line_search_options=None)\n\noptimizer = Optimizer(\n    model=mlp_model,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=optim_method,\n    end_trigger=MaxEpoch(20),\n    batch_size=32)", 
            "title": "Adagrad"
        }, 
        {
            "location": "/APIGuide/Triggers/", 
            "text": "A trigger specifies a timespot or several timespots during training,\nand a corresponding action will be taken when the timespot(s)\ns reached.\n\n\n\n\nEvery Epoch\n\n\nScala:\n\n\n val trigger = Trigger.everyEpoch\n\n\n\n\nPython:\n\n\n trigger = EveryEpoch()\n\n\n\n\nA trigger that triggers an action when each epoch finishs.\n   Could be used as trigger in \nsetValidation\n and \nsetCheckpoint\n\n   in Optimizer, and also in \nTrainSummary.setSummaryTrigger\n.\n\n\n\n\nSeveral Iteration\n\n\nScala:\n\n\n val trigger = Trigger.severalIteration(n)\n\n\n\n\nPython:\n\n\n trigger = SeveralIteration(n)\n\n\n\n\nA trigger that triggers an action every \nn\n iterations.\n Could be used as trigger in \nsetValidation\n and \nsetCheckpoint\n \n in Optimizer, and also in \nTrainSummary.setSummaryTrigger\n.\n\n\n\n\nMax Epoch\n\n\nScala:\n\n\n val trigger = Trigger.maxEpoch(max)\n\n\n\n\nPython:\n\n\n trigger = MaxEpoch(max)\n\n\n\n\nA trigger that triggers an action when training reaches\n  the number of epochs specified by \"max\".\n  Usually used in \nOptimizer.setEndWhen\n.\n\n\n\n\nMax Iteration\n\n\nScala:\n\n\n val trigger = Trigger.maxIteration(max)\n\n\n\n\nPython:\n\n\n trigger = MaxIteration(max)\n\n\n\n\nA trigger that triggers an action when training reaches\n  the number of iterations specified by \"max\".\n  Usually used in \nOptimizer.setEndWhen\n.\n\n\n\n\nMax Score\n\n\nScala:\n\n\n val trigger = Trigger.maxScore(max)\n\n\n\n\nPython:\n\n\n trigger = MaxScore(max)\n\n\n\n\nA trigger that triggers an action when validation score\n larger than \"max\" score\n\n\n\n\nMin Loss\n\n\nScala:\n\n\n val trigger = Trigger.minLoss(min)\n\n\n\n\nPython:\n\n\n trigger = MinLoss(min)\n\n\n\n\nA trigger that triggers an action when training loss\n less than \"min\" loss", 
            "title": "Triggers"
        }, 
        {
            "location": "/APIGuide/Triggers/#every-epoch", 
            "text": "Scala:   val trigger = Trigger.everyEpoch  Python:   trigger = EveryEpoch()  A trigger that triggers an action when each epoch finishs.\n   Could be used as trigger in  setValidation  and  setCheckpoint \n   in Optimizer, and also in  TrainSummary.setSummaryTrigger .", 
            "title": "Every Epoch"
        }, 
        {
            "location": "/APIGuide/Triggers/#several-iteration", 
            "text": "Scala:   val trigger = Trigger.severalIteration(n)  Python:   trigger = SeveralIteration(n)  A trigger that triggers an action every  n  iterations.\n Could be used as trigger in  setValidation  and  setCheckpoint  \n in Optimizer, and also in  TrainSummary.setSummaryTrigger .", 
            "title": "Several Iteration"
        }, 
        {
            "location": "/APIGuide/Triggers/#max-epoch", 
            "text": "Scala:   val trigger = Trigger.maxEpoch(max)  Python:   trigger = MaxEpoch(max)  A trigger that triggers an action when training reaches\n  the number of epochs specified by \"max\".\n  Usually used in  Optimizer.setEndWhen .", 
            "title": "Max Epoch"
        }, 
        {
            "location": "/APIGuide/Triggers/#max-iteration", 
            "text": "Scala:   val trigger = Trigger.maxIteration(max)  Python:   trigger = MaxIteration(max)  A trigger that triggers an action when training reaches\n  the number of iterations specified by \"max\".\n  Usually used in  Optimizer.setEndWhen .", 
            "title": "Max Iteration"
        }, 
        {
            "location": "/APIGuide/Triggers/#max-score", 
            "text": "Scala:   val trigger = Trigger.maxScore(max)  Python:   trigger = MaxScore(max)  A trigger that triggers an action when validation score\n larger than \"max\" score", 
            "title": "Max Score"
        }, 
        {
            "location": "/APIGuide/Triggers/#min-loss", 
            "text": "Scala:   val trigger = Trigger.minLoss(min)  Python:   trigger = MinLoss(min)  A trigger that triggers an action when training loss\n less than \"min\" loss", 
            "title": "Min Loss"
        }, 
        {
            "location": "/APIGuide/Metrics/", 
            "text": "ValidationMethod is a method to validate the model during model trainning or evaluation.\nThe trait can be extended by user-defined method. Now we have defined Top1Accuracy, Top5Accuracy, Loss.\nYou can use those methods to evaluate model, please refer to \nModule\n for details.\n\n\n\n\nLoss\n\n\nCalculate loss of output and target with criterion. The default criterion is ClassNLLCriterion.\n\n\nScala:\n\n\nval loss = new Loss(criterion)\n\n\n\n\nexample\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.models.lenet.LeNet5\n\nval conf = Engine.createSparkConf()\nval sc = new SparkContext(conf)\nEngine.init\n\nval data = new Array[Sample[Float]](10)\nvar i = 0\nwhile (i \n data.length) {\n  val input = Tensor[Float](28, 28).fill(0.8f)\n  val label = Tensor[Float](1).fill(1.0f)\n  data(i) = Sample(input, label)\n  i += 1\n}\nval model = LeNet5(classNum = 10)\nval dataSet = sc.parallelize(data, 4)\n\nval result = model.evaluate(dataSet, Array(new Loss[Float]().asInstanceOf[ValidationMethod[Float]]))\n\nscala\n result\nres12: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array(((Loss: 9.339776, count: 4, Average Loss: 2.334944),Loss))\n\n\n\n\nPython:\n\n\nloss = Loss(cri)\n\n\n\n\nexample\n\n\nfrom pyspark.context import SparkContext\nfrom bigdl.util.common import *\nfrom bigdl.nn.layer import *\nfrom bigdl.optim.optimizer import *\n\nsc = get_spark_context(conf=create_spark_conf())\ninit_engine()\n\ndata_len = 10\nbatch_size = 8\nFEATURES_DIM = 4\n\ndef gen_rand_sample():\n    features = np.random.uniform(0, 1, (FEATURES_DIM))\n    label = features.sum() + 0.4\n    return Sample.from_ndarray(features, label)\n\ntrainingData = sc.parallelize(range(0, data_len)).map(\n    lambda i: gen_rand_sample())\n\nmodel = Sequential()\nmodel.add(Linear(4, 5))\ntest_results = model.test(trainingData, batch_size, [Loss()])\n\n\n print test_results[0]\nTest result: 0.116546951234, total_num: 10, method: Loss\n\n\n\n\n\n\nTop1Accuracy\n\n\nCaculate the percentage that output's max probability index equals target.\n\n\nScala:\n\n\nval top1accuracy = new Top1Accuracy()\n\n\n\n\nset validation method as Top1Accuracy\n\n\nval result = model.evaluate(dataSet, Array(new Top1Accuracy[Float]().asInstanceOf[ValidationMethod[Float]]))\n\nscala\n result\nres13: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array((Accuracy(correct: 0, count: 10, accuracy: 0.0),Top1Accuracy))\n\n\n\n\nPython:\n\n\ntop1accuracy = Top1Accuracy()\n\n\n\n\ntest_results = model.test(trainingData, batch_size, [Top1Accuracy()])\n\n\n print test_results[0]\nTest result: 0.0, total_num: 10, method: Top1Accuracy\n\n\n\n\n\n\nTop5Accuracy\n\n\nCaculate the percentage that target in output's top5 probability indexes.\n\n\nScala:\n\n\nval top5accuracy = new Top5Accuracy()\n\n\n\n\nset validation method as Top5Accuracy\n\n\nval result = model.evaluate(dataSet, Array(new Top5Accuracy[Float]().asInstanceOf[ValidationMethod[Float]]))\n\nscala\n result\nres18: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array((Accuracy(correct: 10, count: 10, accuracy: 1.0),Top5Accuracy))\n\n\n\n\nPython:\n\n\ntop5accuracy = Top5Accuracy()\n\n\n\n\ntest_results = model.test(trainingData, batch_size, [Top5Accuracy()])\n\n\n print test_results[1]\nTest result: 0.0, total_num: 10, method: Top5Accuracy", 
            "title": "Metrics"
        }, 
        {
            "location": "/APIGuide/Metrics/#loss", 
            "text": "Calculate loss of output and target with criterion. The default criterion is ClassNLLCriterion.  Scala:  val loss = new Loss(criterion)  example  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils.Engine\nimport org.apache.spark.SparkContext\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.models.lenet.LeNet5\n\nval conf = Engine.createSparkConf()\nval sc = new SparkContext(conf)\nEngine.init\n\nval data = new Array[Sample[Float]](10)\nvar i = 0\nwhile (i   data.length) {\n  val input = Tensor[Float](28, 28).fill(0.8f)\n  val label = Tensor[Float](1).fill(1.0f)\n  data(i) = Sample(input, label)\n  i += 1\n}\nval model = LeNet5(classNum = 10)\nval dataSet = sc.parallelize(data, 4)\n\nval result = model.evaluate(dataSet, Array(new Loss[Float]().asInstanceOf[ValidationMethod[Float]]))\n\nscala  result\nres12: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array(((Loss: 9.339776, count: 4, Average Loss: 2.334944),Loss))  Python:  loss = Loss(cri)  example  from pyspark.context import SparkContext\nfrom bigdl.util.common import *\nfrom bigdl.nn.layer import *\nfrom bigdl.optim.optimizer import *\n\nsc = get_spark_context(conf=create_spark_conf())\ninit_engine()\n\ndata_len = 10\nbatch_size = 8\nFEATURES_DIM = 4\n\ndef gen_rand_sample():\n    features = np.random.uniform(0, 1, (FEATURES_DIM))\n    label = features.sum() + 0.4\n    return Sample.from_ndarray(features, label)\n\ntrainingData = sc.parallelize(range(0, data_len)).map(\n    lambda i: gen_rand_sample())\n\nmodel = Sequential()\nmodel.add(Linear(4, 5))\ntest_results = model.test(trainingData, batch_size, [Loss()])  print test_results[0]\nTest result: 0.116546951234, total_num: 10, method: Loss", 
            "title": "Loss"
        }, 
        {
            "location": "/APIGuide/Metrics/#top1accuracy", 
            "text": "Caculate the percentage that output's max probability index equals target.  Scala:  val top1accuracy = new Top1Accuracy()  set validation method as Top1Accuracy  val result = model.evaluate(dataSet, Array(new Top1Accuracy[Float]().asInstanceOf[ValidationMethod[Float]]))\n\nscala  result\nres13: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array((Accuracy(correct: 0, count: 10, accuracy: 0.0),Top1Accuracy))  Python:  top1accuracy = Top1Accuracy()  test_results = model.test(trainingData, batch_size, [Top1Accuracy()])  print test_results[0]\nTest result: 0.0, total_num: 10, method: Top1Accuracy", 
            "title": "Top1Accuracy"
        }, 
        {
            "location": "/APIGuide/Metrics/#top5accuracy", 
            "text": "Caculate the percentage that target in output's top5 probability indexes.  Scala:  val top5accuracy = new Top5Accuracy()  set validation method as Top5Accuracy  val result = model.evaluate(dataSet, Array(new Top5Accuracy[Float]().asInstanceOf[ValidationMethod[Float]]))\n\nscala  result\nres18: Array[(com.intel.analytics.bigdl.optim.ValidationResult, com.intel.analytics.bigdl.optim.ValidationMethod[Float])] = Array((Accuracy(correct: 10, count: 10, accuracy: 1.0),Top5Accuracy))  Python:  top5accuracy = Top5Accuracy()  test_results = model.test(trainingData, batch_size, [Top5Accuracy()])  print test_results[1]\nTest result: 0.0, total_num: 10, method: Top5Accuracy", 
            "title": "Top5Accuracy"
        }, 
        {
            "location": "/APIGuide/scaladoc/", 
            "text": "javadoc", 
            "title": "Scala Docs"
        }, 
        {
            "location": "/APIGuide/scaladoc/#javadoc", 
            "text": "", 
            "title": "javadoc"
        }, 
        {
            "location": "/APIGuide/python-api-doc/", 
            "text": "pythonapidoc", 
            "title": "Python API Docs"
        }, 
        {
            "location": "/APIGuide/python-api-doc/#pythonapidoc", 
            "text": "", 
            "title": "pythonapidoc"
        }, 
        {
            "location": "/powered-by/", 
            "text": "Intel\u2019s BigDL on Databricks\n\n\nUse BigDL on AZure HDInsight\n\n\nA more detailed post for \nHow to use BigDL on Apache Spark for Azure HDInsight\n\n\n\n\n\n\nBigDL on AliCloud E-MapReduce (in Chinese)\n\n\nRunning BigDL, Deep Learning for Apache Spark, on AWS\n\n\nBigDL on CDH and Cloudera Data Science Workbench\n\n\nRunning BigDL on Microsoft Data Science Virtual Machine\n\n\nUsing Apache Spark with Intel BigDL on Mesosphere DC/OS\n by Lightbend\n\n\nDeep Learning on Qubole Using BigDL for Apache Spark (\nPart 1\n and \nPart 2\n)", 
            "title": "Powered by"
        }, 
        {
            "location": "/known-issues/", 
            "text": "Currently, BigDL uses synchronous mini-batch SGD in model training. The mini-batch size is expected to be a multiple of \ntotal cores\n used in the job.\n\n\n\n\n\n\nYou may observe very poor performance when running BigDL for Spark 2.0 with Java 7; it is highly recommended to use Java 8 when building and running BigDL for Spark 2.0.\n\n\n\n\n\n\nOn Spark 2.0, please use default Java serializer instead of Kryo because of \nKryo Issue 341\n. The issue has been fixed in Kryo 4.0. However, Spark 2.0 uses Kryo 3.0.3. Spark 1.5 and 1.6 do not have this problem.\n\n\n\n\n\n\nOn CentOS 6 and 7, please increase the max user processes to a larger value (e.g., 514585); otherwise, you may see errors like \"unable to create new native thread\".\n\n\n\n\n\n\nCurrently, BigDL will load all the training and validation data into memory during training. You may encounter errors if it runs out of memory.\n\n\n\n\n\n\nIf you meet the program stuck after \nSave model...\n on Mesos, check the \nspark.driver.memory\n and increase the value. Eg, VGG on Cifar10 may need 20G+.\n\n\n\n\n\n\nIf you meet \ncan't find executor core number\n on Mesos, you should pass the executor cores through \n--conf spark.executor.cores=xxx\n\n\n\n\n\n\nOn Windows, if you meet \"Could not locate executable null\\bin\\winutils.exe\" error, you need to install winutils.exe. Please refer this \npost\n.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/other-resource/", 
            "text": "Talks:\n\n\n\n\n\n\n Strata 2017 London:Building deep learning-powered big data\n \n[Slides Download]\n\n\n\n\n\n\nStrata 2017 Beijing: Building deep learning power big data analytics on Apache Spark using BigDL\n \n[Slides Download]\n\n\n\n\n\n\nStrata 2017 Beijing: Distributed deep learning at scale on Apache Spark with BigDL\n \n[Slides Download]\n\n\n\n\n\n\nSpark Summit East 2017: Building Deep Learning Powered Big Data\n  \n[Slides Download]\n\n\n\n\n\n\nSpark Summit East 2017: BigDL: A Distributed Deep Learning Library on Spark\n \n[Slides Download]\n\n\n\n\n\n\nSpark Summit 2017: BigDL: Bringing Ease of Use of Deep Learning for Apache Spark\n \n\n[Slides Download]\n\n\n\n\n\n\nSpark Summit 2017: Deep Learning to Big Data Analytics on Apache Spark Using BigDL\n \n[Slides Download]\n\n\n\n\n\n\n\n\n Others:\n \n\n\n\n\n\n\nBigDL Overview\n \n\n\n\n\n\n\nIntroduction to BigDL on Apache Spark* :\n\n\n\n\n\n\nPart1\n \n\n\n\n\n\n\nPart2 \n\n\n\n\n\n\nPart3", 
            "title": "Other Resource"
        }
    ]
}