<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Losses - BigDL Project</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "L1Cost", url: "#l1cost", children: [
          ]},
          {title: "TimeDistributedCriterion", url: "#timedistributedcriterion", children: [
          ]},
          {title: "MarginRankingCriterion", url: "#marginrankingcriterion", children: [
          ]},
          {title: "ClassNLLCriterion", url: "#classnllcriterion", children: [
          ]},
          {title: "SoftmaxWithCriterion", url: "#softmaxwithcriterion", children: [
          ]},
          {title: "SmoothL1Criterion", url: "#smoothl1criterion", children: [
          ]},
          {title: "SmoothL1CriterionWithWeights", url: "#smoothl1criterionwithweights", children: [
          ]},
          {title: "MultiMarginCriterion", url: "#multimargincriterion", children: [
          ]},
          {title: "HingeEmbeddingCriterion", url: "#hingeembeddingcriterion", children: [
          ]},
          {title: "MarginCriterion", url: "#margincriterion", children: [
          ]},
          {title: "CosineEmbeddingCriterion", url: "#cosineembeddingcriterion", children: [
          ]},
          {title: "BCECriterion", url: "#bcecriterion", children: [
          ]},
          {title: "DiceCoefficientCriterion", url: "#dicecoefficientcriterion", children: [
          ]},
          {title: "MSECriterion", url: "#msecriterion", children: [
          ]},
          {title: "SoftMarginCriterion", url: "#softmargincriterion", children: [
          ]},
          {title: "DistKLDivCriterion", url: "#distkldivcriterion", children: [
          ]},
          {title: "ClassSimplexCriterion", url: "#classsimplexcriterion", children: [
          ]},
          {title: "L1HingeEmbeddingCriterion", url: "#l1hingeembeddingcriterion", children: [
          ]},
          {title: "CrossEntropyCriterion", url: "#crossentropycriterion", children: [
          ]},
          {title: "ParallelCriterion", url: "#parallelcriterion", children: [
          ]},
          {title: "MultiLabelMarginCriterion", url: "#multilabelmargincriterion", children: [
          ]},
          {title: "MultiLabelSoftMarginCriterion", url: "#multilabelsoftmargincriterion", children: [
          ]},
          {title: "AbsCriterion", url: "#abscriterion", children: [
          ]},
          {title: "MultiCriterion", url: "#multicriterion", children: [
          ]},
          {title: "GaussianCriterion", url: "#gaussiancriterion", children: [
          ]},
          {title: "KLDCriterion", url: "#kldcriterion", children: [
          ]},
          {title: "CosineProximityCriterion", url: "#cosineproximitycriterion", children: [
          ]},
          {title: "MeanSquaredLogarithmicCriterion", url: "#meansquaredlogarithmiccriterion", children: [
          ]},
          {title: "MeanAbsolutePercentageCriterion", url: "#meanabsolutepercentagecriterion", children: [
          ]},
          {title: "KullbackLeiblerDivergenceCriterion", url: "#kullbackleiblerdivergencecriterion", children: [
          ]},
          {title: "PoissonCriterion", url: "#poissoncriterion", children: [
          ]},
          {title: "TransformerCriterion", url: "#transformercriterion", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

    <h2 id="l1cost">L1Cost</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val layer = L1Cost[Float]()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">layer = L1Cost()
</code></pre>

<p>Compute L1 norm for input, and sign of input</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val layer = L1Cost[Float]()
val input = Tensor[Float](2, 2).rand
val target = Tensor[Float](2, 2).rand

val output = layer.forward(input, target)
val gradInput = layer.backward(input, target)

&gt; println(input)
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.48145306      0.476887
0.23729686      0.5169516
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

&gt; println(target)
target: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.42999148      0.22272833
0.49723643      0.17884709
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]

&gt; println(output)
output: Float = 1.7125885
&gt; println(gradInput)
gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0     1.0
1.0     1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">layer = L1Cost()

input = np.random.uniform(0, 1, (2, 2)).astype(&quot;float32&quot;)
target = np.random.uniform(0, 1, (2, 2)).astype(&quot;float32&quot;)

output = layer.forward(input, target)
gradInput = layer.backward(input, target)

&gt; output
2.522411
&gt; gradInput
[array([[ 1.,  1.],
        [ 1.,  1.]], dtype=float32)]
</code></pre>

<hr />
<h2 id="timedistributedcriterion">TimeDistributedCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = TimeDistributedCriterion(critrn, sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = TimeDistributedCriterion(critrn, sizeAverage)
</code></pre>

<p>This class is intended to support inputs with 3 or more dimensions.
Apply Any Provided Criterion to every temporal slice of an input.</p>
<ul>
<li><code>critrn</code> embedded criterion</li>
<li><code>sizeAverage</code> whether to divide the sequence length. Default is false.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage

val criterion = ClassNLLCriterion[Double]()
val layer = TimeDistributedCriterion[Double](criterion, true)
val input = Tensor[Double](Storage(Array(
    1.0262627674932,
    -1.2412600935171,
    -1.0423174168648,
    -1.0262627674932,
    -1.2412600935171,
    -1.0423174168648,
    -0.90330565804228,
    -1.3686840144413,
    -1.0778380454479,
    -0.90330565804228,
    -1.3686840144413,
    -1.0778380454479,
    -0.99131220658219,
    -1.0559142847536,
    -1.2692712660404,
    -0.99131220658219,
    -1.0559142847536,
    -1.2692712660404))).resize(3, 2, 3)
val target = Tensor[Double](3, 2)
    target(Array(1, 1)) = 1
    target(Array(1, 2)) = 1
    target(Array(2, 1)) = 2
    target(Array(2, 2)) = 2
    target(Array(3, 1)) = 3
    target(Array(3, 2)) = 3
&gt; print(layer.forward(input, target))
0.8793184268272332
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *

criterion = ClassNLLCriterion()
layer = TimeDistributedCriterion(criterion, True)
input = np.array([1.0262627674932,
                      -1.2412600935171,
                      -1.0423174168648,
                      -1.0262627674932,
                      -1.2412600935171,
                      -1.0423174168648,
                      -0.90330565804228,
                      -1.3686840144413,
                      -1.0778380454479,
                      -0.90330565804228,
                      -1.3686840144413,
                      -1.0778380454479,
                      -0.99131220658219,
                      -1.0559142847536,
                      -1.2692712660404,
                      -0.99131220658219,
                      -1.0559142847536,
                      -1.2692712660404]).reshape(3,2,3)
target = np.array([[1,1],[2,2],[3,3]])                      
&gt;layer.forward(input, target)
0.8793184
</code></pre>

<hr />
<h2 id="marginrankingcriterion">MarginRankingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val mse = new MarginRankingCriterion(margin=1.0, sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">mse = MarginRankingCriterion(margin=1.0, size_average=true)
</code></pre>

<p>Creates a criterion that measures the loss given an input <code>x = {x1, x2}</code>,
a table of two Tensors of size 1 (they contain only scalars), and a label y (1 or -1).
In batch mode, x is a table of two Tensors of size batchsize, and y is a Tensor of size
batchsize containing 1 or -1 for each corresponding pair of elements in the input Tensor.
If <code>y == 1</code> then it assumed the first input should be ranked higher (have a larger value) than
the second input, and vice-versa for <code>y == -1</code>.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.MarginRankingCriterion
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

import scala.util.Random

val input1Arr = Array(1, 2, 3, 4, 5)
val input2Arr = Array(5, 4, 3, 2, 1)

val target1Arr = Array(-1, 1, -1, 1, 1)

val input1 = Tensor(Storage(input1Arr.map(x =&gt; x.toFloat)))
val input2 = Tensor(Storage(input2Arr.map(x =&gt; x.toFloat)))

val input = T((1.toFloat, input1), (2.toFloat, input2))

val target1 = Tensor(Storage(target1Arr.map(x =&gt; x.toFloat)))
val target = T((1.toFloat, target1))

val mse = new MarginRankingCriterion()

val output = mse.forward(input, target)
val gradInput = mse.backward(input, target)

println(output)
println(gradInput)
</code></pre>

<p>Gives the output</p>
<pre><code>output: Float = 0.8                                                                                                                                                                    [21/154]
</code></pre>

<p>Gives the gradInput,</p>
<pre><code>gradInput: com.intel.analytics.bigdl.utils.Table =
 {
        2: -0.0
           0.2
           -0.2
           0.0
           0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
        1: 0.0
           -0.2
           0.2
           -0.0
           -0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
 }
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

mse = MarginRankingCriterion()

input1 = np.array([1, 2, 3, 4, 5]).astype(&quot;float32&quot;)
input2 = np.array([5, 4, 3, 2, 1]).astype(&quot;float32&quot;)
input = [input1, input2]

target1 = np.array([-1, 1, -1, 1, 1]).astype(&quot;float32&quot;)
target = [target1, target1]

output = mse.forward(input, target)
gradInput = mse.backward(input, target)

print output
print gradInput
</code></pre>

<p>Gives the output,</p>
<pre><code>0.8
</code></pre>

<p>Gives the gradInput,</p>
<pre><code>[array([ 0. , -0.2,  0.2, -0. , -0. ], dtype=float32), array([-0. ,  0.2, -0.2,  0. ,  0. ], dtype=float32)] 
</code></pre>

<hr />
<h2 id="classnllcriterion">ClassNLLCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = ClassNLLCriterion(weights = null, sizeAverage = true, logProbAsInput=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = ClassNLLCriterion(weights=None, size_average=True, logProbAsInput=true)
</code></pre>

<p>The negative log likelihood criterion. It is useful to train a classification problem with n
classes. If provided, the optional argument weights should be a 1D Tensor assigning weight to
each of the classes. This is particularly useful when you have an unbalanced training set.</p>
<p>The input given through a <code>forward()</code> is expected to contain log-probabilities/probabilities of each class:
input has to be a 1D Tensor of size <code>n</code>. Obtaining log-probabilities/probabilities in a neural network is easily
achieved by adding a <code>LogSoftMax</code>/<code>SoftMax</code> layer in the last layer of your neural network. You may use
<code>CrossEntropyCriterion</code> instead, if you prefer not to add an extra layer to your network. This
criterion expects a class index (1 to the number of class) as target when calling
<code>forward(input, target)</code> and <code>backward(input, target)</code>.</p>
<p>In the log-probabilities case,
 The loss can be described as:
     <code>loss(x, class) = -x[class]</code>
 or in the case of the weights argument it is specified as follows:
     <code>loss(x, class) = -weights[class] * x[class]</code>
 Due to the behaviour of the backend code, it is necessary to set sizeAverage to false when
 calculating losses in non-batch mode.</p>
<p>Note that if the target is <code>-1</code>, the training process will skip this sample.
 In other words, the forward process will return zero output and the backward process
 will also return zero <code>gradInput</code>.</p>
<p>By default, the losses are averaged over observations for each minibatch. However, if the field
 <code>sizeAverage</code> is set to false, the losses are instead summed for each minibatch.</p>
<p>Parameters:</p>
<ul>
<li><code>weights</code> weights of each element of the input</li>
<li><code>sizeAverage</code> A boolean indicating whether normalizing by the number of elements in the input.
                  Default: true</li>
<li><code>logProbAsInput</code> indicating whether to accept log-probabilities or probabilities as input. True means accepting
               log-probabilities as input.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn.ClassNLLCriterion
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.utils.T

val criterion = ClassNLLCriterion()
val input = Tensor(T(
              T(1f, 2f, 3f),
              T(2f, 3f, 4f),
              T(3f, 4f, 5f)
          ))

val target = Tensor(T(1f, 2f, 3f))

val loss = criterion.forward(input, target)
val grad = criterion.backward(input, target)

print(loss)
-3.0
println(grad)
-0.33333334 0.0 0.0
0.0 -0.33333334 0.0
0.0 0.0 -0.33333334
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *

criterion = ClassNLLCriterion()
input = np.array([
              [1.0, 2.0, 3.0],
              [2.0, 3.0, 4.0],
              [3.0, 4.0, 5.0]
          ])

target = np.array([1.0, 2.0, 3.0])

loss = criterion.forward(input, target)
gradient= criterion.backward(input, target)

print loss
-3.0
print gradient
-3.0
[[-0.33333334  0.          0.        ]
 [ 0.         -0.33333334  0.        ]
 [ 0.          0.         -0.33333334]]
</code></pre>

<hr />
<h2 id="softmaxwithcriterion">SoftmaxWithCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = SoftmaxWithCriterion(ignoreLabel, normalizeMode)
</code></pre>

<p>Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued predictions through a softmax to
get a probability distribution over classes. It should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as 
its gradient computation is more numerically stable.</p>
<ul>
<li><code>ignoreLabel</code>   (optional) Specify a label value that should be ignored when computing the loss.</li>
<li><code>normalizeMode</code> How to normalize the output loss.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.{Storage, Tensor}

val input = Tensor(1, 5, 2, 3).rand()
val target = Tensor(Storage(Array(2.0f, 4.0f, 2.0f, 4.0f, 1.0f, 2.0f))).resize(1, 1, 2, 3)

val model = SoftmaxWithCriterion[Float]()
val output = model.forward(input, target)

scala&gt; print(input)
(1,1,.,.) =
0.65131104  0.9332143   0.5618989   
0.9965054   0.9370902   0.108070895 

(1,2,.,.) =
0.46066576  0.9636703   0.8123812   
0.31076035  0.16386998  0.37894428  

(1,3,.,.) =
0.49111295  0.3704862   0.9938375   
0.87996656  0.8695406   0.53354675  

(1,4,.,.) =
0.8502225   0.9033509   0.8518651   
0.0692618   0.10121379  0.970959    

(1,5,.,.) =
0.9397213   0.49688303  0.75739735  
0.25074655  0.11416598  0.6594504   

[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 1x5x2x3]

scala&gt; print(output)
1.6689054
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">input = np.random.randn(1, 5, 2, 3)
target = np.array([[[[2.0, 4.0, 2.0], [4.0, 1.0, 2.0]]]])

model = SoftmaxWithCriterion()
output = model.forward(input, target)

&gt;&gt;&gt; print input
[[[[ 0.78455689  0.01402084  0.82539628]
   [-1.06448238  2.58168413  0.60053703]]

  [[-0.48617618  0.44538094  0.46611658]
   [-1.41509329  0.40038991 -0.63505732]]

  [[ 0.91266769  1.68667933  0.92423611]
   [ 0.1465411   0.84637557  0.14917515]]

  [[-0.7060493  -2.02544114  0.89070726]
   [ 0.14535539  0.73980064 -0.33130613]]

  [[ 0.64538791 -0.44384233 -0.40112523]
   [ 0.44346658 -2.22303621  0.35715986]]]]

&gt;&gt;&gt; print output
2.1002123

</code></pre>

<hr />
<h2 id="smoothl1criterion">SmoothL1Criterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val slc = SmoothL1Criterion(sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">slc = SmoothL1Criterion(size_average=True)
</code></pre>

<p>Creates a criterion that can be thought of as a smooth version of the AbsCriterion.
It uses a squared term if the absolute element-wise error falls below 1.
It is less sensitive to outliers than the MSECriterion and in some
cases prevents exploding gradients (e.g. see "Fast R-CNN" paper by Ross Girshick).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.{Tensor, Storage}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.SmoothL1Criterion

val slc = SmoothL1Criterion()

val inputArr = Array(
  0.17503996845335,
  0.83220188552514,
  0.48450597329065,
  0.64701424003579,
  0.62694586534053,
  0.34398410236463,
  0.55356747563928,
  0.20383032318205
)
val targetArr = Array(
  0.69956525065936,
  0.86074831243604,
  0.54923197557218,
  0.57388074393384,
  0.63334444304928,
  0.99680578662083,
  0.49997645849362,
  0.23869121982716
)

val input = Tensor(Storage(inputArr.map(x =&gt; x.toFloat))).reshape(Array(2, 2, 2))
val target = Tensor(Storage(targetArr.map(x =&gt; x.toFloat))).reshape(Array(2, 2, 2))

val output = slc.forward(input, target)
val gradInput = slc.backward(input, target)
</code></pre>

<p>Gives the output,</p>
<pre><code>output: Float = 0.0447365
</code></pre>

<p>Gives the gradInput,</p>
<pre><code>gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
-0.06556566     -0.003568299
-0.008090746    0.009141691

(2,.,.) =
-7.998273E-4    -0.08160271
0.0066988766    -0.0043576136
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

slc = SmoothL1Criterion()

input = np.array([
    0.17503996845335,
    0.83220188552514,
    0.48450597329065,
    0.64701424003579,
    0.62694586534053,
    0.34398410236463,
    0.55356747563928,
    0.20383032318205
])
input.reshape(2, 2, 2)

target = np.array([
    0.69956525065936,
    0.86074831243604,
    0.54923197557218,
    0.57388074393384,
    0.63334444304928,
    0.99680578662083,
    0.49997645849362,
    0.23869121982716
])

target.reshape(2, 2, 2)

output = slc.forward(input, target)
gradInput = slc.backward(input, target)

print output
print gradInput
</code></pre>

<hr />
<h2 id="smoothl1criterionwithweights">SmoothL1CriterionWithWeights</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val smcod = SmoothL1CriterionWithWeights[Float](sigma: Float = 2.4f, num: Int = 2)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">smcod = SmoothL1CriterionWithWeights(sigma, num)
</code></pre>

<p>a smooth version of the AbsCriterion
It uses a squared term if the absolute element-wise error falls below 1.
It is less sensitive to outliers than the MSECriterion and in some cases
prevents exploding gradients (e.g. see "Fast R-CNN" paper by Ross Girshick).</p>
<pre><code>   d = (x - y) * w_in

  loss(x, y, w_in, w_out)
              | 0.5 * (sigma * d_i)^2 * w_out          if |d_i| &lt; 1 / sigma / sigma
   = 1/n \sum |
              | (|d_i| - 0.5 / sigma / sigma) * w_out   otherwise
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">val smcod = SmoothL1CriterionWithWeights[Float](2.4f, 2)

val inputArr = Array(1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03)
val targetArr = Array(0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58)
val inWArr = Array(-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8)
val outWArr = Array(-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1)

val input = Tensor(Storage(inputArr.map(x =&gt; x.toFloat)))
val target = T()
target.insert(Tensor(Storage(targetArr.map(x =&gt; x.toFloat))))
target.insert(Tensor(Storage(inWArr.map(x =&gt; x.toFloat))))
target.insert(Tensor(Storage(outWArr.map(x =&gt; x.toFloat))))

val output = smcod.forward(input, target)
val gradInput = smcod.backward(input, target)

&gt; println(output)
  output: Float = -2.17488
&gt; println(gradInput)
-0.010944003
0.425
0.63037443
-0.95
-0.1
0.07
0.120000005
-0.44000003
[com.intel.analytics.bigdl.tensor.DenseTensor of size 8]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">smcod = SmoothL1CriterionWithWeights(2.4, 2)

input = np.array([1.1, -0.8, 0.1, 0.4, 1.3, 0.2, 0.2, 0.03]).astype(&quot;float32&quot;)
targetArr = np.array([0.9, 1.5, -0.08, -1.68, -0.68, -1.17, -0.92, 1.58]).astype(&quot;float32&quot;)
inWArr = np.array([-0.1, 1.7, -0.8, -1.9, 1.0, 1.4, 0.8, 0.8]).astype(&quot;float32&quot;)
outWArr = np.array([-1.9, -0.5, 1.9, -1.0, -0.2, 0.1, 0.3, 1.1]).astype(&quot;float32&quot;)
target = [targetArr, inWArr, outWArr]

output = smcod.forward(input, target)
gradInput = smcod.backward(input, target)

&gt; output
-2.17488
&gt; gradInput
[array([-0.010944  ,  0.42500001,  0.63037443, -0.94999999, -0.1       ,
         0.07      ,  0.12      , -0.44000003], dtype=float32)]
</code></pre>

<hr />
<h2 id="multimargincriterion">MultiMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val loss = MultiMarginCriterion(p=1,weights=null,margin=1.0,sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">loss = MultiMarginCriterion(p=1,weights=None,margin=1.0,size_average=True)
</code></pre>

<p>MultiMarginCriterion is a loss function that optimizes a multi-class classification hinge loss (margin-based loss) between input <code>x</code> and output <code>y</code> (<code>y</code> is the target class index).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.tensor.Storage

val input = Tensor(3,2).randn()
val target = Tensor(Storage(Array(2.0f, 1.0f, 2.0f)))
val loss = MultiMarginCriterion(1)
val output = loss.forward(input,target)
val grad = loss.backward(input,target)

scala&gt; print(input)
-0.45896783     -0.80141246
0.22560088      -0.13517438
0.2601126       0.35492152
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

scala&gt; print(target)
2.0
1.0
2.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]

scala&gt; print(output)
0.4811434

scala&gt; print(grad)
0.16666667      -0.16666667
-0.16666667     0.16666667
0.16666667      -0.16666667
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x2]


</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

input  = np.random.randn(3,2)
target = np.array([2,1,2])
print &quot;input=&quot;,input
print &quot;target=&quot;,target

loss = MultiMarginCriterion(1)
out = loss.forward(input, target)
print &quot;output of loss is : &quot;,out

grad_out = loss.backward(input,target)
print &quot;grad out of loss is : &quot;,grad_out
</code></pre>

<p>Gives the output,</p>
<pre><code>input= [[ 0.46868305 -2.28562261]
 [ 0.8076243  -0.67809689]
 [-0.20342555 -0.66264743]]
target= [2 1 2]
creating: createMultiMarginCriterion
output of loss is :  0.8689213
grad out of loss is :  [[ 0.16666667 -0.16666667]
 [ 0.          0.        ]
 [ 0.16666667 -0.16666667]]


</code></pre>

<hr />
<h2 id="hingeembeddingcriterion">HingeEmbeddingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val m = HingeEmbeddingCriterion(margin = 1, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">m = HingeEmbeddingCriterion(margin=1, size_average=True)
</code></pre>

<p>Creates a criterion that measures the loss given an input <code>x</code> which is a 1-dimensional vector and a label <code>y</code> (<code>1</code> or <code>-1</code>).
This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>
<pre><code>                 ⎧ x_i,                  if y_i ==  1
loss(x, y) = 1/n ⎨
                 ⎩ max(0, margin - x_i), if y_i == -1
</code></pre>

<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils.{T}
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.{T}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val loss = HingeEmbeddingCriterion(1, sizeAverage = false)
val input = Tensor(T(0.1f, 2.0f, 2.0f, 2.0f))
println(&quot;input: \n&quot; + input)
println(&quot;ouput: &quot;)

println(&quot;Target=1: &quot; + loss.forward(input, Tensor(4, 1).fill(1f)))

println(&quot;Target=-1: &quot; + loss.forward(input, Tensor(4, 1).fill(-1f)))
</code></pre>

<pre><code>input: 
0.1
2.0
2.0
2.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]
ouput: 
Target=1: 6.1
Target=-1: 0.9

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
input = np.array([0.1, 2.0, 2.0, 2.0])
target = np.full(4, 1)
print(&quot;input: &quot; )
print(input)
print(&quot;target: &quot;)
print(target)
print(&quot;output: &quot;)
print(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, target))
print(HingeEmbeddingCriterion(1.0, size_average= False).forward(input, np.full(4, -1)))
</code></pre>

<pre><code>input: 
[ 0.1  2.   2.   2. ]
target: 
[1 1 1 1]
output: 
creating: createHingeEmbeddingCriterion
6.1
creating: createHingeEmbeddingCriterion
0.9
</code></pre>

<hr />
<h2 id="margincriterion">MarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">criterion = MarginCriterion(margin=1.0, sizeAverage=true, squared=false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MarginCriterion(margin=1.0, sizeAverage=True, squared=False, bigdl_type=&quot;float&quot;)
</code></pre>

<p>Creates a criterion that optimizes a two-class classification (squared) hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.
 * <code>margin</code> if unspecified, is by default 1.
 * <code>sizeAverage</code> whether to average the loss, is by default true
 * <code>squared</code> whether to calculate the squared hinge loss</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">val criterion = MarginCriterion(margin=1.0, sizeAverage=true)

val input = Tensor(3, 2).rand()
input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.33753583      0.3575501
0.23477706      0.7240361
0.92835575      0.4737949
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

val target = Tensor(3, 2).rand()
target: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.27280563      0.7022703
0.3348442       0.43332106
0.08935371      0.17876455
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3x2]

criterion.forward(input, target)
res5: Float = 0.84946966
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">criterion = MarginCriterion(margin=1.0,size_average=True,bigdl_type=&quot;float&quot;)
input = np.random.rand(3, 2)
array([[ 0.20824672,  0.67299837],
       [ 0.80561452,  0.19564743],
       [ 0.42501441,  0.19408184]])

target = np.random.rand(3, 2)
array([[ 0.67882632,  0.61257846],
       [ 0.10111138,  0.75225082],
       [ 0.60404296,  0.31373273]])

criterion.forward(input, target)
0.8166871
</code></pre>

<hr />
<h2 id="cosineembeddingcriterion">CosineEmbeddingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val cosineEmbeddingCriterion = CosineEmbeddingCriterion(margin  = 0.0, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">cosineEmbeddingCriterion = CosineEmbeddingCriterion( margin=0.0,size_average=True)
</code></pre>

<p>CosineEmbeddingCriterion creates a criterion that measures the loss given an input x = {x1, x2},
a table of two Tensors, and a Tensor label y with values 1 or -1.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
impot com.intel.analytics.bigdl.utils.T
val cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, false)
val input1 = Tensor(5).rand()
val input2 = Tensor(5).rand()
val input = T()
input(1.0) = input1
input(2.0) = input2
val target1 = Tensor(Storage(Array(-0.5f)))
val target = T()
target(1.0) = target1

&gt; print(input)
 {
    2.0: 0.4110882
         0.57726574
         0.1949834
         0.67670715
         0.16984987
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
    1.0: 0.16878392
         0.24124223
         0.8964794
         0.11156334
         0.5101486
         [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]
 }

&gt; print(cosineEmbeddingCriterion.forward(input, target))
0.49919847

&gt; print(cosineEmbeddingCriterion.backward(input, target))
 {
    2: -0.045381278
       -0.059856333
       0.72547954
       -0.2268434
       0.3842142
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
    1: 0.30369008
       0.42463788
       -0.20637506
       0.5712836
       -0.06355385
       [com.intel.analytics.bigdl.tensor.DenseTensor of size 5]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
cosineEmbeddingCriterion = CosineEmbeddingCriterion(0.0, False)
&gt; cosineEmbeddingCriterion.forward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])
0.6363636
&gt; cosineEmbeddingCriterion.backward([np.array([1.0, 2.0, 3.0, 4.0 ,5.0]),np.array([5.0, 4.0, 3.0, 2.0, 1.0])],[np.array(-0.5)])
[array([ 0.07933884,  0.04958678,  0.01983471, -0.00991735, -0.03966942], dtype=float32), array([-0.03966942, -0.00991735,  0.01983471,  0.04958678,  0.07933884], dtype=float32)]

</code></pre>

<hr />
<h2 id="bcecriterion">BCECriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = BCECriterion[Float]()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = BCECriterion()
</code></pre>

<p>This loss function measures the Binary Cross Entropy between the target and the output</p>
<pre><code> loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
</code></pre>

<p>or in the case of the weights argument being specified:</p>
<pre><code> loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
</code></pre>

<p>By default, the losses are averaged for each mini-batch over observations as well as over
 dimensions. However, if the field sizeAverage is set to false, the losses are instead summed.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
val criterion = BCECriterion[Float]()
val input = Tensor[Float](3, 1).rand

val target = Tensor[Float](3)
target(1) = 1
target(2) = 0
target(3) = 1

val output = criterion.forward(input, target)
val gradInput = criterion.backward(input, target)

&gt; println(target)
res25: com.intel.analytics.bigdl.tensor.Tensor[Float] =
1.0
0.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3]

&gt; println(output)
output: Float = 0.9009579

&gt; println(gradInput)
gradInput: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-1.5277504
1.0736246
-0.336957
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">
criterion = BCECriterion()
input = np.random.uniform(0, 1, (3, 1)).astype(&quot;float32&quot;)
target = np.array([1, 0, 1])
output = criterion.forward(input, target)
gradInput = criterion.backward(input, target)

&gt; output
1.9218739
&gt; gradInput
[array([[-4.3074522 ],
        [ 2.24244714],
        [-1.22368968]], dtype=float32)]

</code></pre>

<hr />
<h2 id="dicecoefficientcriterion">DiceCoefficientCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val loss = DiceCoefficientCriterion(sizeAverage=true, epsilon=1.0f)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">loss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)
</code></pre>

<p>DiceCoefficientCriterion is the Dice-Coefficient objective function. </p>
<p>Both <code>forward</code> and <code>backward</code> accept two tensors : input and target. The <code>forward</code> result is formulated as 
          <code>1 - (2 * (input intersection target) / (input union target))</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.tensor.Storage

val input = Tensor(2).randn()
val target = Tensor(Storage(Array(2.0f, 1.0f)))
val loss = DiceCoefficientCriterion(epsilon = 1.0f)
val output = loss.forward(input,target)
val grad = loss.backward(input,target)

scala&gt; print(input)
-0.50278
0.51387966
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(target)
2.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(output)
0.9958517

scala&gt; print(grad)
-0.99619853     -0.49758217
[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

input  = np.random.randn(2)
target = np.array([2,1],dtype='float64')

print &quot;input=&quot;, input
print &quot;target=&quot;, target
loss = DiceCoefficientCriterion(size_average=True,epsilon=1.0)
out = loss.forward(input,target)
print &quot;output of loss is :&quot;,out

grad_out = loss.backward(input,target)
print &quot;grad out of loss is :&quot;,grad_out
</code></pre>

<p>produces output:</p>
<pre><code class="python">input= [ 0.4440505  2.9430301]
target= [ 2.  1.]
creating: createDiceCoefficientCriterion
output of loss is : -0.17262316
grad out of loss is : [[-0.38274616 -0.11200322]]
</code></pre>

<hr />
<h2 id="msecriterion">MSECriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MSECriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MSECriterion()
</code></pre>

<p>The mean squared error criterion e.g. input: a, target: b, total elements: n</p>
<pre><code>loss(a, b) = 1/n * sum(|a_i - b_i|^2)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>sizeAverage</code> a boolean indicating whether to divide the sum of squared error by n. 
                 Default: true</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = MSECriterion()
val input = Tensor(T(
 T(1.0f, 2.0f),
 T(3.0f, 4.0f))
)
val target = Tensor(T(
 T(2.0f, 3.0f),
 T(4.0f, 5.0f))
)
val output = criterion.forward(input, target)
val gradient = criterion.backward(input, target)
-&gt; print(output)
1.0
-&gt; print(gradient)
-0.5    -0.5    
-0.5    -0.5    
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
criterion = MSECriterion()
input = np.array([
          [1.0, 2.0],
          [3.0, 4.0]
        ])
target = np.array([
           [2.0, 3.0],
           [4.0, 5.0]
         ])
output = criterion.forward(input, target)
gradient= criterion.backward(input, target)
-&gt; print output
1.0
-&gt; print gradient
[[-0.5 -0.5]
 [-0.5 -0.5]]
</code></pre>

<hr />
<h2 id="softmargincriterion">SoftMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = SoftMarginCriterion(sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = SoftMarginCriterion(size_average)
</code></pre>

<p>Creates a criterion that optimizes a two-class classification logistic loss between
input x (a Tensor of dimension 1) and output y (which is a tensor containing either
1s or -1s).</p>
<pre><code>loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()
</code></pre>

<p>Parameters:
* <code>sizeAverage</code> A boolean indicating whether normalizing by the number of elements in the input.
                    Default: true</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = SoftMarginCriterion()
val input = Tensor(T(
 T(1.0f, 2.0f),
 T(3.0f, 4.0f))
)
val target = Tensor(T(
 T(1.0f, -1.0f),
 T(-1.0f, 1.0f))
)
val output = criterion.forward(input, target)
val gradient = criterion.backward(input, target)
-&gt; print(output)
1.3767318
-&gt; print(gradient)
-0.06723536     0.22019927      
0.23814353      -0.0044965525   
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
criterion = SoftMarginCriterion()
input = np.array([
          [1.0, 2.0],
          [3.0, 4.0]
        ])
target = np.array([
           [2.0, 3.0],
           [4.0, 5.0]
         ])
output = criterion.forward(input, target)
gradient = criterion.backward(input, target)
-&gt; print output
1.3767318
-&gt; print gradient
[[-0.06723536  0.22019927]
 [ 0.23814353 -0.00449655]]
</code></pre>

<hr />
<h2 id="distkldivcriterion">DistKLDivCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val loss = DistKLDivCriterion[T](sizeAverage=true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">loss = DistKLDivCriterion(size_average=True)
</code></pre>

<p>DistKLDivCriterion is the Kullback–Leibler divergence loss.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">
scala&gt;
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
import com.intel.analytics.bigdl.tensor.Storage

val input = Tensor(2).randn()
val target = Tensor(Storage(Array(2.0f, 1.0f)))
val loss = DistKLDivCriterion()
val output = loss.forward(input,target)
val grad = loss.backward(input,target)

scala&gt; print(input)
-0.3854126
-0.7707398
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(target)
2.0
1.0
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2]

scala&gt; print(output)
1.4639297

scala&gt; print(grad)
-1.0
-0.5
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2]

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

input  = np.random.randn(2)
target = np.array([2,1])

print &quot;input=&quot;, input
print &quot;target=&quot;, target
loss = DistKLDivCriterion()
out = loss.forward(input,target)
print &quot;output of loss is :&quot;,out

grad_out = loss.backward(input,target)
print &quot;grad out of loss is :&quot;,grad_out
</code></pre>

<p>Gives the output</p>
<pre><code class="python">input= [-1.14333924  0.97662296]
target= [2 1]
creating: createDistKLDivCriterion
output of loss is : 1.348175
grad out of loss is : [-1.  -0.5]
</code></pre>

<hr />
<h2 id="classsimplexcriterion">ClassSimplexCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = ClassSimplexCriterion(nClasses)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = ClassSimplexCriterion(nClasses)
</code></pre>

<p>ClassSimplexCriterion implements a criterion for classification.
It learns an embedding per class, where each class' embedding is a
point on an (N-1)-dimensional simplex, where N is the number of classes.</p>
<p>Parameters:
* <code>nClasses</code> An integer, the number of classes.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = ClassSimplexCriterion(5)
val input = Tensor(T(
 T(1.0f, 2.0f, 3.0f, 4.0f, 5.0f),
 T(4.0f, 5.0f, 6.0f, 7.0f, 8.0f)
))
val target = Tensor(2)
target(1) = 2.0f
target(2) = 1.0f
val output = criterion.forward(input, target)
val gradient = criterion.backward(input, target)
-&gt; print(output)
23.562702
-&gt; print(gradient)
0.25    0.20635083      0.6     0.8     1.0     
0.6     1.0     1.2     1.4     1.6     
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
import numpy as np
criterion = ClassSimplexCriterion(5)
input = np.array([
   [1.0, 2.0, 3.0, 4.0, 5.0],
   [4.0, 5.0, 6.0, 7.0, 8.0]
])
target = np.array([2.0, 1.0])
output = criterion.forward(input, target)
gradient = criterion.backward(input, target)
-&gt; print output
23.562702
-&gt; print gradient
[[ 0.25        0.20635083  0.60000002  0.80000001  1.        ]
 [ 0.60000002  1.          1.20000005  1.39999998  1.60000002]]
</code></pre>

<hr />
<h2 id="l1hingeembeddingcriterion">L1HingeEmbeddingCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val model = L1HingeEmbeddingCriterion(margin)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model = L1HingeEmbeddingCriterion(margin)
</code></pre>

<p>Creates a criterion that measures the loss given an input <code>x = {x1, x2}</code>, a table of two Tensors, and a label y (1 or -1).
This is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>
<pre><code>             ⎧ ||x1 - x2||_1,                  if y ==  1
loss(x, y) = ⎨
             ⎩ max(0, margin - ||x1 - x2||_1), if y == -1
</code></pre>

<p>The margin has a default value of 1, or can be set in the constructor.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val model = L1HingeEmbeddingCriterion(0.6)
val input1 = Tensor(T(1.0f, -0.1f))
val input2 = Tensor(T(2.0f, -0.2f))
val input = T(input1, input2)
val target = Tensor(1)
target(Array(1)) = 1.0f

val output = model.forward(input, target)

scala&gt; print(output)
1.1
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">model = L1HingeEmbeddingCriterion(0.6)
input1 = np.array(1.0, -0.1)
input2 = np.array(2.0, -0.2)
input = [input1, input2]
target = np.array([1.0])

output = model.forward(input, target)

&gt;&gt;&gt; print output
1.1
</code></pre>

<hr />
<h2 id="crossentropycriterion">CrossEntropyCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val module = CrossEntropyCriterion(weights, sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">module = CrossEntropyCriterion(weights, sizeAverage)
</code></pre>

<p>This criterion combines LogSoftMax and ClassNLLCriterion in one single class.</p>
<ul>
<li><code>weights</code> A tensor assigning weight to each of the classes</li>
<li><code>sizeAverage</code> whether to divide the sequence length. Default is true.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Storage

val layer = CrossEntropyCriterion[Double]()
val input = Tensor[Double](Storage(Array(
    1.0262627674932,
    -1.2412600935171,
    -1.0423174168648,
    -0.90330565804228,
    -1.3686840144413,
    -1.0778380454479,
    -0.99131220658219,
    -1.0559142847536,
    -1.2692712660404
    ))).resize(3, 3)
val target = Tensor[Double](3)
    target(Array(1)) = 1
    target(Array(2)) = 2
    target(Array(3)) = 3
&gt; print(layer.forward(input, target))
0.9483051199107635
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *

layer = CrossEntropyCriterion()
input = np.array([1.0262627674932,
                      -1.2412600935171,
                      -1.0423174168648,
                      -0.90330565804228,
                      -1.3686840144413,
                      -1.0778380454479,
                      -0.99131220658219,
                      -1.0559142847536,
                      -1.2692712660404
                      ]).reshape(3,3)
target = np.array([1, 2, 3])                      
&gt;layer.forward(input, target)
0.94830513
</code></pre>

<hr />
<h2 id="parallelcriterion">ParallelCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val pc = ParallelCriterion(repeatTarget=false)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">pc = ParallelCriterion(repeat_target=False)
</code></pre>

<p>ParallelCriterion is a weighted sum of other criterions each applied to a different input
and target. Set repeatTarget = true to share the target for criterions.
Use add(criterion[, weight]) method to add criterion. Where weight is a scalar(default 1).</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.{Tensor, Storage}
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.{ParallelCriterion, ClassNLLCriterion, MSECriterion}

val pc = ParallelCriterion()

val input = T(Tensor(2, 10), Tensor(2, 10))
var i = 0
input[Tensor](1).apply1(_ =&gt; {i += 1; i})
input[Tensor](2).apply1(_ =&gt; {i -= 1; i})
val target = T(Tensor(Storage(Array(1.0f, 8.0f))), Tensor(2, 10).fill(1.0f))

val nll = ClassNLLCriterion()
val mse = MSECriterion()
pc.add(nll, 0.5).add(mse)

val output = pc.forward(input, target)
val gradInput = pc.backward(input, target)

println(output)
println(gradInput)

</code></pre>

<p>Gives the output,</p>
<pre><code>100.75

</code></pre>

<p>Gives the gradInput,</p>
<pre><code> {
        2: 1.8000001    1.7     1.6     1.5     1.4     1.3000001       1.2     1.1     1.0     0.90000004
           0.8  0.7     0.6     0.5     0.4     0.3     0.2     0.1     0.0     -0.1
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]
        1: -0.25        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0
           0.0  0.0     0.0     0.0     0.0     0.0     0.0     -0.25   0.0     0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x10]
 }

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

pc = ParallelCriterion()

input1 = np.arange(1, 21, 1).astype(&quot;float32&quot;)
input2 = np.arange(0, 20, 1).astype(&quot;float32&quot;)[::-1]
input1 = input1.reshape(2, 10)
input2 = input2.reshape(2, 10)

input = [input1, input2]

target1 = np.array([1.0, 8.0]).astype(&quot;float32&quot;)
target1 = target1.reshape(2)
target2 = np.full([2, 10], 1).astype(&quot;float32&quot;)
target2 = target2.reshape(2, 10)
target = [target1, target2]

nll = ClassNLLCriterion()
mse = MSECriterion()

pc.add(nll, weight = 0.5).add(mse)

print &quot;input = \n %s &quot; % input
print &quot;target = \n %s&quot; % target

output = pc.forward(input, target)
gradInput = pc.backward(input, target)

print &quot;output = %s &quot; % output
print &quot;gradInput = %s &quot; % gradInput
</code></pre>

<p>Gives the output,</p>
<pre><code>input = 
 [array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],
       [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]], dtype=float32), array([[ 19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,  10.],
       [  9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.]], dtype=float32)] 
target = 
 [array([ 1.,  8.], dtype=float32), array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)]
output = 100.75 
gradInput = [array([[-0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ]], dtype=float32), array([[ 1.80000007,  1.70000005,  1.60000002,  1.5       ,  1.39999998,
         1.30000007,  1.20000005,  1.10000002,  1.        ,  0.90000004],
       [ 0.80000001,  0.69999999,  0.60000002,  0.5       ,  0.40000001,
         0.30000001,  0.2       ,  0.1       ,  0.        , -0.1       ]], dtype=float32)]
</code></pre>

<hr />
<h2 id="multilabelmargincriterion">MultiLabelMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val multiLabelMarginCriterion = MultiLabelMarginCriterion(sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">multiLabelMarginCriterion = MultiLabelMarginCriterion(size_average=True)
</code></pre>

<p>MultiLabelMarginCriterion creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x and output y </p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor._
val multiLabelMarginCriterion = MultiLabelMarginCriterion(false)
val input = Tensor(4).rand()
val target = Tensor(4)
target(Array(1)) = 3
target(Array(2)) = 2
target(Array(3)) = 1
target(Array(4)) = 0

&gt; print(input)
0.40267515
0.5913795
0.84936756
0.05999674

&gt;  print(multiLabelMarginCriterion.forward(input, target))
0.33414197

&gt; print(multiLabelMarginCriterion.backward(input, target))
-0.25
-0.25
-0.25
0.75
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4]


</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.layer import *
multiLabelMarginCriterion = MultiLabelMarginCriterion(False)

&gt; multiLabelMarginCriterion.forward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))
0.975

&gt; multiLabelMarginCriterion.backward(np.array([0.3, 0.4, 0.2, 0.6]), np.array([3, 2, 1, 0]))
[array([-0.25, -0.25, -0.25,  0.75], dtype=float32)]

</code></pre>

<hr />
<h2 id="multilabelsoftmargincriterion">MultiLabelSoftMarginCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MultiLabelSoftMarginCriterion(weights = null, sizeAverage = true)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MultiLabelSoftMarginCriterion(weights=None, size_average=True)
</code></pre>

<p>MultiLabelSoftMarginCriterion is a multiLabel multiclass criterion based on sigmoid:</p>
<pre><code>l(x,y) = - sum_i y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i])
</code></pre>

<p>where <code>p[i] = exp(x[i]) / (1 + exp(x[i]))</code></p>
<p>If with weights,
 <code>l(x,y) = - sum_i weights[i] (y[i] * log(p[i]) + (1 - y[i]) * log (1 - p[i]))</code></p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = MultiLabelSoftMarginCriterion()
val input = Tensor(3)
input(Array(1)) = 0.4f
input(Array(2)) = 0.5f
input(Array(3)) = 0.6f
val target = Tensor(3)
target(Array(1)) = 0
target(Array(2)) = 1
target(Array(3)) = 1

&gt; criterion.forward(input, target)
res0: Float = 0.6081934
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

criterion = MultiLabelSoftMarginCriterion()
input = np.array([0.4, 0.5, 0.6])
target = np.array([0, 1, 1])

&gt; criterion.forward(input, target)
0.6081934
</code></pre>

<hr />
<h2 id="abscriterion">AbsCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = AbsCriterion(sizeAverage)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = AbsCriterion(sizeAverage)
</code></pre>

<p>Measures the mean absolute value of the element-wise difference between input and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = AbsCriterion()
val input = Tensor(T(1.0f, 2.0f, 3.0f))
val target = Tensor(T(4.0f, 5.0f, 6.0f))
val output = criterion.forward(input, target)

scala&gt; print(output)
3.0
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">criterion = AbsCriterion()
input = np.array([1.0, 2.0, 3.0])
target = np.array([4.0, 5.0, 6.0])
output=criterion.forward(input, target)

&gt;&gt;&gt; print output
3.0
</code></pre>

<hr />
<h2 id="multicriterion">MultiCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MultiCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MultiCriterion()
</code></pre>

<p>MultiCriterion is a weighted sum of other criterions each applied to the same input and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

val criterion = MultiCriterion()
val nll = ClassNLLCriterion()
val mse = MSECriterion()
criterion.add(nll, 0.5)
criterion.add(mse)

val input = Tensor(5).randn()
val target = Tensor(5)
target(Array(1)) = 1
target(Array(2)) = 2
target(Array(3)) = 3
target(Array(4)) = 2
target(Array(5)) = 1

val output = criterion.forward(input, target)

&gt; input
1.0641425
-0.33507252
1.2345984
0.08065767
0.531199
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 5]


&gt; output
res7: Float = 1.9633228
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">from bigdl.nn.criterion import *
import numpy as np

criterion = MultiCriterion()
nll = ClassNLLCriterion()
mse = MSECriterion()
criterion.add(nll, 0.5)
criterion.add(mse)

input = np.array([0.9682213801388531,
0.35258855644097503,
0.04584479998452568,
-0.21781499692588918,
-1.02721844006879])
target = np.array([1, 2, 3, 2, 1])

output = criterion.forward(input, target)

&gt; output
3.6099546
</code></pre>

<h2 id="gaussiancriterion">GaussianCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = GaussianCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = GaussianCriterion()
</code></pre>

<p>GaussianCriterion computes the log-likelihood of a sample given a Gaussian distribution.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.GaussianCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = GaussianCriterion()

val input1 = Tensor[Float](2, 3).range(1, 6, 1)
val input2 = Tensor[Float](2, 3).range(1, 12, 2)
val input = T(input1, input2)

val target = Tensor[Float](2, 3).range(2, 13, 2)

val loss = criterion.forward(input, target)

&gt; loss
loss: Float = 23.836603
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.layer import *
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = GaussianCriterion()

input1 = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input2 = np.arange(1, 12, 2).astype(&quot;float32&quot;)
input1 = input1.reshape(2, 3)
input2 = input2.reshape(2, 3)
input = [input1, input2]

target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

loss = criterion.forward(input, target)

&gt; output
23.836603
</code></pre>

<h2 id="kldcriterion">KLDCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = KLDCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = KLDCriterion()
</code></pre>

<p>Computes the KL-divergence of the input normal distribution to a standard normal distribution.
The input has to be a table. The first element of input is the mean of the distribution,
the second element of input is the log_variance of the distribution. The input distribution is
assumed to be diagonal.</p>
<p>The mean and log_variance are both assumed to be two dimensional tensors. The first dimension are
interpreted as batch. The output is the average/sum of each observation</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.KLDCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = KLDCriterion()

val input1 = Tensor[Float](2, 3).range(1, 6, 1)
val input2 = Tensor[Float](2, 3).range(1, 12, 2)
val input = T(input1, input2)

val target = Tensor[Float](2, 3).range(2, 13, 2)

val loss = criterion.forward(input, target)

&gt; loss
loss: Float = 34647.04
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = KLDCriterion()

input1 = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input2 = np.arange(1, 12, 2).astype(&quot;float32&quot;)
input1 = input1.reshape(2, 3)
input2 = input2.reshape(2, 3)
input = [input1, input2]

target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

loss = criterion.forward(input, target)

&gt; loss
34647.04
</code></pre>

<h2 id="cosineproximitycriterion">CosineProximityCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = CosineProximityCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = CosineProximityCriterion()
</code></pre>

<p>Computes the negative of the mean cosine proximity between predictions and targets.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.CosineProximityCriterion

val criterion = CosineProximityCriterion()

val input = Tensor[Float](2, 3).rand()

val target = Tensor[Float](2, 3).rand()

val loss = criterion.forward(input, target)

&gt; loss
loss: Float = -0.28007346
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *

criterion = CosineProximityCriterion()

input = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input = input.reshape(2, 3)
target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

loss = criterion.forward(input, target)

&gt; loss
-0.3333333
</code></pre>

<h2 id="meansquaredlogarithmiccriterion">MeanSquaredLogarithmicCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MeanSquaredLogarithmicCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MeanSquaredLogarithmicCriterion()
</code></pre>

<p>compute mean squared logarithmic error for input and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.MeanSquaredLogarithmicCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = MeanSquaredLogarithmicCriterion()
val input = Tensor[Float](2, 3).range(1, 6, 1)
val target = Tensor[Float](2, 3).range(2, 13, 2)
val loss = criterion.forward(input, target)

&gt; loss
loss: Float = 0.30576965
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = MeanSquaredLogarithmicCriterion()

input = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input = input.reshape(2, 3)
target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

loss = criterion.forward(input, target)

&gt; loss
0.30576965
</code></pre>

<h2 id="meanabsolutepercentagecriterion">MeanAbsolutePercentageCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = MeanAbsolutePercentageCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = MeanAbsolutePercentageCriterion()
</code></pre>

<p>compute mean absolute percentage error for intput and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.MeanAbsolutePercentageCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = MeanAbsolutePercentageCriterion()

val input = Tensor[Float](2, 3).range(1, 6, 1)
val target = Tensor[Float](2, 3).range(2, 13, 2)
val loss = criterion.forward(input, target)

&gt; loss
loss: Float = 50.0
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = MeanAbsolutePercentageCriterion()

input = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input = input.reshape(2, 3)
target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

loss = criterion.forward(input, target)

&gt; loss
50.0
</code></pre>

<h2 id="kullbackleiblerdivergencecriterion">KullbackLeiblerDivergenceCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = KullbackLeiblerDivergenceCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = KullbackLeiblerDivergenceCriterion()
</code></pre>

<p>compute Kullback Leibler Divergence Criterion error for intput and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.KullbackLeiblerDivergenceCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = KullbackLeiblerDivergenceCriterion[Float]()
val input = Tensor[Float](Array(0.1f, 0.2f, 0.3f, 0.4f, 0.5f, 0.6f), Array(2, 3))
val target = Tensor[Float](Array(0.6f, 0.5f, 0.4f, 0.3f, 0.2f, 0.1f), Array(2, 3))
val loss = criterion.forward(input, target)

&gt; loss
loss: Float = 0.59976757
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = KullbackLeiblerDivergenceCriterion()

y_pred = np.matrix('0.1 0.2 0.3; 0.4 0.5 0.6')
y_true = np.matrix('0.6 0.5 0.4; 0.3 0.2 0.1')

loss = criterion.forward(y_pred, y_true)

&gt; loss
0.59976757
</code></pre>

<h2 id="poissoncriterion">PoissonCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = PoissonCriterion()
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = PoissonCriterion()
</code></pre>

<p>compute Poisson error for intput and target</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.PoissonCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = PoissonCriterion()
val input = Tensor[Float](2, 3).range(1, 6, 1)
val target = Tensor[Float](2, 3).range(2, 13, 2)
val loss = criterion.forward(input, target)

&gt; loss
loss = -6.1750183

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = PoissonCriterion()
input = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input = input.reshape(2, 3)
target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

loss = criterion.forward(input, target)

&gt; loss
-6.1750183
</code></pre>

<h2 id="transformercriterion">TransformerCriterion</h2>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val criterion = TransformerCriterion(criterion, Some(inputTransformer), Some(targetTransformer))
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">criterion = TransformerCriterion(criterion, input_transformer, targetTransformer)
</code></pre>

<p>The criterion that takes two modules (optional) to transform input and target, and take
one criterion to compute the loss with the transformed input and target.</p>
<p>This criterion can be used to construct complex criterion. For example, the
<code>inputTransformer</code> and <code>targetTransformer</code> can be pre-trained CNN networks,
and we can use the networks' output to compute the high-level feature
reconstruction loss, which is commonly used in areas like neural style transfer
(https://arxiv.org/abs/1508.06576), texture synthesis (https://arxiv.org/abs/1505.07376),
.etc.</p>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.nn.TransformerCriterion
import com.intel.analytics.bigdl.utils.T

val criterion = MSECriterion()
val input = Tensor[Float](2, 3).range(1, 6, 1)
val target = Tensor[Float](2, 3).range(2, 13, 2)
val inputTransformer = Identity()
val targetTransformer = Identity()
val transCriterion = TransformerCriterion(criterion,
     Some(inputTransformer), Some(targetTransformer))
val loss = transCriterion.forward(input, target)

&gt; loss
15.166667

</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from bigdl.nn.criterion import *
from bigdl.optim.optimizer import *
from bigdl.util.common import *

criterion = MSECriterion()
input = np.arange(1, 7, 1).astype(&quot;float32&quot;)
input = input.reshape(2, 3)
target = np.arange(2, 13, 2).astype(&quot;float32&quot;)
target = target.reshape(2, 3)

inputTransformer = Identity()
targetTransformer = Identity()
transCriterion = TransformerCriterion(criterion, inputTransformer, targetTransformer)
loss = transCriterion.forward(input, target)


&gt; loss
15.166667
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>