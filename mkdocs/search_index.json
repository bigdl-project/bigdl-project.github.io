{
    "docs": [
        {
            "location": "/",
            "text": "BigDL\n\n\n\n\nBigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.\n\n\n\n\n\n\nRich deep learning support.\n Modeled after \nTorch\n, BigDL provides comprehensive support for deep learning, including numeric computing (via \nTensor\n) and high level \nneural networks\n; in addition, users can load pre-trained \nCaffe\n or \nTorch\n models into Spark programs using BigDL.\n\n\n\n\n\n\nExtremely high performance.\n To achieve high performance, BigDL uses \nIntel MKL\n and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source \nCaffe\n, \nTorch\n or \nTensorFlow\n on a single-node Xeon (i.e., comparable with mainstream GPU).\n\n\n\n\n\n\nEfficiently scale-out.\n BigDL can efficiently scale out to perform data analytics at \"Big Data scale\", by leveraging \nApache Spark\n (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark. \n\n\n\n\n\n\nWhy BigDL?\n\n\nYou may want to write your deep learning programs using BigDL if:\n\n\n\n\n\n\nYou want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).\n\n\n\n\n\n\nYou want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.\n\n\n\n\n\n\nYou want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)\n\n\n\n\n\n\nHow to use BigDL?\n\n\n\n\n\n\nTo learn how to install and build BigDL (on both Linux and macOS), you can check out the \nBuild Page\n\n\n\n\n\n\nTo learn how to run BigDL programs (as either a local Java program or a Spark program), you can check out the \nGetting Started Page\n\n\n\n\n\n\nTo learn the details of Python support in BigDL, you can check out the \nPython Support Page\n\n\n\n\n\n\nTo try BigDL out on EC2, you can check out the \nRunning on EC2 Page\n\n\n\n\n\n\nTo learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the \nExamples Page\n\n\n\n\n\n\nFor more details, you can check out the latest \nDocuments Page\n\n\n\n\n\n\nGetting Help\n\n\n\n\n\n\nYou can join the \nBigDL Google Group\n (or subscribe to the \nMail List\n) for more questions and discussions on BigDL\n\n\n\n\n\n\nYou can post bug reports and feature requests at the \nIssue Page",
            "title": "Home"
        },
        {
            "location": "/#bigdl",
            "text": "BigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.    Rich deep learning support.  Modeled after  Torch , BigDL provides comprehensive support for deep learning, including numeric computing (via  Tensor ) and high level  neural networks ; in addition, users can load pre-trained  Caffe  or  Torch  models into Spark programs using BigDL.    Extremely high performance.  To achieve high performance, BigDL uses  Intel MKL  and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source  Caffe ,  Torch  or  TensorFlow  on a single-node Xeon (i.e., comparable with mainstream GPU).    Efficiently scale-out.  BigDL can efficiently scale out to perform data analytics at \"Big Data scale\", by leveraging  Apache Spark  (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark.",
            "title": "BigDL"
        },
        {
            "location": "/#why-bigdl",
            "text": "You may want to write your deep learning programs using BigDL if:    You want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).    You want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.    You want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)",
            "title": "Why BigDL?"
        },
        {
            "location": "/#how-to-use-bigdl",
            "text": "To learn how to install and build BigDL (on both Linux and macOS), you can check out the  Build Page    To learn how to run BigDL programs (as either a local Java program or a Spark program), you can check out the  Getting Started Page    To learn the details of Python support in BigDL, you can check out the  Python Support Page    To try BigDL out on EC2, you can check out the  Running on EC2 Page    To learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the  Examples Page    For more details, you can check out the latest  Documents Page",
            "title": "How to use BigDL?"
        },
        {
            "location": "/#getting-help",
            "text": "You can join the  BigDL Google Group  (or subscribe to the  Mail List ) for more questions and discussions on BigDL    You can post bug reports and feature requests at the  Issue Page",
            "title": "Getting Help"
        },
        {
            "location": "/release/",
            "text": "Release \n\n\n\n\n\n\n\n\n\n\nDownload\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\nBigDL Nightly Build Download\n\n\nlatest Docs\n\n\n\n\n\n\nBigDL 0.1.1 Download\n\n\nBigDL 0.1.1 Docs\n\n\n\n\n\n\nBigDL 0.1.0 Download\n\n\nN/A\n\n\n\n\n\n\n\n\nThese are built BigDL packages including dependency and python files. You can download these packages instead of building them by yourself. This is useful when you want to do something like run some examples or develop python code.\n\n\nBigDL Nightly Build\n\n\nHere are the folders for nightly build packages. The packages are built from latest master code. You can download the .zip files with a timestamp suffix in the name. \n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\nBigDL 0.1.1\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\nBigDL 0.1.0\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.5.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload",
            "title": "Release"
        },
        {
            "location": "/release/#release",
            "text": "Download  Docs      BigDL Nightly Build Download  latest Docs    BigDL 0.1.1 Download  BigDL 0.1.1 Docs    BigDL 0.1.0 Download  N/A     These are built BigDL packages including dependency and python files. You can download these packages instead of building them by yourself. This is useful when you want to do something like run some examples or develop python code.",
            "title": "Release"
        },
        {
            "location": "/release/#bigdl-nightly-build",
            "text": "Here are the folders for nightly build packages. The packages are built from latest master code. You can download the .zip files with a timestamp suffix in the name.       Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download",
            "title": "BigDL Nightly Build"
        },
        {
            "location": "/release/#bigdl-011",
            "text": "Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download",
            "title": "BigDL 0.1.1"
        },
        {
            "location": "/release/#bigdl-010",
            "text": "Linux x64  Mac      Spark 1.5.1  download  download    Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download",
            "title": "BigDL 0.1.0"
        },
        {
            "location": "/UserGuide/build/",
            "text": "Build\n\n\n\n\nThis page shows how to install and build BigDL (on both Linux and macOS), including:\n\n\n\n\nDownload\n\n\nLinking\n\n\nLinking with BigDL releases\n\n\nLinking with development version\n\n\n\n\n\n\nGet Source Code\n\n\nBuild\n\n\nBuild with make-dist.sh\n\n\nUsing the \nmake-dist.sh\n script\n\n\nBuild for macOS\n\n\nBuild for Spark 2.0 and above\n\n\nBuild using Scala 2.10 or 2.11\n\n\nFull Build\n\n\n\n\n\n\nBuild with Maven\n\n\n\n\n\n\nIDE Settings\n\n\nNext Steps\n\n\n\n\nDownload\n\n\nYou may download the BigDL release (currently v0.1.0) and nightly build from the \nRelease Page\n\n\nLinking\n\n\nLinking with BigDL releases\n\n\nCurrently, BigDL releases are hosted on maven central; here's an example to add the BigDL dependency to your own project:\n\n\n<dependency>\n    <groupId>com.intel.analytics.bigdl</groupId>\n    <artifactId>bigdl</artifactId>\n    <version>${BIGDL_VERSION}</version>\n</dependency>\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \"com.intel.analytics.bigdl\" % \"bigdl\" % \"${BIGDL_VERSION}\"\n\n\n\n\nSince currently only BigDL 0.1.0 is released, ${BIGDL_VERSION} must be set to 0.1.0 here.\n\n\nNote\n: the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this\n\n\n<dependency>\n    <groupId>com.intel.analytics.bigdl</groupId>\n    <artifactId>bigdl-SPARK_2.0</artifactId>\n    <version>${BIGDL_VERSION}</version>\n</dependency>\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \"com.intel.analytics.bigdl\" % \"bigdl-SPARK_2.0\" % \"${BIGDL_VERSION}\"\n\n\n\n\nIf your project runs on MacOS, you should add the dependency below,\n\n\n<dependency>\n    <groupId>com.intel.analytics.bigdl.native</groupId>\n    <artifactId>mkl-java-mac</artifactId>\n    <version>${BIGDL_VERSION}</version>\n    <exclusions>\n        <exclusion>\n            <groupId>com.intel.analytics.bigdl.native</groupId>\n            <artifactId>bigdl-native</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \"com.intel.analytics.bigdl.native\" % \"mkl-java-mac\" % \"${BIGDL_VERSION}\" from \"http://repo1.maven.org/maven2/com/intel/analytics/bigdl/native/mkl-java-mac/${BIGDL_VERSION}/mkl-java-mac-${BIGDL_VERSION}.jar\"\n\n\n\n\nLinking with development version\n\n\nCurrently, BigDL development version is hosted on \nSonaType\n. \n\n\nTo link your application with the latest BigDL development version, you should add some dependencies like \nLinking with BigDL releases\n, but set ${BIGDL_VERSION} to 0.2.0-SNAPSHOT, and add below repository to your pom.xml.\n\n\n<repository>\n    <id>sonatype</id>\n    <name>sonatype repository</name>\n    <url>https://oss.sonatype.org/content/groups/public/</url>\n    <releases>\n        <enabled>true</enabled>\n    </releases>\n    <snapshots>\n        <enabled>true</enabled>\n    </snapshots>\n</repository>\n\n\n\n\nSBT developers can use\n\n\nresolvers += \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\"\n\n\n\n\nGet Source Code\n\n\nBigDL source code is available at \nGitHub\n\n\n$ git clone https://github.com/intel-analytics/BigDL.git\n\n\n\n\nBy default, \ngit clone\n will download the development version of BigDL, if you want a release version, you can use command \ngit checkout\n to change the version. Available release versions is \nBigDL releases\n.\n\n\nBuild\n\n\nThe following instructions are aligned with master code.\n\n\nMaven 3 is needed to build BigDL, you can download it from the \nmaven website\n.\n\n\nAfter installing Maven 3, please set the environment variable MAVEN_OPTS as follows:\n```{r, engine='sh'}\n$ export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"\n\n\nWhen compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d. \n\n### **Build with `make-dist.sh`**\nIt is highly recommended that you build BigDL using the [make-dist.sh script](https://github.com/intel-analytics/BigDL/blob/master/make-dist.sh). And it will handle the MAVEN_OPTS variable.\n\nOnce downloaded, you can build BigDL with the following commands:\n```sbt\n$ bash make-dist.sh\n\n\n\n\nAfter that, you can find a \ndist\n folder, which contains all the needed files to run a BigDL program. The files in \ndist\n include:\n\n \ndist/bin/bigdl.sh\n: A script used to set up proper environment variables and launch the BigDL program.\n\n \ndist/lib/bigdl-VERSION-jar-with-dependencies.jar\n: This jar package contains all dependencies except Spark classes.\n\n \ndist/lib/bigdl-VERSION-python-api.zip\n: This zip package contains all Python files of BigDL.\n\n \ndist/conf/spark-bigdl.conf\n: This file contains necessary property configurations. \nEngine.createSparkConf\n will populate these properties, so try to use that method in your code. Or you need to pass the file to Spark with the \"--properties-file\" option. \n\n\nBuild for macOS\n\n\nThe instructions above will only build for Linux. To build BigDL for macOS, pass \n-P mac\n to the \nmake-dist.sh\n script as follows:\n\n\n$ bash make-dist.sh -P mac\n\n\n\n\nBuild for Spark 2.0 and above\n\n\nThe instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 and above (which uses Scala 2.11 by default), pass \n-P spark_2.x\n to the \nmake-dist.sh\n script:\n\n\n$ bash make-dist.sh -P spark_2.x\n\n\n\n\nIt is highly recommended to use \nJava 8\n when running with Spark 2.x; otherwise you may observe very poor performance.\n\n\nBuild using Scala 2.10 or 2.11\n\n\nBy default, \nmake-dist.sh\n uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0.x or 2.1.x. To override the default behaviors, you can pass \n-P scala_2.10\n or \n-P scala_2.11\n to \nmake-dist.sh\n as appropriate.\n\n\nFull Build\n\n\nNote that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:\n\n\n\n\n\n\nDownload and install \nIntel Parallel Studio XE\n in your Linux box.\n\n\n\n\n\n\nPrepare build environment as follows:\n    \n{r, engine='sh'}\n    $ source <install-dir>/bin/compilervars.sh intel64\n    $ source PATH_TO_MKL/bin/mklvars.sh intel64\n\n    where the \nPATH_TO_MKL\n is the installation directory of the MKL.\n\n\n\n\n\n\nFull build\n\n\n\n\n\n\nClone BigDL as follows:\n   \n{r, engine='sh'}\n   git clone git@github.com:intel-analytics/BigDL.git --recursive\n\n   For already cloned repos, just use:\n   \n{r, engine='sh'}\n   git submodule update --init --recursive\n\n   If the Intel MKL is not installed to the default path \n/opt/intel\n, please pass your libiomp5.so's directory path to\n   the \nmake-dist.sh\n script:\n   \n{r, engine='sh'}\n   $ bash make-dist.sh -P full-build -DiompLibDir=<PATH_TO_LIBIOMP5_DIR>\n\n   Otherwise, only pass \n-P full-build\n to the \nmake-dist.sh\n script:\n   \n{r, engine='sh'}\n   $ bash make-dist.sh -P full-build\n\n\nBuild with Maven\n\n\nTo build BigDL directly using Maven, run the command below:\n\n\n$ mvn clean package -DskipTests\n\n\n\n\nAfter that, you can find that the three jar packages in \nPATH_To_BigDL\n/target/, where \nPATH_To_BigDL\n is the path to the directory of the BigDL. \n\n\nNote that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:\n\n\n\n\n-P mac\n: build for maxOS\n\n\n-P spark_2.x\n: build for Spark 2.0 and above (using Scala 2.11). (Again, it is highly recommended to use \nJava 8\n when running with Spark 2.0; otherwise you may observe very poor performance.)\n\n\n-P full-build\n: full build\n\n\n-P scala_2.10\n (or \n-P scala_2.11\n): build using Scala 2.10 (or Scala 2.11) \n\n\n\n\nIDE Settings\n\n\nWe set the scope of spark related library to \nprovided\n in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.\n\n\nThis will cause a problem in IDE. When you run applications, it will throw \nNoClassDefFoundError\n because the library scope is \nprovided\n.\n\n\nYou can easily change the scopes by the \nall-in-one\n profile.\n\n\n\n\nIn Intellij, go to View -> Tools Windows -> Maven Projects. Then in the Maven Projects panel, Profiles -> click \"all-in-one\". \n\n\n\n\nNext Steps\n\n\n\n\nTo learn how to run BigDL programs (as either a local Java program or a Spark program), you can check out the \nGetting Started Page\n.\n\n\nTo learn the details of Python support in BigDL, you can check out the \nPython Support Page",
            "title": "Build"
        },
        {
            "location": "/UserGuide/build/#build",
            "text": "This page shows how to install and build BigDL (on both Linux and macOS), including:   Download  Linking  Linking with BigDL releases  Linking with development version    Get Source Code  Build  Build with make-dist.sh  Using the  make-dist.sh  script  Build for macOS  Build for Spark 2.0 and above  Build using Scala 2.10 or 2.11  Full Build    Build with Maven    IDE Settings  Next Steps",
            "title": "Build"
        },
        {
            "location": "/UserGuide/build/#download",
            "text": "You may download the BigDL release (currently v0.1.0) and nightly build from the  Release Page",
            "title": "Download"
        },
        {
            "location": "/UserGuide/build/#linking",
            "text": "",
            "title": "Linking"
        },
        {
            "location": "/UserGuide/build/#linking-with-bigdl-releases",
            "text": "Currently, BigDL releases are hosted on maven central; here's an example to add the BigDL dependency to your own project:  <dependency>\n    <groupId>com.intel.analytics.bigdl</groupId>\n    <artifactId>bigdl</artifactId>\n    <version>${BIGDL_VERSION}</version>\n</dependency>  SBT developers can use  libraryDependencies += \"com.intel.analytics.bigdl\" % \"bigdl\" % \"${BIGDL_VERSION}\"  Since currently only BigDL 0.1.0 is released, ${BIGDL_VERSION} must be set to 0.1.0 here.  Note : the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this  <dependency>\n    <groupId>com.intel.analytics.bigdl</groupId>\n    <artifactId>bigdl-SPARK_2.0</artifactId>\n    <version>${BIGDL_VERSION}</version>\n</dependency>  SBT developers can use  libraryDependencies += \"com.intel.analytics.bigdl\" % \"bigdl-SPARK_2.0\" % \"${BIGDL_VERSION}\"  If your project runs on MacOS, you should add the dependency below,  <dependency>\n    <groupId>com.intel.analytics.bigdl.native</groupId>\n    <artifactId>mkl-java-mac</artifactId>\n    <version>${BIGDL_VERSION}</version>\n    <exclusions>\n        <exclusion>\n            <groupId>com.intel.analytics.bigdl.native</groupId>\n            <artifactId>bigdl-native</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>  SBT developers can use  libraryDependencies += \"com.intel.analytics.bigdl.native\" % \"mkl-java-mac\" % \"${BIGDL_VERSION}\" from \"http://repo1.maven.org/maven2/com/intel/analytics/bigdl/native/mkl-java-mac/${BIGDL_VERSION}/mkl-java-mac-${BIGDL_VERSION}.jar\"",
            "title": "Linking with BigDL releases"
        },
        {
            "location": "/UserGuide/build/#linking-with-development-version",
            "text": "Currently, BigDL development version is hosted on  SonaType .   To link your application with the latest BigDL development version, you should add some dependencies like  Linking with BigDL releases , but set ${BIGDL_VERSION} to 0.2.0-SNAPSHOT, and add below repository to your pom.xml.  <repository>\n    <id>sonatype</id>\n    <name>sonatype repository</name>\n    <url>https://oss.sonatype.org/content/groups/public/</url>\n    <releases>\n        <enabled>true</enabled>\n    </releases>\n    <snapshots>\n        <enabled>true</enabled>\n    </snapshots>\n</repository>  SBT developers can use  resolvers += \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\"",
            "title": "Linking with development version"
        },
        {
            "location": "/UserGuide/build/#get-source-code",
            "text": "BigDL source code is available at  GitHub  $ git clone https://github.com/intel-analytics/BigDL.git  By default,  git clone  will download the development version of BigDL, if you want a release version, you can use command  git checkout  to change the version. Available release versions is  BigDL releases .",
            "title": "Get Source Code"
        },
        {
            "location": "/UserGuide/build/#build_1",
            "text": "The following instructions are aligned with master code.  Maven 3 is needed to build BigDL, you can download it from the  maven website .  After installing Maven 3, please set the environment variable MAVEN_OPTS as follows:\n```{r, engine='sh'}\n$ export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"  When compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d. \n\n### **Build with `make-dist.sh`**\nIt is highly recommended that you build BigDL using the [make-dist.sh script](https://github.com/intel-analytics/BigDL/blob/master/make-dist.sh). And it will handle the MAVEN_OPTS variable.\n\nOnce downloaded, you can build BigDL with the following commands:\n```sbt\n$ bash make-dist.sh  After that, you can find a  dist  folder, which contains all the needed files to run a BigDL program. The files in  dist  include:   dist/bin/bigdl.sh : A script used to set up proper environment variables and launch the BigDL program.   dist/lib/bigdl-VERSION-jar-with-dependencies.jar : This jar package contains all dependencies except Spark classes.   dist/lib/bigdl-VERSION-python-api.zip : This zip package contains all Python files of BigDL.   dist/conf/spark-bigdl.conf : This file contains necessary property configurations.  Engine.createSparkConf  will populate these properties, so try to use that method in your code. Or you need to pass the file to Spark with the \"--properties-file\" option.",
            "title": "Build"
        },
        {
            "location": "/UserGuide/build/#build-for-macos",
            "text": "The instructions above will only build for Linux. To build BigDL for macOS, pass  -P mac  to the  make-dist.sh  script as follows:  $ bash make-dist.sh -P mac",
            "title": "Build for macOS"
        },
        {
            "location": "/UserGuide/build/#build-for-spark-20-and-above",
            "text": "The instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 and above (which uses Scala 2.11 by default), pass  -P spark_2.x  to the  make-dist.sh  script:  $ bash make-dist.sh -P spark_2.x  It is highly recommended to use  Java 8  when running with Spark 2.x; otherwise you may observe very poor performance.",
            "title": "Build for Spark 2.0 and above"
        },
        {
            "location": "/UserGuide/build/#build-using-scala-210-or-211",
            "text": "By default,  make-dist.sh  uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0.x or 2.1.x. To override the default behaviors, you can pass  -P scala_2.10  or  -P scala_2.11  to  make-dist.sh  as appropriate.",
            "title": "Build using Scala 2.10 or 2.11"
        },
        {
            "location": "/UserGuide/build/#full-build",
            "text": "Note that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:    Download and install  Intel Parallel Studio XE  in your Linux box.    Prepare build environment as follows:\n     {r, engine='sh'}\n    $ source <install-dir>/bin/compilervars.sh intel64\n    $ source PATH_TO_MKL/bin/mklvars.sh intel64 \n    where the  PATH_TO_MKL  is the installation directory of the MKL.    Full build    Clone BigDL as follows:\n    {r, engine='sh'}\n   git clone git@github.com:intel-analytics/BigDL.git --recursive \n   For already cloned repos, just use:\n    {r, engine='sh'}\n   git submodule update --init --recursive \n   If the Intel MKL is not installed to the default path  /opt/intel , please pass your libiomp5.so's directory path to\n   the  make-dist.sh  script:\n    {r, engine='sh'}\n   $ bash make-dist.sh -P full-build -DiompLibDir=<PATH_TO_LIBIOMP5_DIR> \n   Otherwise, only pass  -P full-build  to the  make-dist.sh  script:\n    {r, engine='sh'}\n   $ bash make-dist.sh -P full-build",
            "title": "Full Build"
        },
        {
            "location": "/UserGuide/build/#build-with-maven",
            "text": "To build BigDL directly using Maven, run the command below:  $ mvn clean package -DskipTests  After that, you can find that the three jar packages in  PATH_To_BigDL /target/, where  PATH_To_BigDL  is the path to the directory of the BigDL.   Note that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:   -P mac : build for maxOS  -P spark_2.x : build for Spark 2.0 and above (using Scala 2.11). (Again, it is highly recommended to use  Java 8  when running with Spark 2.0; otherwise you may observe very poor performance.)  -P full-build : full build  -P scala_2.10  (or  -P scala_2.11 ): build using Scala 2.10 (or Scala 2.11)",
            "title": "Build with Maven"
        },
        {
            "location": "/UserGuide/build/#ide-settings",
            "text": "We set the scope of spark related library to  provided  in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.  This will cause a problem in IDE. When you run applications, it will throw  NoClassDefFoundError  because the library scope is  provided .  You can easily change the scopes by the  all-in-one  profile.   In Intellij, go to View -> Tools Windows -> Maven Projects. Then in the Maven Projects panel, Profiles -> click \"all-in-one\".",
            "title": "IDE Settings"
        },
        {
            "location": "/UserGuide/build/#next-steps",
            "text": "To learn how to run BigDL programs (as either a local Java program or a Spark program), you can check out the  Getting Started Page .  To learn the details of Python support in BigDL, you can check out the  Python Support Page",
            "title": "Next Steps"
        },
        {
            "location": "/UserGuide/getting-started/",
            "text": "Getting Started\n\n\n\n\nThis page shows how to run a BigDL program, including\n\n\n\n\nBefore running a BigDL program\n\n\nInteractive Spark Shell\n\n\nSpark Program\n\n\nNext Steps\n\n\n\n\nBefore running a BigDL program\n\n\nBefore running a BigDL program, you need to set proper environment variables first.\n\n\nSetting Environment Variables\n\n\nTo achieve high performance, BigDL uses Intel MKL and multi-threaded programming; therefore, you need to first set the environment variables by running the provided script in \nPATH_To_BigDL/bin/bigdl.sh\n as follows:\n\n\n$ source PATH_To_BigDL/bin/bigdl.sh\n\n\n\n\nAlternatively, you can also use the \nPATH_To_BigDL/bin/bigdl.sh\n script to launch your BigDL program; see the details below.\n\n\nInteractive Spark shell\n\n\nYou can quickly experiment with BigDL codes as a Spark program using the interactive Spark shell by running:\n\n\n$ source PATH_To_BigDL/bin/bigdl.sh\n$ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \\\n  --jars bigdl-VERSION-jar-with-dependencies.jar\n\n\n\n\nThen you can see something like:\n\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala> \n\n\n\n\nFor instance, to experiment with the \nTensor\n APIs in BigDL, you may then try:\n\n\nscala> import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala> Tensor[Double](2,2).fill(1.0)\nres9: com.intel.analytics.bigdl.tensor.Tensor[Double] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nFor more details about the BigDL APIs, please refer to the \nProgramming Guide\n.\n\n\nSpark Program\n\n\nYou can run a BigDL program, e.g., the \nVGG\n training, as a standard Spark program (running in either local mode or cluster mode) as follows:\n\n\n\n\n\n\nDownload the CIFAR-10 data from \nhere\n. Remember to choose the binary version.\n\n\n\n\n\n\nUse the \nbigdl.sh\n script to launch the example as a Spark program as follows:\n\n\n\n\n\n\n  # Spark local mode\n  ./dist/bin/bigdl.sh -- \\\n  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark standalone mode\n  ./dist/bin/bigdl.sh -- \\\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark yarn mode\n  ./dist/bin/bigdl.sh -- \\\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n\n\n\nThe parameters used in the above command are:\n\n\n\n\n\n\n-f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.\n\n\n\n\n\n\n-b: The mini-batch size. The mini-batch size is expected to be a multiple of \ntotal cores\n used in the job. In this example, the mini-batch size is suggested to be set to \ntotal cores * 4\n\n\n\n\n\n\nNext Steps\n\n\n\n\n\n\nTo learn the details of Python support in BigDL, you can check out the \nPython Support Page\n\n\n\n\n\n\nTo learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the \nExamples Page\n\n\n\n\n\n\nYou can check out the \nDocuments Page\n for more details (including Models, Examples, Programming Guide, etc.)\n\n\n\n\n\n\nYou can join the \nBigDL Google Group\n (or subscribe to the \nmail list\n) for more questions and discussions on BigDL\n\n\n\n\n\n\nYou can post bug reports and feature requests at the \nIssue Page",
            "title": "Getting Started"
        },
        {
            "location": "/UserGuide/getting-started/#getting-started",
            "text": "This page shows how to run a BigDL program, including   Before running a BigDL program  Interactive Spark Shell  Spark Program  Next Steps",
            "title": "Getting Started"
        },
        {
            "location": "/UserGuide/getting-started/#before-running-a-bigdl-program",
            "text": "Before running a BigDL program, you need to set proper environment variables first.",
            "title": "Before running a BigDL program"
        },
        {
            "location": "/UserGuide/getting-started/#setting-environment-variables",
            "text": "To achieve high performance, BigDL uses Intel MKL and multi-threaded programming; therefore, you need to first set the environment variables by running the provided script in  PATH_To_BigDL/bin/bigdl.sh  as follows:  $ source PATH_To_BigDL/bin/bigdl.sh  Alternatively, you can also use the  PATH_To_BigDL/bin/bigdl.sh  script to launch your BigDL program; see the details below.",
            "title": "Setting Environment Variables"
        },
        {
            "location": "/UserGuide/getting-started/#interactive-spark-shell",
            "text": "You can quickly experiment with BigDL codes as a Spark program using the interactive Spark shell by running:  $ source PATH_To_BigDL/bin/bigdl.sh\n$ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \\\n  --jars bigdl-VERSION-jar-with-dependencies.jar  Then you can see something like:  Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala>   For instance, to experiment with the  Tensor  APIs in BigDL, you may then try:  scala> import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala> Tensor[Double](2,2).fill(1.0)\nres9: com.intel.analytics.bigdl.tensor.Tensor[Double] =\n1.0     1.0\n1.0     1.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  For more details about the BigDL APIs, please refer to the  Programming Guide .",
            "title": "Interactive Spark shell"
        },
        {
            "location": "/UserGuide/getting-started/#spark-program",
            "text": "You can run a BigDL program, e.g., the  VGG  training, as a standard Spark program (running in either local mode or cluster mode) as follows:    Download the CIFAR-10 data from  here . Remember to choose the binary version.    Use the  bigdl.sh  script to launch the example as a Spark program as follows:      # Spark local mode\n  ./dist/bin/bigdl.sh -- \\\n  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark standalone mode\n  ./dist/bin/bigdl.sh -- \\\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size\n\n  # Spark yarn mode\n  ./dist/bin/bigdl.sh -- \\\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.bigdl.models.vgg.Train \\\n  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \\\n  -f path_to_your_cifar_folder \\\n  -b batch_size  The parameters used in the above command are:    -f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.    -b: The mini-batch size. The mini-batch size is expected to be a multiple of  total cores  used in the job. In this example, the mini-batch size is suggested to be set to  total cores * 4",
            "title": "Spark Program"
        },
        {
            "location": "/UserGuide/getting-started/#next-steps",
            "text": "To learn the details of Python support in BigDL, you can check out the  Python Support Page    To learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the  Examples Page    You can check out the  Documents Page  for more details (including Models, Examples, Programming Guide, etc.)    You can join the  BigDL Google Group  (or subscribe to the  mail list ) for more questions and discussions on BigDL    You can post bug reports and feature requests at the  Issue Page",
            "title": "Next Steps"
        },
        {
            "location": "/UserGuide/examples/",
            "text": "Examples\n\n\n\n\nThis page shows how to build simple deep learning programs using BigDL, including:\n\n\n\n\nTraining LeNet on MNIST\n - the \"hello world\" for deep learning\n\n\nText Classification\n - working with Spark RDD transformations\n\n\nImage Classification\n - working with Spark DataFrame and ML pipeline\n\n\nPython Text Classifier\n - text classification using BigDL Python APIs\n\n\nBigDL Tutorials Notebooks\n - A series of notebooks that step-by-step introduce you how to do data science on Apache Spark and BigDL framework\n\n\nJupyter Notebook Tutorial\n - using BigDL Python APIs in Jupyter notebook\n\n\n\n\nTraining LeNet on MNIST\n\n\nThis tutorial is an explanation of what is happening in the \nlenet\n example, which trains \nLeNet-5\n on the \nMNIST data\n using BigDL.\n\n\nA BigDL program starts with \nimport com.intel.analytics.bigdl._\n; it then \ncreates the \nSparkContext\n using the \nSparkConf\n returned by the \nEngine\n; after that, it \ninitializes the \nEngine\n.\n\n\n  val conf = Engine.createSparkConf()\n      .setAppName(\"Train Lenet on MNIST\")\n      .set(\"spark.task.maxFailures\", \"1\")\n  val sc = new SparkContext(conf)\n  Engine.init\n\n\n\n\nEngine.createSparkConf\n will return a \nSparkConf\n populated with some appropriate configuration. And \nEngine.init\n will verify and read some environment information(e.g. executor numbers and executor cores) from the \nSparkContext\n. You can find more information about the initialization in the \nProgramming Guilde\n\n\nAfter the initialization, we need to:\n\n\n\n\nCreate the LeNet model\n by calling the \nLeNet5()\n, which creates the LeNet-5 convolutional network model as follows:\n\n\n\n\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())\n\n\n\n\n\n\nLoad the data by \ncreating the \nDataSet\n (either a distributed or local one depending on whether it runs on Spark or not), and then \napplying a series of \nTransformer\n (e.g., \nSampleToGreyImg\n, \nGreyImgNormalizer\n and \nGreyImgToBatch\n):\n\n\n\n\n    val trainSet = (if (sc.isDefined) {\n        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)\n      } else {\n        DataSet.array(load(trainData, trainLabel))\n      }) -> SampleToGreyImg(28, 28) -> GreyImgNormalizer(trainMean, trainStd) -> GreyImgToBatch(\n        param.batchSize)\n\n\n\n\nAfter that, we \ncreate the \nOptimizer\n (either a distributed or local one depending on whether it runs on Spark or not) by specifying the \nDataSet\n, the model and the \nCriterion\n (which, given input and target, computes gradient per given loss function):\n\n\n  val optimizer = Optimizer(\n    model = model,\n    dataset = trainSet,\n    criterion = ClassNLLCriterion[Float]())\n\n\n\n\nFinally (after optionally specifying the validation data and methods for the \nOptimizer\n), we \ntrain the model by calling \nOptimizer.optimize()\n:\n\n\n  optimizer\n    .setValidation(\n      trigger = Trigger.everyEpoch,\n      dataset = validationSet,\n      vMethods = Array(new Top1Accuracy))\n    .setState(state)\n    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))\n    .optimize()\n\n\n\n\nText Classification - Working with Spark RDD\n\n\nThis tutorial describes the \ntext_classification\n example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by \nthis Keras tutorial\n).\n\n\nAfter importing \ncom.intel.analytics.bigdl._\n and some initialization, the \nexample\n broadcasts the pre-trained world embedding and loads the input data using RDD transformations:\n\n\n  // For large dataset, you might want to get such RDD[(String, Float)] from HDFS\n  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)\n  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)\n  val word2MetaBC = sc.broadcast(word2Meta)\n  val word2VecBC = sc.broadcast(word2Vec)\n  val vectorizedRdd = dataRdd\n      .map {case (text, label) => (toTokens(text, word2MetaBC.value), label)}\n      .map {case (tokens, label) => (shaping(tokens, sequenceLen), label)}\n      .map {case (tokens, label) => (vectorization(\n        tokens, embeddingDim, word2VecBC.value), label)}\n\n\n\n\nThe \nexample\n then converts the processed data (\nvectorizedRdd\n) to an RDD of Sample, and randomly splits the sample RDD (\nsampleRDD\n) into training data (\ntrainingRDD\n) and validation data (\nvalRDD\n):\n\n\n  val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) =>\n        Sample(\n          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))\n            .transpose(1, 2).contiguous(),\n          labelTensor = Tensor(Array(label), Array(1)))\n      }\n\n  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(\n    Array(trainingSplit, 1 - trainingSplit))\n\n\n\n\nAfter that, the \nexample\n builds the CNN model, creates the \nOptimizer\n, pass the RDD of training data (\ntrainingRDD\n) to the \nOptimizer\n (with specific batch size), and finally trains the model (using \nAdagrad\n as the optimization method, and setting relevant hyper parameters in \nstate\n):\n\n\n  val optimizer = Optimizer(\n    model = buildModel(classNum),\n    sampleRDD = trainingRDD,\n    criterion = new ClassNLLCriterion[Float](),\n    batchSize = param.batchSize\n  )\n  val state = T(\"learningRate\" -> 0.01, \"learningRateDecay\" -> 0.0002)\n  optimizer\n    .setState(state)\n    .setOptimMethod(new Adagrad())\n    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n    .setEndWhen(Trigger.maxEpoch(2))\n    .optimize()\n\n\n\n\nImage Classification\n - Working with Spark DataFrame and ML pipeline\n\n\nThis tutorial describes the \nimage_classification\n example, which loads a BigDL (\nInception\n) model or Torch (\nResnet\n) model that is trained on \nImageNet\n data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark \nML pipeline\n.\n\n\nAfter importing \ncom.intel.analytics.bigdl._\n and some initialization, the \nexample\n first \nloads\n the specified model:\n\n\n  def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)\n    (implicit ev: TensorNumeric[T]): Module[T] = {\n    val model = param.modelType match {\n      case TorchModel =>\n        Module.loadTorch[T](param.modelPath)\n      case BigDlModel =>\n        Module.load[T](param.modelPath)\n      case _ => throw new IllegalArgumentException(s\"${param.modelType}\")\n    }\n    model\n  }\n\n\n\n\nIt then creates \nDLClassifer\n (a Spark ML pipelines \nTransformer\n) that predicts the input value based on the specified deep learning model:\n\n\n  val model = loadModel(param)\n  val valTrans = new DLClassifier()\n    .setInputCol(\"features\")\n    .setOutputCol(\"predict\")\n\n  val paramsTrans = ParamMap(\n    valTrans.modelTrain -> model,\n    valTrans.batchShape ->\n    Array(param.batchSize, 3, imageSize, imageSize))\n\n\n\n\nAfter that, the \nexample\n  loads the input images into a \nDataFrame\n, and then predicts the class of each each image using the \nDLClassifer\n:\n\n\n  val valRDD = sc.parallelize(imageSet).repartition(partitionNum)\n  val transf = RowToByteRecords() ->\n      SampleToBGRImg() ->\n      BGRImgCropper(imageSize, imageSize) ->\n      BGRImgNormalizer(testMean, testStd) ->\n      BGRImgToImageVector()\n\n  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)\n\n  valTrans.transform(valDF, paramsTrans)\n      .select(\"imageName\", \"predict\")\n      .show(param.showNum)\n\n\n\n\nTutorial: Text Classification using BigDL Python API\n\n\nThis tutorial describes the \ntextclassifier\n example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by \nthis Keras tutorial\n)\n\n\nThe example first creates the \nSparkContext\n using the SparkConf\nreturn by the\ncreate_spark_conf()` method, and then initialize the engine:\n\n\n  sc = SparkContext(appName=\"text_classifier\",\n                    conf=create_spark_conf())\n  init_engine()\n\n\n\n\nIt then loads the \n20 Newsgroup dataset\n into RDD, and transforms the input data into an RDD of \nSample\n. (Each \nSample\n in essence contains a tuple of two NumPy ndarray representing the feature and label).\n\n\n  texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])   \n\n\n\n\nAfter that, the example creates the neural network model as follows:\n\n\ndef build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \"cnn\":\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \"lstm\":\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \"gru\":\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model\n\n\n\n\nFinally the example creates the \nOptimizer\n (which accepts both the model and the training Sample RDD) and trains the model by calling \nOptimizer.optimize()\n:\n\n\noptimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=\"Adagrad\",\n    state=state)\n...\ntrain_model = optimizer.optimize()",
            "title": "Examples"
        },
        {
            "location": "/UserGuide/examples/#examples",
            "text": "This page shows how to build simple deep learning programs using BigDL, including:   Training LeNet on MNIST  - the \"hello world\" for deep learning  Text Classification  - working with Spark RDD transformations  Image Classification  - working with Spark DataFrame and ML pipeline  Python Text Classifier  - text classification using BigDL Python APIs  BigDL Tutorials Notebooks  - A series of notebooks that step-by-step introduce you how to do data science on Apache Spark and BigDL framework  Jupyter Notebook Tutorial  - using BigDL Python APIs in Jupyter notebook",
            "title": "Examples"
        },
        {
            "location": "/UserGuide/examples/#training-lenet-on-mnist",
            "text": "This tutorial is an explanation of what is happening in the  lenet  example, which trains  LeNet-5  on the  MNIST data  using BigDL.  A BigDL program starts with  import com.intel.analytics.bigdl._ ; it then  creates the  SparkContext  using the  SparkConf  returned by the  Engine ; after that, it  initializes the  Engine .    val conf = Engine.createSparkConf()\n      .setAppName(\"Train Lenet on MNIST\")\n      .set(\"spark.task.maxFailures\", \"1\")\n  val sc = new SparkContext(conf)\n  Engine.init  Engine.createSparkConf  will return a  SparkConf  populated with some appropriate configuration. And  Engine.init  will verify and read some environment information(e.g. executor numbers and executor cores) from the  SparkContext . You can find more information about the initialization in the  Programming Guilde  After the initialization, we need to:   Create the LeNet model  by calling the  LeNet5() , which creates the LeNet-5 convolutional network model as follows:       val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())   Load the data by  creating the  DataSet  (either a distributed or local one depending on whether it runs on Spark or not), and then  applying a series of  Transformer  (e.g.,  SampleToGreyImg ,  GreyImgNormalizer  and  GreyImgToBatch ):       val trainSet = (if (sc.isDefined) {\n        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)\n      } else {\n        DataSet.array(load(trainData, trainLabel))\n      }) -> SampleToGreyImg(28, 28) -> GreyImgNormalizer(trainMean, trainStd) -> GreyImgToBatch(\n        param.batchSize)  After that, we  create the  Optimizer  (either a distributed or local one depending on whether it runs on Spark or not) by specifying the  DataSet , the model and the  Criterion  (which, given input and target, computes gradient per given loss function):    val optimizer = Optimizer(\n    model = model,\n    dataset = trainSet,\n    criterion = ClassNLLCriterion[Float]())  Finally (after optionally specifying the validation data and methods for the  Optimizer ), we  train the model by calling  Optimizer.optimize() :    optimizer\n    .setValidation(\n      trigger = Trigger.everyEpoch,\n      dataset = validationSet,\n      vMethods = Array(new Top1Accuracy))\n    .setState(state)\n    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))\n    .optimize()",
            "title": "Training LeNet on MNIST"
        },
        {
            "location": "/UserGuide/examples/#text-classification-working-with-spark-rdd",
            "text": "This tutorial describes the  text_classification  example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by  this Keras tutorial ).  After importing  com.intel.analytics.bigdl._  and some initialization, the  example  broadcasts the pre-trained world embedding and loads the input data using RDD transformations:    // For large dataset, you might want to get such RDD[(String, Float)] from HDFS\n  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)\n  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)\n  val word2MetaBC = sc.broadcast(word2Meta)\n  val word2VecBC = sc.broadcast(word2Vec)\n  val vectorizedRdd = dataRdd\n      .map {case (text, label) => (toTokens(text, word2MetaBC.value), label)}\n      .map {case (tokens, label) => (shaping(tokens, sequenceLen), label)}\n      .map {case (tokens, label) => (vectorization(\n        tokens, embeddingDim, word2VecBC.value), label)}  The  example  then converts the processed data ( vectorizedRdd ) to an RDD of Sample, and randomly splits the sample RDD ( sampleRDD ) into training data ( trainingRDD ) and validation data ( valRDD ):    val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) =>\n        Sample(\n          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))\n            .transpose(1, 2).contiguous(),\n          labelTensor = Tensor(Array(label), Array(1)))\n      }\n\n  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(\n    Array(trainingSplit, 1 - trainingSplit))  After that, the  example  builds the CNN model, creates the  Optimizer , pass the RDD of training data ( trainingRDD ) to the  Optimizer  (with specific batch size), and finally trains the model (using  Adagrad  as the optimization method, and setting relevant hyper parameters in  state ):    val optimizer = Optimizer(\n    model = buildModel(classNum),\n    sampleRDD = trainingRDD,\n    criterion = new ClassNLLCriterion[Float](),\n    batchSize = param.batchSize\n  )\n  val state = T(\"learningRate\" -> 0.01, \"learningRateDecay\" -> 0.0002)\n  optimizer\n    .setState(state)\n    .setOptimMethod(new Adagrad())\n    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n    .setEndWhen(Trigger.maxEpoch(2))\n    .optimize()",
            "title": "Text Classification - Working with Spark RDD"
        },
        {
            "location": "/UserGuide/examples/#image-classification-working-with-spark-dataframe-and-ml-pipeline",
            "text": "This tutorial describes the  image_classification  example, which loads a BigDL ( Inception ) model or Torch ( Resnet ) model that is trained on  ImageNet  data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark  ML pipeline .  After importing  com.intel.analytics.bigdl._  and some initialization, the  example  first  loads  the specified model:    def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)\n    (implicit ev: TensorNumeric[T]): Module[T] = {\n    val model = param.modelType match {\n      case TorchModel =>\n        Module.loadTorch[T](param.modelPath)\n      case BigDlModel =>\n        Module.load[T](param.modelPath)\n      case _ => throw new IllegalArgumentException(s\"${param.modelType}\")\n    }\n    model\n  }  It then creates  DLClassifer  (a Spark ML pipelines  Transformer ) that predicts the input value based on the specified deep learning model:    val model = loadModel(param)\n  val valTrans = new DLClassifier()\n    .setInputCol(\"features\")\n    .setOutputCol(\"predict\")\n\n  val paramsTrans = ParamMap(\n    valTrans.modelTrain -> model,\n    valTrans.batchShape ->\n    Array(param.batchSize, 3, imageSize, imageSize))  After that, the  example   loads the input images into a  DataFrame , and then predicts the class of each each image using the  DLClassifer :    val valRDD = sc.parallelize(imageSet).repartition(partitionNum)\n  val transf = RowToByteRecords() ->\n      SampleToBGRImg() ->\n      BGRImgCropper(imageSize, imageSize) ->\n      BGRImgNormalizer(testMean, testStd) ->\n      BGRImgToImageVector()\n\n  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)\n\n  valTrans.transform(valDF, paramsTrans)\n      .select(\"imageName\", \"predict\")\n      .show(param.showNum)",
            "title": "Image Classification - Working with Spark DataFrame and ML pipeline"
        },
        {
            "location": "/UserGuide/examples/#tutorial-text-classification-using-bigdl-python-api",
            "text": "This tutorial describes the  textclassifier  example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by  this Keras tutorial )  The example first creates the  SparkContext  using the SparkConf return by the create_spark_conf()` method, and then initialize the engine:    sc = SparkContext(appName=\"text_classifier\",\n                    conf=create_spark_conf())\n  init_engine()  It then loads the  20 Newsgroup dataset  into RDD, and transforms the input data into an RDD of  Sample . (Each  Sample  in essence contains a tuple of two NumPy ndarray representing the feature and label).    texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])     After that, the example creates the neural network model as follows:  def build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \"cnn\":\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \"lstm\":\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \"gru\":\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model  Finally the example creates the  Optimizer  (which accepts both the model and the training Sample RDD) and trains the model by calling  Optimizer.optimize() :  optimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=\"Adagrad\",\n    state=state)\n...\ntrain_model = optimizer.optimize()",
            "title": "Tutorial: Text Classification using BigDL Python API"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/",
            "text": "Visualization with TensorBoard\n\n\n\n\nGenerating summary info in BigDL\n\n\nTo enable visualization support, you need first properly configure the \nOptimizer\n to generate summary info for training (\nTrainSummary\n) and/or validation (\nValidationSummary\n) before invoking \nOptimizer.optimize()\n, as illustrated below: \n\n\nGenerating summary info in Scala\n\n\nval optimizer = Optimizer(...)\n...\nval logdir = \"mylogdir\"\nval appName = \"myapp\"\nval trainSummary = TrainSummary(logdir, appName)\nval validationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n...\nval trained_model = optimizer.optimize()\n\n\n\n\nGenerating summary info in Python\n\n\noptimizer = Optimizer(...)\n...\nlog_dir = 'mylogdir'\napp_name = 'myapp'\ntrain_summary = TrainSummary(log_dir=log_dir, app_name=app_name)\nval_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)\noptimizer.set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n...\ntrainedModel = optimizer.optimize()\n\n\n\n\nAfter you start to run your spark job, the train and validation summary will be saved to \nmylogdir/myapp/train\n and \nmylogdir/myapp/validation\n respectively (Note: you may want to use different \nappName\n for different job runs to avoid possible conflicts.) You may then read the summary info as follows:\n\n\nReading summary info in Scala\n\n\nval trainLoss = trainSummary.readScalar(\"Loss\")\nval validationLoss = validationSummary.readScalar(\"Loss\")\n...\n\n\n\n\nReading summary info in Python\n\n\nloss = np.array(train_summary.read_scalar('Loss'))\nvalloss = np.array(val_summary.read_scalar('Loss'))\n...\n\n\n\n\nVisualizing training with TensorBoard\n\n\nWith the summary info generated, we can then use \nTensorBoard\n to visualize the behaviors of the BigDL program.  \n\n\nInstalling TensorBoard\n\n\nPrerequisites:\n\n Python verison: 2.7, 3.4, 3.5, or 3.6\n\n Pip version >= 9.0.1\n\n\nTo install TensorBoard using Python 2, you may run the command:\n\n\npip install tensorboard==1.0.0a4\n\n\n\n\nTo install TensorBoard using Python 3, you may run the command:\n\n\npip3 install tensorboard==1.0.0a4\n\n\n\n\nPlease refer to \nthis page\n for possible issues when installing TensorBoard.\n\n\nLaunching TensorBoard\n\n\nYou can launch TensorBoard using the command below:\n\n\ntensorboard --logdir=/tmp/bigdl_summaries\n\n\n\n\nAfter that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006\n\n\nVisualizations in TensorBoard\n\n\nWithin the TensorBoard dashboard, you will be able to read the visualizations of each run, including the \u201cLoss\u201d and \u201cThroughput\u201d curves under the SCALARS tab (as illustrated below):\n\n\n\nAnd \u201cweights\u201d, \u201cbias\u201d, \u201cgradientWeights\u201d and \u201cgradientBias\u201d under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):",
            "title": "Visualization With Tensorboard"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/#visualization-with-tensorboard",
            "text": "",
            "title": "Visualization with TensorBoard"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/#generating-summary-info-in-bigdl",
            "text": "To enable visualization support, you need first properly configure the  Optimizer  to generate summary info for training ( TrainSummary ) and/or validation ( ValidationSummary ) before invoking  Optimizer.optimize() , as illustrated below:   Generating summary info in Scala  val optimizer = Optimizer(...)\n...\nval logdir = \"mylogdir\"\nval appName = \"myapp\"\nval trainSummary = TrainSummary(logdir, appName)\nval validationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n...\nval trained_model = optimizer.optimize()  Generating summary info in Python  optimizer = Optimizer(...)\n...\nlog_dir = 'mylogdir'\napp_name = 'myapp'\ntrain_summary = TrainSummary(log_dir=log_dir, app_name=app_name)\nval_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)\noptimizer.set_train_summary(train_summary)\noptimizer.set_val_summary(val_summary)\n...\ntrainedModel = optimizer.optimize()  After you start to run your spark job, the train and validation summary will be saved to  mylogdir/myapp/train  and  mylogdir/myapp/validation  respectively (Note: you may want to use different  appName  for different job runs to avoid possible conflicts.) You may then read the summary info as follows:  Reading summary info in Scala  val trainLoss = trainSummary.readScalar(\"Loss\")\nval validationLoss = validationSummary.readScalar(\"Loss\")\n...  Reading summary info in Python  loss = np.array(train_summary.read_scalar('Loss'))\nvalloss = np.array(val_summary.read_scalar('Loss'))\n...",
            "title": "Generating summary info in BigDL"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/#visualizing-training-with-tensorboard",
            "text": "With the summary info generated, we can then use  TensorBoard  to visualize the behaviors of the BigDL program.",
            "title": "Visualizing training with TensorBoard"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/#installing-tensorboard",
            "text": "Prerequisites:  Python verison: 2.7, 3.4, 3.5, or 3.6  Pip version >= 9.0.1  To install TensorBoard using Python 2, you may run the command:  pip install tensorboard==1.0.0a4  To install TensorBoard using Python 3, you may run the command:  pip3 install tensorboard==1.0.0a4  Please refer to  this page  for possible issues when installing TensorBoard.",
            "title": "Installing TensorBoard"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/#launching-tensorboard",
            "text": "You can launch TensorBoard using the command below:  tensorboard --logdir=/tmp/bigdl_summaries  After that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006",
            "title": "Launching TensorBoard"
        },
        {
            "location": "/UserGuide/visualization-with-tensorboard/#visualizations-in-tensorboard",
            "text": "Within the TensorBoard dashboard, you will be able to read the visualizations of each run, including the \u201cLoss\u201d and \u201cThroughput\u201d curves under the SCALARS tab (as illustrated below):  And \u201cweights\u201d, \u201cbias\u201d, \u201cgradientWeights\u201d and \u201cgradientBias\u201d under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):",
            "title": "Visualizations in TensorBoard"
        },
        {
            "location": "/UserGuide/running-on-EC2/",
            "text": "Running on EC2\n\n\n\n\n\n\n1. AMI\n\n\n2. Before You Start\n\n\n3. Run BigDL examples\n\n\n3.1 Run the \"inception-v1\" example\n\n\n3.2 Run the \"perf\" example\n\n\n\n\n\n\n\n\n1. AMI\n\n\nTo make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.\n\n\n\n\n\n\n\n\nBigDL version\n\n\nAMI version\n\n\nDate\n\n\nAMI ID\n\n\nAMI Name\n\n\nRegion\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nmaster\n\n\n0.2S\n\n\nMar 13, 2017\n\n\nami-37b73957\n\n\nBigDL Client 0.2S\n\n\nUS West (Oregon)\n\n\nActive\n\n\n\n\n\n\nmaster\n\n\n0.2S\n\n\nApr 10, 2017\n\n\nami-8c87099a\n\n\nBigDL Client 0.2S\n\n\nUS East (N. Virginia)\n\n\nActive\n\n\n\n\n\n\n0.1.0\n\n\n0.1.0\n\n\nApr 10, 2017\n\n\nami-9a8818fa\n\n\nBigDL Client 0.1.0\n\n\nUS West (Oregon)\n\n\nActive\n\n\n\n\n\n\n0.1.0\n\n\n0.1.0\n\n\nApr 10, 2017\n\n\nami-6476f872\n\n\nBigDL Client 0.1.0\n\n\nUS East (N. Virginia)\n\n\nActive\n\n\n\n\n\n\n\n\nPlease note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.\n\n\nAfter launching the AMI on EC2, please log on to the instance and run a \"bootstrap.sh\" script to download example scripts.\n\n\n./bootstrap.sh\n\n\n\n\n2. Before You Start\n\n\nBefore running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to \nhttps://github.com/amplab/spark-ec2\n for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.\n\n\n3. Run BigDL examples\n\n\nYou can run BigDL examples using the \nrun.example.sh\n script in home directory of your BigDL Client instance (e.g. \n/home/ubuntu/\n) with the following parameters:\n* Mandatory parameters:\n  * \n-m|--model\n which model to train, including\n    * lenet: train the \nLeNet\n example\n    * vgg: train the \nVGG\n example\n    * inception-v1: train the \nInception v1\n example\n    * perf: test the training speed using the \nInception v1\n model with dummy data\n\n\n\n\n\n\n-s|--spark-url\n the master URL for the Spark cluster\n\n\n\n\n\n\n-n|--nodes\n number of Spark slave nodes\n\n\n\n\n\n\n-o|--cores\n number of cores used on each node\n\n\n\n\n\n\n-r|--memory\n memory used on each node, e.g. 200g\n\n\n\n\n\n\n-b|--batch-size\n batch size when training the model; it is expected to be a multiple of \"nodes * cores\"\n\n\n\n\n\n\n-f|--hdfs-data-dir\n HDFS directory for the input images (for the \"inception-v1\" model training only)\n\n\n\n\n\n\nOptional parameters:\n\n\n\n\n\n\n-e|--max-epoch\n the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified\n\n\n\n\n\n\n-p|--spark\n by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify \"spark_2.0\" here (it is highly recommended to use \nJava 8\n when running BigDL for Spark 2.0, otherwise you may observe very poor performance)\n\n\n\n\n\n\n-l|--learning-rate\n by default the the example will use an initial learning rate of \"0.01\"; you can specify a different value here\n\n\n\n\n\n\nAfter the training, you can check the log files and generated models in the home directory (e.g., \n/home/ubuntu/\n).  \n\n\n3.1 Run the \"inception-v1\" example\n\n\nYou can refer to the \nInception v1\n example to prepare the input \nImageNet\n data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.\n\n\n\n\nDownload and prepare the input image data (a subset of the \nFlickr Style\n data)\n\n\n\n\n  ./download.sh $HDFS-NAMENODE\n\n\n\n\nAfter the download completes, the downloaded images are stored in \nhdfs://HDFS-NAMENODE:9000/seq\n. (If the download fails with error \"Unable to establish SSL connection.\" please check your network connection and retry this later.)\n\n\n\n\nTo run the \"inception-v1\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), run the example command below: \n\n\n\n\n  nohup bash ./run.example.sh --model inception-v1  \\\n         --spark-url spark://SPARK-MASTER:7077    \\\n         --nodes 4 --cores 20 --memory 150g       \\\n         --batch-size 400 --learning-rate 0.0898  \\\n         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \\\n         --spark spark_2.0 --max-epoch 4 \\\n         > incep.log 2>&1 &     \n\n\n\n\n\n\nView output of the training in the log file generated by the previous step:\n\n\n\n\n  $ tail -f incep.log\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n\n\n\n\n3.2 Run the \"perf\" example\n\n\nTo run the \"perf\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), you may try the example command below: \n\n\n  nohup bash ./run.example.sh --model perf  \\\n       --spark-url spark://SPARK-MASTER:7077    \\\n       --nodes 4 --cores 20 --memory 150g       \\\n       --spark spark_2.0 --max-epoch 4 \\\n       > perf.log 2>&1 &",
            "title": "Running On EC2"
        },
        {
            "location": "/UserGuide/running-on-EC2/#running-on-ec2",
            "text": "1. AMI  2. Before You Start  3. Run BigDL examples  3.1 Run the \"inception-v1\" example  3.2 Run the \"perf\" example",
            "title": "Running on EC2"
        },
        {
            "location": "/UserGuide/running-on-EC2/#1-ami",
            "text": "To make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.     BigDL version  AMI version  Date  AMI ID  AMI Name  Region  Status      master  0.2S  Mar 13, 2017  ami-37b73957  BigDL Client 0.2S  US West (Oregon)  Active    master  0.2S  Apr 10, 2017  ami-8c87099a  BigDL Client 0.2S  US East (N. Virginia)  Active    0.1.0  0.1.0  Apr 10, 2017  ami-9a8818fa  BigDL Client 0.1.0  US West (Oregon)  Active    0.1.0  0.1.0  Apr 10, 2017  ami-6476f872  BigDL Client 0.1.0  US East (N. Virginia)  Active     Please note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.  After launching the AMI on EC2, please log on to the instance and run a \"bootstrap.sh\" script to download example scripts.  ./bootstrap.sh",
            "title": "1. AMI"
        },
        {
            "location": "/UserGuide/running-on-EC2/#2-before-you-start",
            "text": "Before running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to  https://github.com/amplab/spark-ec2  for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.",
            "title": "2. Before You Start"
        },
        {
            "location": "/UserGuide/running-on-EC2/#3-run-bigdl-examples",
            "text": "You can run BigDL examples using the  run.example.sh  script in home directory of your BigDL Client instance (e.g.  /home/ubuntu/ ) with the following parameters:\n* Mandatory parameters:\n  *  -m|--model  which model to train, including\n    * lenet: train the  LeNet  example\n    * vgg: train the  VGG  example\n    * inception-v1: train the  Inception v1  example\n    * perf: test the training speed using the  Inception v1  model with dummy data    -s|--spark-url  the master URL for the Spark cluster    -n|--nodes  number of Spark slave nodes    -o|--cores  number of cores used on each node    -r|--memory  memory used on each node, e.g. 200g    -b|--batch-size  batch size when training the model; it is expected to be a multiple of \"nodes * cores\"    -f|--hdfs-data-dir  HDFS directory for the input images (for the \"inception-v1\" model training only)    Optional parameters:    -e|--max-epoch  the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified    -p|--spark  by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify \"spark_2.0\" here (it is highly recommended to use  Java 8  when running BigDL for Spark 2.0, otherwise you may observe very poor performance)    -l|--learning-rate  by default the the example will use an initial learning rate of \"0.01\"; you can specify a different value here    After the training, you can check the log files and generated models in the home directory (e.g.,  /home/ubuntu/ ).",
            "title": "3. Run BigDL examples"
        },
        {
            "location": "/UserGuide/running-on-EC2/#31-run-the-inception-v1-example",
            "text": "You can refer to the  Inception v1  example to prepare the input  ImageNet  data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.   Download and prepare the input image data (a subset of the  Flickr Style  data)     ./download.sh $HDFS-NAMENODE  After the download completes, the downloaded images are stored in  hdfs://HDFS-NAMENODE:9000/seq . (If the download fails with error \"Unable to establish SSL connection.\" please check your network connection and retry this later.)   To run the \"inception-v1\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), run the example command below:      nohup bash ./run.example.sh --model inception-v1  \\\n         --spark-url spark://SPARK-MASTER:7077    \\\n         --nodes 4 --cores 20 --memory 150g       \\\n         --batch-size 400 --learning-rate 0.0898  \\\n         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \\\n         --spark spark_2.0 --max-epoch 4 \\\n         > incep.log 2>&1 &        View output of the training in the log file generated by the previous step:     $ tail -f incep.log\n  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.",
            "title": "3.1 Run the \"inception-v1\" example"
        },
        {
            "location": "/UserGuide/running-on-EC2/#32-run-the-perf-example",
            "text": "To run the \"perf\" example on a 4-worker Spark cluster (using, say, the \"m4.10xlarge\" instance), you may try the example command below:     nohup bash ./run.example.sh --model perf  \\\n       --spark-url spark://SPARK-MASTER:7077    \\\n       --nodes 4 --cores 20 --memory 150g       \\\n       --spark spark_2.0 --max-epoch 4 \\\n       > perf.log 2>&1 &",
            "title": "3.2 Run the \"perf\" example"
        },
        {
            "location": "/UserGuide/models/",
            "text": "Models\n\n\n\n\nBigDL provides many popular \nneural network models\n and \ndeep learning examples\n for Apache Spark, including: \n\n\nModels\n\n\n\n\nLeNet\n: it demonstrates how to use BigDL to train and evaluate the \nLeNet-5\n network on MNIST data.\n\n\nInception\n: it demonstrates how to use BigDL to train and evaluate \nInception v1\n and \nInception v2\n architecture on the ImageNet data.\n\n\nVGG\n: it demonstrates how to use BigDL to train and evaluate a \nVGG-like\n network on CIFAR-10 data.\n\n\nResNet\n: it demonstrates how to use BigDL to train and evaluate the \nResNet\n architecture on CIFAR-10 data.\n\n\nRNN\n: it demonstrates how to use BigDL to build and train a simple recurrent neural network \n(RNN) for language model\n.\n\n\nAuto-encoder\n: it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.\n\n\n\n\nExamples\n\n\n\n\ntext_classification\n: it demonstrates how to use BigDL to build a \ntext classifier\n using a simple convolutional neural network (CNN) model.\n\n\nimage_classification\n: it demonstrates how to load a BigDL or \nTorch\n model trained on ImageNet data (e.g., \nInception\n or \nResNet\n), and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.\n\n\nload_model\n: it demonstrates how to use BigDL to load a pre-trained \nTorch\n or \nCaffe\n model into Spark program for prediction.\n\n\n\n\nPython Examples\n\n\n\n\nLeNet\n: it demonstrates how to use BigDL Python APIs to train and evaluate the \nLeNet-5\n network on MNIST data.\n\n\nText Classifier\n:  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.\n\n\nJupyter tutorial\n: it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.",
            "title": "Models"
        },
        {
            "location": "/UserGuide/models/#models",
            "text": "BigDL provides many popular  neural network models  and  deep learning examples  for Apache Spark, including:",
            "title": "Models"
        },
        {
            "location": "/UserGuide/models/#models_1",
            "text": "LeNet : it demonstrates how to use BigDL to train and evaluate the  LeNet-5  network on MNIST data.  Inception : it demonstrates how to use BigDL to train and evaluate  Inception v1  and  Inception v2  architecture on the ImageNet data.  VGG : it demonstrates how to use BigDL to train and evaluate a  VGG-like  network on CIFAR-10 data.  ResNet : it demonstrates how to use BigDL to train and evaluate the  ResNet  architecture on CIFAR-10 data.  RNN : it demonstrates how to use BigDL to build and train a simple recurrent neural network  (RNN) for language model .  Auto-encoder : it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.",
            "title": "Models"
        },
        {
            "location": "/UserGuide/models/#examples",
            "text": "text_classification : it demonstrates how to use BigDL to build a  text classifier  using a simple convolutional neural network (CNN) model.  image_classification : it demonstrates how to load a BigDL or  Torch  model trained on ImageNet data (e.g.,  Inception  or  ResNet ), and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.  load_model : it demonstrates how to use BigDL to load a pre-trained  Torch  or  Caffe  model into Spark program for prediction.",
            "title": "Examples"
        },
        {
            "location": "/UserGuide/models/#python-examples",
            "text": "LeNet : it demonstrates how to use BigDL Python APIs to train and evaluate the  LeNet-5  network on MNIST data.  Text Classifier :  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.  Jupyter tutorial : it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.",
            "title": "Python Examples"
        },
        {
            "location": "/UserGuide/programming-guide/",
            "text": "Programming Guide\n\n\n\n\nOverview\n\n\nBefore starting the programming guide, you may have checked out the \nGetting Started Page\n and the \nTutorials page\n. This section will introduce the BigDL concepts and APIs for building deep learning applications on Spark.\n\n\n\n\nTensor\n\n\nTable\n\n\nModule\n\n\nCreate modules\n\n\nConstruct complex networks\n\n\nBuild neural network models\n\n\n\n\n\n\nCriterion\n\n\nRegularizers\n\n\nTransformer\n\n\nSample and MiniBatch\n\n\nEngine\n\n\nOptimizer\n\n\nHow BigDL train models in a distributed cluster\n\n\n\n\n\n\nValidator\n\n\nModel Persist\n\n\nLogging\n\n\nVisualization via TensorBoard\n\n\n\n\nTensor\n\n\nModeled after the \nTensor\n class in \nTorch\n, the \nTensor\n \npackage\n (written in Scala and leveraging \nIntel MKL\n) in BigDL provides numeric computing support for the deep learning applications (e.g., the input, output, weight, bias and gradient of the neural networks).\n\n\nA \nTensor\n is essentially a multi-dimensional array of numeric types (e.g., \nInt\n, \nFloat\n, \nDouble\n, etc.); you may check it out in the interactive Scala shell (by typing \nscala -cp bigdl_0.1-0.1.0-SNAPSHOT-jar-with-dependencies.jar\n), for instance:\n\n\nscala> import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala> val tensor = Tensor[Float](2, 3)\ntensor: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0     0.0     0.0\n0.0     0.0     0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nTable\n\n\nModeled after the \nTable\n class in \nTorch\n, the \nTable\n class (defined in package \ncom.intel.analytics.bigdl.utils\n) is widely used in BigDL (e.g., a \nTable\n of \nTensor\n can be used as the input or output of neural networks). In essence, a \nTable\n can be considered as a key-value map, and there is also a syntax sugar to create a \nTable\n using \nT()\n in BigDL.\n\n\nscala> import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.utils.T\n\nscala> T(Tensor[Float](2,2), Tensor[Float](2,2))\nres2: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.0  0.0\n           0.0  0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n        1: 0.0  0.0\n           0.0  0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n }\n\n\n\n\n\nModule\n\n\nModeled after the \nnn\n package in \nTorch\n, the \nModule\n class in BigDL represents individual layers of the neural network (such as \nReLU\n, \nLinear\n, \nSpatialConvolution\n, \nSequential\n, etc.).\n\n\nCreate modules\n\n\nFor instance, we can create a \nLinear\n module as follows:\n\n\nscala> import com.intel.analytics.bigdl.numeric.NumericFloat // import global float tensor numeric type\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nscala> import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.nn._\n\nscala> val f = Linear(3,4) // create the module\nmlp: com.intel.analytics.bigdl.nn.Linear[Float] = nn.Linear(3 -> 4)\n\n// let's see what f's parameters were initialized to. ('nn' always inits to something reasonable)\nscala> f.weight\nres5: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.008662592    0.543819        -0.028795477\n-0.30469555     -0.3909278      -0.10871882\n0.114964925     0.1411745       0.35646403\n-0.16590376     -0.19962183     -0.18782845\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x3]\n\n\n\n\nConstruct complex networks\n\n\nWe can use the \nContainer\n module (e.g., \nSequential\n, \nConcat\n, \nConcatTable\n, etc.) to combine individual models to build complex networks, for instance\n\n\nscala> val g = Sum()\ng: com.intel.analytics.bigdl.nn.Sum[Float] = nn.Sum\n\nscala> val mlp = Sequential().add(f).add(g)\nmlp: com.intel.analytics.bigdl.nn.Sequential[Float] =\nnn.Sequential {\n  [input -> (1) -> (2) -> output]\n  (1): nn.Linear(3 -> 4)\n  (2): nn.Sum\n}\n\n\n\n\nBuild neural network models\n\n\nWe can create neural network models, e.g., \nLeNet-5\n, using different \nModule\n as follows:\n\n\nimport com.intel.analytics.bigdl._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nobject LeNet5 {\n  def apply(classNum: Int): Module[Float] = {\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())\n  }\n}\n\n\n\n\nCriterion\n\n\nModeled after the \nCriterion\n class in \nTorch\n, the \nCriterion\n class in BigDL will compute loss and gradient (given prediction and target). See \nBigDL Criterions\n for a list of supported criterions. \n\n\nscala> val mse = MSECriterion() // mean square error lost, usually used for regression loss\nmse: com.intel.analytics.bigdl.nn.MSECriterion[Float] = com.intel.analytics.bigdl.nn.MSECriterion@0\n\nscala> val target = Tensor(3).rand() // create a target tensor randomly\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.33631626\n0.2535103\n0.94784033\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala> val prediction = Tensor(3).rand() // create a predicted tensor randomly\nprediction: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.91918194\n0.6019384\n0.38315287\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala> mse.forward(prediction, target) // use mse to get the loss, returns 1/n sum_i (yhat_i - t_i)^2\nres11: Float = 0.2600022\n\n\n\n\n\nRegularizers\n\n\nRegularizers allow user to apply penalties to the parameters of layers during the optimization process. The penalties are aggregated to the loss function that the network optimizes.\n\n\nBigDL provides layer wise and parameter separated regularizers. User can apply different penalties to different layers and even different parameters of the same layer. Hence the exact API will depend on the layers.\n\n\nExample\n\n\nLinear(inputN, outputN, \n       wRegularizer = L2Regularizer(0.1),\n       bRegularizer = L2Regularizer(0.1))\n\n\n\n\nAvailable penalties\n\n\nThe defined regularizers are located in \ncom.intel.analytics.bigdl.optim\n package.\n\n\nThree pre-defined regularizers are available:\n\n\nL1L2Regularizer(0.)\nL1Regularizer(0.)\nL2Regularizer(0.)\n\n\n\n\nDeveloping new regularizers\n\n\nUsers can define their own customized regularizers by inheriting the \nRegularizer\n trait and overriding the \naccRegularization\n function. The \naccRegularization\n function takes two arguments, one is the parameters to be penalized and the other is the gradient of the parameters. The derivatives of the penalty function should be defined in \naccRegularization\n.\n\n\nTransformer\n\n\nTransformer is for pre-processing. In many deep learning workload, input data need to be pre-processed before fed into model. For example, in CNN, the image file need to be decoded from some compressed format(e.g. jpeg) to float arrays, normalized and cropped to some fixed shape. You can also find pre-processing in other types of deep learning work load(e.g. NLP, speech recognition). In BigDL, we provide many pre-process procedures for user. They're implemented as Transformer.\n\n\nThe transformer interface is\n\n\ntrait Transformer[A, B] extends Serializable {\n  def apply(prev: Iterator[A]): Iterator[B]\n}\n\n\n\n\nIt's simple, right? What a transformer do is convert a sequence of objects of Class A to a sequence of objects of Class B.\n\n\nTransformer is flexible. You can chain them together to do pre-processing. Let's still use the CNN example, say first we need read image files from given paths, then extract the image binaries to array of float, then normalized the image content and crop a fixed size from the image at a random position. Here we need 4 transformers, \nPathToImage\n, \nImageToArray\n, \nNormalizor\n and \nCropper\n. And then chain them together.\n\n\nclass PathToImage extends Transformer[Path, Image]\nclass ImageToArray extends Transformer[Image, Array]\nclass Normalizor extends Transformer[Array, Array]\nclass Cropper extends Transformer[Array, Array]\n\nPathToImage -> ImageToArray -> Normalizor -> Cropper\n\n\n\n\nAnother benefit from \nTransformer\n is code reuse. You may find that for similar tasks, although there's a little difference, many pre-processing steps are same. So instead of a big single pre-process function, break it into small steps can improve the code reuse and save your time.\n\n\nTransformer can work with Spark easily. For example, to transform RDD[A] to RDD[B]\n\n\nval rddA : RDD[A] = ...\nval tran : Transformer[A, B] = ...\nval rddB : RDD[B] = rdd.mapPartitions(tran(_))\n\n\n\n\nTransformer here is different from \nSpark ML pipeline Transformer\n. But they serve similar purpose. \n\n\nSample and MiniBatch\n\n\nSample\n represent one \nitem\n of your data set. For example, one image in image classification, one word in word2vec and one sentence in RNN language model.\n\n\nMiniBatch\n represent \na batch of samples\n. For computing efficiency, we would like to train/inference data in batches.\n\n\nYou need to convert your data type to Sample or MiniBatch by transformers, and then do optimization or inference. Please note that if you provide Sample format, BigDL will still convert it to MiniBatch automatically before optimization or inference.\n\n\nEngine\n\n\nBigDL need some environment variables be set correctly to get a good performance. \nEngine.init\n method can help you set and verify them.\n\n\nHow to do in the code?\n\n\n// Scala code example\nval conf = Engine.createSparkConf()\nval sc = new SparkContext(conf)\nEngine.init\n\n\n\n\n# Python code example\nconf=create_spark_conf()\nsc = SparkContext(conf)\ninit_engine()\n\n\n\n\n\n\nIf you're in spark-shell, Jupyter notebook or yarn-cluster\n\n\n\n\nAs the spark context is pre-created, you need start spark-shell or pyspark with \ndist/conf/spark-bigdl.conf\n file\n\n\n# Spark shell\nspark-shell --properties-file dist/conf/spark-bigdl.conf ...\n# Jupyter notebook\npyspark --properties-file dist/conf/spark-bigdl.conf ...\n\n\n\n\nIn your code\n\n\nEngine.init    // scala: check spark conf values\n\n\n\n\ninit_engine()    # python: check spark conf values\n\n\n\n\nOptimizer\n\n\nOptimizer\n represent a optimization process, aka training. \n\n\nYou need to provide the model, train data set and loss function to start a optimization.\n\n\nval optimizer = Optimizer(\n  model = model,\n  dataset = trainDataSet,\n  criterion = new ClassNLLCriterion[Float]()\n)\n\n\n\n\nYou can set other properties of a optimization process. Here's some examples:\n* Hyper Parameter\n\n\noptimizer.setState(\n  T(\n    \"learningRate\" -> 0.01,\n    \"weightDecay\" -> 0.0005,\n    \"momentum\" -> 0.9,\n    \"dampening\" -> 0.0,\n    \"learningRateSchedule\" -> SGD.EpochStep(25, 0.5)\n  )\n)\n\n\n\n\n\n\nOptimization method, the default one is SGD. See \nOptimization Algorithms\n for a list of supported optimization methods and their usage.\n\n\n\n\n// Change optimization method to adagrad\noptimizer.setOptimMethod(new Adagrad())\n\n\n\n\n\n\nWhen to stop, the default one is stopped after 100 iteration\n\n\n\n\n// Stop after 10 epoch\noptimizer.setEndWhen(Trigger.maxEpoch(10))\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n// Every 50 iteration save current model and training status to ./checkpoint\noptimizer.setCheckpoint(\"./checkpoint\", Trigger.severalIteration(50))\n\n\n\n\n\n\nValidation\nYou can provide a separated data set for validation.\n\n\n\n\n// Every epoch do a validation on valData, use Top1 accuracy metrics\noptimizer.setValidation(Trigger.everyEpoch, valData, Array(new Top1Accuracy[Float]))\n\n\n\n\nHow BigDL train models in a distributed cluster?\n\n\nBigDL distributed training is data parallelism. The training data is split among workers and cached in memory. A complete model is also cached on each worker. The model only uses the data of the same worker in the training.\n\n\nBigDL employs a synchronous distributed training. In each iteration, each worker will sync the latest weights, calculate\ngradients with local data and local model, sync the gradients and update the weights with a given optimization method(e.g. SGD, Adagrad).\n\n\nIn gradients and weights sync, BigDL doesn't use the RDD APIs like(broadcast, reduce, aggregate, treeAggregate). The problem of these methods is every worker needs to communicate with driver, so the driver will become the bottleneck if the parameter is too large or the workers are too many. Instead, BigDL implement a P2P algorithm for parameter sync to remove the bottleneck. For detail of the algorithm, please see the \ncode\n\n\nValidator\n\n\nValidator represent testing the model with some metrics. The model can be loaded from disk or trained from optimization. The metrics can be Top1 accuracy, Loss, etc. See \nValidation Methods\n for a list of supported validation methods\n\n\n// Test the model with validationSet and Top1 accuracy\nval validator = Validator(model, validationSet)\nval result = validator.test(Array(new Top1Accuracy[Float]))\n\n\n\n\nModel Persist\n\n\nYou can save your model like this\n\n\n// Save as Java object\nmodel.save(\"./model\")\n\n// Save as Torch object\nmodel.saveTorch(\"./model.t7\")\n\n\n\n\nYou can read your model file like this\n\n\n// Load from Java object file\nModule.load(\"./model\")\n\n// Load from torch file\nModule.loadTorch(\"./model.t7\")\n\n\n\n\nLogging\n\n\nIn the training, BigDL provide a straight forward logging like this. You can see epoch/iteration/loss/throughput directly from the log.\n\n\n2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n\n\n\n\nThe DistriOptimizer log level is INFO. Currently, we implement a method named with \nredirectSparkInfoLogs\n in \nspark/utils/LoggerFilter.scala\n. You can import and redirect at first.\n\n\nimport com.intel.analytics.bigdl.utils.LoggerFilter\nLoggerFilter.redirectSparkInfoLogs()\n\n\n\n\nThis method will redirect all logs of \norg\n, \nakka\n, \nbreeze\n to \nbigdl.log\n with \nINFO\n level, except \norg.apache.spark.SparkContext\n. And it will output all \nERROR\n message in console too.\n\n\n\n\nYou can disable the redirection with java property \n-Dbigdl.utils.LoggerFilter.disable=true\n. By default, it will do redirect of all examples and models in our code.\n\n\nYou can set where the \nbigdl.log\n will be generated with \n-Dbigdl.utils.LoggerFilter.logFile=<path>\n. By default, it will be generated under current workspace.\n\n\n\n\nVisualization via TensorBoard\n\n\nTo enable visualization, you need to \ninstall tensorboard\n first, then \nsetTrainSummary()\n and \nsetValidationSummary()\n to your optimizer before you call \noptimize()\n:\n\n\nval logdir = \"mylogdir\"\nval appName = \"myapp\"\nval trainSummary = TrainSummary(logdir, appName)\nval talidationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)\n\n\n\n\nAfter you start to run your spark job, the train and validation log will be saved to \"mylogdir/myapp/train\" and \"mylogdir/myapp/validation\". Notice: please change the appName before you start a new job, or the log files will conflict.\n\n\nAs the training started, use command \ntensorboard --logdir mylogdir\n to start tensorboard. Then open http://[ip]:6006 to watch the training.\n\n\n\n\nTrainSummary will show \"Loss\" and \"Throughput\" each iteration by default. You can use \nsetSummaryTrigger()\n to enable \"LearningRate\" and \"Parameters\", or change the \"Loss\" and \"Throughput\"'s trigger:\n\n\n\n\ntrainSummary.setSummaryTrigger(\"LearningRate\", Trigger.severalIteration(1))\ntrainSummary.setSummaryTrigger(\"Parameters\", Trigger.severalIteration(20))\n\n\n\n\nNotice: \"Parameters\" show the histogram of parameters and gradParameters in the model. But getting parameters from workers is a heavy operation, recommend setting the trigger interval to at least 10 iterations. For a better visualization, please give names to the layers in model.\n\n\n\n\n\n\nValidationSummary will show the result of ValidationMethod set in optimizer.setValidation(), like \"Loss\" and \"Top1Accuracy\".\n\n\n\n\n\n\nSummary also provide readScalar function to read scalar summary by tag name. Reading \"Loss\" from summary:\n\n\n\n\n\n\nval trainLoss = trainSummary.readScalar(\"Loss\")\nval validationLoss = validationSummary.readScalar(\"Loss\")",
            "title": "Programming Guide"
        },
        {
            "location": "/UserGuide/programming-guide/#programming-guide",
            "text": "",
            "title": "Programming Guide"
        },
        {
            "location": "/UserGuide/programming-guide/#overview",
            "text": "Before starting the programming guide, you may have checked out the  Getting Started Page  and the  Tutorials page . This section will introduce the BigDL concepts and APIs for building deep learning applications on Spark.   Tensor  Table  Module  Create modules  Construct complex networks  Build neural network models    Criterion  Regularizers  Transformer  Sample and MiniBatch  Engine  Optimizer  How BigDL train models in a distributed cluster    Validator  Model Persist  Logging  Visualization via TensorBoard",
            "title": "Overview"
        },
        {
            "location": "/UserGuide/programming-guide/#tensor",
            "text": "Modeled after the  Tensor  class in  Torch , the  Tensor   package  (written in Scala and leveraging  Intel MKL ) in BigDL provides numeric computing support for the deep learning applications (e.g., the input, output, weight, bias and gradient of the neural networks).  A  Tensor  is essentially a multi-dimensional array of numeric types (e.g.,  Int ,  Float ,  Double , etc.); you may check it out in the interactive Scala shell (by typing  scala -cp bigdl_0.1-0.1.0-SNAPSHOT-jar-with-dependencies.jar ), for instance:  scala> import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nscala> val tensor = Tensor[Float](2, 3)\ntensor: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.0     0.0     0.0\n0.0     0.0     0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]",
            "title": "Tensor"
        },
        {
            "location": "/UserGuide/programming-guide/#table",
            "text": "Modeled after the  Table  class in  Torch , the  Table  class (defined in package  com.intel.analytics.bigdl.utils ) is widely used in BigDL (e.g., a  Table  of  Tensor  can be used as the input or output of neural networks). In essence, a  Table  can be considered as a key-value map, and there is also a syntax sugar to create a  Table  using  T()  in BigDL.  scala> import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.utils.T\n\nscala> T(Tensor[Float](2,2), Tensor[Float](2,2))\nres2: com.intel.analytics.bigdl.utils.Table =\n {\n        2: 0.0  0.0\n           0.0  0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n        1: 0.0  0.0\n           0.0  0.0\n           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n }",
            "title": "Table"
        },
        {
            "location": "/UserGuide/programming-guide/#module",
            "text": "Modeled after the  nn  package in  Torch , the  Module  class in BigDL represents individual layers of the neural network (such as  ReLU ,  Linear ,  SpatialConvolution ,  Sequential , etc.).",
            "title": "Module"
        },
        {
            "location": "/UserGuide/programming-guide/#create-modules",
            "text": "For instance, we can create a  Linear  module as follows:  scala> import com.intel.analytics.bigdl.numeric.NumericFloat // import global float tensor numeric type\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nscala> import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.nn._\n\nscala> val f = Linear(3,4) // create the module\nmlp: com.intel.analytics.bigdl.nn.Linear[Float] = nn.Linear(3 -> 4)\n\n// let's see what f's parameters were initialized to. ('nn' always inits to something reasonable)\nscala> f.weight\nres5: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.008662592    0.543819        -0.028795477\n-0.30469555     -0.3909278      -0.10871882\n0.114964925     0.1411745       0.35646403\n-0.16590376     -0.19962183     -0.18782845\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x3]",
            "title": "Create modules"
        },
        {
            "location": "/UserGuide/programming-guide/#construct-complex-networks",
            "text": "We can use the  Container  module (e.g.,  Sequential ,  Concat ,  ConcatTable , etc.) to combine individual models to build complex networks, for instance  scala> val g = Sum()\ng: com.intel.analytics.bigdl.nn.Sum[Float] = nn.Sum\n\nscala> val mlp = Sequential().add(f).add(g)\nmlp: com.intel.analytics.bigdl.nn.Sequential[Float] =\nnn.Sequential {\n  [input -> (1) -> (2) -> output]\n  (1): nn.Linear(3 -> 4)\n  (2): nn.Sum\n}",
            "title": "Construct complex networks"
        },
        {
            "location": "/UserGuide/programming-guide/#build-neural-network-models",
            "text": "We can create neural network models, e.g.,  LeNet-5 , using different  Module  as follows:  import com.intel.analytics.bigdl._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\nobject LeNet5 {\n  def apply(classNum: Int): Module[Float] = {\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100))\n      .add(Tanh())\n      .add(Linear(100, classNum))\n      .add(LogSoftMax())\n  }\n}",
            "title": "Build neural network models"
        },
        {
            "location": "/UserGuide/programming-guide/#criterion",
            "text": "Modeled after the  Criterion  class in  Torch , the  Criterion  class in BigDL will compute loss and gradient (given prediction and target). See  BigDL Criterions  for a list of supported criterions.   scala> val mse = MSECriterion() // mean square error lost, usually used for regression loss\nmse: com.intel.analytics.bigdl.nn.MSECriterion[Float] = com.intel.analytics.bigdl.nn.MSECriterion@0\n\nscala> val target = Tensor(3).rand() // create a target tensor randomly\ntarget: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.33631626\n0.2535103\n0.94784033\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala> val prediction = Tensor(3).rand() // create a predicted tensor randomly\nprediction: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.91918194\n0.6019384\n0.38315287\n[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]\n\nscala> mse.forward(prediction, target) // use mse to get the loss, returns 1/n sum_i (yhat_i - t_i)^2\nres11: Float = 0.2600022",
            "title": "Criterion"
        },
        {
            "location": "/UserGuide/programming-guide/#regularizers",
            "text": "Regularizers allow user to apply penalties to the parameters of layers during the optimization process. The penalties are aggregated to the loss function that the network optimizes.  BigDL provides layer wise and parameter separated regularizers. User can apply different penalties to different layers and even different parameters of the same layer. Hence the exact API will depend on the layers.",
            "title": "Regularizers"
        },
        {
            "location": "/UserGuide/programming-guide/#example",
            "text": "Linear(inputN, outputN, \n       wRegularizer = L2Regularizer(0.1),\n       bRegularizer = L2Regularizer(0.1))",
            "title": "Example"
        },
        {
            "location": "/UserGuide/programming-guide/#available-penalties",
            "text": "The defined regularizers are located in  com.intel.analytics.bigdl.optim  package.  Three pre-defined regularizers are available:  L1L2Regularizer(0.)\nL1Regularizer(0.)\nL2Regularizer(0.)",
            "title": "Available penalties"
        },
        {
            "location": "/UserGuide/programming-guide/#developing-new-regularizers",
            "text": "Users can define their own customized regularizers by inheriting the  Regularizer  trait and overriding the  accRegularization  function. The  accRegularization  function takes two arguments, one is the parameters to be penalized and the other is the gradient of the parameters. The derivatives of the penalty function should be defined in  accRegularization .",
            "title": "Developing new regularizers"
        },
        {
            "location": "/UserGuide/programming-guide/#transformer",
            "text": "Transformer is for pre-processing. In many deep learning workload, input data need to be pre-processed before fed into model. For example, in CNN, the image file need to be decoded from some compressed format(e.g. jpeg) to float arrays, normalized and cropped to some fixed shape. You can also find pre-processing in other types of deep learning work load(e.g. NLP, speech recognition). In BigDL, we provide many pre-process procedures for user. They're implemented as Transformer.  The transformer interface is  trait Transformer[A, B] extends Serializable {\n  def apply(prev: Iterator[A]): Iterator[B]\n}  It's simple, right? What a transformer do is convert a sequence of objects of Class A to a sequence of objects of Class B.  Transformer is flexible. You can chain them together to do pre-processing. Let's still use the CNN example, say first we need read image files from given paths, then extract the image binaries to array of float, then normalized the image content and crop a fixed size from the image at a random position. Here we need 4 transformers,  PathToImage ,  ImageToArray ,  Normalizor  and  Cropper . And then chain them together.  class PathToImage extends Transformer[Path, Image]\nclass ImageToArray extends Transformer[Image, Array]\nclass Normalizor extends Transformer[Array, Array]\nclass Cropper extends Transformer[Array, Array]\n\nPathToImage -> ImageToArray -> Normalizor -> Cropper  Another benefit from  Transformer  is code reuse. You may find that for similar tasks, although there's a little difference, many pre-processing steps are same. So instead of a big single pre-process function, break it into small steps can improve the code reuse and save your time.  Transformer can work with Spark easily. For example, to transform RDD[A] to RDD[B]  val rddA : RDD[A] = ...\nval tran : Transformer[A, B] = ...\nval rddB : RDD[B] = rdd.mapPartitions(tran(_))  Transformer here is different from  Spark ML pipeline Transformer . But they serve similar purpose.",
            "title": "Transformer"
        },
        {
            "location": "/UserGuide/programming-guide/#sample-and-minibatch",
            "text": "Sample  represent one  item  of your data set. For example, one image in image classification, one word in word2vec and one sentence in RNN language model.  MiniBatch  represent  a batch of samples . For computing efficiency, we would like to train/inference data in batches.  You need to convert your data type to Sample or MiniBatch by transformers, and then do optimization or inference. Please note that if you provide Sample format, BigDL will still convert it to MiniBatch automatically before optimization or inference.",
            "title": "Sample and MiniBatch"
        },
        {
            "location": "/UserGuide/programming-guide/#engine",
            "text": "BigDL need some environment variables be set correctly to get a good performance.  Engine.init  method can help you set and verify them.  How to do in the code?  // Scala code example\nval conf = Engine.createSparkConf()\nval sc = new SparkContext(conf)\nEngine.init  # Python code example\nconf=create_spark_conf()\nsc = SparkContext(conf)\ninit_engine()   If you're in spark-shell, Jupyter notebook or yarn-cluster   As the spark context is pre-created, you need start spark-shell or pyspark with  dist/conf/spark-bigdl.conf  file  # Spark shell\nspark-shell --properties-file dist/conf/spark-bigdl.conf ...\n# Jupyter notebook\npyspark --properties-file dist/conf/spark-bigdl.conf ...  In your code  Engine.init    // scala: check spark conf values  init_engine()    # python: check spark conf values",
            "title": "Engine"
        },
        {
            "location": "/UserGuide/programming-guide/#optimizer",
            "text": "Optimizer  represent a optimization process, aka training.   You need to provide the model, train data set and loss function to start a optimization.  val optimizer = Optimizer(\n  model = model,\n  dataset = trainDataSet,\n  criterion = new ClassNLLCriterion[Float]()\n)  You can set other properties of a optimization process. Here's some examples:\n* Hyper Parameter  optimizer.setState(\n  T(\n    \"learningRate\" -> 0.01,\n    \"weightDecay\" -> 0.0005,\n    \"momentum\" -> 0.9,\n    \"dampening\" -> 0.0,\n    \"learningRateSchedule\" -> SGD.EpochStep(25, 0.5)\n  )\n)   Optimization method, the default one is SGD. See  Optimization Algorithms  for a list of supported optimization methods and their usage.   // Change optimization method to adagrad\noptimizer.setOptimMethod(new Adagrad())   When to stop, the default one is stopped after 100 iteration   // Stop after 10 epoch\noptimizer.setEndWhen(Trigger.maxEpoch(10))   Checkpoint   // Every 50 iteration save current model and training status to ./checkpoint\noptimizer.setCheckpoint(\"./checkpoint\", Trigger.severalIteration(50))   Validation\nYou can provide a separated data set for validation.   // Every epoch do a validation on valData, use Top1 accuracy metrics\noptimizer.setValidation(Trigger.everyEpoch, valData, Array(new Top1Accuracy[Float]))",
            "title": "Optimizer"
        },
        {
            "location": "/UserGuide/programming-guide/#how-bigdl-train-models-in-a-distributed-cluster",
            "text": "BigDL distributed training is data parallelism. The training data is split among workers and cached in memory. A complete model is also cached on each worker. The model only uses the data of the same worker in the training.  BigDL employs a synchronous distributed training. In each iteration, each worker will sync the latest weights, calculate\ngradients with local data and local model, sync the gradients and update the weights with a given optimization method(e.g. SGD, Adagrad).  In gradients and weights sync, BigDL doesn't use the RDD APIs like(broadcast, reduce, aggregate, treeAggregate). The problem of these methods is every worker needs to communicate with driver, so the driver will become the bottleneck if the parameter is too large or the workers are too many. Instead, BigDL implement a P2P algorithm for parameter sync to remove the bottleneck. For detail of the algorithm, please see the  code",
            "title": "How BigDL train models in a distributed cluster?"
        },
        {
            "location": "/UserGuide/programming-guide/#validator",
            "text": "Validator represent testing the model with some metrics. The model can be loaded from disk or trained from optimization. The metrics can be Top1 accuracy, Loss, etc. See  Validation Methods  for a list of supported validation methods  // Test the model with validationSet and Top1 accuracy\nval validator = Validator(model, validationSet)\nval result = validator.test(Array(new Top1Accuracy[Float]))",
            "title": "Validator"
        },
        {
            "location": "/UserGuide/programming-guide/#model-persist",
            "text": "You can save your model like this  // Save as Java object\nmodel.save(\"./model\")\n\n// Save as Torch object\nmodel.saveTorch(\"./model.t7\")  You can read your model file like this  // Load from Java object file\nModule.load(\"./model\")\n\n// Load from torch file\nModule.loadTorch(\"./model.t7\")",
            "title": "Model Persist"
        },
        {
            "location": "/UserGuide/programming-guide/#logging",
            "text": "In the training, BigDL provide a straight forward logging like this. You can see epoch/iteration/loss/throughput directly from the log.  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.\n2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.  The DistriOptimizer log level is INFO. Currently, we implement a method named with  redirectSparkInfoLogs  in  spark/utils/LoggerFilter.scala . You can import and redirect at first.  import com.intel.analytics.bigdl.utils.LoggerFilter\nLoggerFilter.redirectSparkInfoLogs()  This method will redirect all logs of  org ,  akka ,  breeze  to  bigdl.log  with  INFO  level, except  org.apache.spark.SparkContext . And it will output all  ERROR  message in console too.   You can disable the redirection with java property  -Dbigdl.utils.LoggerFilter.disable=true . By default, it will do redirect of all examples and models in our code.  You can set where the  bigdl.log  will be generated with  -Dbigdl.utils.LoggerFilter.logFile=<path> . By default, it will be generated under current workspace.",
            "title": "Logging"
        },
        {
            "location": "/UserGuide/programming-guide/#visualization-via-tensorboard",
            "text": "To enable visualization, you need to  install tensorboard  first, then  setTrainSummary()  and  setValidationSummary()  to your optimizer before you call  optimize() :  val logdir = \"mylogdir\"\nval appName = \"myapp\"\nval trainSummary = TrainSummary(logdir, appName)\nval talidationSummary = ValidationSummary(logdir, appName)\noptimizer.setTrainSummary(trainSummary)\noptimizer.setValidationSummary(validationSummary)  After you start to run your spark job, the train and validation log will be saved to \"mylogdir/myapp/train\" and \"mylogdir/myapp/validation\". Notice: please change the appName before you start a new job, or the log files will conflict.  As the training started, use command  tensorboard --logdir mylogdir  to start tensorboard. Then open http://[ip]:6006 to watch the training.   TrainSummary will show \"Loss\" and \"Throughput\" each iteration by default. You can use  setSummaryTrigger()  to enable \"LearningRate\" and \"Parameters\", or change the \"Loss\" and \"Throughput\"'s trigger:   trainSummary.setSummaryTrigger(\"LearningRate\", Trigger.severalIteration(1))\ntrainSummary.setSummaryTrigger(\"Parameters\", Trigger.severalIteration(20))  Notice: \"Parameters\" show the histogram of parameters and gradParameters in the model. But getting parameters from workers is a heavy operation, recommend setting the trigger interval to at least 10 iterations. For a better visualization, please give names to the layers in model.    ValidationSummary will show the result of ValidationMethod set in optimizer.setValidation(), like \"Loss\" and \"Top1Accuracy\".    Summary also provide readScalar function to read scalar summary by tag name. Reading \"Loss\" from summary:    val trainLoss = trainSummary.readScalar(\"Loss\")\nval validationLoss = validationSummary.readScalar(\"Loss\")",
            "title": "Visualization via TensorBoard"
        },
        {
            "location": "/UserGuide/known-issues/",
            "text": "Known Issues\n\n\n\n\n\n\n\n\nCurrently, BigDL uses synchronous mini-batch SGD in model training. The mini-batch size is expected to be a multiple of \ntotal cores\n used in the job.\n\n\n\n\n\n\nYou may observe very poor performance when running BigDL for Spark 2.0 with Java 7; it is highly recommended to use Java 8 when building and running BigDL for Spark 2.0.\n\n\n\n\n\n\nOn Spark 2.0, please use default Java serializer instead of Kryo because of \nKryo Issue 341\n. The issue has been fixed in Kryo 4.0. However, Spark 2.0 uses Kryo 3.0.3. Spark 1.5 and 1.6 do not have this problem.\n\n\n\n\n\n\nOn CentOS 6 and 7, please increase the max user processes to a larger value (e.g., 514585); otherwise, you may see errors like \"unable to create new native thread\".\n\n\n\n\n\n\nCurrently, BigDL will load all the training and validation data into memory during training. You may encounter errors if it runs out of memory.\n\n\n\n\n\n\nIf you meet the program stuck after \nSave model...\n on Mesos, check the \nspark.driver.memory\n and increase the value. Eg, VGG on Cifar10 may need 20G+.\n\n\n\n\n\n\nIf you meet \ncan't find executor core number\n on Mesos, you should pass the executor cores through \n--conf spark.executor.cores=xxx",
            "title": "Known Issues"
        },
        {
            "location": "/UserGuide/known-issues/#known-issues",
            "text": "Currently, BigDL uses synchronous mini-batch SGD in model training. The mini-batch size is expected to be a multiple of  total cores  used in the job.    You may observe very poor performance when running BigDL for Spark 2.0 with Java 7; it is highly recommended to use Java 8 when building and running BigDL for Spark 2.0.    On Spark 2.0, please use default Java serializer instead of Kryo because of  Kryo Issue 341 . The issue has been fixed in Kryo 4.0. However, Spark 2.0 uses Kryo 3.0.3. Spark 1.5 and 1.6 do not have this problem.    On CentOS 6 and 7, please increase the max user processes to a larger value (e.g., 514585); otherwise, you may see errors like \"unable to create new native thread\".    Currently, BigDL will load all the training and validation data into memory during training. You may encounter errors if it runs out of memory.    If you meet the program stuck after  Save model...  on Mesos, check the  spark.driver.memory  and increase the value. Eg, VGG on Cifar10 may need 20G+.    If you meet  can't find executor core number  on Mesos, you should pass the executor cores through  --conf spark.executor.cores=xxx",
            "title": "Known Issues"
        },
        {
            "location": "/PythonSupport/python-support/",
            "text": "Python Support\n\n\n\n\nBigDL Python API\n\n\nTutorial: Text Classification using BigDL Python API\n\n\n\n\n\n\nRunning BigDL Python Programs\n\n\nAutomatically Packaging Python Dependency\n\n\n\n\n\n\nRunning BigDL Python Code in Notebooks\n\n\n\n\nBigDL Python API\n\n\nPython is one of the most widely used language in the big data and data science community, and BigDL provides full support for Python APIs (built on top of PySpark), which are similar to the \nScala interface\n. (Please note currently the Python support has been tested with Python 2.7 and Spark 1.6 / Spark 2.0). \n\n\nWith the full Python API support in BigDL, users can use deep learning models in BigDL together with existing Python libraries (e.g., Numpy and Pandas), which automatically runs in a distributed fashion to process large volumes of data on Spark.  \n\n\nTutorial: Text Classification using BigDL Python API\n\n\nThis tutorial describes the \ntextclassifier\n example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by \nthis Keras tutorial\n)\n\n\nThe example first creates the \nSparkContext\n using the SparkConf\nreturn by the\ncreate_spark_conf()` method, and then initialize the engine:\n\n\n  sc = SparkContext(appName=\"text_classifier\",\n                    conf=create_spark_conf())\n  init_engine()\n\n\n\n\nIt then loads the \n20 Newsgroup dataset\n into RDD, and transforms the input data into an RDD of \nSample\n. (Each \nSample\n in essence contains a tuple of two NumPy ndarray representing the feature and label).\n\n\n  texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])   \n\n\n\n\nAfter that, the example creates the neural network model as follows:\n\n\ndef build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \"cnn\":\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \"lstm\":\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \"gru\":\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model\n\n\n\n\nFinally the example creates the \nOptimizer\n (which accepts both the model and the training Sample RDD) and trains the model by calling \nOptimizer.optimize()\n:\n\n\noptimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=\"Adagrad\",\n    state=state)\n...\ntrain_model = optimizer.optimize()\n\n\n\n\nRunning BigDL Python Programs\n\n\nA BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. One can run the BigDL \nlenet Python example\n using \nspark-submit\n as follows:\n\n\nPYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-jar-with-dependencies.jar\nPYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\nsource ${BigDL_HOME}/dist/bin/bigdl.sh\n\n${SPARK_HOME}/bin/spark-submit \\\n    --master ${MASTER} \\\n    --driver-memory 10g  \\\n    --driver-cores 4  \\\n    --executor-memory 20g \\\n    --total-executor-cores ${TOTAL_CORES}\\\n    --executor-cores 10 ${EXECUTOR_CORES} \\\n    --py-files ${PYTHON_API_PATH},${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py  \\\n    --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n    --jars ${BigDL_JAR_PATH} \\\n    --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n    --conf spark.executor.extraClassPath=bigdl-VERSION-jar-with-dependencies.jar \\\n    ${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py\n\n\n\n\nAutomatically Packaging Python Dependency\n\n\nYou can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the Python dependency). You  can first package all the required Python dependency into a virtual environment on the local node (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program on the YARN cluster (using that virtual environment). Please refer to this \npatch\n for more details.\n\n\nRunning BigDL Python Code in Notebooks\n\n\nWith the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.\n\n\nFirst, install all the necessary libraries on the local node where you will run Jupyter, e.g., \n\n\nsudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud\n\n\n\n\nThen, you can launch the Jupyter notebook as follows:\n\n\nPYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-jar-with-dependencies.jar\n\nexport PYTHONPATH=${PYTHON_API_PATH}:$PYTHONPATH\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --notebook-dir=./ --ip=* --no-browser\"\n\nsource ${BigDL_HOME}/dist/bin/bigdl.sh\n\n${SPARK_HOME}/bin/pyspark \\\n  --master ${MASTER} \\\n  --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n  --driver-memory 10g  \\\n  --driver-cores 4  \\\n  --executor-memory 20g \\\n  --total-executor-cores {TOTAL_CORES} \\\n  --executor-cores {EXECUTOR_CORES} \\\n  --py-files ${PYTHON_API_PATH} \\\n  --jars ${BigDL_JAR_PATH} \\\n  --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n  --conf spark.executor.extraClassPath=bigdl-0.1.0-jar-with-dependencies.jar\n\n\n\n\nAfter successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/",
            "title": "Python Support"
        },
        {
            "location": "/PythonSupport/python-support/#python-support",
            "text": "BigDL Python API  Tutorial: Text Classification using BigDL Python API    Running BigDL Python Programs  Automatically Packaging Python Dependency    Running BigDL Python Code in Notebooks",
            "title": "Python Support"
        },
        {
            "location": "/PythonSupport/python-support/#bigdl-python-api",
            "text": "Python is one of the most widely used language in the big data and data science community, and BigDL provides full support for Python APIs (built on top of PySpark), which are similar to the  Scala interface . (Please note currently the Python support has been tested with Python 2.7 and Spark 1.6 / Spark 2.0).   With the full Python API support in BigDL, users can use deep learning models in BigDL together with existing Python libraries (e.g., Numpy and Pandas), which automatically runs in a distributed fashion to process large volumes of data on Spark.",
            "title": "BigDL Python API"
        },
        {
            "location": "/PythonSupport/python-support/#tutorial-text-classification-using-bigdl-python-api",
            "text": "This tutorial describes the  textclassifier  example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by  this Keras tutorial )  The example first creates the  SparkContext  using the SparkConf return by the create_spark_conf()` method, and then initialize the engine:    sc = SparkContext(appName=\"text_classifier\",\n                    conf=create_spark_conf())\n  init_engine()  It then loads the  20 Newsgroup dataset  into RDD, and transforms the input data into an RDD of  Sample . (Each  Sample  in essence contains a tuple of two NumPy ndarray representing the feature and label).    texts = news20.get_news20()\n  data_rdd = sc.parallelize(texts, 2)\n  ...\n  sample_rdd = vector_rdd.map(\n      lambda (vectors, label): to_sample(vectors, label, embedding_dim))\n  train_rdd, val_rdd = sample_rdd.randomSplit(\n      [training_split, 1-training_split])     After that, the example creates the neural network model as follows:  def build_model(class_num):\n    model = Sequential()\n\n    if model_type.lower() == \"cnn\":\n        model.add(Reshape([embedding_dim, 1, sequence_len]))\n        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(SpatialConvolution(128, 128, 5, 1))\n        model.add(ReLU())\n        model.add(SpatialMaxPooling(5, 1, 5, 1))\n        model.add(Reshape([128]))\n    elif model_type.lower() == \"lstm\":\n        model.add(Recurrent()\n                  .add(LSTM(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    elif model_type.lower() == \"gru\":\n        model.add(Recurrent()\n                  .add(GRU(embedding_dim, 128)))\n        model.add(Select(2, -1))\n    else:\n        raise ValueError('model can only be cnn, lstm, or gru')\n\n    model.add(Linear(128, 100))\n    model.add(Linear(100, class_num))\n    model.add(LogSoftMax())\n    return model  Finally the example creates the  Optimizer  (which accepts both the model and the training Sample RDD) and trains the model by calling  Optimizer.optimize() :  optimizer = Optimizer(\n    model=build_model(news20.CLASS_NUM),\n    training_rdd=train_rdd,\n    criterion=ClassNLLCriterion(),\n    end_trigger=MaxEpoch(max_epoch),\n    batch_size=batch_size,\n    optim_method=\"Adagrad\",\n    state=state)\n...\ntrain_model = optimizer.optimize()",
            "title": "Tutorial: Text Classification using BigDL Python API"
        },
        {
            "location": "/PythonSupport/python-support/#running-bigdl-python-programs",
            "text": "A BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. One can run the BigDL  lenet Python example  using  spark-submit  as follows:  PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-jar-with-dependencies.jar\nPYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH\n\nsource ${BigDL_HOME}/dist/bin/bigdl.sh\n\n${SPARK_HOME}/bin/spark-submit \\\n    --master ${MASTER} \\\n    --driver-memory 10g  \\\n    --driver-cores 4  \\\n    --executor-memory 20g \\\n    --total-executor-cores ${TOTAL_CORES}\\\n    --executor-cores 10 ${EXECUTOR_CORES} \\\n    --py-files ${PYTHON_API_PATH},${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py  \\\n    --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n    --jars ${BigDL_JAR_PATH} \\\n    --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n    --conf spark.executor.extraClassPath=bigdl-VERSION-jar-with-dependencies.jar \\\n    ${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py",
            "title": "Running BigDL Python Programs"
        },
        {
            "location": "/PythonSupport/python-support/#automatically-packaging-python-dependency",
            "text": "You can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the Python dependency). You  can first package all the required Python dependency into a virtual environment on the local node (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program on the YARN cluster (using that virtual environment). Please refer to this  patch  for more details.",
            "title": "Automatically Packaging Python Dependency"
        },
        {
            "location": "/PythonSupport/python-support/#running-bigdl-python-code-in-notebooks",
            "text": "With the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.  First, install all the necessary libraries on the local node where you will run Jupyter, e.g.,   sudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud  Then, you can launch the Jupyter notebook as follows:  PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-python-api.zip\nBigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-jar-with-dependencies.jar\n\nexport PYTHONPATH=${PYTHON_API_PATH}:$PYTHONPATH\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --notebook-dir=./ --ip=* --no-browser\"\n\nsource ${BigDL_HOME}/dist/bin/bigdl.sh\n\n${SPARK_HOME}/bin/pyspark \\\n  --master ${MASTER} \\\n  --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n  --driver-memory 10g  \\\n  --driver-cores 4  \\\n  --executor-memory 20g \\\n  --total-executor-cores {TOTAL_CORES} \\\n  --executor-cores {EXECUTOR_CORES} \\\n  --py-files ${PYTHON_API_PATH} \\\n  --jars ${BigDL_JAR_PATH} \\\n  --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n  --conf spark.executor.extraClassPath=bigdl-0.1.0-jar-with-dependencies.jar  After successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/",
            "title": "Running BigDL Python Code in Notebooks"
        },
        {
            "location": "/PythonSupport/Python-turtorial/",
            "text": "Python Tuturial\n\n\n\n\n\n\nBigDL Python Tuturial",
            "title": "Python Tutorial"
        },
        {
            "location": "/PythonSupport/Python-turtorial/#python-tuturial",
            "text": "BigDL Python Tuturial",
            "title": "Python Tuturial"
        },
        {
            "location": "/APIdocs/scaladoc/",
            "text": "javadoc",
            "title": "Scala Docs"
        },
        {
            "location": "/APIdocs/scaladoc/#javadoc",
            "text": "",
            "title": "javadoc"
        },
        {
            "location": "/APIdocs/python-api-doc/",
            "text": "pythonapidoc",
            "title": "Python API Docs"
        },
        {
            "location": "/APIdocs/python-api-doc/#pythonapidoc",
            "text": "",
            "title": "pythonapidoc"
        },
        {
            "location": "/powered-by/",
            "text": "Intel\u2019s BigDL on Databricks\n\n\n\n\n\n\nUse BigDL on AZure HDInsight\n\n\n\n\nA more detailed post for \nHow to use BigDL on Apache Spark for Azure HDInsight\n\n\n\n\n\n\n\n\nBigDL on AliCloud E-MapReduce (in Chinese)\n\n\n\n\n\n\nRunning BigDL, Deep Learning for Apache Spark, on AWS\n\n\n\n\n\n\nBigDL on CDH and Cloudera Data Science Workbench",
            "title": "Powered by"
        }
    ]
}